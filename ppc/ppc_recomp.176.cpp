#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_826A6EEC"))) PPC_WEAK_FUNC(sub_826A6EEC);
PPC_FUNC_IMPL(__imp__sub_826A6EEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A6EF0"))) PPC_WEAK_FUNC(sub_826A6EF0);
PPC_FUNC_IMPL(__imp__sub_826A6EF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x826a6f08
	if (!cr6.gt) goto loc_826A6F08;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
loc_826A6F08:
	// bge cr6,0x826a6f1c
	if (!cr6.lt) goto loc_826A6F1C;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// blr 
	return;
loc_826A6F1C:
	// lbz r11,12(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a6f30
	if (cr0.eq) goto loc_826A6F30;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826A6F30:
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A6F38"))) PPC_WEAK_FUNC(sub_826A6F38);
PPC_FUNC_IMPL(__imp__sub_826A6F38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x826a6f60
	if (!cr6.eq) goto loc_826A6F60;
	// lbz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x826a6f5c
	if (!cr0.eq) goto loc_826A6F5C;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826A6F5C:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
loc_826A6F60:
	// blt cr6,0x826a6f68
	if (cr6.lt) goto loc_826A6F68;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_826A6F68:
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A6F70"))) PPC_WEAK_FUNC(sub_826A6F70);
PPC_FUNC_IMPL(__imp__sub_826A6F70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A6F80"))) PPC_WEAK_FUNC(sub_826A6F80);
PPC_FUNC_IMPL(__imp__sub_826A6F80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x826a6fa8
	if (!cr6.eq) goto loc_826A6FA8;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
loc_826A6FA8:
	// stb r11,12(r3)
	PPC_STORE_U8(ctx.r3.u32 + 12, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A6FB0"))) PPC_WEAK_FUNC(sub_826A6FB0);
PPC_FUNC_IMPL(__imp__sub_826A6FB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x826a6ef0
	sub_826A6EF0(ctx, base);
	// cmpw cr6,r3,r29
	cr6.compare<int32_t>(ctx.r3.s32, r29.s32, xer);
	// li r11,1
	r11.s64 = 1;
	// blt cr6,0x826a6fdc
	if (cr6.lt) goto loc_826A6FDC;
	// li r11,0
	r11.s64 = 0;
loc_826A6FDC:
	// clrlwi. r27,r11,24
	r27.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// bne 0x826a7068
	if (!cr0.eq) goto loc_826A7068;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bgt cr6,0x826a700c
	if (cr6.gt) goto loc_826A700C;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// b 0x826a702c
	goto loc_826A702C;
loc_826A700C:
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r5,r30,r29
	ctx.r5.s64 = r29.s64 - r30.s64;
	// add r4,r30,r28
	ctx.r4.u64 = r30.u64 + r28.u64;
loc_826A702C:
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// blt cr6,0x826a7050
	if (cr6.lt) goto loc_826A7050;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_826A7050:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
loc_826A7068:
	// cntlzw r11,r27
	r11.u64 = r27.u32 == 0 ? 32 : __builtin_clz(r27.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_826A707C"))) PPC_WEAK_FUNC(sub_826A707C);
PPC_FUNC_IMPL(__imp__sub_826A707C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A7080"))) PPC_WEAK_FUNC(sub_826A7080);
PPC_FUNC_IMPL(__imp__sub_826A7080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	// cntlzd r5,r3
	ctx.r5.u64 = ctx.r3.u64 == 0 ? 64 : __builtin_clzll(ctx.r3.u64);
	// sld r3,r3,r5
	ctx.r3.u64 = ctx.r5.u8 & 0x40 ? 0 : (ctx.r3.u64 << (ctx.r5.u8 & 0x7F));
	// cmpdi r3,0
	cr0.compare<int64_t>(ctx.r3.s64, 0, xer);
	// beq 0x826a709c
	if (cr0.eq) goto loc_826A709C;
	// subfic r5,r5,1086
	xer.ca = ctx.r5.u32 <= 1086;
	ctx.r5.s64 = 1086 - ctx.r5.s64;
	// rldicl r3,r3,53,12
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u64, 53) & 0xFFFFFFFFFFFFF;
	// rldimi r3,r5,52,1
	ctx.r3.u64 = (__builtin_rotateleft64(ctx.r5.u64, 52) & 0x7FF0000000000000) | (ctx.r3.u64 & 0x800FFFFFFFFFFFFF);
loc_826A709C:
	// std r3,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r3.u64);
	// lfd f1,-8(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A70A8"))) PPC_WEAK_FUNC(sub_826A70A8);
PPC_FUNC_IMPL(__imp__sub_826A70A8) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A70AC"))) PPC_WEAK_FUNC(sub_826A70AC);
PPC_FUNC_IMPL(__imp__sub_826A70AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A70B0"))) PPC_WEAK_FUNC(sub_826A70B0);
PPC_FUNC_IMPL(__imp__sub_826A70B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr1{};
	PPCCRRegister cr2{};
	PPCCRRegister cr3{};
	PPCCRRegister cr4{};
	PPCCRRegister cr5{};
	PPCCRRegister cr6{};
	PPCCRRegister cr7{};
	PPCRegister r0{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f14{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r0
	// stwu r1,-80(r1)
	ea = -80 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r0,8(r1)
	PPC_STORE_U32(ctx.r1.u32 + 8, r0.u32);
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// cmpwi r4,0
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r0,312(r3)
	r0.u64 = PPC_LOAD_U32(ctx.r3.u32 + 312);
	// cmpwi cr1,r0,0
	cr1.compare<int32_t>(r0.s32, 0, xer);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// bne 0x826a70dc
	if (!cr0.eq) goto loc_826A70DC;
	// li r6,1
	ctx.r6.s64 = 1;
loc_826A70DC:
	// bne cr1,0x826a719c
	if (!cr1.eq) goto loc_826A719C;
	// lwz r3,308(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// lwz r4,144(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 144);
	// bl 0x823b4ac0
	sub_823B4AC0(ctx, base);
	// lfd f14,0(r7)
	ctx.fpscr.disableFlushMode();
	f14.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// lfd f15,8(r7)
	f15.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
	// lfd f16,16(r7)
	f16.u64 = PPC_LOAD_U64(ctx.r7.u32 + 16);
	// lfd f17,24(r7)
	f17.u64 = PPC_LOAD_U64(ctx.r7.u32 + 24);
	// lfd f18,32(r7)
	f18.u64 = PPC_LOAD_U64(ctx.r7.u32 + 32);
	// lfd f19,40(r7)
	f19.u64 = PPC_LOAD_U64(ctx.r7.u32 + 40);
	// lfd f20,48(r7)
	f20.u64 = PPC_LOAD_U64(ctx.r7.u32 + 48);
	// lfd f21,56(r7)
	f21.u64 = PPC_LOAD_U64(ctx.r7.u32 + 56);
	// lfd f22,64(r7)
	f22.u64 = PPC_LOAD_U64(ctx.r7.u32 + 64);
	// lfd f23,72(r7)
	f23.u64 = PPC_LOAD_U64(ctx.r7.u32 + 72);
	// lfd f24,80(r7)
	f24.u64 = PPC_LOAD_U64(ctx.r7.u32 + 80);
	// lfd f25,88(r7)
	f25.u64 = PPC_LOAD_U64(ctx.r7.u32 + 88);
	// lfd f26,96(r7)
	f26.u64 = PPC_LOAD_U64(ctx.r7.u32 + 96);
	// lfd f27,104(r7)
	f27.u64 = PPC_LOAD_U64(ctx.r7.u32 + 104);
	// lfd f28,112(r7)
	f28.u64 = PPC_LOAD_U64(ctx.r7.u32 + 112);
	// lfd f29,120(r7)
	f29.u64 = PPC_LOAD_U64(ctx.r7.u32 + 120);
	// lfd f30,128(r7)
	f30.u64 = PPC_LOAD_U64(ctx.r7.u32 + 128);
	// lfd f31,136(r7)
	f31.u64 = PPC_LOAD_U64(ctx.r7.u32 + 136);
	// ld r13,152(r7)
	ctx.r13.u64 = PPC_LOAD_U64(ctx.r7.u32 + 152);
	// ld r14,160(r7)
	r14.u64 = PPC_LOAD_U64(ctx.r7.u32 + 160);
	// ld r15,168(r7)
	r15.u64 = PPC_LOAD_U64(ctx.r7.u32 + 168);
	// ld r16,176(r7)
	r16.u64 = PPC_LOAD_U64(ctx.r7.u32 + 176);
	// ld r17,184(r7)
	r17.u64 = PPC_LOAD_U64(ctx.r7.u32 + 184);
	// ld r18,192(r7)
	r18.u64 = PPC_LOAD_U64(ctx.r7.u32 + 192);
	// ld r19,200(r7)
	r19.u64 = PPC_LOAD_U64(ctx.r7.u32 + 200);
	// ld r20,208(r7)
	r20.u64 = PPC_LOAD_U64(ctx.r7.u32 + 208);
	// ld r21,216(r7)
	r21.u64 = PPC_LOAD_U64(ctx.r7.u32 + 216);
	// ld r22,224(r7)
	r22.u64 = PPC_LOAD_U64(ctx.r7.u32 + 224);
	// ld r23,232(r7)
	r23.u64 = PPC_LOAD_U64(ctx.r7.u32 + 232);
	// ld r24,240(r7)
	r24.u64 = PPC_LOAD_U64(ctx.r7.u32 + 240);
	// ld r25,248(r7)
	r25.u64 = PPC_LOAD_U64(ctx.r7.u32 + 248);
	// ld r26,256(r7)
	r26.u64 = PPC_LOAD_U64(ctx.r7.u32 + 256);
	// ld r27,264(r7)
	r27.u64 = PPC_LOAD_U64(ctx.r7.u32 + 264);
	// ld r28,272(r7)
	r28.u64 = PPC_LOAD_U64(ctx.r7.u32 + 272);
	// ld r29,280(r7)
	r29.u64 = PPC_LOAD_U64(ctx.r7.u32 + 280);
	// ld r30,288(r7)
	r30.u64 = PPC_LOAD_U64(ctx.r7.u32 + 288);
	// ld r31,296(r7)
	r31.u64 = PPC_LOAD_U64(ctx.r7.u32 + 296);
	// lwz r5,308(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// lwz r4,304(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// mtlr r5
	// ld r1,144(r7)
	ctx.r1.u64 = PPC_LOAD_U64(ctx.r7.u32 + 144);
	// mtcr r4
	cr0.lt = (ctx.r4.u32 & 0x80000000) != 0;
	cr0.gt = (ctx.r4.u32 & 0x40000000) != 0;
	cr0.eq = (ctx.r4.u32 & 0x20000000) != 0;
	cr0.so = (ctx.r4.u32 & 0x10000000) != 0;
	cr1.lt = (ctx.r4.u32 & 0x8000000) != 0;
	cr1.gt = (ctx.r4.u32 & 0x4000000) != 0;
	cr1.eq = (ctx.r4.u32 & 0x2000000) != 0;
	cr1.so = (ctx.r4.u32 & 0x1000000) != 0;
	cr2.lt = (ctx.r4.u32 & 0x800000) != 0;
	cr2.gt = (ctx.r4.u32 & 0x400000) != 0;
	cr2.eq = (ctx.r4.u32 & 0x200000) != 0;
	cr2.so = (ctx.r4.u32 & 0x100000) != 0;
	cr3.lt = (ctx.r4.u32 & 0x80000) != 0;
	cr3.gt = (ctx.r4.u32 & 0x40000) != 0;
	cr3.eq = (ctx.r4.u32 & 0x20000) != 0;
	cr3.so = (ctx.r4.u32 & 0x10000) != 0;
	cr4.lt = (ctx.r4.u32 & 0x8000) != 0;
	cr4.gt = (ctx.r4.u32 & 0x4000) != 0;
	cr4.eq = (ctx.r4.u32 & 0x2000) != 0;
	cr4.so = (ctx.r4.u32 & 0x1000) != 0;
	cr5.lt = (ctx.r4.u32 & 0x800) != 0;
	cr5.gt = (ctx.r4.u32 & 0x400) != 0;
	cr5.eq = (ctx.r4.u32 & 0x200) != 0;
	cr5.so = (ctx.r4.u32 & 0x100) != 0;
	cr6.lt = (ctx.r4.u32 & 0x80) != 0;
	cr6.gt = (ctx.r4.u32 & 0x40) != 0;
	cr6.eq = (ctx.r4.u32 & 0x20) != 0;
	cr6.so = (ctx.r4.u32 & 0x10) != 0;
	cr7.lt = (ctx.r4.u32 & 0x8) != 0;
	cr7.gt = (ctx.r4.u32 & 0x4) != 0;
	cr7.eq = (ctx.r4.u32 & 0x2) != 0;
	cr7.so = (ctx.r4.u32 & 0x1) != 0;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// blr 
	return;
loc_826A719C:
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// bl 0x823b4ac0
	sub_823B4AC0(ctx, base);
	// lwz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// bl 0x826e4bcc
	__imp__RtlUnwind(ctx, base);
	// lwz r0,8(r1)
	r0.u64 = PPC_LOAD_U32(ctx.r1.u32 + 8);
	// mtlr r0
	// addi r1,r1,80
	ctx.r1.s64 = ctx.r1.s64 + 80;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A71C4"))) PPC_WEAK_FUNC(sub_826A71C4);
PPC_FUNC_IMPL(__imp__sub_826A71C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A71C8"))) PPC_WEAK_FUNC(sub_826A71C8);
PPC_FUNC_IMPL(__imp__sub_826A71C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// stfd f31,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, f31.u64);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826a7228
	if (!cr6.eq) goto loc_826A7228;
	// bl 0x8239ff90
	sub_8239FF90(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823a3f38
	sub_823A3F38(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f1,-31368(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// b 0x826a72d8
	goto loc_826A72D8;
loc_826A7228:
	// lis r4,-16377
	ctx.r4.s64 = -1073283072;
	// li r3,248
	ctx.r3.s64 = 248;
	// ori r4,r4,65279
	ctx.r4.u64 = ctx.r4.u64 | 65279;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lhz r11,128(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 128);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// rlwinm r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x826a72b8
	if (!cr6.eq) goto loc_826A72B8;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// bl 0x823abd50
	sub_823ABD50(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble 0x826a72a8
	if (!cr0.gt) goto loc_826A72A8;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x826a7288
	if (!cr6.gt) goto loc_826A7288;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x826a72a8
	if (!cr6.eq) goto loc_826A72A8;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r3,23
	ctx.r3.s64 = 23;
	// bl 0x823b2448
	sub_823B2448(ctx, base);
	// b 0x826a72d8
	goto loc_826A72D8;
loc_826A7288:
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lfd f2,-22584(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -22584);
loc_826A7290:
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r4,23
	ctx.r4.s64 = 23;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x823b2550
	sub_823B2550(ctx, base);
	// b 0x826a72d8
	goto loc_826A72D8;
loc_826A72A8:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f2,f31,f0
	ctx.f2.f64 = f31.f64 + f0.f64;
	// b 0x826a7290
	goto loc_826A7290;
loc_826A72B8:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x823abdd8
	sub_823ABDD8(ctx, base);
	// lis r4,-16377
	ctx.r4.s64 = -1073283072;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// ori r4,r4,65279
	ctx.r4.u64 = ctx.r4.u64 | 65279;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
loc_826A72D8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A72F4"))) PPC_WEAK_FUNC(sub_826A72F4);
PPC_FUNC_IMPL(__imp__sub_826A72F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A72F8"))) PPC_WEAK_FUNC(sub_826A72F8);
PPC_FUNC_IMPL(__imp__sub_826A72F8) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A72FC"))) PPC_WEAK_FUNC(sub_826A72FC);
PPC_FUNC_IMPL(__imp__sub_826A72FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A7300"))) PPC_WEAK_FUNC(sub_826A7300);
PPC_FUNC_IMPL(__imp__sub_826A7300) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr1{};
	PPCCRRegister cr2{};
	PPCCRRegister cr3{};
	PPCCRRegister cr4{};
	PPCCRRegister cr5{};
	PPCCRRegister cr6{};
	PPCCRRegister cr7{};
	PPCRegister r0{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f14{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lis r4,-32126
	ctx.r4.s64 = -2105409536;
	// lwz r0,14856(r4)
	r0.u64 = PPC_LOAD_U32(ctx.r4.u32 + 14856);
	// cmpwi r0,0
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bnectr 
	if (!cr0.eq) {
		PPC_CALL_INDIRECT_FUNC(ctr.u32);
		return;
	}
	// mflr r0
	// mfcr r4
	ctx.r4.u64 = cr0.lt ? 0x80000000 : 0;
	ctx.r4.u64 |= cr0.gt ? 0x40000000 : 0;
	ctx.r4.u64 |= cr0.eq ? 0x20000000 : 0;
	ctx.r4.u64 |= cr0.so ? 0x10000000 : 0;
	ctx.r4.u64 |= cr1.lt ? 0x8000000 : 0;
	ctx.r4.u64 |= cr1.gt ? 0x4000000 : 0;
	ctx.r4.u64 |= cr1.eq ? 0x2000000 : 0;
	ctx.r4.u64 |= cr1.so ? 0x1000000 : 0;
	ctx.r4.u64 |= cr2.lt ? 0x800000 : 0;
	ctx.r4.u64 |= cr2.gt ? 0x400000 : 0;
	ctx.r4.u64 |= cr2.eq ? 0x200000 : 0;
	ctx.r4.u64 |= cr2.so ? 0x100000 : 0;
	ctx.r4.u64 |= cr3.lt ? 0x80000 : 0;
	ctx.r4.u64 |= cr3.gt ? 0x40000 : 0;
	ctx.r4.u64 |= cr3.eq ? 0x20000 : 0;
	ctx.r4.u64 |= cr3.so ? 0x10000 : 0;
	ctx.r4.u64 |= cr4.lt ? 0x8000 : 0;
	ctx.r4.u64 |= cr4.gt ? 0x4000 : 0;
	ctx.r4.u64 |= cr4.eq ? 0x2000 : 0;
	ctx.r4.u64 |= cr4.so ? 0x1000 : 0;
	ctx.r4.u64 |= cr5.lt ? 0x800 : 0;
	ctx.r4.u64 |= cr5.gt ? 0x400 : 0;
	ctx.r4.u64 |= cr5.eq ? 0x200 : 0;
	ctx.r4.u64 |= cr5.so ? 0x100 : 0;
	ctx.r4.u64 |= cr6.lt ? 0x80 : 0;
	ctx.r4.u64 |= cr6.gt ? 0x40 : 0;
	ctx.r4.u64 |= cr6.eq ? 0x20 : 0;
	ctx.r4.u64 |= cr6.so ? 0x10 : 0;
	ctx.r4.u64 |= cr7.lt ? 0x8 : 0;
	ctx.r4.u64 |= cr7.gt ? 0x4 : 0;
	ctx.r4.u64 |= cr7.eq ? 0x2 : 0;
	ctx.r4.u64 |= cr7.so ? 0x1 : 0;
	// stfd f14,0(r3)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r3.u32 + 0, f14.u64);
	// stfd f15,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, f15.u64);
	// stfd f16,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, f16.u64);
	// stfd f17,24(r3)
	PPC_STORE_U64(ctx.r3.u32 + 24, f17.u64);
	// stfd f18,32(r3)
	PPC_STORE_U64(ctx.r3.u32 + 32, f18.u64);
	// stfd f19,40(r3)
	PPC_STORE_U64(ctx.r3.u32 + 40, f19.u64);
	// stfd f20,48(r3)
	PPC_STORE_U64(ctx.r3.u32 + 48, f20.u64);
	// stfd f21,56(r3)
	PPC_STORE_U64(ctx.r3.u32 + 56, f21.u64);
	// stfd f22,64(r3)
	PPC_STORE_U64(ctx.r3.u32 + 64, f22.u64);
	// stfd f23,72(r3)
	PPC_STORE_U64(ctx.r3.u32 + 72, f23.u64);
	// stfd f24,80(r3)
	PPC_STORE_U64(ctx.r3.u32 + 80, f24.u64);
	// stfd f25,88(r3)
	PPC_STORE_U64(ctx.r3.u32 + 88, f25.u64);
	// stfd f26,96(r3)
	PPC_STORE_U64(ctx.r3.u32 + 96, f26.u64);
	// stfd f27,104(r3)
	PPC_STORE_U64(ctx.r3.u32 + 104, f27.u64);
	// stfd f28,112(r3)
	PPC_STORE_U64(ctx.r3.u32 + 112, f28.u64);
	// stfd f29,120(r3)
	PPC_STORE_U64(ctx.r3.u32 + 120, f29.u64);
	// stfd f30,128(r3)
	PPC_STORE_U64(ctx.r3.u32 + 128, f30.u64);
	// stfd f31,136(r3)
	PPC_STORE_U64(ctx.r3.u32 + 136, f31.u64);
	// std r13,152(r3)
	PPC_STORE_U64(ctx.r3.u32 + 152, ctx.r13.u64);
	// std r14,160(r3)
	PPC_STORE_U64(ctx.r3.u32 + 160, r14.u64);
	// std r15,168(r3)
	PPC_STORE_U64(ctx.r3.u32 + 168, r15.u64);
	// std r16,176(r3)
	PPC_STORE_U64(ctx.r3.u32 + 176, r16.u64);
	// std r17,184(r3)
	PPC_STORE_U64(ctx.r3.u32 + 184, r17.u64);
	// std r18,192(r3)
	PPC_STORE_U64(ctx.r3.u32 + 192, r18.u64);
	// std r19,200(r3)
	PPC_STORE_U64(ctx.r3.u32 + 200, r19.u64);
	// std r20,208(r3)
	PPC_STORE_U64(ctx.r3.u32 + 208, r20.u64);
	// std r21,216(r3)
	PPC_STORE_U64(ctx.r3.u32 + 216, r21.u64);
	// std r22,224(r3)
	PPC_STORE_U64(ctx.r3.u32 + 224, r22.u64);
	// std r23,232(r3)
	PPC_STORE_U64(ctx.r3.u32 + 232, r23.u64);
	// std r24,240(r3)
	PPC_STORE_U64(ctx.r3.u32 + 240, r24.u64);
	// std r25,248(r3)
	PPC_STORE_U64(ctx.r3.u32 + 248, r25.u64);
	// std r26,256(r3)
	PPC_STORE_U64(ctx.r3.u32 + 256, r26.u64);
	// std r27,264(r3)
	PPC_STORE_U64(ctx.r3.u32 + 264, r27.u64);
	// std r28,272(r3)
	PPC_STORE_U64(ctx.r3.u32 + 272, r28.u64);
	// std r29,280(r3)
	PPC_STORE_U64(ctx.r3.u32 + 280, r29.u64);
	// std r30,288(r3)
	PPC_STORE_U64(ctx.r3.u32 + 288, r30.u64);
	// std r31,296(r3)
	PPC_STORE_U64(ctx.r3.u32 + 296, r31.u64);
	// stw r0,308(r3)
	PPC_STORE_U32(ctx.r3.u32 + 308, r0.u32);
	// stw r4,304(r3)
	PPC_STORE_U32(ctx.r3.u32 + 304, ctx.r4.u32);
	// std r1,144(r3)
	PPC_STORE_U64(ctx.r3.u32 + 144, ctx.r1.u64);
	// li r0,0
	r0.s64 = 0;
	// stw r0,312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 312, r0.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A73CC"))) PPC_WEAK_FUNC(sub_826A73CC);
PPC_FUNC_IMPL(__imp__sub_826A73CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A73D0"))) PPC_WEAK_FUNC(sub_826A73D0);
PPC_FUNC_IMPL(__imp__sub_826A73D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826a7458
	if (cr6.eq) goto loc_826A7458;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826A73F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x826a73f0
	if (!cr6.eq) goto loc_826A73F0;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239d488
	sub_8239D488(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x826a7458
	if (cr0.eq) goto loc_826A7458;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239d3a0
	sub_8239D3A0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x826a7450
	if (cr0.eq) goto loc_826A7450;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823a3f80
	sub_823A3F80(ctx, base);
loc_826A7450:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x826a745c
	goto loc_826A745C;
loc_826A7458:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A745C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826A7464"))) PPC_WEAK_FUNC(sub_826A7464);
PPC_FUNC_IMPL(__imp__sub_826A7464) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A7468"))) PPC_WEAK_FUNC(sub_826A7468);
PPC_FUNC_IMPL(__imp__sub_826A7468) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826a7498
	if (cr6.eq) goto loc_826A7498;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// extsb. r11,r11
	r11.s64 = r11.s8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a7498
	if (cr0.eq) goto loc_826A7498;
	// cmpwi cr6,r11,67
	cr6.compare<int32_t>(r11.s32, 67, xer);
	// bne cr6,0x826a7490
	if (!cr6.eq) goto loc_826A7490;
	// lbz r11,1(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826a7498
	if (cr6.eq) goto loc_826A7498;
loc_826A7490:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826A7498:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r3,r11,9280
	ctx.r3.s64 = r11.s64 + 9280;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A74A4"))) PPC_WEAK_FUNC(sub_826A74A4);
PPC_FUNC_IMPL(__imp__sub_826A74A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A74A8"))) PPC_WEAK_FUNC(sub_826A74A8);
PPC_FUNC_IMPL(__imp__sub_826A74A8) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stfd f30,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, f30.u64);
	// stfd f31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f30,f1
	f30.f64 = ctx.f1.f64;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31368(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fabs f13,f30
	ctx.f13.u64 = f30.u64 & ~0x8000000000000000;
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// bne cr6,0x826a74d8
	if (!cr6.eq) goto loc_826A74D8;
	// b 0x826a7590
	goto loc_826A7590;
loc_826A74D8:
	// lis r11,-32242
	r11.s64 = -2113011712;
	// addi r11,r11,-27672
	r11.s64 = r11.s64 + -27672;
	// lfd f0,-32(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x826a7538
	if (!cr6.gt) goto loc_826A7538;
	// lfd f0,-40(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -40);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x826a7504
	if (!cr6.gt) goto loc_826A7504;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// b 0x826a7588
	goto loc_826A7588;
loc_826A7504:
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f31,264(r11)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f1,f13,f31
	ctx.f1.f64 = ctx.f13.f64 * f31.f64;
	// bl 0x823a05b0
	sub_823A05B0(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fadd f13,f1,f0
	ctx.f13.f64 = ctx.f1.f64 + f0.f64;
	// fdiv f13,f0,f13
	ctx.f13.f64 = f0.f64 / ctx.f13.f64;
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// fsub f0,f0,f13
	f0.f64 = f0.f64 - ctx.f13.f64;
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// b 0x826a7588
	goto loc_826A7588;
loc_826A7538:
	// fmul f0,f13,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f13.f64 * ctx.f13.f64;
	// lis r10,-32242
	ctx.r10.s64 = -2113011712;
	// lfd f11,16(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// lfd f12,-27632(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + -27632);
	// lis r10,-32242
	ctx.r10.s64 = -2113011712;
	// fadd f10,f0,f11
	ctx.f10.f64 = f0.f64 + ctx.f11.f64;
	// lfd f11,-16(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + -16);
	// fnmsub f11,f0,f12,f11
	ctx.f11.f64 = -(f0.f64 * ctx.f12.f64 - ctx.f11.f64);
	// lfd f12,8(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// fmadd f10,f10,f0,f12
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f12.f64;
	// lfd f12,-27640(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + -27640);
	// fmsub f11,f11,f0,f12
	ctx.f11.f64 = ctx.f11.f64 * f0.f64 - ctx.f12.f64;
	// lfd f12,0(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmadd f12,f10,f0,f12
	ctx.f12.f64 = ctx.f10.f64 * f0.f64 + ctx.f12.f64;
	// fmul f0,f11,f0
	f0.f64 = ctx.f11.f64 * f0.f64;
	// fdiv f12,f0,f12
	ctx.f12.f64 = f0.f64 / ctx.f12.f64;
	// lfd f0,-31360(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f0,f12,f0
	f0.f64 = ctx.f12.f64 + f0.f64;
	// fmul f0,f0,f13
	f0.f64 = f0.f64 * ctx.f13.f64;
loc_826A7588:
	// fneg f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = f0.u64 ^ 0x8000000000000000;
	// fsel f1,f30,f0,f13
	ctx.f1.f64 = f30.f64 >= 0.0 ? f0.f64 : ctx.f13.f64;
loc_826A7590:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-24(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// lfd f31,-16(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A75A8"))) PPC_WEAK_FUNC(sub_826A75A8);
PPC_FUNC_IMPL(__imp__sub_826A75A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-16377
	r11.s64 = -1073283072;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// li r3,248
	ctx.r3.s64 = 248;
	// stfd f31,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, f31.u64);
	// ori r29,r11,65279
	r29.u64 = r11.u64 | 65279;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lhz r11,160(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x826a7648
	if (!cr6.eq) goto loc_826A7648;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823abd50
	sub_823ABD50(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble 0x826a7634
	if (!cr0.gt) goto loc_826A7634;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x826a7620
	if (!cr6.gt) goto loc_826A7620;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x826a7634
	if (!cr6.eq) goto loc_826A7634;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r3,31
	ctx.r3.s64 = 31;
	// bl 0x823b2448
	sub_823B2448(ctx, base);
	// b 0x826a7808
	goto loc_826A7808;
loc_826A7620:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// b 0x826a7808
	goto loc_826A7808;
loc_826A7634:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// li r3,8
	ctx.r3.s64 = 8;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f2,f31,f0
	ctx.f2.f64 = f31.f64 + f0.f64;
	// b 0x826a77f8
	goto loc_826A77F8;
loc_826A7648:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31368(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// beq cr6,0x826a7620
	if (cr6.eq) goto loc_826A7620;
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bge cr6,0x826a7668
	if (!cr6.lt) goto loc_826A7668;
	// fneg f1,f31
	ctx.f1.u64 = f31.u64 ^ 0x8000000000000000;
	// b 0x826a766c
	goto loc_826A766C;
loc_826A7668:
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
loc_826A766C:
	// fcmpu cr6,f31,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f0.f64);
	// li r31,-1
	r31.s64 = -1;
	// blt cr6,0x826a767c
	if (cr6.lt) goto loc_826A767C;
	// li r31,1
	r31.s64 = 1;
loc_826A767C:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f30,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32242
	r11.s64 = -2113011712;
	// fcmpu cr6,f1,f30
	cr6.compare(ctx.f1.f64, f30.f64);
	// addi r11,r11,-27624
	r11.s64 = r11.s64 + -27624;
	// ble cr6,0x826a772c
	if (!cr6.gt) goto loc_826A772C;
	// lfd f0,8(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x826a76f8
	if (!cr6.gt) goto loc_826A76F8;
	// lfd f0,16(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x826a76c0
	if (!cr6.gt) goto loc_826A76C0;
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lfd f0,-22592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -22592);
	// fmul f2,f0,f31
	ctx.f2.f64 = f0.f64 * f31.f64;
loc_826A76B8:
	// li r3,17
	ctx.r3.s64 = 17;
	// b 0x826a77f8
	goto loc_826A77F8;
loc_826A76C0:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x823a0690
	sub_823A0690(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r11,-1
	ctx.r4.s64 = r11.s64 + -1;
	// cmpwi cr6,r4,1024
	cr6.compare<int32_t>(ctx.r4.s32, 1024, xer);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// ble cr6,0x826a76ec
	if (!cr6.gt) goto loc_826A76EC;
	// addi r4,r4,-1536
	ctx.r4.s64 = ctx.r4.s64 + -1536;
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// fmr f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f1.f64;
	// b 0x826a76b8
	goto loc_826A76B8;
loc_826A76EC:
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = ctx.f1.f64;
	// b 0x826a771c
	goto loc_826A771C;
loc_826A76F8:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x823a0690
	sub_823A0690(ctx, base);
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// fdiv f0,f30,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f30.f64 / ctx.f1.f64;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fsub f13,f1,f0
	ctx.f13.f64 = ctx.f1.f64 - f0.f64;
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// fmul f30,f13,f0
	f30.f64 = ctx.f13.f64 * f0.f64;
loc_826A771C:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x826a77d4
	if (!cr6.lt) goto loc_826A77D4;
	// fneg f30,f30
	ctx.fpscr.disableFlushMode();
	f30.u64 = f30.u64 ^ 0x8000000000000000;
	// b 0x826a77d4
	goto loc_826A77D4;
loc_826A772C:
	// lfd f0,0(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x826a777c
	if (!cr6.lt) goto loc_826A777C;
	// fmr f30,f31
	f30.f64 = f31.f64;
	// stfd f30,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, f30.u64);
	// lhz r11,88(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 88);
	// rlwinm. r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x826a77d4
	if (!cr0.eq) goto loc_826A77D4;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// clrlwi. r11,r11,12
	r11.u64 = r11.u32 & 0xFFFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x826a7764
	if (!cr0.eq) goto loc_826A7764;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826a77d4
	if (cr6.eq) goto loc_826A77D4;
loc_826A7764:
	// li r4,1536
	ctx.r4.s64 = 1536;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823abd18
	sub_823ABD18(ctx, base);
	// li r3,18
	ctx.r3.s64 = 18;
	// fmr f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f1.f64;
	// b 0x826a77f8
	goto loc_826A77F8;
loc_826A777C:
	// lis r10,-32242
	ctx.r10.s64 = -2113011712;
	// fmul f0,f31,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 * f31.f64;
	// lfd f13,-27512(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -27512);
	// lis r10,-32242
	ctx.r10.s64 = -2113011712;
	// lfd f12,-27520(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + -27520);
	// fsub f11,f0,f12
	ctx.f11.f64 = f0.f64 - ctx.f12.f64;
	// lfd f12,40(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 40);
	// fnmsub f12,f0,f13,f12
	ctx.f12.f64 = -(f0.f64 * ctx.f13.f64 - ctx.f12.f64);
	// lfd f13,64(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 64);
	// lis r11,-32242
	r11.s64 = -2113011712;
	// fmadd f11,f11,f0,f13
	ctx.f11.f64 = ctx.f11.f64 * f0.f64 + ctx.f13.f64;
	// lfd f13,-27528(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -27528);
	// lis r11,-32242
	r11.s64 = -2113011712;
	// fmsub f12,f12,f0,f13
	ctx.f12.f64 = ctx.f12.f64 * f0.f64 - ctx.f13.f64;
	// lfd f13,-27536(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -27536);
	// lis r11,-32242
	r11.s64 = -2113011712;
	// fmsub f11,f11,f0,f13
	ctx.f11.f64 = ctx.f11.f64 * f0.f64 - ctx.f13.f64;
	// lfd f13,-27544(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -27544);
	// fmsub f13,f12,f0,f13
	ctx.f13.f64 = ctx.f12.f64 * f0.f64 - ctx.f13.f64;
	// fdiv f13,f13,f11
	ctx.f13.f64 = ctx.f13.f64 / ctx.f11.f64;
	// fmadd f0,f13,f0,f30
	f0.f64 = ctx.f13.f64 * f0.f64 + f30.f64;
	// fmul f30,f0,f31
	f30.f64 = f0.f64 * f31.f64;
loc_826A77D4:
	// rlwinm. r11,r30,0,28,28
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a77f0
	if (cr0.eq) goto loc_826A77F0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// b 0x826a7808
	goto loc_826A7808;
loc_826A77F0:
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f30.f64;
	// li r3,16
	ctx.r3.s64 = 16;
loc_826A77F8:
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r4,31
	ctx.r4.s64 = 31;
	// bl 0x823b2550
	sub_823B2550(ctx, base);
loc_826A7808:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826A7818"))) PPC_WEAK_FUNC(sub_826A7818);
PPC_FUNC_IMPL(__imp__sub_826A7818) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-16377
	r11.s64 = -1073283072;
	// fmr f30,f1
	f30.f64 = ctx.f1.f64;
	// li r3,248
	ctx.r3.s64 = 248;
	// stfd f30,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, f30.u64);
	// ori r30,r11,65279
	r30.u64 = r11.u64 | 65279;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lhz r11,144(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 144);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x826a78c4
	if (!cr6.eq) goto loc_826A78C4;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// bl 0x823abd50
	sub_823ABD50(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble 0x826a78b0
	if (!cr0.gt) goto loc_826A78B0;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x826a7898
	if (!cr6.gt) goto loc_826A7898;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x826a78b0
	if (!cr6.eq) goto loc_826A78B0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// li r3,19
	ctx.r3.s64 = 19;
	// bl 0x823b2448
	sub_823B2448(ctx, base);
	// b 0x826a79c8
	goto loc_826A79C8;
loc_826A7898:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lfd f1,-22592(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -22592);
	// b 0x826a79c8
	goto loc_826A79C8;
loc_826A78B0:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// li r3,8
	ctx.r3.s64 = 8;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f2,f30,f0
	ctx.f2.f64 = f30.f64 + f0.f64;
	// b 0x826a79b8
	goto loc_826A79B8;
loc_826A78C4:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31368(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// bne cr6,0x826a78ec
	if (!cr6.eq) goto loc_826A78EC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f1,-31360(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// b 0x826a79c8
	goto loc_826A79C8;
loc_826A78EC:
	// fcmpu cr6,f30,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f0.f64);
	// bge cr6,0x826a78fc
	if (!cr6.lt) goto loc_826A78FC;
	// fneg f1,f30
	ctx.f1.u64 = f30.u64 ^ 0x8000000000000000;
	// b 0x826a7900
	goto loc_826A7900;
loc_826A78FC:
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
loc_826A7900:
	// lis r11,-32242
	r11.s64 = -2113011712;
	// addi r11,r11,-27608
	r11.s64 = r11.s64 + -27608;
	// lfd f0,-8(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -8);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x826a7968
	if (!cr6.gt) goto loc_826A7968;
	// lfd f0,0(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x826a7930
	if (!cr6.gt) goto loc_826A7930;
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lfd f2,-22592(r11)
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -22592);
loc_826A7928:
	// li r3,17
	ctx.r3.s64 = 17;
	// b 0x826a79b8
	goto loc_826A79B8;
loc_826A7930:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x823a0690
	sub_823A0690(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r11,-1
	ctx.r4.s64 = r11.s64 + -1;
	// cmpwi cr6,r4,1024
	cr6.compare<int32_t>(ctx.r4.s32, 1024, xer);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// ble cr6,0x826a795c
	if (!cr6.gt) goto loc_826A795C;
	// addi r4,r4,-1536
	ctx.r4.s64 = ctx.r4.s64 + -1536;
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// fmr f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f1.f64;
	// b 0x826a7928
	goto loc_826A7928;
loc_826A795C:
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// b 0x826a7994
	goto loc_826A7994;
loc_826A7968:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x823a0690
	sub_823A0690(ctx, base);
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x823abcd8
	sub_823ABCD8(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fdiv f0,f0,f1
	f0.f64 = f0.f64 / ctx.f1.f64;
	// fadd f13,f0,f1
	ctx.f13.f64 = f0.f64 + ctx.f1.f64;
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// fmul f31,f13,f0
	f31.f64 = ctx.f13.f64 * f0.f64;
loc_826A7994:
	// rlwinm. r11,r31,0,28,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a79b0
	if (cr0.eq) goto loc_826A79B0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// b 0x826a79c8
	goto loc_826A79C8;
loc_826A79B0:
	// fmr f2,f31
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f31.f64;
	// li r3,16
	ctx.r3.s64 = 16;
loc_826A79B8:
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// li r4,19
	ctx.r4.s64 = 19;
	// bl 0x823b2550
	sub_823B2550(ctx, base);
loc_826A79C8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A79E8"))) PPC_WEAK_FUNC(sub_826A79E8);
PPC_FUNC_IMPL(__imp__sub_826A79E8) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A79EC"))) PPC_WEAK_FUNC(sub_826A79EC);
PPC_FUNC_IMPL(__imp__sub_826A79EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp____savevmx_14"))) PPC_WEAK_FUNC(__savevmx_14);
PPC_FUNC_IMPL(__imp____savevmx_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-288
	r11.s64 = -288;
	// stvx v14,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx v15,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_15"))) PPC_WEAK_FUNC(__savevmx_15);
PPC_FUNC_IMPL(__imp____savevmx_15) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-272
	r11.s64 = -272;
	// stvx v15,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_16"))) PPC_WEAK_FUNC(__savevmx_16);
PPC_FUNC_IMPL(__imp____savevmx_16) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_17"))) PPC_WEAK_FUNC(__savevmx_17);
PPC_FUNC_IMPL(__imp____savevmx_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_18"))) PPC_WEAK_FUNC(__savevmx_18);
PPC_FUNC_IMPL(__imp____savevmx_18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_19"))) PPC_WEAK_FUNC(__savevmx_19);
PPC_FUNC_IMPL(__imp____savevmx_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_20"))) PPC_WEAK_FUNC(__savevmx_20);
PPC_FUNC_IMPL(__imp____savevmx_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_21"))) PPC_WEAK_FUNC(__savevmx_21);
PPC_FUNC_IMPL(__imp____savevmx_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_22"))) PPC_WEAK_FUNC(__savevmx_22);
PPC_FUNC_IMPL(__imp____savevmx_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_23"))) PPC_WEAK_FUNC(__savevmx_23);
PPC_FUNC_IMPL(__imp____savevmx_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_24"))) PPC_WEAK_FUNC(__savevmx_24);
PPC_FUNC_IMPL(__imp____savevmx_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_25"))) PPC_WEAK_FUNC(__savevmx_25);
PPC_FUNC_IMPL(__imp____savevmx_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_26"))) PPC_WEAK_FUNC(__savevmx_26);
PPC_FUNC_IMPL(__imp____savevmx_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_27"))) PPC_WEAK_FUNC(__savevmx_27);
PPC_FUNC_IMPL(__imp____savevmx_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_28"))) PPC_WEAK_FUNC(__savevmx_28);
PPC_FUNC_IMPL(__imp____savevmx_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_29"))) PPC_WEAK_FUNC(__savevmx_29);
PPC_FUNC_IMPL(__imp____savevmx_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_30"))) PPC_WEAK_FUNC(__savevmx_30);
PPC_FUNC_IMPL(__imp____savevmx_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_31"))) PPC_WEAK_FUNC(__savevmx_31);
PPC_FUNC_IMPL(__imp____savevmx_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v31{};
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_64"))) PPC_WEAK_FUNC(__savevmx_64);
PPC_FUNC_IMPL(__imp____savevmx_64) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v64{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1024
	r11.s64 = -1024;
	// stvx128 v64,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v64.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-1008
	r11.s64 = -1008;
	// stvx128 v65,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v65.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_65"))) PPC_WEAK_FUNC(__savevmx_65);
PPC_FUNC_IMPL(__imp____savevmx_65) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1008
	r11.s64 = -1008;
	// stvx128 v65,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v65.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_66"))) PPC_WEAK_FUNC(__savevmx_66);
PPC_FUNC_IMPL(__imp____savevmx_66) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_67"))) PPC_WEAK_FUNC(__savevmx_67);
PPC_FUNC_IMPL(__imp____savevmx_67) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_68"))) PPC_WEAK_FUNC(__savevmx_68);
PPC_FUNC_IMPL(__imp____savevmx_68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_69"))) PPC_WEAK_FUNC(__savevmx_69);
PPC_FUNC_IMPL(__imp____savevmx_69) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_70"))) PPC_WEAK_FUNC(__savevmx_70);
PPC_FUNC_IMPL(__imp____savevmx_70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_71"))) PPC_WEAK_FUNC(__savevmx_71);
PPC_FUNC_IMPL(__imp____savevmx_71) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_72"))) PPC_WEAK_FUNC(__savevmx_72);
PPC_FUNC_IMPL(__imp____savevmx_72) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_73"))) PPC_WEAK_FUNC(__savevmx_73);
PPC_FUNC_IMPL(__imp____savevmx_73) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_74"))) PPC_WEAK_FUNC(__savevmx_74);
PPC_FUNC_IMPL(__imp____savevmx_74) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_75"))) PPC_WEAK_FUNC(__savevmx_75);
PPC_FUNC_IMPL(__imp____savevmx_75) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_76"))) PPC_WEAK_FUNC(__savevmx_76);
PPC_FUNC_IMPL(__imp____savevmx_76) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_77"))) PPC_WEAK_FUNC(__savevmx_77);
PPC_FUNC_IMPL(__imp____savevmx_77) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_78"))) PPC_WEAK_FUNC(__savevmx_78);
PPC_FUNC_IMPL(__imp____savevmx_78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_79"))) PPC_WEAK_FUNC(__savevmx_79);
PPC_FUNC_IMPL(__imp____savevmx_79) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_80"))) PPC_WEAK_FUNC(__savevmx_80);
PPC_FUNC_IMPL(__imp____savevmx_80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_81"))) PPC_WEAK_FUNC(__savevmx_81);
PPC_FUNC_IMPL(__imp____savevmx_81) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_82"))) PPC_WEAK_FUNC(__savevmx_82);
PPC_FUNC_IMPL(__imp____savevmx_82) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_83"))) PPC_WEAK_FUNC(__savevmx_83);
PPC_FUNC_IMPL(__imp____savevmx_83) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_84"))) PPC_WEAK_FUNC(__savevmx_84);
PPC_FUNC_IMPL(__imp____savevmx_84) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_85"))) PPC_WEAK_FUNC(__savevmx_85);
PPC_FUNC_IMPL(__imp____savevmx_85) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_86"))) PPC_WEAK_FUNC(__savevmx_86);
PPC_FUNC_IMPL(__imp____savevmx_86) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_87"))) PPC_WEAK_FUNC(__savevmx_87);
PPC_FUNC_IMPL(__imp____savevmx_87) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_88"))) PPC_WEAK_FUNC(__savevmx_88);
PPC_FUNC_IMPL(__imp____savevmx_88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_89"))) PPC_WEAK_FUNC(__savevmx_89);
PPC_FUNC_IMPL(__imp____savevmx_89) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_90"))) PPC_WEAK_FUNC(__savevmx_90);
PPC_FUNC_IMPL(__imp____savevmx_90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_91"))) PPC_WEAK_FUNC(__savevmx_91);
PPC_FUNC_IMPL(__imp____savevmx_91) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_92"))) PPC_WEAK_FUNC(__savevmx_92);
PPC_FUNC_IMPL(__imp____savevmx_92) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_93"))) PPC_WEAK_FUNC(__savevmx_93);
PPC_FUNC_IMPL(__imp____savevmx_93) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_94"))) PPC_WEAK_FUNC(__savevmx_94);
PPC_FUNC_IMPL(__imp____savevmx_94) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_95"))) PPC_WEAK_FUNC(__savevmx_95);
PPC_FUNC_IMPL(__imp____savevmx_95) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_96"))) PPC_WEAK_FUNC(__savevmx_96);
PPC_FUNC_IMPL(__imp____savevmx_96) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_97"))) PPC_WEAK_FUNC(__savevmx_97);
PPC_FUNC_IMPL(__imp____savevmx_97) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_98"))) PPC_WEAK_FUNC(__savevmx_98);
PPC_FUNC_IMPL(__imp____savevmx_98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_99"))) PPC_WEAK_FUNC(__savevmx_99);
PPC_FUNC_IMPL(__imp____savevmx_99) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_100"))) PPC_WEAK_FUNC(__savevmx_100);
PPC_FUNC_IMPL(__imp____savevmx_100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_101"))) PPC_WEAK_FUNC(__savevmx_101);
PPC_FUNC_IMPL(__imp____savevmx_101) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_102"))) PPC_WEAK_FUNC(__savevmx_102);
PPC_FUNC_IMPL(__imp____savevmx_102) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_103"))) PPC_WEAK_FUNC(__savevmx_103);
PPC_FUNC_IMPL(__imp____savevmx_103) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_104"))) PPC_WEAK_FUNC(__savevmx_104);
PPC_FUNC_IMPL(__imp____savevmx_104) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_105"))) PPC_WEAK_FUNC(__savevmx_105);
PPC_FUNC_IMPL(__imp____savevmx_105) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_106"))) PPC_WEAK_FUNC(__savevmx_106);
PPC_FUNC_IMPL(__imp____savevmx_106) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_107"))) PPC_WEAK_FUNC(__savevmx_107);
PPC_FUNC_IMPL(__imp____savevmx_107) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_108"))) PPC_WEAK_FUNC(__savevmx_108);
PPC_FUNC_IMPL(__imp____savevmx_108) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_109"))) PPC_WEAK_FUNC(__savevmx_109);
PPC_FUNC_IMPL(__imp____savevmx_109) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_110"))) PPC_WEAK_FUNC(__savevmx_110);
PPC_FUNC_IMPL(__imp____savevmx_110) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_111"))) PPC_WEAK_FUNC(__savevmx_111);
PPC_FUNC_IMPL(__imp____savevmx_111) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_112"))) PPC_WEAK_FUNC(__savevmx_112);
PPC_FUNC_IMPL(__imp____savevmx_112) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_113"))) PPC_WEAK_FUNC(__savevmx_113);
PPC_FUNC_IMPL(__imp____savevmx_113) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_114"))) PPC_WEAK_FUNC(__savevmx_114);
PPC_FUNC_IMPL(__imp____savevmx_114) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_115"))) PPC_WEAK_FUNC(__savevmx_115);
PPC_FUNC_IMPL(__imp____savevmx_115) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_116"))) PPC_WEAK_FUNC(__savevmx_116);
PPC_FUNC_IMPL(__imp____savevmx_116) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_117"))) PPC_WEAK_FUNC(__savevmx_117);
PPC_FUNC_IMPL(__imp____savevmx_117) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_118"))) PPC_WEAK_FUNC(__savevmx_118);
PPC_FUNC_IMPL(__imp____savevmx_118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_119"))) PPC_WEAK_FUNC(__savevmx_119);
PPC_FUNC_IMPL(__imp____savevmx_119) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_120"))) PPC_WEAK_FUNC(__savevmx_120);
PPC_FUNC_IMPL(__imp____savevmx_120) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_121"))) PPC_WEAK_FUNC(__savevmx_121);
PPC_FUNC_IMPL(__imp____savevmx_121) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_122"))) PPC_WEAK_FUNC(__savevmx_122);
PPC_FUNC_IMPL(__imp____savevmx_122) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_123"))) PPC_WEAK_FUNC(__savevmx_123);
PPC_FUNC_IMPL(__imp____savevmx_123) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_124"))) PPC_WEAK_FUNC(__savevmx_124);
PPC_FUNC_IMPL(__imp____savevmx_124) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_125"))) PPC_WEAK_FUNC(__savevmx_125);
PPC_FUNC_IMPL(__imp____savevmx_125) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_126"))) PPC_WEAK_FUNC(__savevmx_126);
PPC_FUNC_IMPL(__imp____savevmx_126) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_127"))) PPC_WEAK_FUNC(__savevmx_127);
PPC_FUNC_IMPL(__imp____savevmx_127) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v127{};
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_14"))) PPC_WEAK_FUNC(__restvmx_14);
PPC_FUNC_IMPL(__imp____restvmx_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-288
	r11.s64 = -288;
	// lvx v14,r11,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx v15,r11,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_15"))) PPC_WEAK_FUNC(__restvmx_15);
PPC_FUNC_IMPL(__imp____restvmx_15) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-272
	r11.s64 = -272;
	// lvx v15,r11,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_16"))) PPC_WEAK_FUNC(__restvmx_16);
PPC_FUNC_IMPL(__imp____restvmx_16) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_17"))) PPC_WEAK_FUNC(__restvmx_17);
PPC_FUNC_IMPL(__imp____restvmx_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_18"))) PPC_WEAK_FUNC(__restvmx_18);
PPC_FUNC_IMPL(__imp____restvmx_18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_19"))) PPC_WEAK_FUNC(__restvmx_19);
PPC_FUNC_IMPL(__imp____restvmx_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_20"))) PPC_WEAK_FUNC(__restvmx_20);
PPC_FUNC_IMPL(__imp____restvmx_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_21"))) PPC_WEAK_FUNC(__restvmx_21);
PPC_FUNC_IMPL(__imp____restvmx_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_22"))) PPC_WEAK_FUNC(__restvmx_22);
PPC_FUNC_IMPL(__imp____restvmx_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_23"))) PPC_WEAK_FUNC(__restvmx_23);
PPC_FUNC_IMPL(__imp____restvmx_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_24"))) PPC_WEAK_FUNC(__restvmx_24);
PPC_FUNC_IMPL(__imp____restvmx_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_25"))) PPC_WEAK_FUNC(__restvmx_25);
PPC_FUNC_IMPL(__imp____restvmx_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_26"))) PPC_WEAK_FUNC(__restvmx_26);
PPC_FUNC_IMPL(__imp____restvmx_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_27"))) PPC_WEAK_FUNC(__restvmx_27);
PPC_FUNC_IMPL(__imp____restvmx_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_28"))) PPC_WEAK_FUNC(__restvmx_28);
PPC_FUNC_IMPL(__imp____restvmx_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_29"))) PPC_WEAK_FUNC(__restvmx_29);
PPC_FUNC_IMPL(__imp____restvmx_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_30"))) PPC_WEAK_FUNC(__restvmx_30);
PPC_FUNC_IMPL(__imp____restvmx_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_31"))) PPC_WEAK_FUNC(__restvmx_31);
PPC_FUNC_IMPL(__imp____restvmx_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v31{};
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_64"))) PPC_WEAK_FUNC(__restvmx_64);
PPC_FUNC_IMPL(__imp____restvmx_64) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v64{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1024
	r11.s64 = -1024;
	// lvx128 v64,r11,r12
	_mm_store_si128((__m128i*)v64.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-1008
	r11.s64 = -1008;
	// lvx128 v65,r11,r12
	_mm_store_si128((__m128i*)v65.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_65"))) PPC_WEAK_FUNC(__restvmx_65);
PPC_FUNC_IMPL(__imp____restvmx_65) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1008
	r11.s64 = -1008;
	// lvx128 v65,r11,r12
	_mm_store_si128((__m128i*)v65.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_66"))) PPC_WEAK_FUNC(__restvmx_66);
PPC_FUNC_IMPL(__imp____restvmx_66) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_67"))) PPC_WEAK_FUNC(__restvmx_67);
PPC_FUNC_IMPL(__imp____restvmx_67) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_68"))) PPC_WEAK_FUNC(__restvmx_68);
PPC_FUNC_IMPL(__imp____restvmx_68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_69"))) PPC_WEAK_FUNC(__restvmx_69);
PPC_FUNC_IMPL(__imp____restvmx_69) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_70"))) PPC_WEAK_FUNC(__restvmx_70);
PPC_FUNC_IMPL(__imp____restvmx_70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_71"))) PPC_WEAK_FUNC(__restvmx_71);
PPC_FUNC_IMPL(__imp____restvmx_71) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_72"))) PPC_WEAK_FUNC(__restvmx_72);
PPC_FUNC_IMPL(__imp____restvmx_72) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_73"))) PPC_WEAK_FUNC(__restvmx_73);
PPC_FUNC_IMPL(__imp____restvmx_73) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_74"))) PPC_WEAK_FUNC(__restvmx_74);
PPC_FUNC_IMPL(__imp____restvmx_74) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_75"))) PPC_WEAK_FUNC(__restvmx_75);
PPC_FUNC_IMPL(__imp____restvmx_75) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_76"))) PPC_WEAK_FUNC(__restvmx_76);
PPC_FUNC_IMPL(__imp____restvmx_76) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_77"))) PPC_WEAK_FUNC(__restvmx_77);
PPC_FUNC_IMPL(__imp____restvmx_77) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_78"))) PPC_WEAK_FUNC(__restvmx_78);
PPC_FUNC_IMPL(__imp____restvmx_78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_79"))) PPC_WEAK_FUNC(__restvmx_79);
PPC_FUNC_IMPL(__imp____restvmx_79) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_80"))) PPC_WEAK_FUNC(__restvmx_80);
PPC_FUNC_IMPL(__imp____restvmx_80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_81"))) PPC_WEAK_FUNC(__restvmx_81);
PPC_FUNC_IMPL(__imp____restvmx_81) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_82"))) PPC_WEAK_FUNC(__restvmx_82);
PPC_FUNC_IMPL(__imp____restvmx_82) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_83"))) PPC_WEAK_FUNC(__restvmx_83);
PPC_FUNC_IMPL(__imp____restvmx_83) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_84"))) PPC_WEAK_FUNC(__restvmx_84);
PPC_FUNC_IMPL(__imp____restvmx_84) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_85"))) PPC_WEAK_FUNC(__restvmx_85);
PPC_FUNC_IMPL(__imp____restvmx_85) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_86"))) PPC_WEAK_FUNC(__restvmx_86);
PPC_FUNC_IMPL(__imp____restvmx_86) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_87"))) PPC_WEAK_FUNC(__restvmx_87);
PPC_FUNC_IMPL(__imp____restvmx_87) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_88"))) PPC_WEAK_FUNC(__restvmx_88);
PPC_FUNC_IMPL(__imp____restvmx_88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_89"))) PPC_WEAK_FUNC(__restvmx_89);
PPC_FUNC_IMPL(__imp____restvmx_89) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_90"))) PPC_WEAK_FUNC(__restvmx_90);
PPC_FUNC_IMPL(__imp____restvmx_90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_91"))) PPC_WEAK_FUNC(__restvmx_91);
PPC_FUNC_IMPL(__imp____restvmx_91) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_92"))) PPC_WEAK_FUNC(__restvmx_92);
PPC_FUNC_IMPL(__imp____restvmx_92) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_93"))) PPC_WEAK_FUNC(__restvmx_93);
PPC_FUNC_IMPL(__imp____restvmx_93) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_94"))) PPC_WEAK_FUNC(__restvmx_94);
PPC_FUNC_IMPL(__imp____restvmx_94) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_95"))) PPC_WEAK_FUNC(__restvmx_95);
PPC_FUNC_IMPL(__imp____restvmx_95) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_96"))) PPC_WEAK_FUNC(__restvmx_96);
PPC_FUNC_IMPL(__imp____restvmx_96) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_97"))) PPC_WEAK_FUNC(__restvmx_97);
PPC_FUNC_IMPL(__imp____restvmx_97) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_98"))) PPC_WEAK_FUNC(__restvmx_98);
PPC_FUNC_IMPL(__imp____restvmx_98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_99"))) PPC_WEAK_FUNC(__restvmx_99);
PPC_FUNC_IMPL(__imp____restvmx_99) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_100"))) PPC_WEAK_FUNC(__restvmx_100);
PPC_FUNC_IMPL(__imp____restvmx_100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_101"))) PPC_WEAK_FUNC(__restvmx_101);
PPC_FUNC_IMPL(__imp____restvmx_101) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_102"))) PPC_WEAK_FUNC(__restvmx_102);
PPC_FUNC_IMPL(__imp____restvmx_102) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_103"))) PPC_WEAK_FUNC(__restvmx_103);
PPC_FUNC_IMPL(__imp____restvmx_103) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_104"))) PPC_WEAK_FUNC(__restvmx_104);
PPC_FUNC_IMPL(__imp____restvmx_104) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_105"))) PPC_WEAK_FUNC(__restvmx_105);
PPC_FUNC_IMPL(__imp____restvmx_105) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_106"))) PPC_WEAK_FUNC(__restvmx_106);
PPC_FUNC_IMPL(__imp____restvmx_106) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_107"))) PPC_WEAK_FUNC(__restvmx_107);
PPC_FUNC_IMPL(__imp____restvmx_107) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_108"))) PPC_WEAK_FUNC(__restvmx_108);
PPC_FUNC_IMPL(__imp____restvmx_108) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_109"))) PPC_WEAK_FUNC(__restvmx_109);
PPC_FUNC_IMPL(__imp____restvmx_109) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_110"))) PPC_WEAK_FUNC(__restvmx_110);
PPC_FUNC_IMPL(__imp____restvmx_110) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_111"))) PPC_WEAK_FUNC(__restvmx_111);
PPC_FUNC_IMPL(__imp____restvmx_111) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_112"))) PPC_WEAK_FUNC(__restvmx_112);
PPC_FUNC_IMPL(__imp____restvmx_112) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_113"))) PPC_WEAK_FUNC(__restvmx_113);
PPC_FUNC_IMPL(__imp____restvmx_113) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_114"))) PPC_WEAK_FUNC(__restvmx_114);
PPC_FUNC_IMPL(__imp____restvmx_114) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_115"))) PPC_WEAK_FUNC(__restvmx_115);
PPC_FUNC_IMPL(__imp____restvmx_115) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_116"))) PPC_WEAK_FUNC(__restvmx_116);
PPC_FUNC_IMPL(__imp____restvmx_116) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_117"))) PPC_WEAK_FUNC(__restvmx_117);
PPC_FUNC_IMPL(__imp____restvmx_117) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_118"))) PPC_WEAK_FUNC(__restvmx_118);
PPC_FUNC_IMPL(__imp____restvmx_118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_119"))) PPC_WEAK_FUNC(__restvmx_119);
PPC_FUNC_IMPL(__imp____restvmx_119) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_120"))) PPC_WEAK_FUNC(__restvmx_120);
PPC_FUNC_IMPL(__imp____restvmx_120) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_121"))) PPC_WEAK_FUNC(__restvmx_121);
PPC_FUNC_IMPL(__imp____restvmx_121) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_122"))) PPC_WEAK_FUNC(__restvmx_122);
PPC_FUNC_IMPL(__imp____restvmx_122) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_123"))) PPC_WEAK_FUNC(__restvmx_123);
PPC_FUNC_IMPL(__imp____restvmx_123) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_124"))) PPC_WEAK_FUNC(__restvmx_124);
PPC_FUNC_IMPL(__imp____restvmx_124) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_125"))) PPC_WEAK_FUNC(__restvmx_125);
PPC_FUNC_IMPL(__imp____restvmx_125) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_126"))) PPC_WEAK_FUNC(__restvmx_126);
PPC_FUNC_IMPL(__imp____restvmx_126) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_127"))) PPC_WEAK_FUNC(__restvmx_127);
PPC_FUNC_IMPL(__imp____restvmx_127) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v127{};
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A7F20"))) PPC_WEAK_FUNC(sub_826A7F20);
PPC_FUNC_IMPL(__imp__sub_826A7F20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// li r4,0
	ctx.r4.s64 = 0;
	// stfd f31,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, f31.u64);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lis r11,-16377
	r11.s64 = -1073283072;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// ori r28,r11,65279
	r28.u64 = r11.u64 | 65279;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,9296(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 9296);
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// lhz r30,160(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// rlwinm r11,r30,0,17,27
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x826a7ffc
	if (!cr6.eq) goto loc_826A7FFC;
	// lis r11,-32139
	r11.s64 = -2106261504;
	// lfd f0,-22584(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -22584);
	// stfd f0,0(r31)
	PPC_STORE_U64(r31.u32 + 0, f0.u64);
	// bl 0x823abd50
	sub_823ABD50(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble 0x826a7fd4
	if (!cr0.gt) goto loc_826A7FD4;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x826a7fb8
	if (!cr6.gt) goto loc_826A7FB8;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x826a7fd4
	if (!cr6.eq) goto loc_826A7FD4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stfd f31,0(r31)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r31.u32 + 0, f31.u64);
	// li r3,28
	ctx.r3.s64 = 28;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x823b2448
	sub_823B2448(ctx, base);
	// b 0x826a8040
	goto loc_826A8040;
loc_826A7FB8:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stfd f31,0(r31)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r31.u32 + 0, f31.u64);
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// lfd f1,-31368(r11)
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// bl 0x823ae030
	sub_823AE030(ctx, base);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// b 0x826a8030
	goto loc_826A8030;
loc_826A7FD4:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// li r4,28
	ctx.r4.s64 = 28;
	// li r3,8
	ctx.r3.s64 = 8;
	// lfd f0,-31360(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f2,f31,f0
	ctx.f2.f64 = f31.f64 + f0.f64;
	// stfd f2,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.f2.u64);
	// bl 0x823b2550
	sub_823B2550(ctx, base);
	// b 0x826a8040
	goto loc_826A8040;
loc_826A7FFC:
	// bl 0x823b0010
	sub_823B0010(ctx, base);
	// fsub f31,f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = f31.f64 - ctx.f1.f64;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stfd f31,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f31.u64);
	// stfd f1,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.f1.u64);
	// lfd f0,-31368(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bne cr6,0x826a8030
	if (!cr6.eq) goto loc_826A8030;
	// lhz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// rlwinm r11,r30,0,16,16
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8000;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// lfd f31,80(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_826A8030:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x823b2710
	sub_823B2710(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
loc_826A8040:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826A804C"))) PPC_WEAK_FUNC(sub_826A804C);
PPC_FUNC_IMPL(__imp__sub_826A804C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8050"))) PPC_WEAK_FUNC(sub_826A8050);
PPC_FUNC_IMPL(__imp__sub_826A8050) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x826a8060
	if (!cr6.eq) goto loc_826A8060;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826A8060:
	// addic. r11,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	r11.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a8090
	if (cr0.eq) goto loc_826A8090;
loc_826A8068:
	// lhz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x826a8090
	if (cr0.eq) goto loc_826A8090;
	// lhz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x826a8090
	if (!cr6.eq) goto loc_826A8090;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// bne 0x826a8068
	if (!cr0.eq) goto loc_826A8068;
loc_826A8090:
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// lhz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A80A0"))) PPC_WEAK_FUNC(sub_826A80A0);
PPC_FUNC_IMPL(__imp__sub_826A80A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
loc_826A80AC:
	// lhz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// beq 0x826a80cc
	if (cr0.eq) goto loc_826A80CC;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne 0x826a80ac
	if (!cr0.eq) goto loc_826A80AC;
loc_826A80CC:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// addic. r10,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r10.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beqlr 
	if (cr0.eq) return;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beqlr 
	if (cr0.eq) return;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826A80EC:
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x826a80ec
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826A80EC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A80FC"))) PPC_WEAK_FUNC(sub_826A80FC);
PPC_FUNC_IMPL(__imp__sub_826A80FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8100"))) PPC_WEAK_FUNC(sub_826A8100);
PPC_FUNC_IMPL(__imp__sub_826A8100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// blt cr6,0x826a812c
	if (cr6.lt) goto loc_826A812C;
	// bl 0x826a84b8
	sub_826A84B8(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x826a8134
	if (cr6.lt) goto loc_826A8134;
loc_826A812C:
	// bl 0x826a84b8
	sub_826A84B8(ctx, base);
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_826A8134:
	// bl 0x826a84c8
	sub_826A84C8(ctx, base);
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8154"))) PPC_WEAK_FUNC(sub_826A8154);
PPC_FUNC_IMPL(__imp__sub_826A8154) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8158"))) PPC_WEAK_FUNC(sub_826A8158);
PPC_FUNC_IMPL(__imp__sub_826A8158) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826a81b8
	if (cr6.eq) goto loc_826A81B8;
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826a81b8
	if (cr6.eq) goto loc_826A81B8;
	// lis r11,-32253
	r11.s64 = -2113732608;
	// addi r31,r11,-24616
	r31.s64 = r11.s64 + -24616;
	// bl 0x823a0e80
	sub_823A0E80(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823a0c48
	sub_823A0C48(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r31,r11,-31844
	r31.s64 = r11.s64 + -31844;
	// bl 0x823a0e80
	sub_823A0E80(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// bl 0x823a0c48
	sub_823A0C48(ctx, base);
loc_826A81B8:
	// bl 0x8239ff90
	sub_8239FF90(ctx, base);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x826a8100
	sub_826A8100(ctx, base);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r31,r11,26416
	r31.s64 = r11.s64 + 26416;
	// bl 0x823a0e80
	sub_823A0E80(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823a0c48
	sub_823A0C48(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A81FC"))) PPC_WEAK_FUNC(sub_826A81FC);
PPC_FUNC_IMPL(__imp__sub_826A81FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8200"))) PPC_WEAK_FUNC(sub_826A8200);
PPC_FUNC_IMPL(__imp__sub_826A8200) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// addi r11,r5,-1
	r11.s64 = ctx.r5.s64 + -1;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// add r24,r11,r4
	r24.u64 = r11.u64 + ctx.r4.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x826a826c
	if (!cr6.eq) goto loc_826A826C;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x826a826c
	if (cr6.eq) goto loc_826A826C;
loc_826A8238:
	// bl 0x8239ff90
	sub_8239FF90(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823a3f38
	sub_823A3F38(ctx, base);
loc_826A8260:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8264:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_826A826C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826a8238
	if (cr6.eq) goto loc_826A8238;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x826a8238
	if (cr6.eq) goto loc_826A8238;
	// cmplw cr6,r4,r24
	cr6.compare<uint32_t>(ctx.r4.u32, r24.u32, xer);
	// bgt cr6,0x826a8260
	if (cr6.gt) goto loc_826A8260;
loc_826A8284:
	// rlwinm. r28,r5,31,1,31
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// beq 0x826a82f4
	if (cr0.eq) goto loc_826A82F4;
	// clrlwi. r27,r5,31
	r27.u64 = ctx.r5.u32 & 0x1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// bne 0x826a829c
	if (!cr0.eq) goto loc_826A829C;
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
loc_826A829C:
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mtctr r26
	ctr.u64 = r26.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x826a82ec
	if (cr0.eq) goto loc_826A82EC;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x826a82d8
	if (!cr6.lt) goto loc_826A82D8;
	// subf r24,r30,r31
	r24.s64 = r31.s64 - r30.s64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x826a82dc
	if (!cr6.eq) goto loc_826A82DC;
	// addi r5,r28,-1
	ctx.r5.s64 = r28.s64 + -1;
	// b 0x826a82e0
	goto loc_826A82E0;
loc_826A82D8:
	// add r29,r31,r30
	r29.u64 = r31.u64 + r30.u64;
loc_826A82DC:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
loc_826A82E0:
	// cmplw cr6,r29,r24
	cr6.compare<uint32_t>(r29.u32, r24.u32, xer);
	// ble cr6,0x826a8284
	if (!cr6.gt) goto loc_826A8284;
	// b 0x826a8260
	goto loc_826A8260;
loc_826A82EC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x826a8264
	goto loc_826A8264;
loc_826A82F4:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x826a8260
	if (cr6.eq) goto loc_826A8260;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r26
	ctr.u64 = r26.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// bne 0x826a8264
	if (!cr0.eq) goto loc_826A8264;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// b 0x826a8264
	goto loc_826A8264;
}

__attribute__((alias("__imp__sub_826A8320"))) PPC_WEAK_FUNC(sub_826A8320);
PPC_FUNC_IMPL(__imp__sub_826A8320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r16{};
	PPCRegister r19{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r19,19276(r14)
	r19.u64 = PPC_LOAD_U32(r14.u32 + 19276);
	// lwz r16,-13552(r14)
	r16.u64 = PPC_LOAD_U32(r14.u32 + -13552);
	// mflr r12
	// bl 0x8239bcf8
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,1
	ctx.r3.s64 = 1;
	// li r28,0
	r28.s64 = 0;
	// stw r28,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r28.u32);
	// bl 0x823a4260
	sub_823A4260(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r30,r11,15140
	r30.s64 = r11.s64 + 15140;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r10,r11,15144
	ctx.r10.s64 = r11.s64 + 15144;
loc_826A835C:
	// stw r28,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r28.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x826a83ec
	if (!cr6.lt) goto loc_826A83EC;
	// rlwinm r29,r28,2,0,29
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r9,r29,r11
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a83e0
	if (cr6.eq) goto loc_826A83E0;
	// rotlwi r4,r9,0
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// andi. r11,r11,131
	r11.u64 = r11.u64 & 131;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a83e0
	if (cr0.eq) goto loc_826A83E0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823a1030
	sub_823A1030(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r3,r29,r11
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// andi. r11,r11,131
	r11.u64 = r11.u64 & 131;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a83d4
	if (cr0.eq) goto loc_826A83D4;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826a83d4
	if (cr6.eq) goto loc_826A83D4;
	// bl 0x8239ed98
	sub_8239ED98(ctx, base);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
loc_826A83D4:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x826a8454
	sub_826A8454(ctx, base);
loc_826A83E0:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x826a835c
	goto loc_826A835C;
loc_826A83EC:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x826a8404
	sub_826A8404(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826A8328"))) PPC_WEAK_FUNC(sub_826A8328);
PPC_FUNC_IMPL(__imp__sub_826A8328) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,1
	ctx.r3.s64 = 1;
	// li r28,0
	r28.s64 = 0;
	// stw r28,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r28.u32);
	// bl 0x823a4260
	sub_823A4260(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r30,r11,15140
	r30.s64 = r11.s64 + 15140;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r10,r11,15144
	ctx.r10.s64 = r11.s64 + 15144;
loc_826A835C:
	// stw r28,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r28.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x826a83ec
	if (!cr6.lt) goto loc_826A83EC;
	// rlwinm r29,r28,2,0,29
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r9,r29,r11
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a83e0
	if (cr6.eq) goto loc_826A83E0;
	// rotlwi r4,r9,0
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// andi. r11,r11,131
	r11.u64 = r11.u64 & 131;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a83e0
	if (cr0.eq) goto loc_826A83E0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823a1030
	sub_823A1030(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r3,r29,r11
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// andi. r11,r11,131
	r11.u64 = r11.u64 & 131;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x826a83d4
	if (cr0.eq) goto loc_826A83D4;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826a83d4
	if (cr6.eq) goto loc_826A83D4;
	// bl 0x8239ed98
	sub_8239ED98(ctx, base);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
loc_826A83D4:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x826a8454
	sub_826A8454(ctx, base);
loc_826A83E0:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x826a835c
	goto loc_826A835C;
loc_826A83EC:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x826a8404
	sub_826A8404(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826A8404"))) PPC_WEAK_FUNC(sub_826A8404);
PPC_FUNC_IMPL(__imp__sub_826A8404) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x823a4100
	sub_823A4100(ctx, base);
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8428"))) PPC_WEAK_FUNC(sub_826A8428);
PPC_FUNC_IMPL(__imp__sub_826A8428) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r28,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r28.u64);
	// mflr r12
	// stw r12,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r30,r11,15140
	r30.s64 = r11.s64 + 15140;
	// lwz r28,80(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// b 0x826a8470
	goto loc_826A8470;
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r28,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r28.u64);
	// mflr r12
	// stw r12,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
loc_826A8470:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r4,r11,r10
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// bl 0x823a1080
	sub_823A1080(ctx, base);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r30,r11,15140
	r30.s64 = r11.s64 + 15140;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r10,r11,15144
	ctx.r10.s64 = r11.s64 + 15144;
	// lwz r28,80(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r28,-24(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// lwz r12,-32(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8454"))) PPC_WEAK_FUNC(sub_826A8454);
PPC_FUNC_IMPL(__imp__sub_826A8454) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r28,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r28.u64);
	// mflr r12
	// stw r12,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwzx r4,r11,r10
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// bl 0x823a1080
	sub_823A1080(ctx, base);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r30,r11,15140
	r30.s64 = r11.s64 + 15140;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r10,r11,15144
	ctx.r10.s64 = r11.s64 + 15144;
	// lwz r28,80(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r28,-24(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// lwz r12,-32(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A84B4"))) PPC_WEAK_FUNC(sub_826A84B4);
PPC_FUNC_IMPL(__imp__sub_826A84B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A84B8"))) PPC_WEAK_FUNC(sub_826A84B8);
PPC_FUNC_IMPL(__imp__sub_826A84B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r3,r11,9488
	ctx.r3.s64 = r11.s64 + 9488;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A84C4"))) PPC_WEAK_FUNC(sub_826A84C4);
PPC_FUNC_IMPL(__imp__sub_826A84C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A84C8"))) PPC_WEAK_FUNC(sub_826A84C8);
PPC_FUNC_IMPL(__imp__sub_826A84C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r3,r11,9312
	ctx.r3.s64 = r11.s64 + 9312;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A84D4"))) PPC_WEAK_FUNC(sub_826A84D4);
PPC_FUNC_IMPL(__imp__sub_826A84D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A84D8"))) PPC_WEAK_FUNC(sub_826A84D8);
PPC_FUNC_IMPL(__imp__sub_826A84D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// clrlwi r10,r4,24
	ctx.r10.u64 = ctx.r4.u32 & 0xFF;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// rlwinm r8,r5,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826a8520
	if (!cr6.eq) goto loc_826A8520;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826a85dc
	if (cr6.eq) goto loc_826A85DC;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_826A8500:
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826a8500
	if (!cr6.eq) goto loc_826A8500;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_826A8520:
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// blt cr6,0x826a8530
	if (cr6.lt) goto loc_826A8530;
	// li r9,1
	ctx.r9.s64 = 1;
loc_826A8530:
	// srawi r7,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 4;
	// vspltisb v13,4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x4)));
	// lvsl v12,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvsl v0,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslb v0,v0,v13
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v13.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v13.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v13.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v13.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v13.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v13.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v13.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v13.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v13.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v13.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v13.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v13.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v13.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v13.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v13.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v13.u8[15] & 0x7);
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vspltb v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xF))));
	// beq cr6,0x826a856c
	if (cr6.eq) goto loc_826A856C;
loc_826A8558:
	// rlwinm r10,r11,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// dcbzl r10,r3
	memset(base + ((ctx.r10.u32 + ctx.r3.u32) & ~127), 0, 128);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x826a8558
	if (cr6.lt) goto loc_826A8558;
loc_826A856C:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826a85dc
	if (cr6.eq) goto loc_826A85DC;
	// addi r11,r3,32
	r11.s64 = ctx.r3.s64 + 32;
loc_826A857C:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bge cr6,0x826a8594
	if (!cr6.lt) goto loc_826A8594;
	// addi r9,r11,-32
	ctx.r9.s64 = r11.s64 + -32;
	// li r7,128
	ctx.r7.s64 = 128;
	// dcbzl r7,r9
	memset(base + ((ctx.r7.u32 + ctx.r9.u32) & ~127), 0, 128);
loc_826A8594:
	// addi r9,r11,-32
	ctx.r9.s64 = r11.s64 + -32;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r11,-16
	ctx.r7.s64 = r11.s64 + -16;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r5,r11,32
	ctx.r5.s64 = r11.s64 + 32;
	// addi r4,r11,48
	ctx.r4.s64 = r11.s64 + 48;
	// addi r31,r11,64
	r31.s64 = r11.s64 + 64;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r11,80
	r30.s64 = r11.s64 + 80;
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x826a857c
	if (cr6.lt) goto loc_826A857C;
loc_826A85DC:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A85E8"))) PPC_WEAK_FUNC(sub_826A85E8);
PPC_FUNC_IMPL(__imp__sub_826A85E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r30,r27
	r30.u64 = r27.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// bge cr6,0x826a86b0
	if (!cr6.lt) goto loc_826A86B0;
	// cmplwi cr6,r29,8
	cr6.compare<uint32_t>(r29.u32, 8, xer);
	// blt cr6,0x826a8634
	if (cr6.lt) goto loc_826A8634;
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826a8634
	if (!cr6.eq) goto loc_826A8634;
	// ld r11,0(r4)
	r11.u64 = PPC_LOAD_U64(ctx.r4.u32 + 0);
	// addi r30,r27,8
	r30.s64 = r27.s64 + 8;
	// addi r31,r4,8
	r31.s64 = ctx.r4.s64 + 8;
	// addi r29,r29,-8
	r29.s64 = r29.s64 + -8;
	// std r11,0(r27)
	PPC_STORE_U64(r27.u32 + 0, r11.u64);
loc_826A8634:
	// clrlwi r11,r31,30
	r11.u64 = r31.u32 & 0x3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826a8678
	if (!cr6.eq) goto loc_826A8678;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// blt cr6,0x826a8678
	if (cr6.lt) goto loc_826A8678;
	// rlwinm r28,r29,30,2,31
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
loc_826A8668:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,-4
	r29.s64 = r29.s64 + -4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x826a8668
	if (!cr6.eq) goto loc_826A8668;
loc_826A8678:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826a8910
	if (cr6.eq) goto loc_826A8910;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r11,9492(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 9492);
loc_826A8688:
	// lbz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stb r10,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r10.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// bne cr6,0x826a8688
	if (!cr6.eq) goto loc_826A8688;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd44
	return;
loc_826A86B0:
	// neg r11,r27
	r11.s64 = -r27.s64;
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// clrlwi r9,r11,28
	ctx.r9.u64 = r11.u32 & 0xF;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// blt cr6,0x826a86d4
	if (cr6.lt) goto loc_826A86D4;
	// li r10,1024
	ctx.r10.s64 = 1024;
loc_826A86D4:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826a86f0
	if (cr6.eq) goto loc_826A86F0;
loc_826A86E0:
	// dcbt r11,r4
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826a86e0
	if (cr6.lt) goto loc_826A86E0;
loc_826A86F0:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a871c
	if (cr6.eq) goto loc_826A871C;
	// addi r11,r4,15
	r11.s64 = ctx.r4.s64 + 15;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v7,r0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// add r30,r9,r27
	r30.u64 = ctx.r9.u64 + r27.u64;
	// add r31,r9,r4
	r31.u64 = ctx.r9.u64 + ctx.r4.u64;
	// subf r29,r9,r29
	r29.s64 = r29.s64 - ctx.r9.s64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v0,v13,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvlx v0,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
loc_826A871C:
	// rlwinm r11,r29,28,4,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x826a8730
	if (!cr6.lt) goto loc_826A8730;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
loc_826A8730:
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// rlwinm r28,r11,0,0,24
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r28,128
	cr6.compare<uint32_t>(r28.u32, 128, xer);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// blt cr6,0x826a8750
	if (cr6.lt) goto loc_826A8750;
	// li r10,128
	ctx.r10.s64 = 128;
loc_826A8750:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x826a8784
	if (cr6.eq) goto loc_826A8784;
	// addi r9,r30,127
	ctx.r9.s64 = r30.s64 + 127;
loc_826A8764:
	// dcbzl r11,r9
	memset(base + ((r11.u32 + ctx.r9.u32) & ~127), 0, 128);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// blt cr6,0x826a8764
	if (cr6.lt) goto loc_826A8764;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826A8784:
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lvsl v0,r0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x826a87c4
	if (cr6.eq) goto loc_826A87C4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_826A879C:
	// addi r10,r31,15
	ctx.r10.s64 = r31.s64 + 15;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bne cr6,0x826a879c
	if (!cr6.eq) goto loc_826A879C;
loc_826A87C4:
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// clrlwi r10,r31,28
	ctx.r10.u64 = r31.u32 & 0xF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826a8890
	if (!cr6.eq) goto loc_826A8890;
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// blt cr6,0x826a88b4
	if (cr6.lt) goto loc_826A88B4;
loc_826A87E0:
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// ble cr6,0x826a87f0
	if (!cr6.gt) goto loc_826A87F0;
	// li r11,1024
	r11.s64 = 1024;
	// dcbt r11,r31
loc_826A87F0:
	// cmplwi cr6,r29,256
	cr6.compare<uint32_t>(r29.u32, 256, xer);
	// ble cr6,0x826a8800
	if (!cr6.gt) goto loc_826A8800;
	// li r11,128
	r11.s64 = 128;
	// dcbzl r11,r30
	memset(base + ((r11.u32 + r30.u32) & ~127), 0, 128);
loc_826A8800:
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r31,32
	ctx.r10.s64 = r31.s64 + 32;
	// stvx v13,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r31,48
	ctx.r9.s64 = r31.s64 + 48;
	// addi r8,r31,64
	ctx.r8.s64 = r31.s64 + 64;
	// addi r7,r31,80
	ctx.r7.s64 = r31.s64 + 80;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r31,96
	ctx.r6.s64 = r31.s64 + 96;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r31,112
	r11.s64 = r31.s64 + 112;
	// addi r10,r30,16
	ctx.r10.s64 = r30.s64 + 16;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r30,32
	ctx.r9.s64 = r30.s64 + 32;
	// lvx128 v9,r0,r7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r30,48
	ctx.r8.s64 = r30.s64 + 48;
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r30,64
	ctx.r7.s64 = r30.s64 + 64;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r30,80
	ctx.r6.s64 = r30.s64 + 80;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r30,96
	r11.s64 = r30.s64 + 96;
	// addi r10,r30,112
	ctx.r10.s64 = r30.s64 + 112;
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r29,-128
	r29.s64 = r29.s64 + -128;
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// stvx v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// stvx v7,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge cr6,0x826a87e0
	if (!cr6.lt) goto loc_826A87E0;
	// b 0x826a88b4
	goto loc_826A88B4;
loc_826A8890:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x826a9000
	sub_826A9000(ctx, base);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r31,r28,r31
	r31.u64 = r28.u64 + r31.u64;
	// subf r29,r28,r29
	r29.s64 = r29.s64 - r28.s64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_826A88B4:
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// blt cr6,0x826a88ec
	if (cr6.lt) goto loc_826A88EC;
	// rlwinm r11,r29,28,4,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
loc_826A88C0:
	// addi r10,r31,15
	ctx.r10.s64 = r31.s64 + 15;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v13,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bne cr6,0x826a88c0
	if (!cr6.eq) goto loc_826A88C0;
loc_826A88EC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826a8910
	if (cr6.eq) goto loc_826A8910;
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvsl v0,r31,r29
	temp.u32 = r31.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v0,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvrx v0,r30,r29
	ea = r30.u32 + r29.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
loc_826A8910:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_826A891C"))) PPC_WEAK_FUNC(sub_826A891C);
PPC_FUNC_IMPL(__imp__sub_826A891C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8920"))) PPC_WEAK_FUNC(sub_826A8920);
PPC_FUNC_IMPL(__imp__sub_826A8920) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// stw r5,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r5.u32);
	// lwz r11,2060(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2060);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826a8978
	if (cr6.eq) goto loc_826A8978;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a8964
	if (cr0.eq) goto loc_826A8964;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x826a8990
	if (!cr0.eq) goto loc_826A8990;
loc_826A8964:
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8968:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_826A8978:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1824(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1824);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a8964
	if (cr0.eq) goto loc_826A8964;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_826A8990:
	// li r3,37
	ctx.r3.s64 = 37;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x826a8968
	goto loc_826A8968;
}

__attribute__((alias("__imp__sub_826A89A4"))) PPC_WEAK_FUNC(sub_826A89A4);
PPC_FUNC_IMPL(__imp__sub_826A89A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A89A8"))) PPC_WEAK_FUNC(sub_826A89A8);
PPC_FUNC_IMPL(__imp__sub_826A89A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r11,2060(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2060);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826a89e0
	if (cr6.eq) goto loc_826A89E0;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a89d8
	if (cr0.eq) goto loc_826A89D8;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x826a89f8
	if (!cr0.eq) goto loc_826A89F8;
loc_826A89D8:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826A89E0:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1824(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1824);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a89d8
	if (cr0.eq) goto loc_826A89D8;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_826A89F8:
	// li r3,38
	ctx.r3.s64 = 38;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_826A8A04"))) PPC_WEAK_FUNC(sub_826A8A04);
PPC_FUNC_IMPL(__imp__sub_826A8A04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8A08"))) PPC_WEAK_FUNC(sub_826A8A08);
PPC_FUNC_IMPL(__imp__sub_826A8A08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// stfs f1,124(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// lwz r11,1824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1824);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x826a8a48
	if (cr0.eq) goto loc_826A8A48;
	// oris r4,r3,544
	ctx.r4.u64 = ctx.r3.u64 | 35651584;
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// li r3,39
	ctx.r3.s64 = 39;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,1824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1824);
loc_826A8A48:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x826a8a68
	if (cr0.eq) goto loc_826A8A68;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r3,39
	ctx.r3.s64 = 39;
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826A8A68:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8A7C"))) PPC_WEAK_FUNC(sub_826A8A7C);
PPC_FUNC_IMPL(__imp__sub_826A8A7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8A80"))) PPC_WEAK_FUNC(sub_826A8A80);
PPC_FUNC_IMPL(__imp__sub_826A8A80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826a8abc
	if (cr6.eq) goto loc_826A8ABC;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x823ba830
	sub_823BA830(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x826a8ac0
	goto loc_826A8AC0;
loc_826A8ABC:
	// li r4,0
	ctx.r4.s64 = 0;
loc_826A8AC0:
	// cntlzw r11,r31
	r11.u64 = r31.u32 == 0 ? 32 : __builtin_clz(r31.u32);
	// clrlwi r6,r30,24
	ctx.r6.u64 = r30.u32 & 0xFF;
	// rlwinm r5,r11,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826e4c5c
	__imp__NtCreateEvent(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8afc
	if (cr0.lt) goto loc_826A8AFC;
	// lis r11,16384
	r11.s64 = 1073741824;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// li r3,183
	ctx.r3.s64 = 183;
	// beq cr6,0x826a8af0
	if (cr6.eq) goto loc_826A8AF0;
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8AF0:
	// bl 0x823ba6c0
	sub_823BA6C0(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x826a8b04
	goto loc_826A8B04;
loc_826A8AFC:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8B04:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8B1C"))) PPC_WEAK_FUNC(sub_826A8B1C);
PPC_FUNC_IMPL(__imp__sub_826A8B1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8B20"))) PPC_WEAK_FUNC(sub_826A8B20);
PPC_FUNC_IMPL(__imp__sub_826A8B20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826e4ccc
	__imp__RtlInitAnsiString(ctx, base);
	// li r11,-3
	r11.s64 = -3;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// li r11,64
	r11.s64 = 64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x826e4cec
	__imp__NtQueryFullAttributesFile(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8b6c
	if (cr0.lt) goto loc_826A8B6C;
	// lwz r3,160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// b 0x826a8b74
	goto loc_826A8B74;
loc_826A8B6C:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
loc_826A8B74:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8B84"))) PPC_WEAK_FUNC(sub_826A8B84);
PPC_FUNC_IMPL(__imp__sub_826A8B84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8B88"))) PPC_WEAK_FUNC(sub_826A8B88);
PPC_FUNC_IMPL(__imp__sub_826A8B88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r7,-1
	ctx.r7.s64 = -1;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x826a90c0
	sub_826A90C0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x826a8bf8
	if (!cr0.eq) goto loc_826A8BF8;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826a8bec
	if (cr6.eq) goto loc_826A8BEC;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x826a8be4
	if (cr6.eq) goto loc_826A8BE4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823a12f0
	sub_823A12F0(ctx, base);
	// b 0x826a8bfc
	goto loc_826A8BFC;
loc_826A8BE4:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a8bfc
	goto loc_826A8BFC;
loc_826A8BEC:
	// subfic r11,r31,0
	xer.ca = r31.u32 <= 0;
	r11.s64 = 0 - r31.s64;
	// subfe r3,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	ctx.r3.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// b 0x826a8bfc
	goto loc_826A8BFC;
loc_826A8BF8:
	// addi r3,r3,-2
	ctx.r3.s64 = ctx.r3.s64 + -2;
loc_826A8BFC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8C14"))) PPC_WEAK_FUNC(sub_826A8C14);
PPC_FUNC_IMPL(__imp__sub_826A8C14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8C18"))) PPC_WEAK_FUNC(sub_826A8C18);
PPC_FUNC_IMPL(__imp__sub_826A8C18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// li r7,34
	ctx.r7.s64 = 34;
	// lwz r11,15160(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 15160);
	// li r6,56
	ctx.r6.s64 = 56;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// beq cr6,0x826a8c60
	if (cr6.eq) goto loc_826A8C60;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x826a8c64
	goto loc_826A8C64;
loc_826A8C60:
	// bl 0x826e4d6c
	__imp__NtQueryInformationFile(ctx, base);
loc_826A8C64:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x826a8ca4
	if (cr6.lt) goto loc_826A8CA4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x826a8c7c
	if (cr6.eq) goto loc_826A8C7C;
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826A8C7C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826a8c8c
	if (cr6.eq) goto loc_826A8C8C;
	// ld r11,104(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_826A8C8C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826a8c9c
	if (cr6.eq) goto loc_826A8C9C;
	// ld r11,112(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
loc_826A8C9C:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a8cac
	goto loc_826A8CAC;
loc_826A8CA4:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8CAC:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_826A8CB4"))) PPC_WEAK_FUNC(sub_826A8CB4);
PPC_FUNC_IMPL(__imp__sub_826A8CB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8CB8"))) PPC_WEAK_FUNC(sub_826A8CB8);
PPC_FUNC_IMPL(__imp__sub_826A8CB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2228(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2228);
	// lwz r3,16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8CC8"))) PPC_WEAK_FUNC(sub_826A8CC8);
PPC_FUNC_IMPL(__imp__sub_826A8CC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mftb r11
	r11.u64 = __rdtsc();
	// rotlwi. r9,r11,0
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 0);
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x826a8cdc
	if (!cr0.eq) goto loc_826A8CDC;
	// mftb r11
	r11.u64 = __rdtsc();
loc_826A8CDC:
	// li r3,1
	ctx.r3.s64 = 1;
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8CE8"))) PPC_WEAK_FUNC(sub_826A8CE8);
PPC_FUNC_IMPL(__imp__sub_826A8CE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r3,-16384
	ctx.r3.s64 = -1073741824;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// bl 0x823ba828
	sub_823BA828(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8D14"))) PPC_WEAK_FUNC(sub_826A8D14);
PPC_FUNC_IMPL(__imp__sub_826A8D14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8D18"))) PPC_WEAK_FUNC(sub_826A8D18);
PPC_FUNC_IMPL(__imp__sub_826A8D18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826e538c
	__imp__NtSetEvent(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8d3c
	if (cr0.lt) goto loc_826A8D3C;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a8d44
	goto loc_826A8D44;
loc_826A8D3C:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8D44:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8D54"))) PPC_WEAK_FUNC(sub_826A8D54);
PPC_FUNC_IMPL(__imp__sub_826A8D54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8D58"))) PPC_WEAK_FUNC(sub_826A8D58);
PPC_FUNC_IMPL(__imp__sub_826A8D58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x826e539c
	__imp__NtClearEvent(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8d78
	if (cr0.lt) goto loc_826A8D78;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a8d80
	goto loc_826A8D80;
loc_826A8D78:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8D80:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8D90"))) PPC_WEAK_FUNC(sub_826A8D90);
PPC_FUNC_IMPL(__imp__sub_826A8D90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// addi r4,r1,124
	ctx.r4.s64 = ctx.r1.s64 + 124;
	// bl 0x826e4dac
	__imp__NtAllocateVirtualMemory(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8dc4
	if (cr0.lt) goto loc_826A8DC4;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// b 0x826a8dcc
	goto loc_826A8DCC;
loc_826A8DC4:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8DCC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A8DDC"))) PPC_WEAK_FUNC(sub_826A8DDC);
PPC_FUNC_IMPL(__imp__sub_826A8DDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8DE0"))) PPC_WEAK_FUNC(sub_826A8DE0);
PPC_FUNC_IMPL(__imp__sub_826A8DE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// rlwinm. r11,r5,0,16,16
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x8000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// beq 0x826a8e24
	if (cr0.eq) goto loc_826A8E24;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826a8e24
	if (cr6.eq) goto loc_826A8E24;
	// lis r3,-16384
	ctx.r3.s64 = -1073741824;
	// ori r3,r3,13
	ctx.r3.u64 = ctx.r3.u64 | 13;
loc_826A8E0C:
	// bl 0x823ba6d8
	sub_823BA6D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A8E14:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_826A8E24:
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r4,r1,124
	ctx.r4.s64 = ctx.r1.s64 + 124;
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// bl 0x826e4dec
	__imp__NtFreeVirtualMemory(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x826a8e0c
	if (cr0.lt) goto loc_826A8E0C;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a8e14
	goto loc_826A8E14;
}

__attribute__((alias("__imp__sub_826A8E44"))) PPC_WEAK_FUNC(sub_826A8E44);
PPC_FUNC_IMPL(__imp__sub_826A8E44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8E48"))) PPC_WEAK_FUNC(sub_826A8E48);
PPC_FUNC_IMPL(__imp__sub_826A8E48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,16
	cr6.compare<uint32_t>(ctx.r5.u32, 16, xer);
	// bge cr6,0x826a8e84
	if (!cr6.lt) goto loc_826A8E84;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x826a8ff0
	if (cr6.eq) goto loc_826A8FF0;
	// extsb r10,r4
	ctx.r10.s64 = ctx.r4.s8;
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq 0x826a8ff0
	if (cr0.eq) goto loc_826A8FF0;
	// mtctr r5
	ctr.u64 = ctx.r5.u64;
loc_826A8E74:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826a8e74
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826A8E74;
	// b 0x826a8ff0
	goto loc_826A8FF0;
loc_826A8E84:
	// neg r9,r3
	ctx.r9.s64 = -ctx.r3.s64;
	// vspltisb v0,4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x4)));
	// lvsl v13,r0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// clrlwi. r10,r9,28
	ctx.r10.u64 = ctx.r9.u32 & 0xF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// srawi r8,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 4;
	// clrlwi r9,r9,29
	ctx.r9.u64 = ctx.r9.u32 & 0x7;
	// lvsl v12,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// vslb v0,v12,v0
	ctx.v0.u8[0] = ctx.v12.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v12.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v12.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v12.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v12.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v12.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v12.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v12.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v12.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v12.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v12.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v12.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v12.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v12.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v12.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v12.u8[15] << (ctx.v0.u8[15] & 0x7);
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vspltb v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xF))));
	// beq 0x826a8ec0
	if (cr0.eq) goto loc_826A8EC0;
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r11,r10,r3
	r11.u64 = ctx.r10.u64 + ctx.r3.u64;
loc_826A8EC0:
	// rlwinm r10,r5,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 28) & 0xFFFFFFF;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x826a8ed0
	if (!cr6.lt) goto loc_826A8ED0;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_826A8ED0:
	// lis r10,-32137
	ctx.r10.s64 = -2106130432;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r6,9496(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 9496);
	// beq cr6,0x826a8ef4
	if (cr6.eq) goto loc_826A8EF4;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_826A8EE4:
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// bne 0x826a8ee4
	if (!cr0.eq) goto loc_826A8EE4;
loc_826A8EF4:
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// clrlwi. r10,r4,24
	ctx.r10.u64 = ctx.r4.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r7,r9,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r9,r7,25,7,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// bne 0x826a8f28
	if (!cr0.eq) goto loc_826A8F28;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a8fc4
	if (cr6.eq) goto loc_826A8FC4;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_826A8F14:
	// dcbzl r0,r11
	memset(base + ((r11.u32) & ~127), 0, 128);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// bne 0x826a8f14
	if (!cr0.eq) goto loc_826A8F14;
	// b 0x826a8fc4
	goto loc_826A8FC4;
loc_826A8F28:
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// blt cr6,0x826a8f38
	if (cr6.lt) goto loc_826A8F38;
	// li r8,4
	ctx.r8.s64 = 4;
loc_826A8F38:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826a8f58
	if (cr6.eq) goto loc_826A8F58;
loc_826A8F44:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbzl r5,r11
	memset(base + ((ctx.r5.u32 + r11.u32) & ~127), 0, 128);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// blt cr6,0x826a8f44
	if (cr6.lt) goto loc_826A8F44;
loc_826A8F58:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a8fc4
	if (cr6.eq) goto loc_826A8FC4;
loc_826A8F64:
	// addi r8,r10,4
	ctx.r8.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x826a8f78
	if (!cr6.lt) goto loc_826A8F78;
	// li r8,512
	ctx.r8.s64 = 512;
	// dcbzl r8,r11
	memset(base + ((ctx.r8.u32 + r11.u32) & ~127), 0, 128);
loc_826A8F78:
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r11,32
	ctx.r5.s64 = r11.s64 + 32;
	// addi r4,r11,48
	ctx.r4.s64 = r11.s64 + 48;
	// addi r31,r11,64
	r31.s64 = r11.s64 + 64;
	// addi r30,r11,80
	r30.s64 = r11.s64 + 80;
	// addi r29,r11,96
	r29.s64 = r11.s64 + 96;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r11,112
	r28.s64 = r11.s64 + 112;
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x826a8f64
	if (cr6.lt) goto loc_826A8F64;
loc_826A8FC4:
	// rlwinm r10,r9,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x826a8fec
	if (cr6.lt) goto loc_826A8FEC;
	// rlwinm r9,r10,28,4,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
loc_826A8FD8:
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// bne 0x826a8fd8
	if (!cr0.eq) goto loc_826A8FD8;
loc_826A8FEC:
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
loc_826A8FF0:
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826A8FF4"))) PPC_WEAK_FUNC(sub_826A8FF4);
PPC_FUNC_IMPL(__imp__sub_826A8FF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8FF8"))) PPC_WEAK_FUNC(sub_826A8FF8);
PPC_FUNC_IMPL(__imp__sub_826A8FF8) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A8FFC"))) PPC_WEAK_FUNC(sub_826A8FFC);
PPC_FUNC_IMPL(__imp__sub_826A8FFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A9000"))) PPC_WEAK_FUNC(sub_826A9000);
PPC_FUNC_IMPL(__imp__sub_826A9000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr7{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister temp{};
	// li r6,16
	ctx.r6.s64 = 16;
	// li r7,32
	ctx.r7.s64 = 32;
	// li r8,48
	ctx.r8.s64 = 48;
	// li r9,63
	ctx.r9.s64 = 63;
	// li r10,1024
	ctx.r10.s64 = 1024;
	// li r12,128
	r12.s64 = 128;
	// cmplwi r5,128
	cr0.compare<uint32_t>(ctx.r5.u32, 128, xer);
	// lvsl v0,r0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)&VectorShiftTableL[(temp.u32 & 0xF) * 16]));
	// bltlr 
	if (cr0.lt) return;
loc_826A9024:
	// cmplwi cr7,r5,256
	cr7.compare<uint32_t>(ctx.r5.u32, 256, xer);
	// cmplwi r5,1024
	cr0.compare<uint32_t>(ctx.r5.u32, 1024, xer);
	// blt cr7,0x826a903c
	if (cr7.lt) goto loc_826A903C;
	// ble 0x826a9038
	if (!cr0.gt) goto loc_826A9038;
	// dcbt r10,r4
loc_826A9038:
	// dcbzl r12,r3
	memset(base + ((r12.u32 + ctx.r3.u32) & ~127), 0, 128);
loc_826A903C:
	// lvx v1,r0,r4
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,64
	r11.s64 = ctx.r4.s64 + 64;
	// lvx v2,r6,r4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx v3,r7,r4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v1,v1,v2,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v4,r8,r4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v2,v2,v3,v0
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v3,v3,v4,v0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v6,r6,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v4,v4,v5,v0
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v7,r7,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v5,v5,v6,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v8,r8,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v6,v6,v7,v0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx v9,r9,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v7,v7,v8,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v1,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v8,v8,v9,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v2,r6,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,64
	r11.s64 = ctx.r3.s64 + 64;
	// stvx v3,r7,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,128
	ctx.r4.s64 = ctx.r4.s64 + 128;
	// stvx v4,r8,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r3,128
	ctx.r3.s64 = ctx.r3.s64 + 128;
	// stvx v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,-128
	ctx.r5.s64 = ctx.r5.s64 + -128;
	// stvx v6,r6,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi r5,128
	cr0.compare<uint32_t>(ctx.r5.u32, 128, xer);
	// stvx v7,r7,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r8,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge 0x826a9024
	if (!cr0.lt) goto loc_826A9024;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A90C0"))) PPC_WEAK_FUNC(sub_826A90C0);
PPC_FUNC_IMPL(__imp__sub_826A90C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// li r31,0
	r31.s64 = 0;
	// clrlwi r29,r3,31
	r29.u64 = ctx.r3.u32 & 0x1;
	// rlwinm r30,r11,20,31,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 20) & 0x1;
	// rlwinm r28,r3,0,29,29
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826a927c
	if (cr6.eq) goto loc_826A927C;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826a927c
	if (cr6.eq) goto loc_826A927C;
	// cmpwi cr6,r5,-1
	cr6.compare<int32_t>(ctx.r5.s32, -1, xer);
	// blt cr6,0x826a927c
	if (cr6.lt) goto loc_826A927C;
	// cmpwi cr6,r7,-1
	cr6.compare<int32_t>(ctx.r7.s32, -1, xer);
	// blt cr6,0x826a927c
	if (cr6.lt) goto loc_826A927C;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x826a9260
	if (cr6.eq) goto loc_826A9260;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// addi r3,r11,9504
	ctx.r3.s64 = r11.s64 + 9504;
loc_826A9110:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826a9258
	if (cr6.eq) goto loc_826A9258;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// extsb. r10,r11
	ctx.r10.s64 = r11.s8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x826a9130
	if (!cr0.eq) goto loc_826A9130;
	// lbz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826a9234
	if (cr6.eq) goto loc_826A9234;
loc_826A9130:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826a9248
	if (cr6.eq) goto loc_826A9248;
	// lbz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x826a9250
	if (cr0.eq) goto loc_826A9250;
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// clrlwi r8,r10,24
	ctx.r8.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r8,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// lwzx r10,r11,r3
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// lwzx r11,r27,r3
	r11.u64 = PPC_LOAD_U32(r27.u32 + ctx.r3.u32);
	// beq cr6,0x826a91c0
	if (cr6.eq) goto loc_826A91C0;
	// rlwinm. r27,r10,0,15,15
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10000;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x826a91a4
	if (cr0.eq) goto loc_826A91A4;
	// rlwinm. r9,r11,0,15,15
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x826a9190
	if (cr0.eq) goto loc_826A9190;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x826a9184
	if (!cr6.lt) goto loc_826A9184;
	// li r31,7
	r31.s64 = 7;
	// b 0x826a9218
	goto loc_826A9218;
loc_826A9184:
	// ble cr6,0x826a9218
	if (!cr6.gt) goto loc_826A9218;
	// li r31,11
	r31.s64 = 11;
	// b 0x826a9218
	goto loc_826A9218;
loc_826A9190:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x826a919c
	if (!cr6.eq) goto loc_826A919C;
	// li r31,4
	r31.s64 = 4;
loc_826A919C:
	// ori r31,r31,1
	r31.u64 = r31.u64 | 1;
	// b 0x826a9220
	goto loc_826A9220;
loc_826A91A4:
	// rlwinm. r27,r11,0,15,15
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x826a91c0
	if (cr0.eq) goto loc_826A91C0;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x826a91b8
	if (!cr6.eq) goto loc_826A91B8;
	// li r31,8
	r31.s64 = 8;
loc_826A91B8:
	// ori r31,r31,2
	r31.u64 = r31.u64 | 2;
	// b 0x826a91d8
	goto loc_826A91D8;
loc_826A91C0:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826a91e4
	if (cr6.eq) goto loc_826A91E4;
	// rlwinm. r27,r10,0,14,14
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// bne 0x826a9220
	if (!cr0.eq) goto loc_826A9220;
	// rlwinm. r27,r11,0,14,14
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x826a91e4
	if (cr0.eq) goto loc_826A91E4;
loc_826A91D8:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// b 0x826a9228
	goto loc_826A9228;
loc_826A91E4:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826a920c
	if (cr6.eq) goto loc_826A920C;
	// rlwinm. r27,r10,0,11,11
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x826a91fc
	if (cr0.eq) goto loc_826A91FC;
	// addi r10,r9,32
	ctx.r10.s64 = ctx.r9.s64 + 32;
	// oris r10,r10,8
	ctx.r10.u64 = ctx.r10.u64 | 524288;
loc_826A91FC:
	// rlwinm. r9,r11,0,11,11
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x826a920c
	if (cr0.eq) goto loc_826A920C;
	// addi r11,r8,32
	r11.s64 = ctx.r8.s64 + 32;
	// oris r11,r11,8
	r11.u64 = r11.u64 | 524288;
loc_826A920C:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x826a9248
	if (cr6.lt) goto loc_826A9248;
	// bgt cr6,0x826a9250
	if (cr6.gt) goto loc_826A9250;
loc_826A9218:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_826A9220:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
loc_826A9228:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x826a9110
	if (!cr6.eq) goto loc_826A9110;
	// b 0x826a9260
	goto loc_826A9260;
loc_826A9234:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,10528
	r11.s64 = r11.s64 + 10528;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// b 0x826a9288
	goto loc_826A9288;
loc_826A9248:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826a9288
	goto loc_826A9288;
loc_826A9250:
	// li r3,3
	ctx.r3.s64 = 3;
	// b 0x826a9288
	goto loc_826A9288;
loc_826A9258:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x826a9268
	if (!cr6.eq) goto loc_826A9268;
loc_826A9260:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826a9234
	if (cr6.eq) goto loc_826A9234;
loc_826A9268:
	// subfic r11,r5,0
	xer.ca = ctx.r5.u32 <= 0;
	r11.s64 = 0 - ctx.r5.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// b 0x826a9288
	goto loc_826A9288;
loc_826A927C:
	// li r3,87
	ctx.r3.s64 = 87;
	// bl 0x823b9af0
	sub_823B9AF0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826A9288:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_826A9290"))) PPC_WEAK_FUNC(sub_826A9290);
PPC_FUNC_IMPL(__imp__sub_826A9290) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// bl 0x826afdf0
	sub_826AFDF0(ctx, base);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r5,r11,-13144
	ctx.r5.s64 = r11.s64 + -13144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826b29b0
	sub_826B29B0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A92DC"))) PPC_WEAK_FUNC(sub_826A92DC);
PPC_FUNC_IMPL(__imp__sub_826A92DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A92E0"))) PPC_WEAK_FUNC(sub_826A92E0);
PPC_FUNC_IMPL(__imp__sub_826A92E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,10624
	r30.s64 = r11.s64 + 10624;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A9344"))) PPC_WEAK_FUNC(sub_826A9344);
PPC_FUNC_IMPL(__imp__sub_826A9344) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A9348"))) PPC_WEAK_FUNC(sub_826A9348);
PPC_FUNC_IMPL(__imp__sub_826A9348) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,10624
	r30.s64 = r11.s64 + 10624;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826A93AC"))) PPC_WEAK_FUNC(sub_826A93AC);
PPC_FUNC_IMPL(__imp__sub_826A93AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A93B0"))) PPC_WEAK_FUNC(sub_826A93B0);
PPC_FUNC_IMPL(__imp__sub_826A93B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,10624
	r30.s64 = r11.s64 + 10624;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826A940C"))) PPC_WEAK_FUNC(sub_826A940C);
PPC_FUNC_IMPL(__imp__sub_826A940C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A9410"))) PPC_WEAK_FUNC(sub_826A9410);
PPC_FUNC_IMPL(__imp__sub_826A9410) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,10624
	r30.s64 = r11.s64 + 10624;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826A947C"))) PPC_WEAK_FUNC(sub_826A947C);
PPC_FUNC_IMPL(__imp__sub_826A947C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826A9480"))) PPC_WEAK_FUNC(sub_826A9480);
PPC_FUNC_IMPL(__imp__sub_826A9480) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,10624
	r30.s64 = r11.s64 + 10624;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// bl 0x826e4b6c
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x826e4b7c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

