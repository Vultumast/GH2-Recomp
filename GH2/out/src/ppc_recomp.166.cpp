#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_8260AA38"))) PPC_WEAK_FUNC(sub_8260AA38);
PPC_FUNC_IMPL(__imp__sub_8260AA38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,12850
	r11.s64 = 842137600;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// ori r8,r11,13392
	ctx.r8.u64 = r11.u64 | 13392;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bgt cr6,0x8260acd4
	if (cr6.gt) goto loc_8260ACD4;
	// beq cr6,0x8260ac60
	if (cr6.eq) goto loc_8260AC60;
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bgt cr6,0x8260aad0
	if (cr6.gt) goto loc_8260AAD0;
	// beq cr6,0x8260aaf0
	if (cr6.eq) goto loc_8260AAF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260aa8c
	if (cr6.eq) goto loc_8260AA8C;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x8260aa98
	if (!cr6.eq) goto loc_8260AA98;
loc_8260AA8C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8260ad24
	if (!cr6.gt) goto loc_8260AD24;
loc_8260AA94:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_8260AA98:
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// mullw r8,r31,r11
	ctx.r8.s64 = int64_t(r31.s32) * int64_t(r11.s32);
	// addi r8,r8,31
	ctx.r8.s64 = ctx.r8.s64 + 31;
	// rlwinm r8,r8,0,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r8,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r26,r8,r3
	r26.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r3.s32);
	// bne cr6,0x8260ad2c
	if (!cr6.eq) goto loc_8260AD2C;
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// mullw r8,r26,r8
	ctx.r8.s64 = int64_t(r26.s32) * int64_t(ctx.r8.s32);
	// b 0x8260ad60
	goto loc_8260AD60;
loc_8260AAD0:
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// beq cr6,0x8260ac00
	if (cr6.eq) goto loc_8260AC00;
	// lis r8,12849
	ctx.r8.s64 = 842072064;
	// ori r8,r8,22105
	ctx.r8.u64 = ctx.r8.u64 | 22105;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x8260aa98
	if (!cr6.eq) goto loc_8260AA98;
loc_8260AAF0:
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r29,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r29.s64 = r31.s32 >> 1;
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// mullw r11,r31,r7
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r7,308(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r3,300(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// srawi r28,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r28.s64 = r30.s32 >> 1;
	// srawi r21,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r21.s64 = r20.s32 >> 1;
	// srawi r19,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r19.s64 = ctx.r8.s32 >> 1;
	// srawi r24,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r24.s64 = ctx.r6.s32 >> 1;
	// mullw r27,r31,r6
	r27.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// mullw r6,r30,r7
	ctx.r6.s64 = int64_t(r30.s32) * int64_t(ctx.r7.s32);
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// srawi r25,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r25.s64 = ctx.r10.s32 >> 1;
	// add r27,r27,r10
	r27.u64 = r27.u64 + ctx.r10.u64;
	// add r26,r6,r3
	r26.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mullw r10,r24,r29
	ctx.r10.s64 = int64_t(r24.s32) * int64_t(r29.s32);
	// srawi r6,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r6.s64 = r11.s32 >> 2;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r25,r6,r11
	r25.u64 = ctx.r6.u64 + r11.u64;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// mullw r6,r7,r28
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
loc_8260AB54:
	// add r7,r11,r10
	ctx.r7.u64 = r11.u64 + ctx.r10.u64;
	// mullw r11,r30,r9
	r11.s64 = int64_t(r30.s32) * int64_t(ctx.r9.s32);
	// add r9,r25,r10
	ctx.r9.u64 = r25.u64 + ctx.r10.u64;
	// add r23,r9,r5
	r23.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r9,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r9.s64 = r11.s32 >> 2;
loc_8260AB68:
	// add r10,r6,r3
	ctx.r10.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r25,r7,r5
	r25.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r7,r10
	r11.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r24,r9,r4
	r24.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r22,r11,r4
	r22.u64 = r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8260abb4
	if (!cr6.gt) goto loc_8260ABB4;
	// mr r18,r8
	r18.u64 = ctx.r8.u64;
loc_8260AB90:
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r18,r18,-1
	r18.s64 = r18.s64 + -1;
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// bne cr6,0x8260ab90
	if (!cr6.eq) goto loc_8260AB90;
loc_8260ABB4:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x8260ae28
	if (!cr6.gt) goto loc_8260AE28;
loc_8260ABBC:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// add r25,r29,r25
	r25.u64 = r29.u64 + r25.u64;
	// add r24,r28,r24
	r24.u64 = r28.u64 + r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// add r23,r29,r23
	r23.u64 = r29.u64 + r23.u64;
	// add r22,r28,r22
	r22.u64 = r28.u64 + r22.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x8260abbc
	if (!cr6.eq) goto loc_8260ABBC;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd20
	return;
loc_8260AC00:
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// srawi r29,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r29.s64 = r31.s32 >> 2;
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r28,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r28.s64 = r30.s32 >> 2;
	// mullw r27,r31,r6
	r27.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lwz r3,308(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// srawi r21,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r21.s64 = r20.s32 >> 2;
	// srawi r25,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r25.s64 = ctx.r10.s32 >> 2;
	// add r27,r27,r10
	r27.u64 = r27.u64 + ctx.r10.u64;
	// mullw r8,r30,r3
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r3.s32);
	// mullw r10,r29,r6
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r6.s32);
	// mullw r11,r31,r7
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// lwz r7,300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// add r26,r8,r4
	r26.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// srawi r25,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r25.s64 = r11.s32 >> 2;
	// mullw r6,r28,r3
	ctx.r6.s64 = int64_t(r28.s32) * int64_t(ctx.r3.s32);
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// add r26,r26,r7
	r26.u64 = r26.u64 + ctx.r7.u64;
	// srawi r3,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 2;
	// add r25,r25,r11
	r25.u64 = r25.u64 + r11.u64;
	// b 0x8260ab54
	goto loc_8260AB54;
loc_8260AC60:
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// srawi r29,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r29.s64 = r31.s32 >> 1;
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r28,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r28.s64 = r30.s32 >> 1;
	// mullw r27,r31,r6
	r27.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lwz r3,308(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// add r27,r27,r5
	r27.u64 = r27.u64 + ctx.r5.u64;
	// srawi r21,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r21.s64 = r20.s32 >> 1;
	// srawi r25,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r25.s64 = ctx.r10.s32 >> 1;
	// add r27,r27,r10
	r27.u64 = r27.u64 + ctx.r10.u64;
	// mullw r10,r29,r6
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r6.s32);
	// mullw r11,r31,r7
	r11.s64 = int64_t(r31.s32) * int64_t(ctx.r7.s32);
	// lwz r7,300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r8,r30,r3
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r3.s32);
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// srawi r25,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r25.s64 = r11.s32 >> 1;
	// add r26,r8,r4
	r26.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r6,r28,r3
	ctx.r6.s64 = int64_t(r28.s32) * int64_t(ctx.r3.s32);
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// srawi r3,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 1;
	// add r25,r25,r11
	r25.u64 = r25.u64 + r11.u64;
	// add r26,r26,r7
	r26.u64 = r26.u64 + ctx.r7.u64;
	// add r7,r11,r10
	ctx.r7.u64 = r11.u64 + ctx.r10.u64;
	// mullw r11,r30,r9
	r11.s64 = int64_t(r30.s32) * int64_t(ctx.r9.s32);
	// add r9,r25,r10
	ctx.r9.u64 = r25.u64 + ctx.r10.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// add r23,r9,r5
	r23.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// b 0x8260ab68
	goto loc_8260AB68;
loc_8260ACD4:
	// lis r8,22101
	ctx.r8.s64 = 1448411136;
	// ori r8,r8,22857
	ctx.r8.u64 = ctx.r8.u64 | 22857;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bgt cr6,0x8260ad0c
	if (cr6.gt) goto loc_8260AD0C;
	// beq cr6,0x8260aaf0
	if (cr6.eq) goto loc_8260AAF0;
	// lis r8,12889
	ctx.r8.s64 = 844693504;
	// ori r8,r8,21849
	ctx.r8.u64 = ctx.r8.u64 | 21849;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// beq cr6,0x8260ad1c
	if (cr6.eq) goto loc_8260AD1C;
	// lis r8,21849
	ctx.r8.s64 = 1431896064;
	// ori r8,r8,22105
	ctx.r8.u64 = ctx.r8.u64 | 22105;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// beq cr6,0x8260ad1c
	if (cr6.eq) goto loc_8260AD1C;
	// b 0x8260aa98
	goto loc_8260AA98;
loc_8260AD0C:
	// lis r8,22870
	ctx.r8.s64 = 1498808320;
	// ori r8,r8,22869
	ctx.r8.u64 = ctx.r8.u64 | 22869;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x8260aa98
	if (!cr6.eq) goto loc_8260AA98;
loc_8260AD1C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8260aa94
	if (!cr6.gt) goto loc_8260AA94;
loc_8260AD24:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8260aa98
	goto loc_8260AA98;
loc_8260AD2C:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// bgt cr6,0x8260ad3c
	if (cr6.gt) goto loc_8260AD3C;
	// neg r6,r26
	ctx.r6.s64 = -r26.s64;
loc_8260AD3C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bgt cr6,0x8260ad48
	if (cr6.gt) goto loc_8260AD48;
	// neg r7,r7
	ctx.r7.s64 = -ctx.r7.s64;
loc_8260AD48:
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// addi r7,r8,-1
	ctx.r7.s64 = ctx.r8.s64 + -1;
	// srawi r8,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 3;
	// mullw r10,r7,r6
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
loc_8260AD60:
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// mullw r10,r30,r11
	ctx.r10.s64 = int64_t(r30.s32) * int64_t(r11.s32);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// mullw r27,r10,r3
	r27.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// bne cr6,0x8260ad9c
	if (!cr6.eq) goto loc_8260AD9C;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,308(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// mullw r9,r27,r9
	ctx.r9.s64 = int64_t(r27.s32) * int64_t(ctx.r9.s32);
	// b 0x8260add4
	goto loc_8260ADD4;
loc_8260AD9C:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// bgt cr6,0x8260adac
	if (cr6.gt) goto loc_8260ADAC;
	// neg r8,r27
	ctx.r8.s64 = -r27.s64;
loc_8260ADAC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt cr6,0x8260adb8
	if (cr6.gt) goto loc_8260ADB8;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_8260ADB8:
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r9,300(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
loc_8260ADD4:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,316(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r29,324(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// add r31,r7,r5
	r31.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// add r30,r10,r4
	r30.u64 = ctx.r10.u64 + ctx.r4.u64;
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r28,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r28.s64 = temp.s64;
	// ble cr6,0x8260ae28
	if (!cr6.gt) goto loc_8260AE28;
loc_8260AE04:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r31,r26,r31
	r31.u64 = r26.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8260ae04
	if (!cr6.eq) goto loc_8260AE04;
loc_8260AE28:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_8260AE30"))) PPC_WEAK_FUNC(sub_8260AE30);
PPC_FUNC_IMPL(__imp__sub_8260AE30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8260aefc
	if (!cr6.eq) goto loc_8260AEFC;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260aeec
	if (cr6.eq) goto loc_8260AEEC;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x8260aeec
	if (cr6.eq) goto loc_8260AEEC;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260aef4
	if (cr6.eq) goto loc_8260AEF4;
	// lis r11,22870
	r11.s64 = 1498808320;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260aef4
	if (cr6.eq) goto loc_8260AEF4;
	// lis r11,12850
	r11.s64 = 842137600;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// beq cr6,0x8260aef4
	if (cr6.eq) goto loc_8260AEF4;
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260aed4
	if (cr6.eq) goto loc_8260AED4;
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260aed4
	if (cr6.eq) goto loc_8260AED4;
	// lis r11,12849
	r11.s64 = 842072064;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260aed4
	if (cr6.eq) goto loc_8260AED4;
	// lis r11,12593
	r11.s64 = 825294848;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bne cr6,0x8260aeec
	if (!cr6.eq) goto loc_8260AEEC;
	// srawi r11,r4,2
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3) != 0);
	r11.s64 = ctx.r4.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8260aeec
	if (cr0.eq) goto loc_8260AEEC;
loc_8260AECC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_8260AED4:
	// clrlwi r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260aecc
	if (!cr6.eq) goto loc_8260AECC;
	// clrlwi r11,r5,31
	r11.u64 = ctx.r5.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260aecc
	if (!cr6.eq) goto loc_8260AECC;
loc_8260AEEC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8260AEF4:
	// clrlwi r3,r4,31
	ctx.r3.u64 = ctx.r4.u32 & 0x1;
	// blr 
	return;
loc_8260AEFC:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260afd8
	if (cr6.eq) goto loc_8260AFD8;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x8260afd8
	if (cr6.eq) goto loc_8260AFD8;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260afb4
	if (cr6.eq) goto loc_8260AFB4;
	// lis r11,22870
	r11.s64 = 1498808320;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260afb4
	if (cr6.eq) goto loc_8260AFB4;
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260af90
	if (cr6.eq) goto loc_8260AF90;
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260af90
	if (cr6.eq) goto loc_8260AF90;
	// lis r11,12849
	r11.s64 = 842072064;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x8260af90
	if (cr6.eq) goto loc_8260AF90;
	// lis r11,12593
	r11.s64 = 825294848;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bne cr6,0x8260aeec
	if (!cr6.eq) goto loc_8260AEEC;
	// clrlwi r11,r4,30
	r11.u64 = ctx.r4.u32 & 0x3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260afd0
	if (!cr6.eq) goto loc_8260AFD0;
	// clrlwi r11,r5,31
	r11.u64 = ctx.r5.u32 & 0x1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_8260AF90:
	// clrlwi r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260afac
	if (!cr6.eq) goto loc_8260AFAC;
	// clrlwi r11,r5,30
	r11.u64 = ctx.r5.u32 & 0x3;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
loc_8260AFAC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_8260AFB4:
	// clrlwi r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260afd0
	if (!cr6.eq) goto loc_8260AFD0;
	// clrlwi r11,r5,31
	r11.u64 = ctx.r5.u32 & 0x1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
loc_8260AFD0:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_8260AFD8:
	// clrlwi r3,r5,31
	ctx.r3.u64 = ctx.r5.u32 & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260AFE0"))) PPC_WEAK_FUNC(sub_8260AFE0);
PPC_FUNC_IMPL(__imp__sub_8260AFE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8260b014
	if (!cr6.eq) goto loc_8260B014;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8260B014:
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260b03c
	if (!cr6.eq) goto loc_8260B03C;
	// bl 0x8260a9d8
	sub_8260A9D8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8260B03C:
	// bl 0x8260a638
	sub_8260A638(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260B060"))) PPC_WEAK_FUNC(sub_8260B060);
PPC_FUNC_IMPL(__imp__sub_8260B060) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8260b074
	if (!cr6.eq) goto loc_8260B074;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8260B074:
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8260b0bc
	if (cr6.eq) goto loc_8260B0BC;
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r10,r10,13385
	ctx.r10.u64 = ctx.r10.u64 | 13385;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8260b0bc
	if (cr6.eq) goto loc_8260B0BC;
	// lis r10,12593
	ctx.r10.s64 = 825294848;
	// ori r10,r10,13392
	ctx.r10.u64 = ctx.r10.u64 | 13392;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x8260b0bc
	if (cr6.eq) goto loc_8260B0BC;
	// lis r10,12849
	ctx.r10.s64 = 842072064;
	// li r3,2
	ctx.r3.s64 = 2;
	// ori r10,r10,22105
	ctx.r10.u64 = ctx.r10.u64 | 22105;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bnelr cr6
	if (!cr6.eq) return;
loc_8260B0BC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260B0C4"))) PPC_WEAK_FUNC(sub_8260B0C4);
PPC_FUNC_IMPL(__imp__sub_8260B0C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260B0C8"))) PPC_WEAK_FUNC(sub_8260B0C8);
PPC_FUNC_IMPL(__imp__sub_8260B0C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r9,r10,22857
	ctx.r9.u64 = ctx.r10.u64 | 22857;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// beq cr6,0x8260b114
	if (cr6.eq) goto loc_8260B114;
	// lis r9,12338
	ctx.r9.s64 = 808583168;
	// ori r9,r9,13385
	ctx.r9.u64 = ctx.r9.u64 | 13385;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// beq cr6,0x8260b114
	if (cr6.eq) goto loc_8260B114;
	// lis r9,12849
	ctx.r9.s64 = 842072064;
	// ori r9,r9,22105
	ctx.r9.u64 = ctx.r9.u64 | 22105;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// beq cr6,0x8260b114
	if (cr6.eq) goto loc_8260B114;
	// lis r9,12593
	ctx.r9.s64 = 825294848;
	// ori r9,r9,13392
	ctx.r9.u64 = ctx.r9.u64 | 13392;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bne cr6,0x8260b134
	if (!cr6.eq) goto loc_8260B134;
loc_8260B114:
	// mullw r11,r11,r4
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
loc_8260B134:
	// mullw r11,r11,r4
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260B164"))) PPC_WEAK_FUNC(sub_8260B164);
PPC_FUNC_IMPL(__imp__sub_8260B164) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260B168"))) PPC_WEAK_FUNC(sub_8260B168);
PPC_FUNC_IMPL(__imp__sub_8260B168) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,32(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// lwz r9,296(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 296);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8260b1f4
	if (!cr6.eq) goto loc_8260B1F4;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r9,36(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 36);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bne cr6,0x8260b1b4
	if (!cr6.eq) goto loc_8260B1B4;
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// lwz r9,40(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// beq cr6,0x8260b1bc
	if (cr6.eq) goto loc_8260B1BC;
loc_8260B1B4:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
loc_8260B1BC:
	// lwz r10,28(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// lwz r9,44(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 44);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bne cr6,0x8260b1d8
	if (!cr6.eq) goto loc_8260B1D8;
	// lwz r10,48(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 48);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x8260b1e4
	if (cr6.eq) goto loc_8260B1E4;
loc_8260B1D8:
	// lwz r11,300(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 300);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, r11.u32);
loc_8260B1E4:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// b 0x8260b218
	goto loc_8260B218;
loc_8260B1F4:
	// lwz r11,16(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// li r10,3
	ctx.r10.s64 = 3;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r11.u32);
loc_8260B218:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,8(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// lwz r4,4(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// bl 0x8260b0c8
	sub_8260B0C8(ctx, base);
	// stw r3,20(r6)
	PPC_STORE_U32(ctx.r6.u32 + 20, ctx.r3.u32);
	// lwz r11,292(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 292);
	// lwz r6,28(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r4,12(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// beq cr6,0x8260b274
	if (cr6.eq) goto loc_8260B274;
	// addi r3,r8,156
	ctx.r3.s64 = ctx.r8.s64 + 156;
	// bl 0x8265bad0
	sub_8265BAD0(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_8260B274:
	// addi r3,r8,52
	ctx.r3.s64 = ctx.r8.s64 + 52;
	// bl 0x8265bc60
	sub_8265BC60(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260B290"))) PPC_WEAK_FUNC(sub_8260B290);
PPC_FUNC_IMPL(__imp__sub_8260B290) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x8260b2c4
	if (!cr6.eq) goto loc_8260B2C4;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B2C4:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8260b308
	if (cr6.eq) goto loc_8260B308;
	// lwz r30,16(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lhz r29,14(r28)
	r29.u64 = PPC_LOAD_U16(r28.u32 + 14);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260b308
	if (!cr6.eq) goto loc_8260B308;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x8260b484
	if (cr6.eq) goto loc_8260B484;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x8260b31c
	if (!cr6.eq) goto loc_8260B31C;
	// cmplwi cr6,r29,32
	cr6.compare<uint32_t>(r29.u32, 32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
loc_8260B308:
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B31C:
	// lis r11,22870
	r11.s64 = 1498808320;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,12849
	r11.s64 = 842072064;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,12850
	r11.s64 = 842137600;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x8260b38c
	if (cr6.eq) goto loc_8260B38C;
	// lis r11,12593
	r11.s64 = 825294848;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x8260b3d4
	if (!cr6.eq) goto loc_8260B3D4;
loc_8260B38C:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,312
	ctx.r3.s64 = 312;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8260b308
	if (cr6.eq) goto loc_8260B308;
	// bl 0x8260a5b0
	sub_8260A5B0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bne cr6,0x8260b3e8
	if (!cr6.eq) goto loc_8260B3E8;
loc_8260B3C4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a638
	sub_8260A638(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8260B3D4:
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B3E8:
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8260B3F4:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8260b3f4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260B3F4;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260b41c
	if (cr6.gt) goto loc_8260B41C;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260B41C:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,20(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8260b444
	if (!cr6.eq) goto loc_8260B444;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// bl 0x8260b0c8
	sub_8260B0C8(ctx, base);
	// stw r3,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, ctx.r3.u32);
loc_8260B444:
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r31,156
	ctx.r3.s64 = r31.s64 + 156;
	// bl 0x8265bad0
	sub_8265BAD0(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r26,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r25.u32);
	// stw r27,292(r31)
	PPC_STORE_U32(r31.u32 + 292, r27.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stw r31,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r31.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B484:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x8260b544
	if (cr6.eq) goto loc_8260B544;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x8260b4c0
	if (!cr6.eq) goto loc_8260B4C0;
	// cmpwi cr6,r29,15
	cr6.compare<int32_t>(r29.s32, 15, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// cmpwi cr6,r29,16
	cr6.compare<int32_t>(r29.s32, 16, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// cmpwi cr6,r29,32
	cr6.compare<int32_t>(r29.s32, 32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B4C0:
	// lis r11,12889
	r11.s64 = 844693504;
	// ori r11,r11,21849
	r11.u64 = r11.u64 | 21849;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,22870
	r11.s64 = 1498808320;
	// ori r11,r11,22869
	r11.u64 = r11.u64 | 22869;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,14677
	r11.s64 = 961871872;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,22101
	r11.s64 = 1448411136;
	// ori r11,r11,22857
	r11.u64 = r11.u64 | 22857;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,12338
	r11.s64 = 808583168;
	// ori r11,r11,13385
	r11.u64 = r11.u64 | 13385;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,12849
	r11.s64 = 842072064;
	// ori r11,r11,22105
	r11.u64 = r11.u64 | 22105;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// lis r11,12850
	r11.s64 = 842137600;
	// ori r11,r11,13392
	r11.u64 = r11.u64 | 13392;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260B544:
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// cmpwi cr6,r29,16
	cr6.compare<int32_t>(r29.s32, 16, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// cmpwi cr6,r29,24
	cr6.compare<int32_t>(r29.s32, 24, xer);
	// beq cr6,0x8260b564
	if (cr6.eq) goto loc_8260B564;
	// cmpwi cr6,r29,32
	cr6.compare<int32_t>(r29.s32, 32, xer);
	// bne cr6,0x8260b308
	if (!cr6.eq) goto loc_8260B308;
loc_8260B564:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,312
	ctx.r3.s64 = 312;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8260b308
	if (cr6.eq) goto loc_8260B308;
	// bl 0x8260a5b0
	sub_8260A5B0(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x8260b5b8
	if (!cr6.eq) goto loc_8260B5B8;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// bne cr6,0x8260b5e8
	if (!cr6.eq) goto loc_8260B5E8;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1064
	ctx.r3.s64 = 1064;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x8260b3c4
	if (cr6.eq) goto loc_8260B3C4;
	// li r5,1064
	ctx.r5.s64 = 1064;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// b 0x8260b624
	goto loc_8260B624;
loc_8260B5B8:
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// bne cr6,0x8260b5e8
	if (!cr6.eq) goto loc_8260B5E8;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,52
	ctx.r3.s64 = 52;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x8260b3c4
	if (cr6.eq) goto loc_8260B3C4;
	// li r5,52
	ctx.r5.s64 = 52;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// b 0x8260b624
	goto loc_8260B624;
loc_8260B5E8:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// beq cr6,0x8260b3c4
	if (cr6.eq) goto loc_8260B3C4;
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8260B610:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8260b610
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260B610;
loc_8260B624:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260b638
	if (cr6.gt) goto loc_8260B638;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260B638:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,20(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8260b660
	if (!cr6.eq) goto loc_8260B660;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// bl 0x8260b0c8
	sub_8260B0C8(ctx, base);
	// stw r3,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, ctx.r3.u32);
loc_8260B660:
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// bl 0x8265bba0
	sub_8265BBA0(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r26,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r26.u32);
	// stw r25,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r25.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stw r10,292(r31)
	PPC_STORE_U32(r31.u32 + 292, ctx.r10.u32);
	// stw r31,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r31.u32);
	// beq cr6,0x8260b6a8
	if (cr6.eq) goto loc_8260B6A8;
	// li r3,0
	ctx.r3.s64 = 0;
loc_8260B6A8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8260B6B0"))) PPC_WEAK_FUNC(sub_8260B6B0);
PPC_FUNC_IMPL(__imp__sub_8260B6B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r28,16(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lhz r27,14(r11)
	r27.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// bne cr6,0x8260b6f8
	if (!cr6.eq) goto loc_8260B6F8;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd30
	return;
loc_8260B6F8:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8260b790
	if (!cr6.eq) goto loc_8260B790;
	// lwz r6,304(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x8260b908
	if (cr6.eq) goto loc_8260B908;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// mr r22,r9
	r22.u64 = ctx.r9.u64;
	// bgt cr6,0x8260b724
	if (cr6.gt) goto loc_8260B724;
	// neg r22,r9
	r22.s64 = -ctx.r9.s64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_8260B724:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r30,0
	r30.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,40(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r6,36(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r28.u32);
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r22.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bl 0x8260aa38
	sub_8260AA38(ctx, base);
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// b 0x8260b8f8
	goto loc_8260B8F8;
loc_8260B790:
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x8260b810
	if (!cr6.eq) goto loc_8260B810;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x8260b908
	if (cr6.eq) goto loc_8260B908;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r11,0
	r11.s64 = 0;
	// lwz r30,24(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r24,20(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,44(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r5,308(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r28.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// bl 0x8260aa38
	sub_8260AA38(ctx, base);
	// b 0x8260b8f8
	goto loc_8260B8F8;
loc_8260B810:
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// bne cr6,0x8260b908
	if (!cr6.eq) goto loc_8260B908;
	// lwz r10,308(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8260b908
	if (cr6.eq) goto loc_8260B908;
	// lwz r6,304(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x8260b908
	if (cr6.eq) goto loc_8260B908;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// mr r22,r9
	r22.u64 = ctx.r9.u64;
	// bgt cr6,0x8260b848
	if (cr6.gt) goto loc_8260B848;
	// neg r22,r9
	r22.s64 = -ctx.r9.s64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_8260B848:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r30,0
	r30.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,40(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r6,36(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r28.u32);
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r22.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bl 0x8260aa38
	sub_8260AA38(ctx, base);
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r24,20(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r8,44(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r5,308(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r28.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// bl 0x8260aa38
	sub_8260AA38(ctx, base);
loc_8260B8F8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260b90c
	if (!cr6.eq) goto loc_8260B90C;
loc_8260B908:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8260B90C:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8260B914"))) PPC_WEAK_FUNC(sub_8260B914);
PPC_FUNC_IMPL(__imp__sub_8260B914) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260B918"))) PPC_WEAK_FUNC(sub_8260B918);
PPC_FUNC_IMPL(__imp__sub_8260B918) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x8260bda4
	if (cr6.eq) goto loc_8260BDA4;
	// lwz r31,304(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 304);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8260bda4
	if (cr6.eq) goto loc_8260BDA4;
	// bl 0x8260b060
	sub_8260B060(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x8260bc00
	if (!cr6.eq) goto loc_8260BC00;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r11,22101
	r11.s64 = 1448411136;
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r30,r11,22857
	r30.u64 = r11.u64 | 22857;
	// li r11,0
	r11.s64 = 0;
	// li r23,0
	r23.s64 = 0;
	// lwz r3,16(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// li r24,0
	r24.s64 = 0;
	// li r21,0
	r21.s64 = 0;
	// li r29,0
	r29.s64 = 0;
	// li r22,0
	r22.s64 = 0;
	// li r20,0
	r20.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpw cr6,r3,r30
	cr6.compare<int32_t>(ctx.r3.s32, r30.s32, xer);
	// beq cr6,0x8260ba78
	if (cr6.eq) goto loc_8260BA78;
	// lis r30,12338
	r30.s64 = 808583168;
	// ori r30,r30,13385
	r30.u64 = r30.u64 | 13385;
	// cmpw cr6,r3,r30
	cr6.compare<int32_t>(ctx.r3.s32, r30.s32, xer);
	// beq cr6,0x8260ba78
	if (cr6.eq) goto loc_8260BA78;
	// lis r30,12849
	r30.s64 = 842072064;
	// ori r30,r30,22105
	r30.u64 = r30.u64 | 22105;
	// cmpw cr6,r3,r30
	cr6.compare<int32_t>(ctx.r3.s32, r30.s32, xer);
	// beq cr6,0x8260ba78
	if (cr6.eq) goto loc_8260BA78;
	// lis r30,12593
	r30.s64 = 825294848;
	// ori r30,r30,13392
	r30.u64 = r30.u64 | 13392;
	// cmpw cr6,r3,r30
	cr6.compare<int32_t>(ctx.r3.s32, r30.s32, xer);
	// bne cr6,0x8260bb54
	if (!cr6.eq) goto loc_8260BB54;
	// lwz r11,36(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// lwz r6,40(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r5,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 2;
	// addze r24,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	r24.s64 = temp.s64;
	// srawi r8,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// mr r21,r24
	r21.u64 = r24.u64;
	// addze r29,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r29.s64 = temp.s64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r22,r29
	r22.u64 = r29.u64;
	// mr r20,r29
	r20.u64 = r29.u64;
	// bne cr6,0x8260ba40
	if (!cr6.eq) goto loc_8260BA40;
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r9,r6,r11
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// rlwinm r3,r9,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// srawi r6,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 2;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// srawi r7,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r3.s32 >> 2;
	// addze r3,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r3.s64 = temp.s64;
	// add r7,r6,r9
	ctx.r7.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r9,r3,r6
	ctx.r9.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r6,r7,r5
	ctx.r6.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r9,r5
	ctx.r7.u64 = ctx.r9.u64 + ctx.r5.u64;
	// b 0x8260bb54
	goto loc_8260BB54;
loc_8260BA40:
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r5,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 2;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addze r3,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r3.s64 = temp.s64;
	// mullw r5,r9,r10
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// b 0x8260bb48
	goto loc_8260BB48;
loc_8260BA78:
	// lwz r11,36(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// lwz r6,40(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r5,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 1;
	// addze r24,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	r24.s64 = temp.s64;
	// srawi r8,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// mr r21,r24
	r21.u64 = r24.u64;
	// addze r29,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r29.s64 = temp.s64;
	// srawi r8,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r8.s64 = r29.s32 >> 1;
	// addze r22,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r22.s64 = temp.s64;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mr r20,r22
	r20.u64 = r22.u64;
	// bne cr6,0x8260bb08
	if (!cr6.eq) goto loc_8260BB08;
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mullw r9,r6,r11
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// mullw r6,r8,r10
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 1;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 1;
	// add r30,r9,r6
	r30.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addze r6,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r6.s64 = temp.s64;
	// srawi r3,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	ctx.r3.s64 = r30.s32 >> 2;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r30.s64 = temp.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r7,r30,r6
	ctx.r7.u64 = r30.u64 + ctx.r6.u64;
	// add r3,r6,r5
	ctx.r3.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r6,r3,r9
	ctx.r6.u64 = ctx.r3.u64 + ctx.r9.u64;
	// b 0x8260bb54
	goto loc_8260BB54;
loc_8260BB08:
	// srawi r7,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 1;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r5,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 1;
	// addi r3,r7,1
	ctx.r3.s64 = ctx.r7.s64 + 1;
	// addze r7,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r7.s64 = temp.s64;
	// mullw r5,r3,r10
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r6,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// srawi r3,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 2;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addze r9,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r9.s64 = temp.s64;
	// add r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 + ctx.r7.u64;
loc_8260BB48:
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r3,r5
	ctx.r7.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r6,r9,r6
	ctx.r6.u64 = ctx.r9.u64 + ctx.r6.u64;
loc_8260BB54:
	// add r30,r8,r4
	r30.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r28,r6,r4
	r28.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r26,r7,r4
	r26.u64 = ctx.r7.u64 + ctx.r4.u64;
	// rlwinm r27,r11,1,0,30
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r10,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8260bb94
	if (!cr6.gt) goto loc_8260BB94;
loc_8260BB70:
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// add r31,r23,r31
	r31.u64 = r23.u64 + r31.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8260bb70
	if (!cr6.eq) goto loc_8260BB70;
loc_8260BB94:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// ble cr6,0x8260bbc4
	if (!cr6.gt) goto loc_8260BBC4;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_8260BBA0:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r28,r25,r28
	r28.u64 = r25.u64 + r28.u64;
	// add r31,r24,r31
	r31.u64 = r24.u64 + r31.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260bba0
	if (!cr6.eq) goto loc_8260BBA0;
loc_8260BBC4:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x8260bd6c
	if (!cr6.gt) goto loc_8260BD6C;
	// mr r30,r20
	r30.u64 = r20.u64;
loc_8260BBD0:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r26,r25,r26
	r26.u64 = r25.u64 + r26.u64;
	// add r31,r21,r31
	r31.u64 = r21.u64 + r31.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260bbd0
	if (!cr6.eq) goto loc_8260BBD0;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260BC00:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x8260bd78
	if (!cr6.eq) goto loc_8260BD78;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mr r29,r31
	r29.u64 = r31.u64;
	// lwz r10,36(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,12(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lhz r11,14(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// lwz r31,16(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// cmplwi cr6,r31,3
	cr6.compare<uint32_t>(r31.u32, 3, xer);
	// addi r31,r10,31
	r31.s64 = ctx.r10.s64 + 31;
	// mullw r10,r3,r11
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// rlwinm r3,r31,0,0,26
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// rlwinm r31,r10,0,0,26
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r3,r31,3
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7) != 0);
	ctx.r3.s64 = r31.s32 >> 3;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r28,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r28.s64 = temp.s64;
	// srawi r3,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 1;
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r30.s64 = temp.s64;
	// bgt cr6,0x8260bc74
	if (cr6.gt) goto loc_8260BC74;
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8260bc74
	if (!cr6.gt) goto loc_8260BC74;
	// li r6,1
	ctx.r6.s64 = 1;
loc_8260BC74:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8260bcd8
	if (!cr6.eq) goto loc_8260BCD8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8260bca0
	if (!cr6.eq) goto loc_8260BCA0;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// b 0x8260bd38
	goto loc_8260BD38;
loc_8260BCA0:
	// lwz r7,40(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// subf r7,r6,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r6.s64;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// subf r11,r8,r7
	r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// b 0x8260bd3c
	goto loc_8260BD3C;
loc_8260BCD8:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8260bd04
	if (!cr6.eq) goto loc_8260BD04;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// b 0x8260bd3c
	goto loc_8260BD3C;
loc_8260BD04:
	// lwz r7,40(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r3,4(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// subfic r9,r6,1
	xer.ca = ctx.r6.u32 <= 1;
	ctx.r9.s64 = 1 - ctx.r6.s64;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r8,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r8.s64 = r11.s32 >> 3;
	// mullw r11,r9,r10
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
loc_8260BD38:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8260BD3C:
	// add r31,r11,r4
	r31.u64 = r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8260bd6c
	if (!cr6.gt) goto loc_8260BD6C;
loc_8260BD48:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260bd48
	if (!cr6.eq) goto loc_8260BD48;
loc_8260BD6C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260BD78:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260bd98
	if (!cr6.eq) goto loc_8260BD98;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,28700
	ctx.r3.s64 = r11.s64 + 28700;
	// bl 0x826a8158
	sub_826A8158(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260BD98:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,28688
	ctx.r3.s64 = r11.s64 + 28688;
	// bl 0x826a8158
	sub_826A8158(ctx, base);
loc_8260BDA4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8260BDB0"))) PPC_WEAK_FUNC(sub_8260BDB0);
PPC_FUNC_IMPL(__imp__sub_8260BDB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x8260c2c8
	if (cr6.eq) goto loc_8260C2C8;
	// lwz r11,304(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 304);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8260c2c8
	if (cr6.eq) goto loc_8260C2C8;
	// bl 0x8260b060
	sub_8260B060(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x8260c124
	if (!cr6.eq) goto loc_8260C124;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// li r30,0
	r30.s64 = 0;
	// ori r7,r10,22857
	ctx.r7.u64 = ctx.r10.u64 | 22857;
	// li r29,0
	r29.s64 = 0;
	// li r27,0
	r27.s64 = 0;
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// li r24,0
	r24.s64 = 0;
	// li r23,0
	r23.s64 = 0;
	// li r21,0
	r21.s64 = 0;
	// li r28,0
	r28.s64 = 0;
	// li r22,0
	r22.s64 = 0;
	// li r20,0
	r20.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// beq cr6,0x8260bf04
	if (cr6.eq) goto loc_8260BF04;
	// lis r7,12338
	ctx.r7.s64 = 808583168;
	// ori r7,r7,13385
	ctx.r7.u64 = ctx.r7.u64 | 13385;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// beq cr6,0x8260bf04
	if (cr6.eq) goto loc_8260BF04;
	// lis r7,12849
	ctx.r7.s64 = 842072064;
	// ori r7,r7,22105
	ctx.r7.u64 = ctx.r7.u64 | 22105;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// beq cr6,0x8260bf04
	if (cr6.eq) goto loc_8260BF04;
	// lis r7,12593
	ctx.r7.s64 = 825294848;
	// ori r7,r7,13392
	ctx.r7.u64 = ctx.r7.u64 | 13392;
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bne cr6,0x8260c080
	if (!cr6.eq) goto loc_8260C080;
	// lwz r24,28(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r10,32(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// srawi r7,r24,2
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3) != 0);
	ctx.r7.s64 = r24.s32 >> 2;
	// lwz r11,44(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,48(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// addze r23,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	r23.s64 = temp.s64;
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// lwz r7,20(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// mr r21,r23
	r21.u64 = r23.u64;
	// addze r28,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r28.s64 = temp.s64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// mr r22,r28
	r22.u64 = r28.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// mr r20,r28
	r20.u64 = r28.u64;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// bne cr6,0x8260bed4
	if (!cr6.eq) goto loc_8260BED4;
	// rlwinm r31,r8,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r5,r6,r10
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// srawi r3,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 2;
	// add r31,r8,r31
	r31.u64 = ctx.r8.u64 + r31.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addze r3,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r6,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r6.s64 = r31.s32 >> 2;
	// add r8,r3,r8
	ctx.r8.u64 = ctx.r3.u64 + ctx.r8.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// b 0x8260c074
	goto loc_8260C074;
loc_8260BED4:
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r5,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 2;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 2;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addze r31,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r31.s64 = temp.s64;
	// mullw r3,r6,r10
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r6,r31,r5
	ctx.r6.u64 = r31.u64 + ctx.r5.u64;
	// b 0x8260c068
	goto loc_8260C068;
loc_8260BF04:
	// lwz r7,124(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 124);
	// lwz r24,28(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r10,32(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// srawi r7,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	ctx.r7.s64 = r24.s32 >> 1;
	// lwz r11,44(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,48(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// addze r23,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	r23.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r21,r23
	r21.u64 = r23.u64;
	// addze r28,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r28.s64 = temp.s64;
	// srawi r10,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r10.s64 = r28.s32 >> 1;
	// addze r22,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r22.s64 = temp.s64;
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// mr r20,r22
	r20.u64 = r22.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// beq cr6,0x8260bfdc
	if (cr6.eq) goto loc_8260BFDC;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r11,144(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 144);
	// lwz r10,148(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 148);
	// lwz r5,152(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 152);
	// lwz r7,20(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// bne cr6,0x8260bf90
	if (!cr6.eq) goto loc_8260BF90;
	// srawi r6,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// mullw r4,r8,r10
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// mullw r5,r8,r5
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// addze r4,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r4.s64 = temp.s64;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// b 0x8260bfb8
	goto loc_8260BFB8;
loc_8260BF90:
	// srawi r6,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 1;
	// addi r4,r8,1
	ctx.r4.s64 = ctx.r8.s64 + 1;
	// addze r8,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r4,r4,r11
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// srawi r6,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// mullw r4,r8,r10
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// mullw r5,r8,r5
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
loc_8260BFB8:
	// add r8,r4,r6
	ctx.r8.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,132(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 132);
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// lwz r4,136(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 136);
	// lwz r5,140(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 140);
	// add r30,r3,r7
	r30.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r29,r4,r8
	r29.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r27,r5,r6
	r27.u64 = ctx.r5.u64 + ctx.r6.u64;
	// b 0x8260c080
	goto loc_8260C080;
loc_8260BFDC:
	// lwz r7,24(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r6,20(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// bne cr6,0x8260c030
	if (!cr6.eq) goto loc_8260C030;
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r31,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r31.s64 = ctx.r6.s32 >> 1;
	// add r30,r8,r3
	r30.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addze r3,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r31,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r31.s64 = r30.s32 >> 2;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// addze r30,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	r30.s64 = temp.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r31,r3,r5
	r31.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r6,r30,r3
	ctx.r6.u64 = r30.u64 + ctx.r3.u64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// b 0x8260c074
	goto loc_8260C074;
loc_8260C030:
	// srawi r5,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 1;
	// addi r31,r5,1
	r31.s64 = ctx.r5.s64 + 1;
	// addze r5,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r5.s64 = temp.s64;
	// mullw r3,r31,r10
	ctx.r3.s64 = int64_t(r31.s32) * int64_t(ctx.r10.s32);
	// rlwinm r31,r8,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r31,r8,r31
	r31.u64 = ctx.r8.u64 + r31.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r31,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r31.s64 = r31.s32 >> 2;
	// addze r6,r31
	temp.s64 = r31.s64 + xer.ca;
	xer.ca = temp.u32 < r31.u32;
	ctx.r6.s64 = temp.s64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
loc_8260C068:
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
loc_8260C074:
	// add r27,r6,r4
	r27.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r29,r8,r4
	r29.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r30,r7,r4
	r30.u64 = ctx.r7.u64 + ctx.r4.u64;
loc_8260C080:
	// lwz r31,308(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 308);
	// rlwinm r26,r11,1,0,30
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r10,1,0,30
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8260c0b8
	if (!cr6.gt) goto loc_8260C0B8;
loc_8260C094:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// add r30,r26,r30
	r30.u64 = r26.u64 + r30.u64;
	// add r31,r24,r31
	r31.u64 = r24.u64 + r31.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8260c094
	if (!cr6.eq) goto loc_8260C094;
loc_8260C0B8:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// ble cr6,0x8260c0e8
	if (!cr6.gt) goto loc_8260C0E8;
	// mr r30,r22
	r30.u64 = r22.u64;
loc_8260C0C4:
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r29,r25,r29
	r29.u64 = r25.u64 + r29.u64;
	// add r31,r23,r31
	r31.u64 = r23.u64 + r31.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260c0c4
	if (!cr6.eq) goto loc_8260C0C4;
loc_8260C0E8:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x8260c290
	if (!cr6.gt) goto loc_8260C290;
	// mr r30,r20
	r30.u64 = r20.u64;
loc_8260C0F4:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r27,r25,r27
	r27.u64 = r25.u64 + r27.u64;
	// add r31,r21,r31
	r31.u64 = r21.u64 + r31.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260c0f4
	if (!cr6.eq) goto loc_8260C0F4;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260C124:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x8260c29c
	if (!cr6.eq) goto loc_8260C29C;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,28(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// lwz r3,44(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// lwz r29,308(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 308);
	// lhz r11,14(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// lwz r31,16(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// cmplwi cr6,r31,3
	cr6.compare<uint32_t>(r31.u32, 3, xer);
	// addi r31,r10,31
	r31.s64 = ctx.r10.s64 + 31;
	// mullw r10,r3,r11
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// rlwinm r3,r31,0,0,26
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// srawi r3,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// addze r28,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r28.s64 = temp.s64;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r3,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 1;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	r30.s64 = temp.s64;
	// bgt cr6,0x8260c198
	if (cr6.gt) goto loc_8260C198;
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8260c198
	if (!cr6.gt) goto loc_8260C198;
	// li r6,1
	ctx.r6.s64 = 1;
loc_8260C198:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8260c1fc
	if (!cr6.eq) goto loc_8260C1FC;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8260c1c4
	if (!cr6.eq) goto loc_8260C1C4;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r9,24(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// b 0x8260c25c
	goto loc_8260C25C;
loc_8260C1C4:
	// lwz r7,48(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r9,20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// subf r7,r6,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r6.s64;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// subf r11,r8,r7
	r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// b 0x8260c260
	goto loc_8260C260;
loc_8260C1FC:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x8260c228
	if (!cr6.eq) goto loc_8260C228;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r9,24(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// mullw r11,r8,r10
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// b 0x8260c260
	goto loc_8260C260;
loc_8260C228:
	// lwz r7,48(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r3,20(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// subfic r9,r6,1
	xer.ca = ctx.r6.u32 <= 1;
	ctx.r9.s64 = 1 - ctx.r6.s64;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r8,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r8.s64 = r11.s32 >> 3;
	// mullw r11,r9,r10
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
loc_8260C25C:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8260C260:
	// add r31,r11,r4
	r31.u64 = r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8260c290
	if (!cr6.gt) goto loc_8260C290;
loc_8260C26C:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8260c26c
	if (!cr6.eq) goto loc_8260C26C;
loc_8260C290:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260C29C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c2bc
	if (!cr6.eq) goto loc_8260C2BC;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,28700
	ctx.r3.s64 = r11.s64 + 28700;
	// bl 0x826a8158
	sub_826A8158(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8260C2BC:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,28688
	ctx.r3.s64 = r11.s64 + 28688;
	// bl 0x826a8158
	sub_826A8158(ctx, base);
loc_8260C2C8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8260C2D4"))) PPC_WEAK_FUNC(sub_8260C2D4);
PPC_FUNC_IMPL(__imp__sub_8260C2D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260C2D8"))) PPC_WEAK_FUNC(sub_8260C2D8);
PPC_FUNC_IMPL(__imp__sub_8260C2D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,304(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260c308
	if (cr6.eq) goto loc_8260C308;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r30.u32);
loc_8260C308:
	// lwz r3,308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8260c31c
	if (cr6.eq) goto loc_8260C31C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,308(r31)
	PPC_STORE_U32(r31.u32 + 308, r30.u32);
loc_8260C31C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,40(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r4,36(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// bl 0x8260b0c8
	sub_8260B0C8(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,48(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bl 0x8260b0c8
	sub_8260B0C8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,304(r31)
	PPC_STORE_U32(r31.u32 + 304, ctx.r3.u32);
	// bne cr6,0x8260c364
	if (!cr6.eq) goto loc_8260C364;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8260c380
	goto loc_8260C380;
loc_8260C364:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// stw r3,308(r31)
	PPC_STORE_U32(r31.u32 + 308, ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
loc_8260C380:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260C398"))) PPC_WEAK_FUNC(sub_8260C398);
PPC_FUNC_IMPL(__imp__sub_8260C398) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r30,16(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// lwz r26,228(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// blt cr6,0x8260c50c
	if (cr6.lt) goto loc_8260C50C;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260c410
	if (cr6.gt) goto loc_8260C410;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260C410:
	// add r6,r29,r28
	ctx.r6.u64 = r29.u64 + r28.u64;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// bgt cr6,0x8260c50c
	if (cr6.gt) goto loc_8260C50C;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260c42c
	if (cr6.gt) goto loc_8260C42C;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260C42C:
	// add r6,r27,r7
	ctx.r6.u64 = r27.u64 + ctx.r7.u64;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// bgt cr6,0x8260c50c
	if (cr6.gt) goto loc_8260C50C;
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260c448
	if (cr6.gt) goto loc_8260C448;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260C448:
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// bgt cr6,0x8260c50c
	if (cr6.gt) goto loc_8260C50C;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8260c464
	if (cr6.gt) goto loc_8260C464;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_8260C464:
	// add r6,r9,r26
	ctx.r6.u64 = ctx.r9.u64 + r26.u64;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// bgt cr6,0x8260c50c
	if (cr6.gt) goto loc_8260C50C;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// lwz r6,296(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c50c
	if (!cr6.eq) goto loc_8260C50C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c50c
	if (!cr6.eq) goto loc_8260C50C;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c50c
	if (!cr6.eq) goto loc_8260C50C;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c50c
	if (!cr6.eq) goto loc_8260C50C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r29,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r29.u32);
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// stw r8,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r8.u32);
	// stw r9,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r9.u32);
	// stw r28,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r28.u32);
	// stw r7,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r7.u32);
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r26,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r26.u32);
	// bl 0x8260b168
	sub_8260B168(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8260C50C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8260C518"))) PPC_WEAK_FUNC(sub_8260C518);
PPC_FUNC_IMPL(__imp__sub_8260C518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// bne cr6,0x8260c58c
	if (!cr6.eq) goto loc_8260C58C;
	// stw r11,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r11.u32);
	// bl 0x8260b6b0
	sub_8260B6B0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_8260C58C:
	// li r26,1
	r26.s64 = 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r11,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r11.u32);
	// stw r26,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r26.u32);
	// stw r11,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r11.u32);
	// bl 0x8260b918
	sub_8260B918(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// addze r29,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r29.s64 = temp.s64;
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addze r30,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r30.s64 = temp.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c6a0
	if (!cr6.eq) goto loc_8260C6A0;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260bdb0
	sub_8260BDB0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r26,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r26.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r26,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r26.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260b918
	sub_8260B918(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 308);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260a6b8
	sub_8260A6B8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x8260c690
	if (!cr6.eq) goto loc_8260C690;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260bdb0
	sub_8260BDB0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260c69c
	if (cr6.eq) goto loc_8260C69C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r9.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
loc_8260C690:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_8260C69C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8260C6A0:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8260C6A8"))) PPC_WEAK_FUNC(sub_8260C6A8);
PPC_FUNC_IMPL(__imp__sub_8260C6A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,22870
	r11.s64 = 1498808320;
	// stw r3,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, ctx.r3.u32);
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// ori r30,r11,22869
	r30.u64 = r11.u64 | 22869;
	// lis r11,12889
	r11.s64 = 844693504;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// ori r16,r11,21849
	r16.u64 = r11.u64 | 21849;
	// lis r11,22101
	r11.s64 = 1448411136;
	// mr r17,r10
	r17.u64 = ctx.r10.u64;
	// ori r7,r11,22857
	ctx.r7.u64 = r11.u64 | 22857;
	// lis r11,12338
	r11.s64 = 808583168;
	// lwz r10,16(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// ori r31,r11,13385
	r31.u64 = r11.u64 | 13385;
	// lis r11,12849
	r11.s64 = 842072064;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// ori r14,r11,22105
	r14.u64 = r11.u64 | 22105;
	// lis r11,12850
	r11.s64 = 842137600;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// ori r15,r11,13392
	r15.u64 = r11.u64 | 13392;
	// lis r11,12593
	r11.s64 = 825294848;
	// mr r18,r9
	r18.u64 = ctx.r9.u64;
	// ori r8,r11,13392
	ctx.r8.u64 = r11.u64 | 13392;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r16
	cr6.compare<int32_t>(ctx.r10.s32, r16.s32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r31
	cr6.compare<int32_t>(ctx.r10.s32, r31.s32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r14
	cr6.compare<int32_t>(ctx.r10.s32, r14.s32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmplw cr6,r10,r15
	cr6.compare<uint32_t>(ctx.r10.u32, r15.u32, xer);
	// beq cr6,0x8260c758
	if (cr6.eq) goto loc_8260C758;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
loc_8260C758:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8260c9f4
	if (cr6.eq) goto loc_8260C9F4;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// cmpwi cr6,r17,1
	cr6.compare<int32_t>(r17.s32, 1, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r20,324(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// cmpwi cr6,r20,1
	cr6.compare<int32_t>(r20.s32, 1, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r21,332(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r22,340(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r23,348(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// cmpwi cr6,r23,1
	cr6.compare<int32_t>(r23.s32, 1, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r24,356(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// cmpwi cr6,r24,1
	cr6.compare<int32_t>(r24.s32, 1, xer);
	// blt cr6,0x8260c9ec
	if (cr6.lt) goto loc_8260C9EC;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// bgt cr6,0x8260c7d0
	if (cr6.gt) goto loc_8260C7D0;
	// neg r11,r4
	r11.s64 = -ctx.r4.s64;
loc_8260C7D0:
	// add r9,r19,r17
	ctx.r9.u64 = r19.u64 + r17.u64;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bgt cr6,0x8260c9ec
	if (cr6.gt) goto loc_8260C9EC;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// bgt cr6,0x8260c7f0
	if (cr6.gt) goto loc_8260C7F0;
	// neg r11,r5
	r11.s64 = -ctx.r5.s64;
loc_8260C7F0:
	// add r9,r18,r20
	ctx.r9.u64 = r18.u64 + r20.u64;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bgt cr6,0x8260c9ec
	if (cr6.gt) goto loc_8260C9EC;
	// add r11,r21,r23
	r11.u64 = r21.u64 + r23.u64;
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// bgt cr6,0x8260c9ec
	if (cr6.gt) goto loc_8260C9EC;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// mr r11,r26
	r11.u64 = r26.u64;
	// bgt cr6,0x8260c818
	if (cr6.gt) goto loc_8260C818;
	// neg r11,r26
	r11.s64 = -r26.s64;
loc_8260C818:
	// add r9,r22,r24
	ctx.r9.u64 = r22.u64 + r24.u64;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bgt cr6,0x8260c9ec
	if (cr6.gt) goto loc_8260C9EC;
	// lwz r29,372(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// lhz r9,14(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 14);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8260ae30
	sub_8260AE30(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9ec
	if (!cr6.eq) goto loc_8260C9EC;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bne cr6,0x8260c8c8
	if (!cr6.eq) goto loc_8260C8C8;
	// li r30,1
	r30.s64 = 1;
	// b 0x8260c910
	goto loc_8260C910;
loc_8260C8C8:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// beq cr6,0x8260c90c
	if (cr6.eq) goto loc_8260C90C;
	// cmpw cr6,r10,r31
	cr6.compare<int32_t>(ctx.r10.s32, r31.s32, xer);
	// beq cr6,0x8260c90c
	if (cr6.eq) goto loc_8260C90C;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8260c8e8
	if (!cr6.eq) goto loc_8260C8E8;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// b 0x8260c904
	goto loc_8260C904;
loc_8260C8E8:
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// beq cr6,0x8260c90c
	if (cr6.eq) goto loc_8260C90C;
	// cmpw cr6,r10,r16
	cr6.compare<int32_t>(ctx.r10.s32, r16.s32, xer);
	// beq cr6,0x8260c90c
	if (cr6.eq) goto loc_8260C90C;
	// cmplw cr6,r10,r15
	cr6.compare<uint32_t>(ctx.r10.u32, r15.u32, xer);
	// beq cr6,0x8260c90c
	if (cr6.eq) goto loc_8260C90C;
	// cmpw cr6,r10,r14
	cr6.compare<int32_t>(ctx.r10.s32, r14.s32, xer);
loc_8260C904:
	// li r30,0
	r30.s64 = 0;
	// bne cr6,0x8260c910
	if (!cr6.eq) goto loc_8260C910;
loc_8260C90C:
	// lwz r30,364(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
loc_8260C910:
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8260b290
	sub_8260B290(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9f8
	if (!cr6.eq) goto loc_8260C9F8;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// stw r25,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r25.u32);
	// stw r26,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r26.u32);
	// stw r30,292(r31)
	PPC_STORE_U32(r31.u32 + 292, r30.u32);
	// stw r29,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r29.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bl 0x8260c2d8
	sub_8260C2D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260c9f4
	if (cr6.eq) goto loc_8260C9F4;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8260c398
	sub_8260C398(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8260c9f4
	if (!cr6.eq) goto loc_8260C9F4;
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r10,21440(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21440);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8260c9e0
	if (cr6.eq) goto loc_8260C9E0;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,124(r31)
	PPC_STORE_U32(r31.u32 + 124, ctx.r10.u32);
	// lwz r10,21444(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21444);
	// stw r10,132(r31)
	PPC_STORE_U32(r31.u32 + 132, ctx.r10.u32);
	// lwz r10,21448(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21448);
	// stw r10,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r10.u32);
	// lwz r10,21452(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21452);
	// stw r10,140(r31)
	PPC_STORE_U32(r31.u32 + 140, ctx.r10.u32);
	// lwz r10,21456(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21456);
	// stw r10,144(r31)
	PPC_STORE_U32(r31.u32 + 144, ctx.r10.u32);
	// lwz r10,21460(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21460);
	// stw r10,148(r31)
	PPC_STORE_U32(r31.u32 + 148, ctx.r10.u32);
	// lwz r11,21464(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 21464);
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
loc_8260C9E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_8260C9EC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_8260C9F4:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8260C9F8:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8260CA00"))) PPC_WEAK_FUNC(sub_8260CA00);
PPC_FUNC_IMPL(__imp__sub_8260CA00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r30,3704(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r3,608(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 608);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r25,r4,r10
	r25.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r29,r5,r11
	r29.u64 = ctx.r5.u64 + r11.u64;
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// add r27,r6,r11
	r27.u64 = ctx.r6.u64 + r11.u64;
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r26,r7,r10
	r26.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r3,608(r30)
	PPC_STORE_U32(r30.u32 + 608, ctx.r3.u32);
	// add r30,r8,r11
	r30.u64 = ctx.r8.u64 + r11.u64;
	// lwz r3,15900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// add r28,r9,r11
	r28.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260cac8
	if (cr6.eq) goto loc_8260CAC8;
	// lwz r11,19712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19712);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260cac8
	if (!cr6.eq) goto loc_8260CAC8;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	r11.s64 = ctx.r9.s64 * 68;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r10,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8260CAC8:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// li r24,0
	r24.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8260cb58
	if (!cr6.gt) goto loc_8260CB58;
loc_8260CAD8:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// cmpw cr6,r24,r10
	cr6.compare<int32_t>(r24.s32, ctx.r10.s32, xer);
	// blt cr6,0x8260cad8
	if (cr6.lt) goto loc_8260CAD8;
loc_8260CB58:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8260CB60"))) PPC_WEAK_FUNC(sub_8260CB60);
PPC_FUNC_IMPL(__imp__sub_8260CB60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r19,r4
	r19.u64 = ctx.r4.u64;
	// mr r16,r5
	r16.u64 = ctx.r5.u64;
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// lwz r11,15900(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r10,3728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r20,r9,r11
	r20.u64 = ctx.r9.u64 + r11.u64;
	// add r17,r10,r11
	r17.u64 = ctx.r10.u64 + r11.u64;
	// add r25,r8,r7
	r25.u64 = ctx.r8.u64 + ctx.r7.u64;
	// beq cr6,0x8260cc44
	if (cr6.eq) goto loc_8260CC44;
	// lwz r11,19712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19712);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8260cc44
	if (!cr6.eq) goto loc_8260CC44;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	r11.s64 = ctx.r9.s64 * 68;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// stw r10,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r10.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// stw r10,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// stw r10,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r10,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r10.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r10,64(r11)
	PPC_STORE_U32(r11.u32 + 64, ctx.r10.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r10,68(r11)
	PPC_STORE_U32(r11.u32 + 68, ctx.r10.u32);
	// lwz r10,248(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// stw r10,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r10.u32);
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// stw r10,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r10.u32);
	// lwz r10,232(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// stw r10,84(r11)
	PPC_STORE_U32(r11.u32 + 84, ctx.r10.u32);
	// lwz r10,15512(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15512);
	// stw r19,92(r11)
	PPC_STORE_U32(r11.u32 + 92, r19.u32);
	// stw r16,96(r11)
	PPC_STORE_U32(r11.u32 + 96, r16.u32);
	// stw r10,88(r11)
	PPC_STORE_U32(r11.u32 + 88, ctx.r10.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_8260CC44:
	// lwz r11,3688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// mr r27,r19
	r27.u64 = r19.u64;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// cmplw cr6,r19,r16
	cr6.compare<uint32_t>(r19.u32, r16.u32, xer);
	// lwz r11,608(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 608);
	// stw r11,608(r10)
	PPC_STORE_U32(ctx.r10.u32 + 608, r11.u32);
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r6,232(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// add r26,r8,r7
	r26.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r8,228(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r10,3784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r15,204(r31)
	r15.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r22,r11,r9
	r22.u64 = r11.u64 + ctx.r9.u64;
	// lwz r21,208(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r18,r10,r11
	r18.u64 = ctx.r10.u64 + r11.u64;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// bge cr6,0x8260cd44
	if (!cr6.lt) goto loc_8260CD44;
	// lis r24,-32126
	r24.s64 = -2105409536;
	// lis r23,-32126
	r23.s64 = -2105409536;
loc_8260CCA0:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// mr r29,r25
	r29.u64 = r25.u64;
	// bne cr6,0x8260ccb0
	if (!cr6.eq) goto loc_8260CCB0;
	// mr r29,r26
	r29.u64 = r26.u64;
loc_8260CCB0:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r28,r25
	r28.u64 = r25.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x8260cd24
	if (!cr6.gt) goto loc_8260CD24;
loc_8260CCC4:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bne cr6,0x8260cce8
	if (!cr6.eq) goto loc_8260CCE8;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// lwz r11,13652(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 13652);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8260CCE8:
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// lwz r11,13660(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 13660);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8260ccc4
	if (cr6.lt) goto loc_8260CCC4;
loc_8260CD24:
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r26,r10,r26
	r26.u64 = ctx.r10.u64 + r26.u64;
	// cmplw cr6,r27,r16
	cr6.compare<uint32_t>(r27.u32, r16.u32, xer);
	// blt cr6,0x8260cca0
	if (cr6.lt) goto loc_8260CCA0;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_8260CD44:
	// mr r27,r19
	r27.u64 = r19.u64;
	// cmplw cr6,r19,r16
	cr6.compare<uint32_t>(r19.u32, r16.u32, xer);
	// lis r26,-32126
	r26.s64 = -2105409536;
	// lis r25,-32126
	r25.s64 = -2105409536;
	// bge cr6,0x8260cdf8
	if (!cr6.lt) goto loc_8260CDF8;
loc_8260CD58:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// mr r29,r20
	r29.u64 = r20.u64;
	// bne cr6,0x8260cd68
	if (!cr6.eq) goto loc_8260CD68;
	// mr r29,r22
	r29.u64 = r22.u64;
loc_8260CD68:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r28,r20
	r28.u64 = r20.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x8260cddc
	if (!cr6.gt) goto loc_8260CDDC;
loc_8260CD7C:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bne cr6,0x8260cda0
	if (!cr6.eq) goto loc_8260CDA0;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// lwz r11,13664(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 13664);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8260CDA0:
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// lwz r11,13656(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 13656);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8260cd7c
	if (cr6.lt) goto loc_8260CD7C;
loc_8260CDDC:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r20,r11,r20
	r20.u64 = r11.u64 + r20.u64;
	// add r22,r6,r22
	r22.u64 = ctx.r6.u64 + r22.u64;
	// cmplw cr6,r27,r16
	cr6.compare<uint32_t>(r27.u32, r16.u32, xer);
	// blt cr6,0x8260cd58
	if (cr6.lt) goto loc_8260CD58;
loc_8260CDF8:
	// mr r27,r19
	r27.u64 = r19.u64;
	// cmplw cr6,r19,r16
	cr6.compare<uint32_t>(r19.u32, r16.u32, xer);
	// bge cr6,0x8260cea4
	if (!cr6.lt) goto loc_8260CEA4;
loc_8260CE04:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// mr r29,r17
	r29.u64 = r17.u64;
	// bne cr6,0x8260ce14
	if (!cr6.eq) goto loc_8260CE14;
	// mr r29,r18
	r29.u64 = r18.u64;
loc_8260CE14:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r28,r17
	r28.u64 = r17.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x8260ce88
	if (!cr6.gt) goto loc_8260CE88;
loc_8260CE28:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bne cr6,0x8260ce4c
	if (!cr6.eq) goto loc_8260CE4C;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// lwz r11,13664(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 13664);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8260CE4C:
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// lwz r11,13656(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 13656);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8260ce28
	if (cr6.lt) goto loc_8260CE28;
loc_8260CE88:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r17,r11,r17
	r17.u64 = r11.u64 + r17.u64;
	// add r18,r6,r18
	r18.u64 = ctx.r6.u64 + r18.u64;
	// cmplw cr6,r27,r16
	cr6.compare<uint32_t>(r27.u32, r16.u32, xer);
	// blt cr6,0x8260ce04
	if (cr6.lt) goto loc_8260CE04;
loc_8260CEA4:
	// lwz r11,15512(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15512);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260cf88
	if (cr6.eq) goto loc_8260CF88;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mr r27,r19
	r27.u64 = r19.u64;
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// cmplw cr6,r19,r16
	cr6.compare<uint32_t>(r19.u32, r16.u32, xer);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r9,3780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r10,3784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r24,r8,r7
	r24.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r25,r11,r9
	r25.u64 = r11.u64 + ctx.r9.u64;
	// add r23,r11,r10
	r23.u64 = r11.u64 + ctx.r10.u64;
	// bge cr6,0x8260cf88
	if (!cr6.lt) goto loc_8260CF88;
	// lis r22,-32126
	r22.s64 = -2105409536;
loc_8260CEE0:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r29,r24
	r29.u64 = r24.u64;
	// mr r30,r25
	r30.u64 = r25.u64;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8260cf6c
	if (cr6.eq) goto loc_8260CF6C;
	// subf r26,r25,r23
	r26.s64 = r23.s64 - r25.s64;
loc_8260CEFC:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8260cf50
	if (cr6.eq) goto loc_8260CF50;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x8260cf50
	if (cr6.eq) goto loc_8260CF50;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8260cf50
	if (cr6.eq) goto loc_8260CF50;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x8260cf50
	if (cr6.eq) goto loc_8260CF50;
	// lwz r6,248(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// lwz r11,13640(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 13640);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
	// add r5,r26,r30
	ctx.r5.u64 = r26.u64 + r30.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8260CF50:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x8260cefc
	if (cr6.lt) goto loc_8260CEFC;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_8260CF6C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// add r25,r6,r25
	r25.u64 = ctx.r6.u64 + r25.u64;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// add r23,r6,r23
	r23.u64 = ctx.r6.u64 + r23.u64;
	// cmplw cr6,r27,r16
	cr6.compare<uint32_t>(r27.u32, r16.u32, xer);
	// blt cr6,0x8260cee0
	if (cr6.lt) goto loc_8260CEE0;
loc_8260CF88:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8260CF90"))) PPC_WEAK_FUNC(sub_8260CF90);
PPC_FUNC_IMPL(__imp__sub_8260CF90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,15548(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15548);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bgtlr cr6
	if (cr6.gt) return;
	// bne cr6,0x8260cfc4
	if (!cr6.eq) goto loc_8260CFC4;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r4,3924(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 3924);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,15508(r11)
	PPC_STORE_U32(r11.u32 + 15508, ctx.r10.u32);
	// stw r10,15512(r11)
	PPC_STORE_U32(r11.u32 + 15512, ctx.r10.u32);
	// b 0x8260f190
	sub_8260F190(ctx, base);
	return;
loc_8260CFC4:
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// bne cr6,0x8260cfe4
	if (!cr6.eq) goto loc_8260CFE4;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r4,3924(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 3924);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r10,15508(r11)
	PPC_STORE_U32(r11.u32 + 15508, ctx.r10.u32);
	// stw r10,15512(r11)
	PPC_STORE_U32(r11.u32 + 15512, ctx.r10.u32);
	// b 0x8260f190
	sub_8260F190(ctx, base);
	return;
loc_8260CFE4:
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x8260d008
	if (!cr6.eq) goto loc_8260D008;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r4,3924(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 3924);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,15508(r11)
	PPC_STORE_U32(r11.u32 + 15508, ctx.r10.u32);
	// stw r9,15512(r11)
	PPC_STORE_U32(r11.u32 + 15512, ctx.r9.u32);
	// b 0x8260f190
	sub_8260F190(ctx, base);
	return;
loc_8260D008:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8260d028
	if (!cr6.eq) goto loc_8260D028;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r4,3924(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 3924);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r10,15508(r11)
	PPC_STORE_U32(r11.u32 + 15508, ctx.r10.u32);
	// stw r9,15512(r11)
	PPC_STORE_U32(r11.u32 + 15512, ctx.r9.u32);
	// b 0x8260f190
	sub_8260F190(ctx, base);
	return;
loc_8260D028:
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,15508(r11)
	PPC_STORE_U32(r11.u32 + 15508, ctx.r10.u32);
	// stw r10,15512(r11)
	PPC_STORE_U32(r11.u32 + 15512, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260D038"))) PPC_WEAK_FUNC(sub_8260D038);
PPC_FUNC_IMPL(__imp__sub_8260D038) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// li r26,0
	r26.s64 = 0;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// add r24,r4,r10
	r24.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// add r29,r5,r11
	r29.u64 = ctx.r5.u64 + r11.u64;
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// add r27,r6,r11
	r27.u64 = ctx.r6.u64 + r11.u64;
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r25,r7,r10
	r25.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lwz r3,200(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r30,r8,r11
	r30.u64 = ctx.r8.u64 + r11.u64;
	// add r28,r9,r11
	r28.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x8260d0fc
	if (!cr6.gt) goto loc_8260D0FC;
loc_8260D098:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x8260d098
	if (cr6.lt) goto loc_8260D098;
loc_8260D0FC:
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mr r27,r23
	r27.u64 = r23.u64;
	// lwz r8,3776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// cmplw cr6,r23,r20
	cr6.compare<uint32_t>(r23.u32, r20.u32, xer);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r9,3780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r10,3784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// add r24,r8,r7
	r24.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r25,r9,r11
	r25.u64 = ctx.r9.u64 + r11.u64;
	// add r21,r10,r11
	r21.u64 = ctx.r10.u64 + r11.u64;
	// bge cr6,0x8260d1d0
	if (!cr6.lt) goto loc_8260D1D0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lis r23,-32126
	r23.s64 = -2105409536;
	// li r22,1
	r22.s64 = 1;
loc_8260D134:
	// mr r28,r24
	r28.u64 = r24.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8260d1ac
	if (cr6.eq) goto loc_8260D1AC;
	// subf r26,r25,r21
	r26.s64 = r21.s64 - r25.s64;
loc_8260D14C:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// lwz r5,248(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// li r8,0
	ctx.r8.s64 = 0;
	// add r6,r26,r29
	ctx.r6.u64 = r26.u64 + r29.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,13648(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 13648);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x8260d14c
	if (cr6.lt) goto loc_8260D14C;
loc_8260D1AC:
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lwz r9,228(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r24,r9,r24
	r24.u64 = ctx.r9.u64 + r24.u64;
	// cmplw cr6,r27,r20
	cr6.compare<uint32_t>(r27.u32, r20.u32, xer);
	// add r25,r10,r25
	r25.u64 = ctx.r10.u64 + r25.u64;
	// add r21,r10,r21
	r21.u64 = ctx.r10.u64 + r21.u64;
	// blt cr6,0x8260d134
	if (cr6.lt) goto loc_8260D134;
loc_8260D1D0:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8260D1D8"))) PPC_WEAK_FUNC(sub_8260D1D8);
PPC_FUNC_IMPL(__imp__sub_8260D1D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,23256(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 23256);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8260d1f4
	if (!cr6.lt) goto loc_8260D1F4;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// stw r7,23256(r10)
	PPC_STORE_U32(ctx.r10.u32 + 23256, ctx.r7.u32);
loc_8260D1F4:
	// lwz r9,3644(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3644);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x8260d214
	if (cr6.lt) goto loc_8260D214;
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// bgt cr6,0x8260d214
	if (cr6.gt) goto loc_8260D214;
	// stw r9,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, ctx.r9.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8260D214:
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// ble cr6,0x8260d338
	if (!cr6.gt) goto loc_8260D338;
	// lwz r8,3656(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3656);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bgt cr6,0x8260d22c
	if (cr6.gt) goto loc_8260D22C;
	// li r8,30
	ctx.r8.s64 = 30;
loc_8260D22C:
	// lwz r9,3660(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3660);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt cr6,0x8260d23c
	if (cr6.gt) goto loc_8260D23C;
	// li r9,500
	ctx.r9.s64 = 500;
loc_8260D23C:
	// lwz r11,23260(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 23260);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8260d344
	if (!cr6.gt) goto loc_8260D344;
	// lis r6,1
	ctx.r6.s64 = 65536;
	// addi r3,r11,-100
	ctx.r3.s64 = r11.s64 + -100;
	// ori r6,r6,34364
	ctx.r6.u64 = ctx.r6.u64 | 34364;
	// cmplw cr6,r3,r6
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r6.u32, xer);
	// bgt cr6,0x8260d344
	if (cr6.gt) goto loc_8260D344;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// lwz r6,15572(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 15572);
	// mullw r8,r8,r5
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// mulli r9,r11,10000
	ctx.r9.s64 = r11.s64 * 10000;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// fsqrts f0,f0
	f0.f64 = double(float(sqrt(f0.f64)));
	// lfd f13,-8(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f0,2868(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2868);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r4
	PPC_STORE_U32(ctx.r4.u32, f0.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// srawi r8,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r8.s64 = r11.s32 >> 1;
	// twllei r11,0
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rotlwi r9,r8,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// divw r8,r8,r11
	ctx.r8.s32 = ctx.r8.s32 / r11.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// andc r11,r11,r9
	r11.u64 = r11.u64 & ~ctx.r9.u64;
	// twlgei r11,-1
	// bne cr6,0x8260d2e0
	if (!cr6.eq) goto loc_8260D2E0;
	// addi r8,r8,-50
	ctx.r8.s64 = ctx.r8.s64 + -50;
loc_8260D2E0:
	// addi r11,r8,-100
	r11.s64 = ctx.r8.s64 + -100;
	// cmpwi cr6,r11,120
	cr6.compare<int32_t>(r11.s32, 120, xer);
	// blt cr6,0x8260d2fc
	if (cr6.lt) goto loc_8260D2FC;
	// li r11,4
	r11.s64 = 4;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, r11.u32);
	// blr 
	return;
loc_8260D2FC:
	// cmpwi cr6,r11,90
	cr6.compare<int32_t>(r11.s32, 90, xer);
	// blt cr6,0x8260d314
	if (cr6.lt) goto loc_8260D314;
	// li r11,3
	r11.s64 = 3;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, r11.u32);
	// blr 
	return;
loc_8260D314:
	// cmpwi cr6,r11,65
	cr6.compare<int32_t>(r11.s32, 65, xer);
	// blt cr6,0x8260d32c
	if (cr6.lt) goto loc_8260D32C;
	// li r11,2
	r11.s64 = 2;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, r11.u32);
	// blr 
	return;
loc_8260D32C:
	// cmpwi cr6,r11,42
	cr6.compare<int32_t>(r11.s32, 42, xer);
	// blt cr6,0x8260d344
	if (cr6.lt) goto loc_8260D344;
	// li r11,1
	r11.s64 = 1;
loc_8260D338:
	// stw r11,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8260D344:
	// stw r7,15548(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15548, ctx.r7.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260D350"))) PPC_WEAK_FUNC(sub_8260D350);
PPC_FUNC_IMPL(__imp__sub_8260D350) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcec
	// add r11,r3,r6
	r11.u64 = ctx.r3.u64 + ctx.r6.u64;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r26,r11,-1
	r26.s64 = r11.s64 + -1;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,255
	ctx.r8.s64 = 255;
	// li r27,-1
	r27.s64 = -1;
	// addi r28,r10,-1
	r28.s64 = ctx.r10.s64 + -1;
	// subf r25,r6,r3
	r25.s64 = ctx.r3.s64 - ctx.r6.s64;
	// add r29,r6,r11
	r29.u64 = ctx.r6.u64 + r11.u64;
loc_8260D384:
	// add r10,r25,r27
	ctx.r10.u64 = r25.u64 + r27.u64;
	// add r30,r28,r6
	r30.u64 = r28.u64 + ctx.r6.u64;
	// mr r31,r28
	r31.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r7,2
	ctx.r7.s64 = 2;
loc_8260D398:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bge cr6,0x8260d3a8
	if (!cr6.lt) goto loc_8260D3A8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8260D3A8:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x8260d3b4
	if (!cr6.gt) goto loc_8260D3B4;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_8260D3B4:
	// lbzx r11,r10,r6
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r6.u32);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bge cr6,0x8260d3c4
	if (!cr6.lt) goto loc_8260D3C4;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8260D3C4:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x8260d3d0
	if (!cr6.gt) goto loc_8260D3D0;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_8260D3D0:
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bge cr6,0x8260d3e0
	if (!cr6.lt) goto loc_8260D3E0;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8260D3E0:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x8260d3ec
	if (!cr6.gt) goto loc_8260D3EC;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_8260D3EC:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bge cr6,0x8260d3fc
	if (!cr6.lt) goto loc_8260D3FC;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8260D3FC:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x8260d408
	if (!cr6.gt) goto loc_8260D408;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_8260D408:
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bge cr6,0x8260d418
	if (!cr6.lt) goto loc_8260D418;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8260D418:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// ble cr6,0x8260d424
	if (!cr6.gt) goto loc_8260D424;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_8260D424:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// add r31,r29,r31
	r31.u64 = r29.u64 + r31.u64;
	// add r30,r29,r30
	r30.u64 = r29.u64 + r30.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x8260d398
	if (!cr6.eq) goto loc_8260D398;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmpwi cr6,r27,9
	cr6.compare<int32_t>(r27.s32, 9, xer);
	// blt cr6,0x8260d384
	if (cr6.lt) goto loc_8260D384;
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8260D474"))) PPC_WEAK_FUNC(sub_8260D474);
PPC_FUNC_IMPL(__imp__sub_8260D474) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260D478"))) PPC_WEAK_FUNC(sub_8260D478);
PPC_FUNC_IMPL(__imp__sub_8260D478) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// li r31,16
	r31.s64 = 16;
loc_8260D498:
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8260d498
	if (!cr6.eq) goto loc_8260D498;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8260D4C4"))) PPC_WEAK_FUNC(sub_8260D4C4);
PPC_FUNC_IMPL(__imp__sub_8260D4C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260D4C8"))) PPC_WEAK_FUNC(sub_8260D4C8);
PPC_FUNC_IMPL(__imp__sub_8260D4C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8260D4D8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x8260d4d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D4D8;
	// add r11,r4,r5
	r11.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D504:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d504
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D504;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D530:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d530
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D530;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D55C:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d55c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D55C;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D588:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d588
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D588;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D5B4:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d5b4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D5B4;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8260D5E0:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8260d5e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D5E0;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8260D604:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x8260d604
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8260D604;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260D61C"))) PPC_WEAK_FUNC(sub_8260D61C);
PPC_FUNC_IMPL(__imp__sub_8260D61C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260D620"))) PPC_WEAK_FUNC(sub_8260D620);
PPC_FUNC_IMPL(__imp__sub_8260D620) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// stw r3,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r3.u32);
	// stw r4,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r4.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r7.u32);
	// stw r8,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r8.u32);
	// beq cr6,0x8260dbd8
	if (cr6.eq) goto loc_8260DBD8;
	// rotlwi r11,r4,0
	r11.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// addi r20,r11,8
	r20.s64 = r11.s64 + 8;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// addi r15,r20,1
	r15.s64 = r20.s64 + 1;
	// addi r19,r20,-1
	r19.s64 = r20.s64 + -1;
	// addi r14,r15,1
	r14.s64 = r15.s64 + 1;
	// addi r16,r19,-1
	r16.s64 = r19.s64 + -1;
	// lwz r27,256(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 256);
	// li r11,0
	r11.s64 = 0;
	// addi r17,r16,-1
	r17.s64 = r16.s64 + -1;
	// addi r18,r17,-1
	r18.s64 = r17.s64 + -1;
	// stw r14,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r14.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addi r11,r14,1
	r11.s64 = r14.s64 + 1;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
loc_8260D694:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// lbz r24,0(r18)
	r24.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// lbz r26,0(r17)
	r26.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lbz r29,0(r16)
	r29.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// lbz r21,0(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r31,0(r19)
	r31.u64 = PPC_LOAD_U8(r19.u32 + 0);
	// lbz r30,0(r20)
	r30.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// lbz r23,0(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lbz r28,0(r15)
	r28.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// lbz r25,0(r14)
	r25.u64 = PPC_LOAD_U8(r14.u32 + 0);
	// lbz r22,0(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// bne cr6,0x8260d820
	if (!cr6.eq) goto loc_8260D820;
	// subf r11,r24,r21
	r11.s64 = r21.s64 - r24.s64;
	// li r3,1
	ctx.r3.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d6f8
	if (!cr6.gt) goto loc_8260D6F8;
	// li r3,0
	ctx.r3.s64 = 0;
loc_8260D6F8:
	// subf r11,r26,r24
	r11.s64 = r24.s64 - r26.s64;
	// li r4,1
	ctx.r4.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d718
	if (!cr6.gt) goto loc_8260D718;
	// li r4,0
	ctx.r4.s64 = 0;
loc_8260D718:
	// subf r11,r29,r26
	r11.s64 = r26.s64 - r29.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d738
	if (!cr6.gt) goto loc_8260D738;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260D738:
	// subf r11,r31,r29
	r11.s64 = r29.s64 - r31.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d758
	if (!cr6.gt) goto loc_8260D758;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260D758:
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d778
	if (!cr6.gt) goto loc_8260D778;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260D778:
	// subf r11,r28,r30
	r11.s64 = r30.s64 - r28.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d798
	if (!cr6.gt) goto loc_8260D798;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260D798:
	// subf r11,r25,r28
	r11.s64 = r28.s64 - r25.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d7b8
	if (!cr6.gt) goto loc_8260D7B8;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260D7B8:
	// subf r11,r23,r25
	r11.s64 = r25.s64 - r23.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260d7d8
	if (!cr6.gt) goto loc_8260D7D8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260D7D8:
	// subf r11,r22,r23
	r11.s64 = r23.s64 - r22.s64;
	// srawi r14,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r14.s64 = r11.s32 >> 31;
	// xor r11,r11,r14
	r11.u64 = r11.u64 ^ r14.u64;
	// subf r11,r14,r11
	r11.s64 = r11.s64 - r14.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260d7f8
	if (!cr6.gt) goto loc_8260D7F8;
	// li r11,0
	r11.s64 = 0;
loc_8260D7F8:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r14,112(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_8260D820:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x8260da48
	if (cr6.lt) goto loc_8260DA48;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82659bf8
	sub_82659BF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260db7c
	if (cr6.eq) goto loc_8260DB7C;
	// subf r11,r21,r24
	r11.s64 = r24.s64 - r21.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260d884
	if (cr6.lt) goto loc_8260D884;
	// mr r21,r24
	r21.u64 = r24.u64;
loc_8260D884:
	// subf r10,r22,r23
	ctx.r10.s64 = r23.s64 - r22.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260d8a0
	if (cr6.lt) goto loc_8260D8A0;
	// mr r22,r23
	r22.u64 = r23.u64;
loc_8260D8A0:
	// add r10,r26,r21
	ctx.r10.u64 = r26.u64 + r21.u64;
	// addi r8,r29,2
	ctx.r8.s64 = r29.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r7,r31,2
	ctx.r7.s64 = r31.s64 + 2;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r9,r31
	ctx.r8.u64 = ctx.r9.u64 + r31.u64;
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// addi r5,r30,2
	ctx.r5.s64 = r30.s64 + 2;
	// add r7,r9,r26
	ctx.r7.u64 = ctx.r9.u64 + r26.u64;
	// add r6,r8,r24
	ctx.r6.u64 = ctx.r8.u64 + r24.u64;
	// add r8,r10,r30
	ctx.r8.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r21
	ctx.r7.u64 = ctx.r7.u64 + r21.u64;
	// add r6,r8,r26
	ctx.r6.u64 = ctx.r8.u64 + r26.u64;
	// add r8,r10,r28
	ctx.r8.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r28,2
	ctx.r4.s64 = r28.s64 + 2;
	// add r5,r9,r30
	ctx.r5.u64 = ctx.r9.u64 + r30.u64;
	// add r7,r8,r31
	ctx.r7.u64 = ctx.r8.u64 + r31.u64;
	// add r8,r10,r25
	ctx.r8.u64 = ctx.r10.u64 + r25.u64;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r6,r8,r28
	ctx.r6.u64 = ctx.r8.u64 + r28.u64;
	// add r8,r9,r25
	ctx.r8.u64 = ctx.r9.u64 + r25.u64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r7,r10,r31
	ctx.r7.u64 = ctx.r10.u64 + r31.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// rlwinm r10,r21,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r24,2
	r11.s64 = r24.s64 + 2;
	// add r4,r9,r24
	ctx.r4.u64 = ctx.r9.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r21,r10
	ctx.r9.u64 = r21.u64 + ctx.r10.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r8,r8,r24
	ctx.r8.u64 = ctx.r8.u64 + r24.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 + r21.u64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// srawi r9,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 4;
	// srawi r7,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 4;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// srawi r6,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 4;
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// stb r11,0(r18)
	PPC_STORE_U8(r18.u32 + 0, r11.u8);
	// lbzx r11,r9,r27
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r27.u32);
	// stb r11,0(r17)
	PPC_STORE_U8(r17.u32 + 0, r11.u8);
	// lbzx r11,r7,r27
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r27.u32);
	// stb r11,0(r16)
	PPC_STORE_U8(r16.u32 + 0, r11.u8);
	// lbzx r11,r8,r27
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + r27.u32);
	// stb r11,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r11.u8);
	// lbzx r11,r6,r27
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + r27.u32);
	// stb r11,0(r20)
	PPC_STORE_U8(r20.u32 + 0, r11.u8);
	// lbzx r11,r10,r27
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// addi r10,r23,2
	ctx.r10.s64 = r23.s64 + 2;
	// stb r11,0(r15)
	PPC_STORE_U8(r15.u32 + 0, r11.u8);
	// add r11,r22,r25
	r11.u64 = r22.u64 + r25.u64;
	// addi r9,r11,2
	ctx.r9.s64 = r11.s64 + 2;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// stb r10,0(r14)
	PPC_STORE_U8(r14.u32 + 0, ctx.r10.u8);
	// rlwinm r10,r22,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// b 0x8260db7c
	goto loc_8260DB7C;
loc_8260DA48:
	// subf r11,r28,r29
	r11.s64 = r29.s64 - r28.s64;
	// subf r6,r30,r31
	ctx.r6.s64 = r31.s64 - r30.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r5,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r5.s64 = temp.s64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r4,r10,r11
	ctx.r4.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// bge cr6,0x8260db7c
	if (!cr6.lt) goto loc_8260DB7C;
	// subf r11,r26,r29
	r11.s64 = r29.s64 - r26.s64;
	// subf r8,r31,r24
	ctx.r8.s64 = r24.s64 - r31.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// subf r10,r28,r25
	ctx.r10.s64 = r25.s64 - r28.s64;
	// subf r9,r23,r30
	ctx.r9.s64 = r30.s64 - r23.s64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r10,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 3;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8260daf4
	if (!cr6.lt) goto loc_8260DAF4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260DAF4:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// bge cr6,0x8260db7c
	if (!cr6.lt) goto loc_8260DB7C;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// li r9,-1
	ctx.r9.s64 = -1;
	// blt cr6,0x8260db0c
	if (cr6.lt) goto loc_8260DB0C;
	// li r9,1
	ctx.r9.s64 = 1;
loc_8260DB0C:
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// ble cr6,0x8260db4c
	if (!cr6.gt) goto loc_8260DB4C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8260db40
	if (!cr6.lt) goto loc_8260DB40;
	// li r11,0
	r11.s64 = 0;
loc_8260DB40:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8260db64
	if (!cr6.gt) goto loc_8260DB64;
	// b 0x8260db60
	goto loc_8260DB60;
loc_8260DB4C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8260db58
	if (!cr6.gt) goto loc_8260DB58;
	// li r11,0
	r11.s64 = 0;
loc_8260DB58:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8260db64
	if (!cr6.lt) goto loc_8260DB64;
loc_8260DB60:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260DB64:
	// subf r10,r11,r31
	ctx.r10.s64 = r31.s64 - r11.s64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// stb r10,0(r19)
	PPC_STORE_U8(r19.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r20)
	PPC_STORE_U8(r20.u32 + 0, r11.u8);
loc_8260DB7C:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// add r18,r18,r11
	r18.u64 = r18.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// stw r14,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r14.u32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lwz r9,108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// blt cr6,0x8260d694
	if (cr6.lt) goto loc_8260D694;
loc_8260DBD8:
	// lwz r11,324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260e15c
	if (cr6.eq) goto loc_8260E15C;
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r19,r11,-1
	r19.s64 = r11.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r16,r19,-1
	r16.s64 = r19.s64 + -1;
	// addi r15,r11,1
	r15.s64 = r11.s64 + 1;
	// addi r17,r16,-1
	r17.s64 = r16.s64 + -1;
	// lwz r27,256(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	// addi r14,r15,1
	r14.s64 = r15.s64 + 1;
	// addi r18,r17,-1
	r18.s64 = r17.s64 + -1;
	// addi r11,r14,1
	r11.s64 = r14.s64 + 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r20,r19,1
	r20.s64 = r19.s64 + 1;
	// stw r14,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r14.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
loc_8260DC2C:
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lbz r24,0(r18)
	r24.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// lbz r26,0(r17)
	r26.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// lbz r29,0(r16)
	r29.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// lbz r31,0(r19)
	r31.u64 = PPC_LOAD_U8(r19.u32 + 0);
	// lbz r21,0(r11)
	r21.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lbz r30,0(r20)
	r30.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// lbz r25,0(r15)
	r25.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// lbz r23,0(r14)
	r23.u64 = PPC_LOAD_U8(r14.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lbz r28,1(r20)
	r28.u64 = PPC_LOAD_U8(r20.u32 + 1);
	// lbz r22,0(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// bne cr6,0x8260ddb4
	if (!cr6.eq) goto loc_8260DDB4;
	// subf r11,r24,r21
	r11.s64 = r21.s64 - r24.s64;
	// li r3,1
	ctx.r3.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dc8c
	if (!cr6.gt) goto loc_8260DC8C;
	// li r3,0
	ctx.r3.s64 = 0;
loc_8260DC8C:
	// subf r11,r26,r24
	r11.s64 = r24.s64 - r26.s64;
	// li r4,1
	ctx.r4.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dcac
	if (!cr6.gt) goto loc_8260DCAC;
	// li r4,0
	ctx.r4.s64 = 0;
loc_8260DCAC:
	// subf r11,r29,r26
	r11.s64 = r26.s64 - r29.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dccc
	if (!cr6.gt) goto loc_8260DCCC;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260DCCC:
	// subf r11,r31,r29
	r11.s64 = r29.s64 - r31.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dcec
	if (!cr6.gt) goto loc_8260DCEC;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260DCEC:
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dd0c
	if (!cr6.gt) goto loc_8260DD0C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260DD0C:
	// subf r11,r28,r30
	r11.s64 = r30.s64 - r28.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dd2c
	if (!cr6.gt) goto loc_8260DD2C;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260DD2C:
	// subf r11,r25,r28
	r11.s64 = r28.s64 - r25.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dd4c
	if (!cr6.gt) goto loc_8260DD4C;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260DD4C:
	// subf r11,r23,r25
	r11.s64 = r25.s64 - r23.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260dd6c
	if (!cr6.gt) goto loc_8260DD6C;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260DD6C:
	// subf r11,r22,r23
	r11.s64 = r23.s64 - r22.s64;
	// srawi r14,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r14.s64 = r11.s32 >> 31;
	// xor r11,r11,r14
	r11.u64 = r11.u64 ^ r14.u64;
	// subf r11,r14,r11
	r11.s64 = r11.s64 - r14.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260dd8c
	if (!cr6.gt) goto loc_8260DD8C;
	// li r11,0
	r11.s64 = 0;
loc_8260DD8C:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r14,112(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
loc_8260DDB4:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x8260dfd8
	if (cr6.lt) goto loc_8260DFD8;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82659bf8
	sub_82659BF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260e10c
	if (cr6.eq) goto loc_8260E10C;
	// subf r11,r21,r24
	r11.s64 = r24.s64 - r21.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260de18
	if (cr6.lt) goto loc_8260DE18;
	// mr r21,r24
	r21.u64 = r24.u64;
loc_8260DE18:
	// subf r10,r22,r23
	ctx.r10.s64 = r23.s64 - r22.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260de34
	if (cr6.lt) goto loc_8260DE34;
	// mr r22,r23
	r22.u64 = r23.u64;
loc_8260DE34:
	// add r10,r26,r21
	ctx.r10.u64 = r26.u64 + r21.u64;
	// addi r8,r29,2
	ctx.r8.s64 = r29.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r7,r31,2
	ctx.r7.s64 = r31.s64 + 2;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r9,r31
	ctx.r8.u64 = ctx.r9.u64 + r31.u64;
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// addi r5,r30,2
	ctx.r5.s64 = r30.s64 + 2;
	// add r7,r9,r26
	ctx.r7.u64 = ctx.r9.u64 + r26.u64;
	// add r6,r8,r24
	ctx.r6.u64 = ctx.r8.u64 + r24.u64;
	// add r8,r10,r30
	ctx.r8.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r21
	ctx.r7.u64 = ctx.r7.u64 + r21.u64;
	// add r6,r8,r26
	ctx.r6.u64 = ctx.r8.u64 + r26.u64;
	// add r8,r10,r28
	ctx.r8.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r28,2
	ctx.r4.s64 = r28.s64 + 2;
	// add r5,r9,r30
	ctx.r5.u64 = ctx.r9.u64 + r30.u64;
	// add r7,r8,r31
	ctx.r7.u64 = ctx.r8.u64 + r31.u64;
	// add r8,r10,r25
	ctx.r8.u64 = ctx.r10.u64 + r25.u64;
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r6,r8,r28
	ctx.r6.u64 = ctx.r8.u64 + r28.u64;
	// add r8,r9,r25
	ctx.r8.u64 = ctx.r9.u64 + r25.u64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r7,r10,r31
	ctx.r7.u64 = ctx.r10.u64 + r31.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// rlwinm r10,r21,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r24,2
	r11.s64 = r24.s64 + 2;
	// add r4,r9,r24
	ctx.r4.u64 = ctx.r9.u64 + r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r21,r10
	ctx.r9.u64 = r21.u64 + ctx.r10.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r8,r8,r24
	ctx.r8.u64 = ctx.r8.u64 + r24.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 + r21.u64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// srawi r9,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r5.s32 >> 4;
	// srawi r7,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 4;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// srawi r6,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 4;
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// stb r11,0(r18)
	PPC_STORE_U8(r18.u32 + 0, r11.u8);
	// lbzx r11,r9,r27
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r27.u32);
	// stb r11,0(r17)
	PPC_STORE_U8(r17.u32 + 0, r11.u8);
	// lbzx r11,r7,r27
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r27.u32);
	// stb r11,0(r16)
	PPC_STORE_U8(r16.u32 + 0, r11.u8);
	// lbzx r11,r8,r27
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + r27.u32);
	// stb r11,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r11.u8);
	// lbzx r11,r6,r27
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + r27.u32);
	// stb r11,0(r20)
	PPC_STORE_U8(r20.u32 + 0, r11.u8);
	// lbzx r11,r10,r27
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// addi r10,r23,2
	ctx.r10.s64 = r23.s64 + 2;
	// stb r11,1(r20)
	PPC_STORE_U8(r20.u32 + 1, r11.u8);
	// add r11,r22,r25
	r11.u64 = r22.u64 + r25.u64;
	// addi r9,r11,2
	ctx.r9.s64 = r11.s64 + 2;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// srawi r10,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// stb r10,0(r15)
	PPC_STORE_U8(r15.u32 + 0, ctx.r10.u8);
	// rlwinm r10,r22,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r14)
	PPC_STORE_U8(r14.u32 + 0, r11.u8);
	// b 0x8260e10c
	goto loc_8260E10C;
loc_8260DFD8:
	// subf r11,r28,r29
	r11.s64 = r29.s64 - r28.s64;
	// subf r6,r30,r31
	ctx.r6.s64 = r31.s64 - r30.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r5,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r5.s64 = temp.s64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r4,r10,r11
	ctx.r4.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// bge cr6,0x8260e10c
	if (!cr6.lt) goto loc_8260E10C;
	// subf r11,r26,r29
	r11.s64 = r29.s64 - r26.s64;
	// subf r8,r31,r24
	ctx.r8.s64 = r24.s64 - r31.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// subf r10,r28,r25
	ctx.r10.s64 = r25.s64 - r28.s64;
	// subf r9,r23,r30
	ctx.r9.s64 = r30.s64 - r23.s64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r10,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 3;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8260e084
	if (!cr6.lt) goto loc_8260E084;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260E084:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// bge cr6,0x8260e10c
	if (!cr6.lt) goto loc_8260E10C;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// li r9,-1
	ctx.r9.s64 = -1;
	// blt cr6,0x8260e09c
	if (cr6.lt) goto loc_8260E09C;
	// li r9,1
	ctx.r9.s64 = 1;
loc_8260E09C:
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// ble cr6,0x8260e0dc
	if (!cr6.gt) goto loc_8260E0DC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8260e0d0
	if (!cr6.lt) goto loc_8260E0D0;
	// li r11,0
	r11.s64 = 0;
loc_8260E0D0:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8260e0f4
	if (!cr6.gt) goto loc_8260E0F4;
	// b 0x8260e0f0
	goto loc_8260E0F0;
loc_8260E0DC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8260e0e8
	if (!cr6.gt) goto loc_8260E0E8;
	// li r11,0
	r11.s64 = 0;
loc_8260E0E8:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8260e0f4
	if (!cr6.lt) goto loc_8260E0F4;
loc_8260E0F0:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260E0F4:
	// subf r10,r11,r31
	ctx.r10.s64 = r31.s64 - r11.s64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// stb r10,0(r19)
	PPC_STORE_U8(r19.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r20)
	PPC_STORE_U8(r20.u32 + 0, r11.u8);
loc_8260E10C:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// add r18,r18,r11
	r18.u64 = r18.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// stw r14,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r14.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// blt cr6,0x8260dc2c
	if (cr6.lt) goto loc_8260DC2C;
loc_8260E15C:
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260e5d4
	if (cr6.eq) goto loc_8260E5D4;
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r16,r11,16
	r16.s64 = r11.s64 + 16;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r21,r16,-1
	r21.s64 = r16.s64 + -1;
	// addi r15,r16,1
	r15.s64 = r16.s64 + 1;
	// addi r17,r21,-1
	r17.s64 = r21.s64 + -1;
	// lwz r22,256(r11)
	r22.u64 = PPC_LOAD_U32(r11.u32 + 256);
	// li r11,16
	r11.s64 = 16;
	// addi r10,r15,1
	ctx.r10.s64 = r15.s64 + 1;
	// addi r18,r17,-1
	r18.s64 = r17.s64 + -1;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r19,r18,-1
	r19.s64 = r18.s64 + -1;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// addi r8,r19,-1
	ctx.r8.s64 = r19.s64 + -1;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
loc_8260E1B4:
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// li r3,1
	ctx.r3.s64 = 1;
	// lbz r30,0(r19)
	r30.u64 = PPC_LOAD_U8(r19.u32 + 0);
	// lbz r24,0(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r10,r30,r31
	ctx.r10.s64 = r31.s64 - r30.s64;
	// lbz r23,0(r9)
	r23.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r27,0(r18)
	r27.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// lbz r29,0(r17)
	r29.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// lbz r25,0(r21)
	r25.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// lbz r28,0(r16)
	r28.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// lbz r26,0(r15)
	r26.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e1fc
	if (!cr6.gt) goto loc_8260E1FC;
	// li r3,0
	ctx.r3.s64 = 0;
loc_8260E1FC:
	// subf r10,r27,r30
	ctx.r10.s64 = r30.s64 - r27.s64;
	// li r4,1
	ctx.r4.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e21c
	if (!cr6.gt) goto loc_8260E21C;
	// li r4,0
	ctx.r4.s64 = 0;
loc_8260E21C:
	// subf r10,r29,r27
	ctx.r10.s64 = r27.s64 - r29.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e23c
	if (!cr6.gt) goto loc_8260E23C;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260E23C:
	// subf r10,r25,r29
	ctx.r10.s64 = r29.s64 - r25.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e25c
	if (!cr6.gt) goto loc_8260E25C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260E25C:
	// subf r20,r28,r25
	r20.s64 = r25.s64 - r28.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e280
	if (!cr6.gt) goto loc_8260E280;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260E280:
	// subf r10,r26,r28
	ctx.r10.s64 = r28.s64 - r26.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e2a0
	if (!cr6.gt) goto loc_8260E2A0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260E2A0:
	// subf r10,r24,r26
	ctx.r10.s64 = r26.s64 - r24.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260e2c0
	if (!cr6.gt) goto loc_8260E2C0;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260E2C0:
	// subf r10,r23,r24
	ctx.r10.s64 = r24.s64 - r23.s64;
	// srawi r14,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	r14.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r14
	ctx.r10.u64 = ctx.r10.u64 ^ r14.u64;
	// subf r10,r14,r10
	ctx.r10.s64 = ctx.r10.s64 - r14.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x8260e2e0
	if (!cr6.gt) goto loc_8260E2E0;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260E2E0:
	// subf r11,r11,r23
	r11.s64 = r23.s64 - r11.s64;
	// srawi r14,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r14.s64 = r11.s32 >> 31;
	// xor r11,r11,r14
	r11.u64 = r11.u64 ^ r14.u64;
	// subf r11,r14,r11
	r11.s64 = r11.s64 - r14.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260e300
	if (!cr6.gt) goto loc_8260E300;
	// li r11,0
	r11.s64 = 0;
loc_8260E300:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x8260e450
	if (cr6.lt) goto loc_8260E450;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82659bf8
	sub_82659BF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260e574
	if (cr6.eq) goto loc_8260E574;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8260e380
	if (cr6.lt) goto loc_8260E380;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_8260E380:
	// add r10,r27,r31
	ctx.r10.u64 = r27.u64 + r31.u64;
	// addi r8,r29,2
	ctx.r8.s64 = r29.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r7,r25,2
	ctx.r7.s64 = r25.s64 + 2;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r9,r25
	ctx.r8.u64 = ctx.r9.u64 + r25.u64;
	// add r9,r10,r28
	ctx.r9.u64 = ctx.r10.u64 + r28.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// add r7,r8,r30
	ctx.r7.u64 = ctx.r8.u64 + r30.u64;
	// add r8,r9,r27
	ctx.r8.u64 = ctx.r9.u64 + r27.u64;
	// add r9,r10,r28
	ctx.r9.u64 = ctx.r10.u64 + r28.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// add r7,r9,r27
	ctx.r7.u64 = ctx.r9.u64 + r27.u64;
	// add r6,r10,r28
	ctx.r6.u64 = ctx.r10.u64 + r28.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r10,r10,r24
	ctx.r10.u64 = ctx.r10.u64 + r24.u64;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// add r8,r10,r31
	ctx.r8.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// srawi r10,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 4;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// stb r11,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r11.u8);
	// lbzx r11,r10,r22
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r22.u32);
	// stb r11,0(r18)
	PPC_STORE_U8(r18.u32 + 0, r11.u8);
	// lbzx r11,r9,r22
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r22.u32);
	// stb r11,0(r17)
	PPC_STORE_U8(r17.u32 + 0, r11.u8);
	// lbzx r11,r8,r22
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + r22.u32);
	// b 0x8260e570
	goto loc_8260E570;
loc_8260E450:
	// subf r11,r26,r29
	r11.s64 = r29.s64 - r26.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r20,2,0,29
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r20,r11
	r11.u64 = r20.u64 + r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r6,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r6.s64 = temp.s64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x8260e574
	if (!cr6.lt) goto loc_8260E574;
	// subf r11,r27,r29
	r11.s64 = r29.s64 - r27.s64;
	// subf r8,r25,r30
	ctx.r8.s64 = r30.s64 - r25.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// subf r10,r26,r24
	ctx.r10.s64 = r24.s64 - r26.s64;
	// subf r9,r23,r28
	ctx.r9.s64 = r28.s64 - r23.s64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r10,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 3;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8260e4f8
	if (!cr6.lt) goto loc_8260E4F8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260E4F8:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x8260e574
	if (!cr6.lt) goto loc_8260E574;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// li r9,-1
	ctx.r9.s64 = -1;
	// blt cr6,0x8260e510
	if (cr6.lt) goto loc_8260E510;
	// li r9,1
	ctx.r9.s64 = 1;
loc_8260E510:
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// srawi r10,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	ctx.r10.s64 = r20.s32 >> 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// ble cr6,0x8260e550
	if (!cr6.gt) goto loc_8260E550;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8260e544
	if (!cr6.lt) goto loc_8260E544;
	// li r11,0
	r11.s64 = 0;
loc_8260E544:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8260e568
	if (!cr6.gt) goto loc_8260E568;
	// b 0x8260e564
	goto loc_8260E564;
loc_8260E550:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8260e55c
	if (!cr6.gt) goto loc_8260E55C;
	// li r11,0
	r11.s64 = 0;
loc_8260E55C:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8260e568
	if (!cr6.lt) goto loc_8260E568;
loc_8260E564:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8260E568:
	// subf r11,r11,r25
	r11.s64 = r25.s64 - r11.s64;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
loc_8260E570:
	// stb r11,0(r21)
	PPC_STORE_U8(r21.u32 + 0, r11.u8);
loc_8260E574:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// add r18,r18,r11
	r18.u64 = r18.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// add r16,r16,r11
	r16.u64 = r16.u64 + r11.u64;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// bne cr6,0x8260e1b4
	if (!cr6.eq) goto loc_8260E1B4;
loc_8260E5D4:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8260E5DC"))) PPC_WEAK_FUNC(sub_8260E5DC);
PPC_FUNC_IMPL(__imp__sub_8260E5DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260E5E0"))) PPC_WEAK_FUNC(sub_8260E5E0);
PPC_FUNC_IMPL(__imp__sub_8260E5E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,324(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// stw r3,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, ctx.r3.u32);
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r7.u32);
	// stw r8,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r8.u32);
	// beq cr6,0x8260e950
	if (cr6.eq) goto loc_8260E950;
	// rotlwi r11,r4,0
	r11.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// li r16,0
	r16.s64 = 0;
	// addi r22,r11,8
	r22.s64 = r11.s64 + 8;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// addi r21,r22,-1
	r21.s64 = r22.s64 + -1;
	// addi r18,r22,1
	r18.s64 = r22.s64 + 1;
	// addi r19,r21,-1
	r19.s64 = r21.s64 + -1;
	// addi r17,r18,1
	r17.s64 = r18.s64 + 1;
	// addi r20,r19,-1
	r20.s64 = r19.s64 + -1;
	// lwz r27,256(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 256);
	// addi r14,r17,1
	r14.s64 = r17.s64 + 1;
	// addi r15,r20,-1
	r15.s64 = r20.s64 + -1;
loc_8260E638:
	// clrlwi r11,r16,30
	r11.u64 = r16.u32 & 0x3;
	// lbz r23,0(r15)
	r23.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// lbz r26,0(r20)
	r26.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// lbz r29,0(r19)
	r29.u64 = PPC_LOAD_U8(r19.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r31,0(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// lbz r30,0(r22)
	r30.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// lbz r28,0(r18)
	r28.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// lbz r25,0(r17)
	r25.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// lbz r24,0(r14)
	r24.u64 = PPC_LOAD_U8(r14.u32 + 0);
	// bne cr6,0x8260e760
	if (!cr6.eq) goto loc_8260E760;
	// subf r11,r26,r23
	r11.s64 = r23.s64 - r26.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e684
	if (!cr6.gt) goto loc_8260E684;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260E684:
	// subf r11,r29,r26
	r11.s64 = r26.s64 - r29.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e6a4
	if (!cr6.gt) goto loc_8260E6A4;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260E6A4:
	// subf r11,r31,r29
	r11.s64 = r29.s64 - r31.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e6c4
	if (!cr6.gt) goto loc_8260E6C4;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260E6C4:
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e6e4
	if (!cr6.gt) goto loc_8260E6E4;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260E6E4:
	// subf r11,r28,r30
	r11.s64 = r30.s64 - r28.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e704
	if (!cr6.gt) goto loc_8260E704;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260E704:
	// subf r11,r25,r28
	r11.s64 = r28.s64 - r25.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e724
	if (!cr6.gt) goto loc_8260E724;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260E724:
	// subf r11,r24,r25
	r11.s64 = r25.s64 - r24.s64;
	// srawi r4,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r11.s32 >> 31;
	// xor r11,r11,r4
	r11.u64 = r11.u64 ^ ctx.r4.u64;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260e744
	if (!cr6.gt) goto loc_8260E744;
	// li r11,0
	r11.s64 = 0;
loc_8260E744:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_8260E760:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x8260e8d8
	if (cr6.lt) goto loc_8260E8D8;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82659ce0
	sub_82659CE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260e920
	if (cr6.eq) goto loc_8260E920;
	// subf r11,r23,r26
	r11.s64 = r26.s64 - r23.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260e7b8
	if (cr6.lt) goto loc_8260E7B8;
	// mr r23,r26
	r23.u64 = r26.u64;
loc_8260E7B8:
	// subf r10,r24,r25
	ctx.r10.s64 = r25.s64 - r24.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260e7d4
	if (cr6.lt) goto loc_8260E7D4;
	// mr r24,r25
	r24.u64 = r25.u64;
loc_8260E7D4:
	// add r11,r29,r23
	r11.u64 = r29.u64 + r23.u64;
	// addi r8,r31,2
	ctx.r8.s64 = r31.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r7,r30,2
	ctx.r7.s64 = r30.s64 + 2;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r8,r9,r30
	ctx.r8.u64 = ctx.r9.u64 + r30.u64;
	// add r9,r11,r28
	ctx.r9.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r7,r8,r26
	ctx.r7.u64 = ctx.r8.u64 + r26.u64;
	// add r8,r24,r28
	ctx.r8.u64 = r24.u64 + r28.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// add r6,r9,r23
	ctx.r6.u64 = ctx.r9.u64 + r23.u64;
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r5,r9,r26
	ctx.r5.u64 = ctx.r9.u64 + r26.u64;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// addi r8,r25,2
	ctx.r8.s64 = r25.s64 + 2;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// addi r10,r26,2
	ctx.r10.s64 = r26.s64 + 2;
	// add r4,r9,r29
	ctx.r4.u64 = ctx.r9.u64 + r29.u64;
	// rlwinm r9,r23,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r23,r9
	ctx.r8.u64 = r23.u64 + ctx.r9.u64;
	// rlwinm r9,r24,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r24,r9
	ctx.r9.u64 = r24.u64 + ctx.r9.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// srawi r9,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 3;
	// srawi r8,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 3;
	// srawi r7,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 3;
	// srawi r6,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 3;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stb r10,0(r20)
	PPC_STORE_U8(r20.u32 + 0, ctx.r10.u8);
	// lbzx r10,r9,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + r27.u32);
	// stb r10,0(r19)
	PPC_STORE_U8(r19.u32 + 0, ctx.r10.u8);
	// lbzx r10,r8,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + r27.u32);
	// stb r10,0(r21)
	PPC_STORE_U8(r21.u32 + 0, ctx.r10.u8);
	// lbzx r10,r7,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r7.u32 + r27.u32);
	// stb r10,0(r22)
	PPC_STORE_U8(r22.u32 + 0, ctx.r10.u8);
	// lbzx r10,r6,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + r27.u32);
	// stb r10,0(r18)
	PPC_STORE_U8(r18.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r17)
	PPC_STORE_U8(r17.u32 + 0, r11.u8);
	// b 0x8260e920
	goto loc_8260E920;
loc_8260E8D8:
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260e920
	if (cr6.eq) goto loc_8260E920;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r9,332(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x8260e920
	if (!cr6.lt) goto loc_8260E920;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// stb r10,0(r21)
	PPC_STORE_U8(r21.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r22)
	PPC_STORE_U8(r22.u32 + 0, r11.u8);
loc_8260E920:
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// addi r16,r16,1
	r16.s64 = r16.s64 + 1;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r22,r22,r11
	r22.u64 = r22.u64 + r11.u64;
	// add r18,r18,r11
	r18.u64 = r18.u64 + r11.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// cmpwi cr6,r16,16
	cr6.compare<int32_t>(r16.s32, 16, xer);
	// blt cr6,0x8260e638
	if (cr6.lt) goto loc_8260E638;
loc_8260E950:
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260ec9c
	if (cr6.eq) goto loc_8260EC9C;
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// li r17,0
	r17.s64 = 0;
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// addi r21,r11,-1
	r21.s64 = r11.s64 + -1;
	// lwz r16,80(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r19,r21,-1
	r19.s64 = r21.s64 + -1;
	// addi r18,r11,1
	r18.s64 = r11.s64 + 1;
	// addi r20,r19,-1
	r20.s64 = r19.s64 + -1;
	// lwz r27,256(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	// addi r22,r21,1
	r22.s64 = r21.s64 + 1;
	// addi r14,r18,1
	r14.s64 = r18.s64 + 1;
	// addi r15,r20,-1
	r15.s64 = r20.s64 + -1;
loc_8260E990:
	// clrlwi r11,r17,30
	r11.u64 = r17.u32 & 0x3;
	// lbz r23,0(r15)
	r23.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// lbz r26,0(r20)
	r26.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// lbz r29,0(r19)
	r29.u64 = PPC_LOAD_U8(r19.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r31,0(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// lbz r30,0(r22)
	r30.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// lbz r25,0(r18)
	r25.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// lbz r24,0(r14)
	r24.u64 = PPC_LOAD_U8(r14.u32 + 0);
	// lbz r28,1(r22)
	r28.u64 = PPC_LOAD_U8(r22.u32 + 1);
	// bne cr6,0x8260eab4
	if (!cr6.eq) goto loc_8260EAB4;
	// subf r11,r26,r23
	r11.s64 = r23.s64 - r26.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e9dc
	if (!cr6.gt) goto loc_8260E9DC;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260E9DC:
	// subf r11,r29,r26
	r11.s64 = r26.s64 - r29.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260e9fc
	if (!cr6.gt) goto loc_8260E9FC;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260E9FC:
	// subf r11,r31,r29
	r11.s64 = r29.s64 - r31.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260ea1c
	if (!cr6.gt) goto loc_8260EA1C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260EA1C:
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260ea3c
	if (!cr6.gt) goto loc_8260EA3C;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260EA3C:
	// subf r11,r28,r30
	r11.s64 = r30.s64 - r28.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260ea5c
	if (!cr6.gt) goto loc_8260EA5C;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260EA5C:
	// subf r11,r25,r28
	r11.s64 = r28.s64 - r25.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x8260ea7c
	if (!cr6.gt) goto loc_8260EA7C;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260EA7C:
	// subf r11,r24,r25
	r11.s64 = r25.s64 - r24.s64;
	// srawi r4,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r11.s32 >> 31;
	// xor r11,r11,r4
	r11.u64 = r11.u64 ^ ctx.r4.u64;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260ea9c
	if (!cr6.gt) goto loc_8260EA9C;
	// li r11,0
	r11.s64 = 0;
loc_8260EA9C:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r16,r11,r5
	r16.u64 = r11.u64 + ctx.r5.u64;
loc_8260EAB4:
	// cmpwi cr6,r16,5
	cr6.compare<int32_t>(r16.s32, 5, xer);
	// blt cr6,0x8260ec28
	if (cr6.lt) goto loc_8260EC28;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82659ce0
	sub_82659CE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260ec70
	if (cr6.eq) goto loc_8260EC70;
	// subf r11,r23,r26
	r11.s64 = r26.s64 - r23.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260eb08
	if (cr6.lt) goto loc_8260EB08;
	// mr r23,r26
	r23.u64 = r26.u64;
loc_8260EB08:
	// subf r10,r24,r25
	ctx.r10.s64 = r25.s64 - r24.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x8260eb24
	if (cr6.lt) goto loc_8260EB24;
	// mr r24,r25
	r24.u64 = r25.u64;
loc_8260EB24:
	// add r11,r29,r23
	r11.u64 = r29.u64 + r23.u64;
	// addi r8,r31,2
	ctx.r8.s64 = r31.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r7,r30,2
	ctx.r7.s64 = r30.s64 + 2;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r8,r9,r30
	ctx.r8.u64 = ctx.r9.u64 + r30.u64;
	// add r9,r11,r28
	ctx.r9.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r7,r8,r26
	ctx.r7.u64 = ctx.r8.u64 + r26.u64;
	// add r8,r24,r28
	ctx.r8.u64 = r24.u64 + r28.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// add r6,r9,r23
	ctx.r6.u64 = ctx.r9.u64 + r23.u64;
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r5,r9,r26
	ctx.r5.u64 = ctx.r9.u64 + r26.u64;
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// addi r8,r25,2
	ctx.r8.s64 = r25.s64 + 2;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// addi r10,r26,2
	ctx.r10.s64 = r26.s64 + 2;
	// add r4,r9,r29
	ctx.r4.u64 = ctx.r9.u64 + r29.u64;
	// rlwinm r9,r23,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r8,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r23,r9
	ctx.r8.u64 = r23.u64 + ctx.r9.u64;
	// rlwinm r9,r24,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r9,r24,r9
	ctx.r9.u64 = r24.u64 + ctx.r9.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// srawi r9,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 3;
	// srawi r8,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 3;
	// srawi r7,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 3;
	// srawi r6,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r4.s32 >> 3;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stb r10,0(r20)
	PPC_STORE_U8(r20.u32 + 0, ctx.r10.u8);
	// lbzx r10,r9,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + r27.u32);
	// stb r10,0(r19)
	PPC_STORE_U8(r19.u32 + 0, ctx.r10.u8);
	// lbzx r10,r8,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + r27.u32);
	// stb r10,0(r21)
	PPC_STORE_U8(r21.u32 + 0, ctx.r10.u8);
	// lbzx r10,r7,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r7.u32 + r27.u32);
	// stb r10,0(r22)
	PPC_STORE_U8(r22.u32 + 0, ctx.r10.u8);
	// lbzx r10,r6,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + r27.u32);
	// stb r10,1(r22)
	PPC_STORE_U8(r22.u32 + 1, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r18)
	PPC_STORE_U8(r18.u32 + 0, r11.u8);
	// b 0x8260ec70
	goto loc_8260EC70;
loc_8260EC28:
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260ec70
	if (cr6.eq) goto loc_8260EC70;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r9,332(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x8260ec70
	if (!cr6.lt) goto loc_8260EC70;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// add r10,r31,r27
	ctx.r10.u64 = r31.u64 + r27.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// stb r10,0(r21)
	PPC_STORE_U8(r21.u32 + 0, ctx.r10.u8);
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,0(r22)
	PPC_STORE_U8(r22.u32 + 0, r11.u8);
loc_8260EC70:
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r20,r20,r11
	r20.u64 = r20.u64 + r11.u64;
	// add r19,r19,r11
	r19.u64 = r19.u64 + r11.u64;
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r22,r22,r11
	r22.u64 = r22.u64 + r11.u64;
	// add r18,r18,r11
	r18.u64 = r18.u64 + r11.u64;
	// add r14,r14,r11
	r14.u64 = r14.u64 + r11.u64;
	// cmpwi cr6,r17,16
	cr6.compare<int32_t>(r17.s32, 16, xer);
	// blt cr6,0x8260e990
	if (cr6.lt) goto loc_8260E990;
loc_8260EC9C:
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260ef08
	if (cr6.eq) goto loc_8260EF08;
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// li r19,0
	r19.s64 = 0;
	// lwz r14,80(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r20,r11,16
	r20.s64 = r11.s64 + 16;
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lwz r4,340(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// addi r25,r20,-1
	r25.s64 = r20.s64 + -1;
	// addi r18,r20,1
	r18.s64 = r20.s64 + 1;
	// addi r23,r25,-1
	r23.s64 = r25.s64 + -1;
	// addi r17,r18,1
	r17.s64 = r18.s64 + 1;
	// lwz r21,256(r11)
	r21.u64 = PPC_LOAD_U32(r11.u32 + 256);
	// addi r24,r23,-1
	r24.s64 = r23.s64 + -1;
	// addi r15,r17,1
	r15.s64 = r17.s64 + 1;
	// addi r16,r24,-1
	r16.s64 = r24.s64 + -1;
loc_8260ECE0:
	// clrlwi r11,r19,30
	r11.u64 = r19.u32 & 0x3;
	// lbz r31,0(r16)
	r31.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// lbz r30,0(r24)
	r30.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r27,0(r23)
	r27.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// lbz r29,0(r25)
	r29.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// lbz r28,0(r20)
	r28.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// lbz r26,0(r18)
	r26.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// lbz r22,0(r17)
	r22.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// lbz r11,0(r15)
	r11.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// bne cr6,0x8260ee04
	if (!cr6.eq) goto loc_8260EE04;
	// subf r10,r30,r31
	ctx.r10.s64 = r31.s64 - r30.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260ed2c
	if (!cr6.gt) goto loc_8260ED2C;
	// li r5,0
	ctx.r5.s64 = 0;
loc_8260ED2C:
	// subf r10,r27,r30
	ctx.r10.s64 = r30.s64 - r27.s64;
	// li r6,1
	ctx.r6.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260ed4c
	if (!cr6.gt) goto loc_8260ED4C;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8260ED4C:
	// subf r10,r29,r27
	ctx.r10.s64 = r27.s64 - r29.s64;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260ed6c
	if (!cr6.gt) goto loc_8260ED6C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8260ED6C:
	// subf r10,r28,r29
	ctx.r10.s64 = r29.s64 - r28.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260ed8c
	if (!cr6.gt) goto loc_8260ED8C;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8260ED8C:
	// subf r10,r26,r28
	ctx.r10.s64 = r28.s64 - r26.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// ble cr6,0x8260edac
	if (!cr6.gt) goto loc_8260EDAC;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8260EDAC:
	// subf r10,r22,r26
	ctx.r10.s64 = r26.s64 - r22.s64;
	// srawi r3,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r3.u64;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x8260edcc
	if (!cr6.gt) goto loc_8260EDCC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8260EDCC:
	// subf r11,r11,r22
	r11.s64 = r22.s64 - r11.s64;
	// srawi r3,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r11.s32 >> 31;
	// xor r11,r11,r3
	r11.u64 = r11.u64 ^ ctx.r3.u64;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,1
	r11.s64 = 1;
	// ble cr6,0x8260edec
	if (!cr6.gt) goto loc_8260EDEC;
	// li r11,0
	r11.s64 = 0;
loc_8260EDEC:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r14,r11,r5
	r14.u64 = r11.u64 + ctx.r5.u64;
loc_8260EE04:
	// cmpwi cr6,r14,5
	cr6.compare<int32_t>(r14.s32, 5, xer);
	// blt cr6,0x8260ef10
	if (cr6.lt) goto loc_8260EF10;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82659ce0
	sub_82659CE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8260eed8
	if (cr6.eq) goto loc_8260EED8;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,332(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8260ee58
	if (cr6.lt) goto loc_8260EE58;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_8260EE58:
	// add r10,r27,r31
	ctx.r10.u64 = r27.u64 + r31.u64;
	// addi r8,r29,2
	ctx.r8.s64 = r29.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + r26.u64;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r8,r10,r31
	ctx.r8.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 3;
	// lbzx r11,r11,r21
	r11.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// stb r11,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r11.u8);
	// lbzx r11,r10,r21
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r21.u32);
	// stb r11,0(r23)
	PPC_STORE_U8(r23.u32 + 0, r11.u8);
	// lbzx r11,r9,r21
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r21.u32);
	// stb r11,0(r25)
	PPC_STORE_U8(r25.u32 + 0, r11.u8);
loc_8260EED8:
	// lwz r4,340(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
loc_8260EEDC:
	// addi r19,r19,1
	r19.s64 = r19.s64 + 1;
	// add r16,r16,r4
	r16.u64 = r16.u64 + ctx.r4.u64;
	// add r24,r24,r4
	r24.u64 = r24.u64 + ctx.r4.u64;
	// add r23,r23,r4
	r23.u64 = r23.u64 + ctx.r4.u64;
	// add r25,r25,r4
	r25.u64 = r25.u64 + ctx.r4.u64;
	// add r20,r20,r4
	r20.u64 = r20.u64 + ctx.r4.u64;
	// add r18,r18,r4
	r18.u64 = r18.u64 + ctx.r4.u64;
	// add r17,r17,r4
	r17.u64 = r17.u64 + ctx.r4.u64;
	// add r15,r15,r4
	r15.u64 = r15.u64 + ctx.r4.u64;
	// cmpwi cr6,r19,16
	cr6.compare<int32_t>(r19.s32, 16, xer);
	// blt cr6,0x8260ece0
	if (cr6.lt) goto loc_8260ECE0;
loc_8260EF08:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_8260EF10:
	// subf r11,r29,r28
	r11.s64 = r28.s64 - r29.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8260eedc
	if (cr6.eq) goto loc_8260EEDC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r9,332(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x8260eedc
	if (!cr6.lt) goto loc_8260EEDC;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// lbzx r11,r11,r21
	r11.u64 = PPC_LOAD_U8(r11.u32 + r21.u32);
	// stb r11,0(r25)
	PPC_STORE_U8(r25.u32 + 0, r11.u8);
	// b 0x8260eedc
	goto loc_8260EEDC;
}

__attribute__((alias("__imp__sub_8260EF50"))) PPC_WEAK_FUNC(sub_8260EF50);
PPC_FUNC_IMPL(__imp__sub_8260EF50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r30,-32126
	r30.s64 = -2105409536;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r22,r28,8
	r22.s64 = r28.s64 + 8;
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// add r27,r11,r28
	r27.u64 = r11.u64 + r28.u64;
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r21,r27,8
	r21.s64 = r27.s64 + 8;
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// addi r5,r1,124
	ctx.r5.s64 = ctx.r1.s64 + 124;
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,13636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13636);
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// addi r5,r1,132
	ctx.r5.s64 = ctx.r1.s64 + 132;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// bgt cr6,0x8260f04c
	if (cr6.gt) goto loc_8260F04C;
	// li r11,1
	r11.s64 = 1;
loc_8260F04C:
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8260f068
	if (cr6.gt) goto loc_8260F068;
	// li r11,2
	r11.s64 = 2;
loc_8260F068:
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8260f084
	if (cr6.gt) goto loc_8260F084;
	// li r11,3
	r11.s64 = 3;
loc_8260F084:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// lwzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
loc_8260F094:
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// cmpwi cr6,r7,32
	cr6.compare<int32_t>(ctx.r7.s32, 32, xer);
	// bge cr6,0x8260f0bc
	if (!cr6.lt) goto loc_8260F0BC;
	// cmpwi cr6,r9,64
	cr6.compare<int32_t>(ctx.r9.s32, 64, xer);
	// blt cr6,0x8260f0bc
	if (cr6.lt) goto loc_8260F0BC;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwzx r7,r10,r7
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	// stwx r7,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r7.u32);
loc_8260F0BC:
	// cmpwi cr6,r9,16
	cr6.compare<int32_t>(ctx.r9.s32, 16, xer);
	// bge cr6,0x8260f0cc
	if (!cr6.lt) goto loc_8260F0CC;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stwx r8,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r8.u32);
loc_8260F0CC:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpwi cr6,r11,16
	cr6.compare<int32_t>(r11.s32, 16, xer);
	// blt cr6,0x8260f094
	if (cr6.lt) goto loc_8260F094;
	// rlwinm r30,r29,1,0,30
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lis r29,-32126
	r29.s64 = -2105409536;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,92(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,13644(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 13644);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_8260F190"))) PPC_WEAK_FUNC(sub_8260F190);
PPC_FUNC_IMPL(__imp__sub_8260F190) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x8260f22c
	if (!cr6.eq) goto loc_8260F22C;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// beq cr6,0x8260f1bc
	if (cr6.eq) goto loc_8260F1BC;
	// lis r11,-32154
	r11.s64 = -2107244544;
	// lis r10,-32153
	ctx.r10.s64 = -2107179008;
	// addi r11,r11,26760
	r11.s64 = r11.s64 + 26760;
	// stw r11,13660(r9)
	PPC_STORE_U32(ctx.r9.u32 + 13660, r11.u32);
	// addi r11,r10,-32624
	r11.s64 = ctx.r10.s64 + -32624;
	// b 0x8260f1d0
	goto loc_8260F1D0;
loc_8260F1BC:
	// lis r11,-32153
	r11.s64 = -2107179008;
	// lis r10,-32153
	ctx.r10.s64 = -2107179008;
	// addi r11,r11,-29504
	r11.s64 = r11.s64 + -29504;
	// stw r11,13660(r9)
	PPC_STORE_U32(ctx.r9.u32 + 13660, r11.u32);
	// addi r11,r10,-21752
	r11.s64 = ctx.r10.s64 + -21752;
loc_8260F1D0:
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// lis r6,-32126
	ctx.r6.s64 = -2105409536;
	// lis r7,-32159
	ctx.r7.s64 = -2107572224;
	// lis r8,-32154
	ctx.r8.s64 = -2107244544;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// stw r11,13656(r9)
	PPC_STORE_U32(ctx.r9.u32 + 13656, r11.u32);
	// lis r11,-32159
	r11.s64 = -2107572224;
	// lis r9,-32159
	ctx.r9.s64 = -2107572224;
	// addi r11,r11,-11064
	r11.s64 = r11.s64 + -11064;
	// stw r11,13664(r6)
	PPC_STORE_U32(ctx.r6.u32 + 13664, r11.u32);
	// addi r11,r7,-11144
	r11.s64 = ctx.r7.s64 + -11144;
	// lis r6,-32126
	ctx.r6.s64 = -2105409536;
	// lis r7,-32126
	ctx.r7.s64 = -2105409536;
	// stw r11,13652(r6)
	PPC_STORE_U32(ctx.r6.u32 + 13652, r11.u32);
	// addi r11,r8,25320
	r11.s64 = ctx.r8.s64 + 25320;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// stw r11,13644(r7)
	PPC_STORE_U32(ctx.r7.u32 + 13644, r11.u32);
	// addi r11,r9,-11440
	r11.s64 = ctx.r9.s64 + -11440;
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// stw r11,13636(r8)
	PPC_STORE_U32(ctx.r8.u32 + 13636, r11.u32);
	// addi r11,r10,-4272
	r11.s64 = ctx.r10.s64 + -4272;
	// stw r11,13640(r9)
	PPC_STORE_U32(ctx.r9.u32 + 13640, r11.u32);
	// blr 
	return;
loc_8260F22C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// beq cr6,0x8260f248
	if (cr6.eq) goto loc_8260F248;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r11,r11,-6688
	r11.s64 = r11.s64 + -6688;
	// stw r11,13648(r10)
	PPC_STORE_U32(ctx.r10.u32 + 13648, r11.u32);
	// blr 
	return;
loc_8260F248:
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r11,r11,-10720
	r11.s64 = r11.s64 + -10720;
	// stw r11,13648(r10)
	PPC_STORE_U32(ctx.r10.u32 + 13648, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8260F258"))) PPC_WEAK_FUNC(sub_8260F258);
PPC_FUNC_IMPL(__imp__sub_8260F258) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r22,348(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// lwz r23,340(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// clrlwi r8,r8,28
	ctx.r8.u64 = ctx.r8.u32 & 0xF;
	// lwz r11,13632(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 13632);
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// stw r21,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r21.u32);
	// rlwinm r6,r11,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// mr r31,r29
	r31.u64 = r29.u64;
	// stw r25,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r25.u32);
	// add r27,r6,r9
	r27.u64 = ctx.r6.u64 + ctx.r9.u64;
	// cmpw cr6,r4,r25
	cr6.compare<int32_t>(ctx.r4.s32, r25.s32, xer);
	// srawi r24,r27,3
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7) != 0);
	r24.s64 = r27.s32 >> 3;
	// subfic r6,r22,0
	xer.ca = r22.u32 <= 0;
	ctx.r6.s64 = 0 - r22.s64;
	// subf r5,r11,r27
	ctx.r5.s64 = r27.s64 - r11.s64;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// subfe r6,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + xer.ca < xer.ca);
	ctx.r6.u64 = ~ctx.r6.u64 + ctx.r6.u64 + xer.ca;
	xer.ca = temp.u8;
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r27.u32);
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r24,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r24.u32);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// andi. r6,r6,20
	ctx.r6.u64 = ctx.r6.u64 & 20;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r26,r6,20
	r26.s64 = ctx.r6.s64 + 20;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// bge cr6,0x8260f37c
	if (!cr6.lt) goto loc_8260F37C;
	// addi r3,r5,-16
	ctx.r3.s64 = ctx.r5.s64 + -16;
	// subf r30,r29,r10
	r30.s64 = ctx.r10.s64 - r29.s64;
	// subf r5,r4,r25
	ctx.r5.s64 = r25.s64 - ctx.r4.s64;
loc_8260F2E8:
	// lbzx r11,r30,r31
	r11.u64 = PPC_LOAD_U8(r30.u32 + r31.u32);
	// add r10,r3,r31
	ctx.r10.u64 = ctx.r3.u64 + r31.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// vspltb v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xC))));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vspltb v13,v13,3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_set1_epi8(char(0xC))));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8260f34c
	if (!cr6.gt) goto loc_8260F34C;
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
loc_8260F338:
	// lbz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stbx r4,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x8260f338
	if (cr6.lt) goto loc_8260F338;
loc_8260F34C:
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r6,16
	ctx.r10.s64 = ctx.r6.s64 + 16;
	// stvx v13,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// add r31,r31,r23
	r31.u64 = r31.u64 + r23.u64;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x8260f2e8
	if (!cr6.eq) goto loc_8260F2E8;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
loc_8260F37C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8260f3fc
	if (cr6.eq) goto loc_8260F3FC;
	// mullw r11,r26,r23
	r11.s64 = int64_t(r26.s32) * int64_t(r23.s32);
	// subf r30,r11,r29
	r30.s64 = r29.s64 - r11.s64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r28,r24,3,0,27
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF0;
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
	// bl 0x826a85e8
	sub_826A85E8(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// ble cr6,0x8260f3fc
	if (!cr6.gt) goto loc_8260F3FC;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
loc_8260F3B8:
	// li r11,0
	r11.s64 = 0;
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r30.u32);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// ble cr6,0x8260f3e4
	if (!cr6.gt) goto loc_8260F3E4;
loc_8260F3CC:
	// lvlx v0,r29,r11
	temp.u32 = r29.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx128 v0,r11,r30
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// blt cr6,0x8260f3cc
	if (cr6.lt) goto loc_8260F3CC;
loc_8260F3E4:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r30,r30,r23
	r30.u64 = r30.u64 + r23.u64;
	// cmpw cr6,r10,r26
	cr6.compare<int32_t>(ctx.r10.s32, r26.s32, xer);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
	// blt cr6,0x8260f3b8
	if (cr6.lt) goto loc_8260F3B8;
loc_8260F3FC:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x8260f49c
	if (cr6.eq) goto loc_8260F49C;
	// subf r29,r23,r31
	r29.s64 = r31.s64 - r23.s64;
	// rlwinm r30,r24,3,0,27
	r30.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF0;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// neg r11,r25
	r11.s64 = -r25.s64;
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r29.u32);
	// stw r30,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r30.u32);
	// beq cr6,0x8260f428
	if (cr6.eq) goto loc_8260F428;
	// clrlwi r11,r11,27
	r11.u64 = r11.u32 & 0x1F;
	// b 0x8260f42c
	goto loc_8260F42C;
loc_8260F428:
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
loc_8260F42C:
	// add r28,r11,r26
	r28.u64 = r11.u64 + r26.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// bl 0x826a85e8
	sub_826A85E8(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// ble cr6,0x8260f49c
	if (!cr6.gt) goto loc_8260F49C;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
loc_8260F458:
	// li r11,0
	r11.s64 = 0;
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// ble cr6,0x8260f484
	if (!cr6.gt) goto loc_8260F484;
loc_8260F46C:
	// lvlx v0,r29,r11
	temp.u32 = r29.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx128 v0,r11,r31
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// blt cr6,0x8260f46c
	if (cr6.lt) goto loc_8260F46C;
loc_8260F484:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r31,r31,r23
	r31.u64 = r31.u64 + r23.u64;
	// cmpw cr6,r10,r28
	cr6.compare<int32_t>(ctx.r10.s32, r28.s32, xer);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// blt cr6,0x8260f458
	if (cr6.lt) goto loc_8260F458;
loc_8260F49C:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_8260F4A4"))) PPC_WEAK_FUNC(sub_8260F4A4);
PPC_FUNC_IMPL(__imp__sub_8260F4A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260F4A8"))) PPC_WEAK_FUNC(sub_8260F4A8);
PPC_FUNC_IMPL(__imp__sub_8260F4A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// stw r9,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r9.u32);
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// stw r8,428(r1)
	PPC_STORE_U32(ctx.r1.u32 + 428, ctx.r8.u32);
	// clrlwi r31,r9,29
	r31.u64 = ctx.r9.u32 & 0x7;
	// stw r6,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r6.u32);
	// add r8,r4,r7
	ctx.r8.u64 = ctx.r4.u64 + ctx.r7.u64;
	// lwz r11,3380(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3380);
	// add r9,r3,r7
	ctx.r9.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r29,r8,r10
	r29.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r5,r28,r31
	ctx.r5.u64 = r28.u64 + r31.u64;
	// stw r8,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r8.u32);
	// subf r4,r11,r9
	ctx.r4.s64 = ctx.r9.s64 - r11.s64;
	// stw r9,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r9.u32);
	// add r28,r5,r10
	r28.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lwz r10,468(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// subf r3,r11,r8
	ctx.r3.s64 = ctx.r8.s64 - r11.s64;
	// subf r5,r11,r28
	ctx.r5.s64 = r28.s64 - r11.s64;
	// srawi r11,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	r11.s64 = r28.s32 >> 2;
	// subfic r10,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// stw r4,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r4.u32);
	// addi r30,r7,-1
	r30.s64 = ctx.r7.s64 + -1;
	// stw r28,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r28.u32);
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// stw r3,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r3.u32);
	// addi r7,r29,-1
	ctx.r7.s64 = r29.s64 + -1;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r30,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r30.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// andi. r10,r10,10
	ctx.r10.u64 = ctx.r10.u64 & 10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r10,r10,10
	ctx.r10.s64 = ctx.r10.s64 + 10;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// bge cr6,0x8260f6d8
	if (!cr6.lt) goto loc_8260F6D8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
loc_8260F55C:
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r11.u32);
	// add r11,r5,r29
	r11.u64 = ctx.r5.u64 + r29.u64;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// stw r11,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r11.u32);
	// add r11,r5,r27
	r11.u64 = ctx.r5.u64 + r27.u64;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// stw r11,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r11.u32);
	// lbz r11,0(r7)
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// addi r11,r1,240
	r11.s64 = ctx.r1.s64 + 240;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xC))));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v13,v13,3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_set1_epi8(char(0xC))));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// vspltb v12,v12,3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_set1_epi8(char(0xC))));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,240
	r11.s64 = ctx.r1.s64 + 240;
	// vspltb v11,v11,3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_set1_epi8(char(0xC))));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,256
	r11.s64 = ctx.r1.s64 + 256;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x8260f610
	if (!cr6.gt) goto loc_8260F610;
	// addi r11,r7,1
	r11.s64 = ctx.r7.s64 + 1;
	// subf r9,r7,r30
	ctx.r9.s64 = r30.s64 - ctx.r7.s64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_8260F5F0:
	// lbz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r8,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r8.u8);
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8260f5f0
	if (!cr6.eq) goto loc_8260F5F0;
loc_8260F610:
	// stvlx v0,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// li r11,16
	r11.s64 = 16;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stvrx v0,r7,r11
	ea = ctx.r7.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// stvrx v13,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stvlx v12,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stvrx v12,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stvlx v11,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stvrx v11,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r29,r10,r31
	r29.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r6,412(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// add r27,r10,r31
	r27.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,168(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// add r8,r10,r31
	ctx.r8.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// add r30,r10,r31
	r30.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// add r7,r10,r31
	ctx.r7.u64 = ctx.r10.u64 + r31.u64;
	// stw r9,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r9.u32);
	// stw r8,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r8.u32);
	// stw r30,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r30.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// blt cr6,0x8260f55c
	if (cr6.lt) goto loc_8260F55C;
	// lwz r4,176(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r3,192(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r28,172(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// b 0x8260f6dc
	goto loc_8260F6DC;
loc_8260F6D8:
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
loc_8260F6DC:
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// li r23,0
	r23.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8260f8a0
	if (cr6.eq) goto loc_8260F8A0;
	// rlwinm r8,r11,2,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFF0;
	// stw r23,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r23.u32);
	// mullw r7,r10,r31
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(r31.s32);
	// stw r8,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r8.u32);
	// addi r9,r11,-4
	ctx.r9.s64 = r11.s64 + -4;
	// subf r8,r7,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r7.s64;
	// subf r7,r7,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r7.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// stw r7,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r7.u32);
	// stw r9,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r9.u32);
	// ble cr6,0x8260f8a0
	if (!cr6.gt) goto loc_8260F8A0;
	// b 0x8260f738
	goto loc_8260F738;
loc_8260F724:
	// lwz r4,176(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r3,192(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r7,156(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r9,208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
loc_8260F738:
	// add r6,r9,r4
	ctx.r6.u64 = ctx.r9.u64 + ctx.r4.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// add r5,r9,r3
	ctx.r5.u64 = ctx.r9.u64 + ctx.r3.u64;
	// stw r23,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r23.u32);
	// add r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stw r6,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r6.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// stw r5,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r5.u32);
	// stw r4,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r4.u32);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// lwz r9,164(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8260f81c
	if (!cr6.gt) goto loc_8260F81C;
loc_8260F780:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r7,16
	ctx.r7.s64 = 16;
	// lvrx v13,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,16
	r11.s64 = 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v0,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// stw r9,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r9.u32);
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// lwz r7,164(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// blt cr6,0x8260f780
	if (cr6.lt) goto loc_8260F780;
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r7,156(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// lwz r6,232(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r5,224(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
loc_8260F81C:
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// li r11,16
	r11.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// add r10,r7,r31
	ctx.r10.u64 = ctx.r7.u64 + r31.u64;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// lvrx v13,r6,r11
	temp.u32 = ctx.r6.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v12,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r10,16
	ctx.r10.s64 = 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v13,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// blt cr6,0x8260f724
	if (cr6.lt) goto loc_8260F724;
	// lwz r6,412(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// lwz r29,92(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r27,88(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r28,172(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
loc_8260F8A0:
	// lwz r9,436(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8260faa4
	if (cr6.eq) goto loc_8260FAA4;
	// addi r9,r11,-4
	ctx.r9.s64 = r11.s64 + -4;
	// rlwinm r11,r11,2,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFF0;
	// subf r26,r31,r29
	r26.s64 = r29.s64 - r31.s64;
	// subf r25,r31,r27
	r25.s64 = r27.s64 - r31.s64;
	// rlwinm r30,r9,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// stw r26,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r26.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r25,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r25.u32);
	// stw r30,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, r30.u32);
	// neg r11,r6
	r11.s64 = -ctx.r6.s64;
	// beq cr6,0x8260f8e8
	if (cr6.eq) goto loc_8260F8E8;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// b 0x8260f8ec
	goto loc_8260F8EC;
loc_8260F8E8:
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
loc_8260F8EC:
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r24,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r24.u32);
	// bl 0x826a85e8
	sub_826A85E8(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x826a85e8
	sub_826A85E8(ctx, base);
	// add r9,r29,r31
	ctx.r9.u64 = r29.u64 + r31.u64;
	// add r8,r27,r31
	ctx.r8.u64 = r27.u64 + r31.u64;
	// li r11,1
	r11.s64 = 1;
	// cmpwi cr6,r24,1
	cr6.compare<int32_t>(r24.s32, 1, xer);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// ble cr6,0x8260faa4
	if (!cr6.gt) goto loc_8260FAA4;
	// b 0x8260f950
	goto loc_8260F950;
loc_8260F938:
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r26,220(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r25,228(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// lwz r30,236(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
loc_8260F950:
	// add r5,r30,r9
	ctx.r5.u64 = r30.u64 + ctx.r9.u64;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// mr r11,r26
	r11.u64 = r26.u64;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// add r7,r30,r26
	ctx.r7.u64 = r30.u64 + r26.u64;
	// stw r23,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r23.u32);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// add r6,r30,r25
	ctx.r6.u64 = r30.u64 + r25.u64;
	// stw r5,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r5.u32);
	// add r5,r30,r8
	ctx.r5.u64 = r30.u64 + ctx.r8.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// stw r7,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r7.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// stw r6,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r6.u32);
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// lwz r5,196(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x8260fa38
	if (!cr6.gt) goto loc_8260FA38;
	// b 0x8260f9a4
	goto loc_8260F9A4;
loc_8260F99C:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_8260F9A4:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r7,16
	ctx.r7.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// lvrx v13,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stvlx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stvrx v0,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stvrx v13,r11,r6
	ea = r11.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r10,196(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8260f99c
	if (cr6.lt) goto loc_8260F99C;
	// lwz r31,460(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r7,212(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// lwz r6,216(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
loc_8260FA38:
	// add r10,r9,r31
	ctx.r10.u64 = ctx.r9.u64 + r31.u64;
	// li r11,16
	r11.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lvrx v13,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v12,r6,r11
	temp.u32 = ctx.r6.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lvlx v0,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r10,16
	ctx.r10.s64 = 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// blt cr6,0x8260f938
	if (cr6.lt) goto loc_8260F938;
loc_8260FAA4:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8260FAAC"))) PPC_WEAK_FUNC(sub_8260FAAC);
PPC_FUNC_IMPL(__imp__sub_8260FAAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8260FAB0"))) PPC_WEAK_FUNC(sub_8260FAB0);
PPC_FUNC_IMPL(__imp__sub_8260FAB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stw r10,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r10.u32);
	// add r10,r3,r6
	ctx.r10.u64 = ctx.r3.u64 + ctx.r6.u64;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// addi r31,r9,13632
	r31.s64 = ctx.r9.s64 + 13632;
	// stw r10,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r10.u32);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r4,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r4.u32);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r9,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r9.u32);
	// subf r27,r8,r10
	r27.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// stw r6,-132(r1)
	PPC_STORE_U32(ctx.r1.u32 + -132, ctx.r6.u32);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// stw r27,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, r27.u32);
	// stw r25,-136(r1)
	PPC_STORE_U32(ctx.r1.u32 + -136, r25.u32);
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// bge cr6,0x8260fc00
	if (!cr6.lt) goto loc_8260FC00;
loc_8260FB1C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// li r10,16
	ctx.r10.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// li r9,16
	ctx.r9.s64 = 16;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// stw r11,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r11.u32);
	// li r11,16
	r11.s64 = 16;
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// vspltb v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xC))));
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// vspltb v13,v13,3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_set1_epi8(char(0xC))));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// stvx v13,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v0,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r7,-160(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// stvrx v0,r7,r11
	ea = ctx.r7.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-160(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,-156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,-152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -152);
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r9,-144(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,-148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -148);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r6,36(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r8,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r8.u32);
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// stw r9,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r9.u32);
	// stw r10,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r10.u32);
	// blt cr6,0x8260fb1c
	if (cr6.lt) goto loc_8260FB1C;
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r27,-140(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// lwz r25,-136(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -136);
	// lwz r6,-132(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -132);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_8260FC00:
	// lwz r10,76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r10,32
	cr6.compare<int32_t>(ctx.r10.s32, 32, xer);
	// beq cr6,0x8260fc24
	if (cr6.eq) goto loc_8260FC24;
	// srawi r10,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 1;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r25,r10,2
	r25.s64 = ctx.r10.s64 + 2;
	// srawi r6,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r6.s64 = r11.s32 >> 1;
	// subf r3,r9,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r9.s64;
loc_8260FC24:
	// lwz r10,52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r10,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 3;
	// rlwinm r26,r10,3,0,27
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF0;
	// beq cr6,0x8260fd10
	if (cr6.eq) goto loc_8260FD10;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8260fd10
	if (!cr6.gt) goto loc_8260FD10;
	// addi r10,r25,-1
	ctx.r10.s64 = r25.s64 + -1;
	// rlwinm r23,r11,3,0,28
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r24,r10,1
	r24.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r28,r10,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
loc_8260FC60:
	// add r10,r28,r9
	ctx.r10.u64 = r28.u64 + ctx.r9.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8260fd00
	if (!cr6.gt) goto loc_8260FD00;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r10,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r5,r10,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r10.s64;
	// addi r3,r26,-1
	ctx.r3.s64 = r26.s64 + -1;
	// add r30,r4,r9
	r30.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r29,r5,r9
	r29.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r3,28,4,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 28) & 0xFFFFFFF;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// subf r22,r11,r3
	r22.s64 = ctx.r3.s64 - r11.s64;
	// subf r3,r10,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r4,r10,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r5,r10,r22
	ctx.r5.s64 = r22.s64 - ctx.r10.s64;
	// subf r8,r28,r10
	ctx.r8.s64 = ctx.r10.s64 - r28.s64;
	// subf r31,r9,r27
	r31.s64 = r27.s64 - ctx.r9.s64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
loc_8260FCC8:
	// lvx128 v0,r8,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r8,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stvx128 v0,r30,r10
	_mm_store_si128((__m128i*)(base + ((r30.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// stvx128 v0,r3,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r29,r10
	_mm_store_si128((__m128i*)(base + ((r29.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r4,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r5,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x8260fcc8
	if (!cr6.eq) goto loc_8260FCC8;
loc_8260FD00:
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// add r9,r23,r9
	ctx.r9.u64 = r23.u64 + ctx.r9.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x8260fc60
	if (!cr6.eq) goto loc_8260FC60;
loc_8260FD10:
	// lwz r10,60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8260fdf4
	if (cr6.eq) goto loc_8260FDF4;
	// subf r28,r11,r7
	r28.s64 = ctx.r7.s64 - r11.s64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8260fdf4
	if (!cr6.gt) goto loc_8260FDF4;
	// addi r10,r25,-1
	ctx.r10.s64 = r25.s64 + -1;
	// rlwinm r25,r11,3,0,28
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r27,r10,1
	r27.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r29,r10,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
loc_8260FD44:
	// add r10,r29,r7
	ctx.r10.u64 = r29.u64 + ctx.r7.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8260fde4
	if (!cr6.gt) goto loc_8260FDE4;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r10,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r6,r10,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r10.s64;
	// addi r4,r26,-1
	ctx.r4.s64 = r26.s64 + -1;
	// add r31,r5,r7
	r31.u64 = ctx.r5.u64 + ctx.r7.u64;
	// add r30,r6,r7
	r30.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r4,28,4,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 28) & 0xFFFFFFF;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// subf r24,r11,r4
	r24.s64 = ctx.r4.s64 - r11.s64;
	// subf r4,r10,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r5,r10,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subf r6,r10,r24
	ctx.r6.s64 = r24.s64 - ctx.r10.s64;
	// subf r9,r29,r10
	ctx.r9.s64 = ctx.r10.s64 - r29.s64;
	// subf r3,r7,r28
	ctx.r3.s64 = r28.s64 - ctx.r7.s64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
loc_8260FDAC:
	// lvx128 v0,r3,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r9,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stvx128 v0,r31,r10
	_mm_store_si128((__m128i*)(base + ((r31.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// stvx128 v0,r4,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r30,r10
	_mm_store_si128((__m128i*)(base + ((r30.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r5,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r6,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x8260fdac
	if (!cr6.eq) goto loc_8260FDAC;
loc_8260FDE4:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r7,r25,r7
	ctx.r7.u64 = r25.u64 + ctx.r7.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8260fd44
	if (!cr6.eq) goto loc_8260FD44;
loc_8260FDF4:
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8260FDF8"))) PPC_WEAK_FUNC(sub_8260FDF8);
PPC_FUNC_IMPL(__imp__sub_8260FDF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// lis r31,-32126
	r31.s64 = -2105409536;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// add r8,r4,r7
	ctx.r8.u64 = ctx.r4.u64 + ctx.r7.u64;
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// add r9,r3,r7
	ctx.r9.u64 = ctx.r3.u64 + ctx.r7.u64;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r28,84(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r25,0
	r25.s64 = 0;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// lwz r31,3380(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 3380);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// addi r30,r7,-1
	r30.s64 = ctx.r7.s64 + -1;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// subf r26,r31,r8
	r26.s64 = ctx.r8.s64 - r31.s64;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// subf r27,r31,r9
	r27.s64 = ctx.r9.s64 - r31.s64;
	// stw r9,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r9.u32);
	// addi r29,r10,-1
	r29.s64 = ctx.r10.s64 + -1;
	// stw r8,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r8.u32);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// stw r25,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r25.u32);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stw r30,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r30.u32);
	// stw r26,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, r26.u32);
	// mr r26,r31
	r26.u64 = r31.u64;
	// subf r24,r28,r11
	r24.s64 = r11.s64 - r28.s64;
	// stw r27,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r27.u32);
	// stw r29,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, r29.u32);
	// stw r7,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r7.u32);
	// cmpwi cr6,r24,16
	cr6.compare<int32_t>(r24.s32, 16, xer);
	// stw r10,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r10.u32);
	// stw r26,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, r26.u32);
	// stw r11,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r11.u32);
	// beq cr6,0x8260fec4
	if (cr6.eq) goto loc_8260FEC4;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// srawi r26,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r26.s64 = r11.s32 >> 1;
	// subf r3,r11,r3
	ctx.r3.s64 = ctx.r3.s64 - r11.s64;
	// subf r4,r11,r4
	ctx.r4.s64 = ctx.r4.s64 - r11.s64;
	// clrlwi r27,r6,29
	r27.u64 = ctx.r6.u32 & 0x7;
	// stw r26,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r26.u32);
	// addi r26,r31,1
	r26.s64 = r31.s64 + 1;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r26,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, r26.u32);
	// beq cr6,0x8260fec4
	if (cr6.eq) goto loc_8260FEC4;
	// subfic r31,r27,8
	xer.ca = r27.u32 <= 8;
	r31.s64 = 8 - r27.s64;
	// stw r31,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r31.u32);
loc_8260FEC4:
	// stw r5,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r5.u32);
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// bge cr6,0x8261000c
	if (!cr6.lt) goto loc_8261000C;
loc_8260FED0:
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// li r9,16
	ctx.r9.s64 = 16;
	// stw r11,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r11.u32);
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// add r11,r10,r28
	r11.u64 = ctx.r10.u64 + r28.u64;
	// stw r11,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, r11.u32);
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// add r11,r7,r28
	r11.u64 = ctx.r7.u64 + r28.u64;
	// li r7,16
	ctx.r7.s64 = 16;
	// stw r11,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, r11.u32);
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// stw r11,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, r11.u32);
	// li r11,16
	r11.s64 = 16;
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// vspltb v0,v0,3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xC))));
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v13,v13,3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_set1_epi8(char(0xC))));
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// lvx128 v12,r0,r6
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v12,v12,3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_set1_epi8(char(0xC))));
	// addi r6,r1,-224
	ctx.r6.s64 = ctx.r1.s64 + -224;
	// lvx128 v11,r0,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// vspltb v11,v11,3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_set1_epi8(char(0xC))));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-112
	ctx.r6.s64 = ctx.r1.s64 + -112;
	// stvx v13,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// stvx v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-224
	ctx.r6.s64 = ctx.r1.s64 + -224;
	// stvx v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,-240(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// stvlx v12,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// lwz r11,-240(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -240);
	// stvrx v12,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// lwz r11,-248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// stvlx v11,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// lwz r11,-248(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -248);
	// stvrx v11,r11,r7
	ea = r11.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// lwz r11,-280(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r9,-268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r6,-176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// add r30,r6,r11
	r30.u64 = ctx.r6.u64 + r11.u64;
	// lwz r6,-196(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -196);
	// lwz r9,-168(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r8,-180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -180);
	// add r29,r6,r11
	r29.u64 = ctx.r6.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r6,44(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// stw r5,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r5.u32);
	// stw r7,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r7.u32);
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// stw r10,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r10.u32);
	// stw r30,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r30.u32);
	// stw r9,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r9.u32);
	// stw r8,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r8.u32);
	// stw r29,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, r29.u32);
	// blt cr6,0x8260fed0
	if (cr6.lt) goto loc_8260FED0;
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r26,-252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
loc_8261000C:
	// lwz r9,60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r9,-208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// rlwinm r5,r9,2,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFF0;
	// srawi r8,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r5.s32 >> 2;
	// stw r9,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, ctx.r9.u32);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// stw r5,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, ctx.r5.u32);
	// rlwinm r31,r9,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, r31.u32);
	// beq cr6,0x826102e0
	if (cr6.eq) goto loc_826102E0;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// stw r9,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, ctx.r9.u32);
	// stw r8,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r8.u32);
	// bne cr6,0x82610184
	if (!cr6.eq) goto loc_82610184;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r4,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r4.u32);
	// ble cr6,0x826102e0
	if (!cr6.gt) goto loc_826102E0;
loc_82610064:
	// add r6,r9,r11
	ctx.r6.u64 = ctx.r9.u64 + r11.u64;
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -152);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r9,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r9.u32);
	// stw r8,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
	// stw r25,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, r25.u32);
	// stw r6,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r6.u32);
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + r11.u64;
	// stw r6,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r6.u32);
	// ble cr6,0x8261015c
	if (!cr6.gt) goto loc_8261015C;
	// b 0x8261009c
	goto loc_8261009C;
loc_82610094:
	// lwz r10,-260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// lwz r7,-256(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
loc_8261009C:
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,16
	ctx.r10.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r11.u32);
	// addi r11,r7,16
	r11.s64 = ctx.r7.s64 + 16;
	// li r7,16
	ctx.r7.s64 = 16;
	// stw r11,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r11.u32);
	// stvlx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r9,-284(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// li r11,16
	r11.s64 = 16;
	// stvrx v0,r9,r11
	ea = ctx.r9.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvrx v13,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,-272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stvrx v13,r11,r7
	ea = r11.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// lwz r11,-188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -188);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lwz r10,-276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lwz r5,-236(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// stw r9,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r9.u32);
	// stw r11,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, r11.u32);
	// stw r10,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r10.u32);
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r10,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r10.u32);
	// lwz r10,-272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r10,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r10.u32);
	// blt cr6,0x82610094
	if (cr6.lt) goto loc_82610094;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r26,-252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// lwz r4,-280(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r9,-232(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// lwz r8,-244(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
loc_8261015C:
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r4,r26
	cr6.compare<int32_t>(ctx.r4.s32, r26.s32, xer);
	// stw r4,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r4.u32);
	// stw r9,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, ctx.r9.u32);
	// stw r8,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r8.u32);
	// blt cr6,0x82610064
	if (cr6.lt) goto loc_82610064;
	// b 0x826102d4
	goto loc_826102D4;
loc_82610184:
	// mr r29,r25
	r29.u64 = r25.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r29,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r29.u32);
	// ble cr6,0x826102e0
	if (!cr6.gt) goto loc_826102E0;
loc_82610194:
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + r11.u64;
	// lwz r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -152);
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// stw r25,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r25.u32);
	// add r31,r8,r11
	r31.u64 = ctx.r8.u64 + r11.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r6,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r6.u32);
	// stw r4,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r4.u32);
	// stw r3,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r3.u32);
	// stw r31,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r31.u32);
	// ble cr6,0x82610290
	if (!cr6.gt) goto loc_82610290;
loc_826101C8:
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, r11.u32);
	// addi r11,r7,16
	r11.s64 = ctx.r7.s64 + 16;
	// stw r11,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r11.u32);
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r7,-284(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// li r11,16
	r11.s64 = 16;
	// stvrx v0,r7,r11
	ea = ctx.r7.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// stvrx v0,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r11,-272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stvrx v13,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// lwz r11,-164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lwz r10,-276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lwz r5,-236(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r7,-256(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lwz r10,-272(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stw r6,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r6.u32);
	// addi r31,r10,16
	r31.s64 = ctx.r10.s64 + 16;
	// lwz r10,-260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// stw r11,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r11.u32);
	// stw r4,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r4.u32);
	// stw r3,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r3.u32);
	// stw r31,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r31.u32);
	// blt cr6,0x826101c8
	if (cr6.lt) goto loc_826101C8;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r26,-252(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -252);
	// lwz r29,-280(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lwz r9,-232(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -232);
	// lwz r8,-244(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -244);
loc_82610290:
	// ld r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// ld r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// cmpw cr6,r29,r26
	cr6.compare<int32_t>(r29.s32, r26.s32, xer);
	// std r10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.r10.u64);
	// std r10,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r10.u64);
	// std r7,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r7.u64);
	// std r10,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r10.u64);
	// std r7,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.r7.u64);
	// stw r29,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r29.u32);
	// std r7,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r7.u64);
	// stw r9,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, ctx.r9.u32);
	// stw r8,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r8.u32);
	// blt cr6,0x82610194
	if (cr6.lt) goto loc_82610194;
loc_826102D4:
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// lwz r6,44(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
loc_826102E0:
	// lwz r9,68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82610518
	if (cr6.eq) goto loc_82610518;
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// lwz r9,-172(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// stw r6,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, ctx.r6.u32);
	// add r9,r26,r9
	ctx.r9.u64 = r26.u64 + ctx.r9.u64;
	// stw r8,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r8.u32);
	// subf r8,r11,r7
	ctx.r8.s64 = ctx.r7.s64 - r11.s64;
	// stw r9,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, ctx.r9.u32);
	// stw r9,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r9.u32);
	// stw r8,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r8.u32);
	// lwz r8,-148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -148);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82610410
	if (!cr6.eq) goto loc_82610410;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r6,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r6.u32);
	// ble cr6,0x82610518
	if (!cr6.gt) goto loc_82610518;
loc_8261032C:
	// lwz r9,-184(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -184);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r8,-204(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -204);
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// stw r7,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r7.u32);
	// stw r25,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r25.u32);
	// stw r9,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r9.u32);
	// stw r8,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r8.u32);
	// ble cr6,0x826103e8
	if (!cr6.gt) goto loc_826103E8;
loc_82610350:
	// li r11,16
	r11.s64 = 16;
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r7,16
	ctx.r7.s64 = 16;
	// lvrx v13,r9,r11
	temp.u32 = ctx.r9.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,16
	ctx.r9.s64 = 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r8,r7
	temp.u32 = ctx.r8.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r10,-260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// lwz r11,-156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lwz r10,-256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// lwz r7,-288(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// lwz r5,-236(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r9.u32);
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// stw r11,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r11.u32);
	// stw r8,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r8.u32);
	// stw r7,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r7.u32);
	// stw r10,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r10.u32);
	// blt cr6,0x82610350
	if (cr6.lt) goto loc_82610350;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// lwz r6,-280(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
loc_826103E8:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// lwz r9,-192(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// stw r6,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r6.u32);
	// stw r10,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r10.u32);
	// stw r7,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r7.u32);
	// blt cr6,0x8261032c
	if (cr6.lt) goto loc_8261032C;
	// b 0x8239bd38
	return;
loc_82610410:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r3,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r3.u32);
	// ble cr6,0x82610518
	if (!cr6.gt) goto loc_82610518;
loc_82610420:
	// lwz r9,-184(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -184);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// lwz r8,-204(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -204);
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// stw r25,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, r25.u32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r9,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r9.u32);
	// stw r8,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r8.u32);
	// stw r6,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r6.u32);
	// stw r4,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r4.u32);
	// ble cr6,0x826104e4
	if (!cr6.gt) goto loc_826104E4;
loc_8261044C:
	// li r11,16
	r11.s64 = 16;
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r10,16
	ctx.r10.s64 = 16;
	// lvrx v13,r9,r11
	temp.u32 = ctx.r9.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,16
	ctx.r9.s64 = 16;
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,-288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stvrx v13,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r10,-260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -260);
	// lwz r11,-200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -200);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lwz r10,-256(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -256);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lwz r5,-236(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -236);
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lwz r10,-284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// stw r9,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r9.u32);
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// stw r11,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, r11.u32);
	// stw r8,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r8.u32);
	// stw r6,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r6.u32);
	// stw r4,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r4.u32);
	// blt cr6,0x8261044c
	if (cr6.lt) goto loc_8261044C;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r10,-264(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -264);
	// lwz r7,-268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// lwz r3,-280(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
loc_826104E4:
	// ld r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r31,-192(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// cmpw cr6,r3,r31
	cr6.compare<int32_t>(ctx.r3.s32, r31.s32, xer);
	// std r9,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.r9.u64);
	// ld r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// stw r3,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r3.u32);
	// stw r10,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r10.u32);
	// stw r7,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r7.u32);
	// std r9,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r9.u64);
	// blt cr6,0x82610420
	if (cr6.lt) goto loc_82610420;
loc_82610518:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8261051C"))) PPC_WEAK_FUNC(sub_8261051C);
PPC_FUNC_IMPL(__imp__sub_8261051C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82610520"))) PPC_WEAK_FUNC(sub_82610520);
PPC_FUNC_IMPL(__imp__sub_82610520) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// vspltisb v0,15
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0xF)));
	// srawi r11,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	r11.s64 = ctx.r6.s32 >> 4;
	// vspltisb v13,1
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_set1_epi8(char(0x1)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// blelr cr6
	if (!cr6.gt) return;
loc_82610538:
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,32
	ctx.r9.s64 = ctx.r3.s64 + 32;
	// lvx128 v11,r0,r4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r3,48
	ctx.r8.s64 = ctx.r3.s64 + 48;
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsububm v12,v12,v0
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// vsububm v11,v11,v0
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsububm v10,v10,v0
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsububm v9,v9,v0
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsububm v8,v8,v0
	// vsububm v7,v7,v0
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vsrab v12,v12,v13
	// vsrab v9,v9,v13
	// vsrab v8,v8,v13
	// vsrab v7,v7,v13
	// vsrab v11,v11,v13
	// vsrab v10,v10,v13
	// vaddubm v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddubm v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddubm v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddubm v7,v7,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddubm v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddubm v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bne cr6,0x82610538
	if (!cr6.eq) goto loc_82610538;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826105D8"))) PPC_WEAK_FUNC(sub_826105D8);
PPC_FUNC_IMPL(__imp__sub_826105D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// vspltisb v0,15
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0xF)));
	// srawi r11,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	r11.s64 = ctx.r6.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// blelr cr6
	if (!cr6.gt) return;
loc_826105EC:
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,32
	ctx.r9.s64 = ctx.r3.s64 + 32;
	// lvx128 v12,r0,r4
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r3,48
	ctx.r8.s64 = ctx.r3.s64 + 48;
	// lvx128 v11,r0,r5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// vxor v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vaddsbs v13,v13,v13
	// vaddsbs v10,v10,v10
	// vaddsbs v9,v9,v9
	// vaddsbs v8,v8,v8
	// vaddsbs v12,v12,v12
	// vaddsbs v11,v11,v11
	// vxor v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// stvx v8,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// stvx v11,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bne cr6,0x826105ec
	if (!cr6.eq) goto loc_826105EC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261068C"))) PPC_WEAK_FUNC(sub_8261068C);
PPC_FUNC_IMPL(__imp__sub_8261068C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82610690"))) PPC_WEAK_FUNC(sub_82610690);
PPC_FUNC_IMPL(__imp__sub_82610690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,14800(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 14800);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826106d8
	if (!cr6.eq) goto loc_826106D8;
	// lwz r11,14796(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82610710
	if (!cr6.eq) goto loc_82610710;
	// lwz r11,216(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 216);
	// lwz r10,208(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 208);
	// lwz r5,3740(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3740);
	// mullw r6,r11,r10
	ctx.r6.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r4,3736(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3736);
	// lwz r3,3732(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3732);
	// bl 0x82610520
	sub_82610520(ctx, base);
	// b 0x82610708
	goto loc_82610708;
loc_826106D8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82610710
	if (!cr6.eq) goto loc_82610710;
	// lwz r11,14796(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 14796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82610710
	if (!cr6.eq) goto loc_82610710;
	// lwz r11,216(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 216);
	// lwz r10,208(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 208);
	// lwz r5,3740(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3740);
	// mullw r6,r11,r10
	ctx.r6.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r4,3736(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3736);
	// lwz r3,3732(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3732);
	// bl 0x826105d8
	sub_826105D8(ctx, base);
loc_82610708:
	// lwz r11,14796(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 14796);
	// stw r11,14800(r7)
	PPC_STORE_U32(ctx.r7.u32 + 14800, r11.u32);
loc_82610710:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82610720"))) PPC_WEAK_FUNC(sub_82610720);
PPC_FUNC_IMPL(__imp__sub_82610720) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,14796(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r10,216(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// vspltisb v0,15
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0xF)));
	// lwz r9,208(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// lwz r11,3776(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3776);
	// mullw r9,r10,r9
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r10,3780(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3780);
	// srawi r8,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// lwz r9,3784(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3784);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
loc_82610758:
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r11,32
	ctx.r6.s64 = r11.s64 + 32;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r11,48
	ctx.r5.s64 = r11.s64 + 48;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vxor v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v10,r0,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v9,r0,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vaddsbs v13,v13,v13
	// vaddsbs v10,v10,v10
	// vaddsbs v9,v9,v9
	// vaddsbs v8,v8,v8
	// vaddsbs v12,v12,v12
	// vaddsbs v11,v11,v11
	// vxor v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vxor v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v10,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvx v8,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82610758
	if (!cr6.eq) goto loc_82610758;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826107F8"))) PPC_WEAK_FUNC(sub_826107F8);
PPC_FUNC_IMPL(__imp__sub_826107F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// rlwinm r11,r4,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// neg r5,r9
	ctx.r5.s64 = -ctx.r9.s64;
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v19,-1
	_mm_store_si128((__m128i*)v19.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vspltish v20,1
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// vspltish v13,2
	// vspltish v12,3
	// vspltish v11,4
	// vspltish v10,15
	// dcbt r5,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// dcbt r8,r11
	// neg r31,r4
	r31.s64 = -ctx.r4.s64;
	// dcbt r31,r11
	// dcbt r0,r11
	// dcbt r4,r11
	// dcbt r10,r11
	// dcbt r9,r11
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lbz r8,2(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bge cr6,0x826108d4
	if (!cr6.lt) goto loc_826108D4;
loc_82610898:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82610c58
	if (!cr6.gt) goto loc_82610C58;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,2(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// blt cr6,0x82610898
	if (cr6.lt) goto loc_82610898;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
loc_826108D4:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82610c60
	if (!cr6.gt) goto loc_82610C60;
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// vpkswss v21,v19,v0
	// beq cr6,0x826108ec
	if (cr6.eq) goto loc_826108EC;
	// vor v21,v19,v19
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)v19.u8));
loc_826108EC:
	// addi r11,r1,36
	r11.s64 = ctx.r1.s64 + 36;
	// stw r3,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, ctx.r3.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r7,r1,36
	ctx.r7.s64 = ctx.r1.s64 + 36;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// addi r11,r3,16
	r11.s64 = ctx.r3.s64 + 16;
	// stw r10,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, ctx.r10.u32);
	// stw r9,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, ctx.r9.u32);
	// vor v4,v8,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v9,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v8,r3,r31
	temp.u32 = ctx.r3.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v7,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r3,r4
	temp.u32 = ctx.r3.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsplth v25,v4,1
	_mm_store_si128((__m128i*)v25.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_set1_epi16(short(0xD0C))));
	// lvlx v5,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v3,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v29,r3,r8
	temp.u32 = ctx.r3.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v28,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// lvrx v2,r11,r5
	temp.u32 = r11.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v31,v9,v2
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvrx v2,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v9,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v2,v8,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvrx v27,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)v27.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v28,v28,v9
	_mm_store_si128((__m128i*)v28.u8, _mm_or_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v29,v29,v27
	_mm_store_si128((__m128i*)v29.u8, _mm_or_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v27.u8)));
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v30,v6,v8
	_mm_store_si128((__m128i*)v30.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v1,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v27,v5,v9
	_mm_store_si128((__m128i*)v27.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v6,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v1,v7,v1
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vor v26,v3,v6
	_mm_store_si128((__m128i*)v26.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vmrghb v6,v0,v28
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v3,v0,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v30
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v23,v0,v27
	_mm_store_si128((__m128i*)v23.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v1
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v4,v3,v6
	// vsubshs v6,v6,v5
	// vmrghb v24,v0,v31
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v5,v5,v23
	// vmrghb v22,v0,v26
	_mm_store_si128((__m128i*)v22.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v9,v8,v7
	// vsubshs v18,v7,v8
	// vsubshs v3,v24,v8
	// vsubshs v7,v7,v22
	// vslh v24,v4,v13
	// vslh v23,v5,v13
	// vslh v22,v9,v13
	// vaddshs v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v4,v24,v4
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v23,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v24,v7,v7
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v23,v22,v9
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v22,v6,v6
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubshs v7,v3,v4
	// vsubshs v6,v24,v5
	// vmaxsh v8,v9,v18
	// vsubshs v5,v22,v23
	// vaddshs v7,v7,v11
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v6,v6,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v8,v8,v20
	// vaddshs v4,v5,v11
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v5,v7,v12
	// vsrah v6,v6,v12
	// vsrah v9,v9,v10
	// vsrah v7,v4,v12
	// vsubshs v3,v0,v5
	// vsubshs v4,v0,v6
	// vsubshs v24,v0,v7
	// vmaxsh v5,v5,v3
	// vmaxsh v23,v6,v4
	// vsrah v4,v7,v10
	// vmaxsh v6,v7,v24
	// vminsh v7,v5,v23
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// vcmpgtsh v3,v25,v6
	// addi r11,r6,-8
	r11.s64 = ctx.r6.s64 + -8;
	// addi r10,r10,29760
	ctx.r10.s64 = ctx.r10.s64 + 29760;
	// vxor v4,v4,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// vcmpgtsh v24,v6,v7
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vsubshs v5,v6,v7
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v6,v24,v3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vperm v3,v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vslh v24,v5,v13
	// vsubshs v3,v0,v3
	// vaddshs v5,v24,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vperm v24,v6,v6,v7
	_mm_store_si128((__m128i*)v24.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsrah v3,v3,v10
	// vsrah v5,v5,v12
	// vand v4,v4,v3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vand v6,v6,v24
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)v24.u8)));
	// vand v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vminsh v8,v8,v6
	// vxor v8,v8,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsubshs v9,v8,v9
	// vand v24,v9,v21
	_mm_store_si128((__m128i*)v24.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v21.u8)));
	// ble cr6,0x82610c24
	if (!cr6.gt) goto loc_82610C24;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x82610ad4
	if (!cr6.eq) goto loc_82610AD4;
	// subf r11,r4,r9
	r11.s64 = ctx.r9.s64 - ctx.r4.s64;
	// lbz r10,2(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbz r11,2(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// blt cr6,0x82610c24
	if (cr6.lt) goto loc_82610C24;
	// vpkswss v21,v19,v0
	// b 0x82610b24
	goto loc_82610B24;
loc_82610AD4:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bne cr6,0x82610b24
	if (!cr6.eq) goto loc_82610B24;
	// subf r11,r4,r9
	r11.s64 = ctx.r9.s64 - ctx.r4.s64;
	// lbz r10,2(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bge cr6,0x82610b20
	if (!cr6.lt) goto loc_82610B20;
	// lbz r11,6(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lbz r10,6(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 6);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// blt cr6,0x82610c24
	if (cr6.lt) goto loc_82610C24;
loc_82610B20:
	// vor v21,v19,v19
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)v19.u8));
loc_82610B24:
	// vmrglb v9,v0,v29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v28
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v30
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v30,v0,v27
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v6,v0,v1
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v29,v0,v31
	_mm_store_si128((__m128i*)v29.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v3,v9,v5
	// vmrglb v8,v0,v2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v5,v5,v4
	// vmrglb v28,v0,v26
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v31,v4,v30
	// vsubshs v9,v8,v6
	// vsubshs v30,v29,v8
	// vsubshs v8,v6,v8
	// vsubshs v29,v6,v28
	// vslh v6,v3,v13
	// vslh v4,v31,v13
	// vaddshs v5,v5,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmaxsh v8,v9,v8
	// vaddshs v6,v6,v3
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v3,v30,v30
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v4,v4,v31
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v31,v29,v29
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsrah v8,v8,v20
	// vsubshs v6,v3,v6
	// vslh v3,v9,v13
	// vsubshs v4,v31,v4
	// vaddshs v6,v6,v11
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v3,v3,v9
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v4,v4,v11
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v9,v9,v10
	// vsrah v6,v6,v12
	// vsubshs v5,v5,v3
	// vaddshs v11,v5,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsrah v5,v4,v12
	// vsubshs v4,v0,v6
	// vsrah v11,v11,v12
	// vsubshs v3,v0,v5
	// vmaxsh v6,v6,v4
	// vsrah v4,v11,v10
	// vmaxsh v5,v5,v3
	// vxor v4,v4,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vminsh v6,v6,v5
	// vsubshs v5,v0,v11
	// vmaxsh v11,v11,v5
	// vsubshs v5,v11,v6
	// vcmpgtsh v3,v25,v11
	// vcmpgtsh v11,v11,v6
	// vslh v13,v5,v13
	// vand v11,v11,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v13,v13,v5
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v13,v13,v12
	// vperm v12,v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vand v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vand v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vperm v12,v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubshs v0,v0,v12
	// vsrah v0,v0,v10
	// vand v0,v4,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vminsh v0,v8,v0
	// vxor v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsubshs v0,v0,v9
	// vand v0,v0,v21
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v21.u8)));
loc_82610C24:
	// vpkshss v0,v24,v0
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// vsububm v13,v2,v0
	// vaddubm v0,v1,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v13,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r11,r10
	ea = r11.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// stvrx v0,r11,r9
	ea = r11.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82610C58:
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
loc_82610C60:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82610C68"))) PPC_WEAK_FUNC(sub_82610C68);
PPC_FUNC_IMPL(__imp__sub_82610C68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// li r7,16
	ctx.r7.s64 = 16;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// addi r8,r8,29760
	ctx.r8.s64 = ctx.r8.s64 + 29760;
	// vspltisb v17,-1
	_mm_store_si128((__m128i*)v17.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// vspltish v16,1
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// vspltish v11,2
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v10,3
	// rlwinm r31,r11,2,0,29
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltish v7,4
	// lvx128 v1,r0,r8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltish v6,15
	// lvx128 v12,r7,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-116(r1)
	PPC_STORE_U32(ctx.r1.u32 + -116, r11.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r6,r11,r8
	ctx.r6.u64 = r11.u64 + ctx.r8.u64;
	// stw r10,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r10.u32);
	// stw r31,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r31.u32);
	// stw r6,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, ctx.r6.u32);
	// ble cr6,0x82610fbc
	if (!cr6.gt) goto loc_82610FBC;
loc_82610CD4:
	// add r8,r10,r3
	ctx.r8.u64 = ctx.r10.u64 + ctx.r3.u64;
	// addi r7,r8,4
	ctx.r7.s64 = ctx.r8.s64 + 4;
loc_82610CDC:
	// lbz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r8,1(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// subf r8,r8,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r8.s64;
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r5.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bge cr6,0x82610d18
	if (!cr6.lt) goto loc_82610D18;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// bgt cr6,0x82610cdc
	if (cr6.gt) goto loc_82610CDC;
	// b 0x8239bd44
	return;
loc_82610D18:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82610fbc
	if (!cr6.gt) goto loc_82610FBC;
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// ble cr6,0x82610d60
	if (!cr6.gt) goto loc_82610D60;
	// add r8,r31,r10
	ctx.r8.u64 = r31.u64 + ctx.r10.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r7,5(r8)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// lbz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bge cr6,0x82610fc0
	if (!cr6.lt) goto loc_82610FC0;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
loc_82610D60:
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// vpkswss v3,v17,v0
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// addi r7,r1,36
	ctx.r7.s64 = ctx.r1.s64 + 36;
	// li r5,16
	ctx.r5.s64 = 16;
	// addi r4,r1,36
	ctx.r4.s64 = ctx.r1.s64 + 36;
	// li r30,16
	r30.s64 = 16;
	// stw r8,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, ctx.r8.u32);
	// stw r9,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, ctx.r9.u32);
	// li r29,16
	r29.s64 = 16;
	// li r28,16
	r28.s64 = 16;
	// lvrx v13,r7,r5
	temp.u32 = ctx.r7.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 + r11.u64;
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r5,r8,r10
	ctx.r5.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vor v23,v9,v13
	_mm_store_si128((__m128i*)v23.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v13,r8,r30
	temp.u32 = ctx.r8.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r4,r8,r6
	ctx.r4.u64 = ctx.r8.u64 + ctx.r6.u64;
	// vor v27,v9,v13
	_mm_store_si128((__m128i*)v27.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r27,16
	r27.s64 = 16;
	// lvrx v13,r7,r29
	temp.u32 = ctx.r7.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsplth v23,v23,1
	_mm_store_si128((__m128i*)v23.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u16), _mm_set1_epi16(short(0xD0C))));
	// vor v26,v9,v13
	_mm_store_si128((__m128i*)v26.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v8,r5,r28
	temp.u32 = ctx.r5.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v25,v13,v8
	_mm_store_si128((__m128i*)v25.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v9,r4,r27
	temp.u32 = ctx.r4.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v24,v13,v9
	_mm_store_si128((__m128i*)v24.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v13,v27,v25
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vmrghb v9,v26,v24
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vmrghb v8,v13,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghw v9,v8,v0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmrghw v5,v13,v0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v13.u32)));
	// vmrglw v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmrglw v4,v13,v0
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v13.u32)));
	// vmrglb v30,v0,v9
	_mm_store_si128((__m128i*)v30.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v31,v0,v9
	_mm_store_si128((__m128i*)v31.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v2,v0,v8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v28,v0,v4
	_mm_store_si128((__m128i*)v28.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v29,v0,v4
	_mm_store_si128((__m128i*)v29.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v13,v0,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v4,v2,v5
	// vsubshs v2,v30,v2
	// vsubshs v29,v5,v29
	// vsubshs v8,v13,v9
	// vsubshs v22,v9,v13
	// vslh v21,v2,v11
	// vsubshs v30,v9,v28
	// vsubshs v31,v31,v13
	// vmaxsh v28,v8,v22
	// vslh v20,v29,v11
	// vaddshs v2,v21,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v21,v4,v4
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vslh v4,v8,v11
	// vaddshs v31,v31,v31
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v30,v30,v30
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vsrah v28,v28,v16
	// vaddshs v29,v20,v29
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vsrah v5,v8,v6
	// vaddshs v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsubshs v4,v31,v2
	// vsplth v22,v28,2
	_mm_store_si128((__m128i*)v22.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u16), _mm_set1_epi16(short(0xB0A))));
	// vsubshs v2,v30,v29
	// vsubshs v8,v21,v8
	// vand v22,v22,v3
	_mm_store_si128((__m128i*)v22.u8, _mm_and_si128(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vaddshs v4,v4,v7
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v2,v2,v7
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubshs v22,v0,v22
	// vsrah v4,v4,v10
	// vsrah v2,v2,v10
	// vsrah v8,v8,v10
	// vsrah v22,v22,v6
	// vsubshs v31,v0,v4
	// vsubshs v30,v0,v2
	// vmaxsh v2,v2,v30
	// vsubshs v30,v0,v8
	// vmaxsh v4,v4,v31
	// vsrah v31,v8,v6
	// vmaxsh v8,v8,v30
	// vminsh v4,v4,v2
	// vcmpgtsh v30,v23,v8
	// vsubshs v2,v8,v4
	// vcmpgtsh v8,v8,v4
	// vxor v4,v31,v5
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vslh v31,v2,v11
	// vand v8,v8,v30
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vand v4,v4,v22
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vaddshs v2,v31,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vperm v31,v8,v8,v1
	_mm_store_si128((__m128i*)v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vsrah v2,v2,v10
	// vand v8,v2,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vand v8,v8,v31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vand v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vminsh v8,v28,v8
	// vxor v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsubshs v8,v8,v5
	// vand v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vcmpequh. v5,v0,v8
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vsubshs v13,v13,v8
	// stw r7,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, ctx.r7.u32);
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// rlwinm r7,r7,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80;
	// stw r8,-120(r1)
	PPC_STORE_U32(ctx.r1.u32 + -120, ctx.r8.u32);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// vmrghh v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vsplth v9,v13,0
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xF0E))));
	// vsplth v8,v13,1
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vsplth v5,v13,2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xB0A))));
	// vsplth v4,v13,3
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x908))));
	// vsldoi v13,v9,v0,1
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v9,v8,v0,1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v8,v5,v0,1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v5,v4,v0,1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// beq cr6,0x82610fb4
	if (cr6.eq) goto loc_82610FB4;
	// vsel v13,v27,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v27.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8))));
	// li r11,16
	r11.s64 = 16;
	// vsel v9,v26,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v26.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8))));
	// li r7,16
	ctx.r7.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// vsel v8,v25,v8,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v25.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8))));
	// vsel v5,v24,v5,v12
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v24.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v5.u8))));
	// li r5,16
	ctx.r5.s64 = 16;
	// stvlx v13,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r10,-120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -120);
	// stvrx v13,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lwz r9,-120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -120);
	// lwz r11,-116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -116);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// stvlx v9,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvlx v8,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r8,r6
	ea = ctx.r8.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// lwz r6,-108(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stvlx v5,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r9,r5
	ea = ctx.r9.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
loc_82610FAC:
	// lwz r9,-104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// lwz r31,-100(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
loc_82610FB4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt cr6,0x82610cd4
	if (cr6.gt) goto loc_82610CD4;
loc_82610FBC:
	// b 0x8239bd44
	return;
loc_82610FC0:
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// add r7,r8,r31
	ctx.r7.u64 = ctx.r8.u64 + r31.u64;
	// addi r4,r1,36
	ctx.r4.s64 = ctx.r1.s64 + 36;
	// stw r8,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, ctx.r8.u32);
	// stw r9,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, ctx.r9.u32);
	// stw r7,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, ctx.r7.u32);
	// lvlx v9,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v3,r7,r10
	temp.u32 = ctx.r7.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v25,r7,r6
	temp.u32 = ctx.r7.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r5,17
	ctx.r7.s64 = ctx.r5.s64 + 17;
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r5,r7,r31
	ctx.r5.u64 = ctx.r7.u64 + r31.u64;
	// lvlx v8,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v4,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v29,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stw r7,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r7.u32);
	// stw r5,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, ctx.r5.u32);
	// lvrx v2,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvrx v28,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)v28.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v2,v13,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// lvrx v27,r7,r10
	temp.u32 = ctx.r7.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)v27.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v5,v28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)v28.u8)));
	// lvrx v26,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)v26.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v28,v4,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v27.u8)));
	// lvrx v24,r7,r6
	temp.u32 = ctx.r7.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)v24.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v27,v3,v26
	_mm_store_si128((__m128i*)v27.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v26.u8)));
	// lvrx v23,r5,r6
	temp.u32 = ctx.r5.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)v23.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v26,v29,v24
	_mm_store_si128((__m128i*)v26.u8, _mm_or_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v24.u8)));
	// lvrx v31,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)v31.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r5,16
	ctx.r5.s64 = 16;
	// lvrx v30,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	_mm_store_si128((__m128i*)v30.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r7,r1,36
	ctx.r7.s64 = ctx.r1.s64 + 36;
	// vor v31,v9,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vor v30,v8,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v30.u8)));
	// lvlx v8,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v25,v25,v23
	_mm_store_si128((__m128i*)v25.u8, _mm_or_si128(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vor v29,v13,v13
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// lvrx v9,r7,r5
	temp.u32 = ctx.r7.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghb v13,v2,v31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v20,v8,v9
	_mm_store_si128((__m128i*)v20.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v28,v27
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vmrghb v5,v26,v25
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vmrghb v9,v30,v29
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vmrghb v4,v13,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v8,v9,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrglb v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v5,v4,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglb v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrghb v4,v13,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrglb v3,v13,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmrghb v24,v0,v5
	_mm_store_si128((__m128i*)v24.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v13,v0,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v22,v0,v5
	_mm_store_si128((__m128i*)v22.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v5,v0,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v21,v0,v3
	_mm_store_si128((__m128i*)v21.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v8,v13,v9
	// vsubshs v19,v9,v13
	// vmrglb v23,v0,v3
	_mm_store_si128((__m128i*)v23.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubshs v3,v24,v13
	// vsubshs v24,v5,v4
	// vsubshs v21,v4,v21
	// vmaxsh v4,v8,v19
	// vsubshs v22,v22,v5
	// vaddshs v19,v24,v24
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// vsubshs v23,v9,v23
	// vsrah v4,v4,v16
	// vslh v15,v22,v11
	// vslh v14,v21,v11
	// vsrah v5,v8,v6
	// vperm v24,v4,v4,v1
	_mm_store_si128((__m128i*)v24.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vaddshs v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vsubshs v24,v0,v24
	// vsrah v18,v24,v6
	// vaddshs v24,v23,v23
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v23.s16)));
	// vaddshs v23,v15,v22
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v22.s16)));
	// vaddshs v22,v14,v21
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v21.s16)));
	// vslh v21,v8,v11
	// vsubshs v24,v24,v22
	// vsubshs v3,v3,v23
	// vaddshs v8,v21,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsplth v21,v20,1
	_mm_store_si128((__m128i*)v21.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u16), _mm_set1_epi16(short(0xD0C))));
	// vaddshs v24,v24,v7
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v3,v3,v7
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubshs v8,v19,v8
	// vsrah v24,v24,v10
	// vsrah v3,v3,v10
	// vaddshs v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsubshs v22,v0,v24
	// vsubshs v23,v0,v3
	// vsrah v8,v8,v10
	// vmaxsh v24,v24,v22
	// vmaxsh v3,v3,v23
	// vsubshs v22,v0,v8
	// vsrah v23,v8,v6
	// vminsh v24,v3,v24
	// vmaxsh v8,v8,v22
	// vcmpgtsh v22,v8,v24
	// vcmpgtsh v3,v21,v8
	// vsubshs v8,v8,v24
	// vxor v24,v23,v5
	_mm_store_si128((__m128i*)v24.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vand v3,v22,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vslh v22,v8,v11
	// vperm v23,v3,v3,v1
	_mm_store_si128((__m128i*)v23.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vaddshs v22,v22,v8
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vand v8,v24,v18
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v18.u8)));
	// vsrah v24,v22,v10
	// vand v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vand v3,v3,v23
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vand v8,v3,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vminsh v8,v4,v8
	// vxor v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsubshs v8,v8,v5
	// vand v8,v8,v17
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v17.u8)));
	// vcmpequh. v5,v0,v8
	// mfocrf r7,2
	ctx.r7.u64 = (cr6.lt << 7) | (cr6.gt << 6) | (cr6.eq << 5) | (cr6.so << 4);
	// vsubshs v13,v13,v8
	// add r5,r8,r31
	ctx.r5.u64 = ctx.r8.u64 + r31.u64;
	// vaddshs v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stw r7,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, ctx.r7.u32);
	// rlwinm r7,r7,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80;
	// stw r8,-124(r1)
	PPC_STORE_U32(ctx.r1.u32 + -124, ctx.r8.u32);
	// cmplwi cr6,r7,128
	cr6.compare<uint32_t>(ctx.r7.u32, 128, xer);
	// vmrghh v8,v13,v9
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// stw r5,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, ctx.r5.u32);
	// vmrglh v13,v13,v9
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vpkshus v13,v8,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vsplth v9,v13,0
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xF0E))));
	// vsplth v8,v13,1
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xD0C))));
	// vsplth v5,v13,2
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xB0A))));
	// vsplth v4,v13,3
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x908))));
	// vsplth v3,v13,4
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x706))));
	// vsplth v24,v13,5
	_mm_store_si128((__m128i*)v24.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x504))));
	// vsplth v23,v13,6
	_mm_store_si128((__m128i*)v23.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x302))));
	// vsplth v13,v13,7
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0x100))));
	// vsldoi v9,v9,v0,1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v3,v3,v0,1
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v8,v8,v0,1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v5,v5,v0,1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v4,v4,v0,1
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsel v9,v2,v9,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v2.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v9.u8))));
	// vsldoi v24,v24,v0,1
	_mm_store_si128((__m128i*)v24.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsel v3,v31,v3,v12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v31.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v3.u8))));
	// vsldoi v23,v23,v0,1
	_mm_store_si128((__m128i*)v23.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsel v8,v30,v8,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v30.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8))));
	// vsldoi v13,v13,v0,1
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsel v5,v28,v5,v12
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v28.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v5.u8))));
	// vsel v4,v26,v4,v12
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v26.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v4.u8))));
	// vsel v2,v29,v24,v12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v29.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v24.u8))));
	// vsel v31,v27,v23,v12
	_mm_store_si128((__m128i*)v31.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v27.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v23.u8))));
	// vsel v13,v25,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)v25.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8))));
	// beq cr6,0x82610fb4
	if (cr6.eq) goto loc_82610FB4;
	// stvlx v9,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// lwz r10,-124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// li r11,16
	r11.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r7,16
	ctx.r7.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stvrx v9,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// lwz r11,-124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// lwz r10,-116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -116);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// li r31,16
	r31.s64 = 16;
	// stvlx v8,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r10,r9
	ea = ctx.r10.u32 + ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stvlx v5,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// stvrx v5,r10,r7
	ea = ctx.r10.u32 + ctx.r7.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v5.u8[i]);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stvlx v4,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// stvrx v4,r11,r6
	ea = r11.u32 + ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v4.u8[i]);
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// stvlx v3,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// stvrx v3,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v3.u8[i]);
	// lwz r9,-128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lwz r11,-116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -116);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// stvlx v2,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v2.u8[15 - i]);
	// stvrx v2,r10,r5
	ea = ctx.r10.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v2.u8[i]);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvlx v31,0,r8
	ea = ctx.r8.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, v31.u8[15 - i]);
	// stvrx v31,r8,r4
	ea = ctx.r8.u32 + ctx.r4.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, v31.u8[i]);
	// lwz r6,-108(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stvlx v13,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r9,r31
	ea = ctx.r9.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// b 0x82610fac
	goto loc_82610FAC;
}

__attribute__((alias("__imp__sub_826112D8"))) PPC_WEAK_FUNC(sub_826112D8);
PPC_FUNC_IMPL(__imp__sub_826112D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd8
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltish v11,4
	// rlwinm r5,r4,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v0,3
	// rlwinm r6,r4,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v13,1
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// vspltish v12,2
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 + ctx.r6.u64;
	// vmrghh v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// addi r11,r3,-4
	r11.s64 = ctx.r3.s64 + -4;
	// vmrghh v6,v11,v0
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// mulli r3,r4,14
	ctx.r3.s64 = ctx.r4.s64 * 14;
	// stw r11,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r11.u32);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r3,r11
	ctx.r4.u64 = ctx.r3.u64 + r11.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// stw r4,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r4.u32);
	// li r31,16
	r31.s64 = 16;
	// stw r10,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r10.u32);
	// li r30,16
	r30.s64 = 16;
	// stw r9,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r9.u32);
	// stw r8,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r8.u32);
	// li r29,16
	r29.s64 = 16;
	// stw r7,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r7.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r6,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r6.u32);
	// li r27,16
	r27.s64 = 16;
	// stw r5,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r5.u32);
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r9,r31
	temp.u32 = ctx.r9.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r26,16
	r26.s64 = 16;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v9,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r3,16
	ctx.r3.s64 = 16;
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r8,r29
	temp.u32 = ctx.r8.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v5,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vmrghh v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvrx v10,r6,r27
	temp.u32 = ctx.r6.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v5,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v4,r7,r26
	temp.u32 = ctx.r7.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghh v10,v8,v9
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvlx v9,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// lvlx v4,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r4,r3
	temp.u32 = ctx.r4.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v4,r5,r31
	temp.u32 = ctx.r5.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vmrghh v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// lvlx v5,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v5,v5,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vmrglh v4,v11,v9
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v8,v5,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrghh v5,v11,v9
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v9,v10,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v8,v10,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghh v11,v5,v9
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrglh v10,v4,v8
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vmrglh v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vmrghh v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v3,v11,v13
	// vaddshs v5,v11,v10
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v4,v9,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v2,v11,v12
	// addi r3,r1,-160
	ctx.r3.s64 = ctx.r1.s64 + -160;
	// vslh v1,v8,v13
	// addi r30,r1,-128
	r30.s64 = ctx.r1.s64 + -128;
	// vslh v8,v8,v12
	// vsubshs v11,v11,v10
	// vaddshs v3,v2,v3
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vslh v2,v9,v13
	// vslh v9,v9,v12
	// vslh v12,v10,v12
	// vslh v13,v10,v13
	// vaddshs v8,v8,v1
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v2,v9,v2
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v9,v3,v5
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vaddshs v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v3,v2,v4
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v12,v13,v5
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vaddshs v13,v9,v6
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsubshs v10,v3,v11
	// vaddshs v9,v8,v11
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v12,v12,v7
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vsrah v13,v13,v0
	// vaddshs v11,v10,v7
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v10,v9,v6
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vsrah v12,v12,v0
	// vsrah v11,v11,v0
	// vsrah v0,v10,v0
	// vmrghh v10,v11,v12
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v11,v13,v0
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrglh v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vmrghh v13,v11,v10
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrghh v10,v0,v12
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// stvx v13,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,-144
	ctx.r3.s64 = ctx.r1.s64 + -144;
	// stvx v10,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r3,-160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r30,-152(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -152);
	// lwz r25,-120(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -120);
	// lwz r28,-156(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// lwz r24,-124(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// addi r26,r1,-144
	r26.s64 = ctx.r1.s64 + -144;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// lwz r31,-144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r29,-136(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -136);
	// lwz r3,-140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// lwz r27,-132(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -132);
	// stvx v0,r0,r26
	_mm_store_si128((__m128i*)(base + ((r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r26,-128(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// stw r31,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r31.u32);
	// lwz r31,-148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -148);
	// lwz r23,-116(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -116);
	// stw r26,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r26.u32);
	// lwz r26,-144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r22,-136(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -136);
	// lwz r21,-140(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// lwz r20,-132(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -132);
	// stw r26,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r26.u32);
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
	// stw r29,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r29.u32);
	// stw r25,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r25.u32);
	// stw r22,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r22.u32);
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// stw r3,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r3.u32);
	// stw r24,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r24.u32);
	// stw r21,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r21.u32);
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// stw r27,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r27.u32);
	// stw r23,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r23.u32);
	// stw r20,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r20.u32);
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_82611550"))) PPC_WEAK_FUNC(sub_82611550);
PPC_FUNC_IMPL(__imp__sub_82611550) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce8
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v0,15
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// stw r10,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r10.u32);
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// beq cr6,0x82611904
	if (cr6.eq) goto loc_82611904;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82611870
	if (cr6.eq) goto loc_82611870;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, r11.u32);
	// subf r31,r7,r6
	r31.s64 = ctx.r6.s64 - ctx.r7.s64;
	// stw r4,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r4.u32);
	// add r30,r6,r7
	r30.u64 = ctx.r6.u64 + ctx.r7.u64;
	// stw r6,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, ctx.r6.u32);
	// subf r28,r8,r6
	r28.s64 = ctx.r6.s64 - ctx.r8.s64;
	// vspltish v13,3
	// li r27,16
	r27.s64 = 16;
	// vspltish v9,4
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v12,1
	// li r24,16
	r24.s64 = 16;
	// stw r31,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, r31.u32);
	// stw r30,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r30.u32);
	// add r3,r9,r4
	ctx.r3.u64 = ctx.r9.u64 + ctx.r4.u64;
	// stw r28,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, r28.u32);
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v10,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r26,16
	r26.s64 = 16;
	// vor v10,v8,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v7,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r4,r24
	temp.u32 = ctx.r4.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r29,r9,r11
	r29.u64 = ctx.r9.u64 + r11.u64;
	// li r25,16
	r25.s64 = 16;
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmrghh v5,v9,v13
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// lvlx v7,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmrghh v6,v13,v9
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// lvrx v9,r3,r26
	temp.u32 = ctx.r3.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vspltish v11,2
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v4,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vslh v31,v8,v12
	// lvrx v7,r29,r25
	temp.u32 = r29.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// vor v7,v4,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// vslh v4,v10,v12
	// vslh v3,v10,v11
	// vslh v1,v7,v12
	// vslh v12,v9,v12
	// vaddshs v2,v3,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v3,v7,v8
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vslh v7,v7,v11
	// vaddshs v4,v10,v9
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vsubshs v10,v10,v9
	// vaddshs v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vslh v1,v8,v11
	// vslh v11,v9,v11
	// vaddshs v2,v2,v4
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v8,v7,v3
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v7,v1,v31
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vaddshs v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vaddshs v11,v2,v5
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubshs v9,v8,v10
	// vaddshs v7,v7,v3
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vaddshs v12,v12,v4
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vsrah v11,v11,v13
	// vaddshs v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v10,v7,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vaddshs v8,v12,v6
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v12,v11,v0
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vsrah v9,v9,v13
	// vaddshs v10,v10,v5
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v11,v8,v13
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vsrah v10,v10,v13
	// vaddshs v13,v11,v0
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v11,v10,v0
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-160
	r11.s64 = ctx.r1.s64 + -160;
	// vaddshs v12,v9,v0
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-144
	r11.s64 = ctx.r1.s64 + -144;
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// addi r10,r1,-160
	ctx.r10.s64 = ctx.r1.s64 + -160;
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lwz r29,-172(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// addi r11,r1,-144
	r11.s64 = ctx.r1.s64 + -144;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// stw r11,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, r11.u32);
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r4,-156(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r27,-144(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r10,-140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// lwz r26,-128(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// stw r27,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r27.u32);
	// lwz r25,-124(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// stw r26,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r26.u32);
	// stw r29,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r29.u32);
	// stw r4,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r4.u32);
	// stw r10,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r10.u32);
	// add r10,r8,r6
	ctx.r10.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r25,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r25.u32);
	// stw r10,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r10.u32);
	// bne cr6,0x82611b5c
	if (!cr6.eq) goto loc_82611B5C;
	// li r4,16
	ctx.r4.s64 = 16;
	// stw r11,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, r11.u32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r31,16
	r31.s64 = 16;
	// add r6,r3,r11
	ctx.r6.u64 = ctx.r3.u64 + r11.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// lvrx v13,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lvrx v12,r9,r31
	temp.u32 = ctx.r9.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lvrx v11,r6,r3
	temp.u32 = ctx.r6.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vaddshs v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// lvlx v10,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// add r5,r7,r9
	ctx.r5.u64 = ctx.r7.u64 + ctx.r9.u64;
	// vaddshs v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// li r30,16
	r30.s64 = 16;
	// vaddshs v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvrx v10,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r10,r7
	r11.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// add r8,r5,r10
	ctx.r8.u64 = ctx.r5.u64 + ctx.r10.u64;
	// vaddshs v0,v10,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v12,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// stvx v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-128
	ctx.r7.s64 = ctx.r1.s64 + -128;
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// vpkshus v13,v12,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v13,v11,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// lwz r7,-128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lwz r5,-124(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-144(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r4,-140(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,-160(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r3,-156(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r6,-176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// lwz r31,-172(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// stw r5,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r5.u32);
	// stw r4,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r4.u32);
	// stw r3,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r3.u32);
	// stw r31,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r31.u32);
	// b 0x8239bd38
	return;
loc_82611870:
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, r11.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// li r5,16
	ctx.r5.s64 = 16;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r9,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r9.u32);
	// stw r10,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r10.u32);
	// lvrx v13,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r10,r7
	r11.u64 = ctx.r10.u64 + ctx.r7.u64;
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v11,r9,r5
	temp.u32 = ctx.r9.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vaddshs v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,-128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lwz r8,-124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// lwz r7,-144(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r6,-140(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// b 0x8239bd38
	return;
loc_82611904:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82611b5c
	if (cr6.eq) goto loc_82611B5C;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// stw r11,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, r11.u32);
	// beq cr6,0x8261199c
	if (cr6.eq) goto loc_8261199C;
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r4,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r4.u32);
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r4,r10
	temp.u32 = ctx.r4.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// vaddshs v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,-128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lwz r9,-124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// lwz r8,-144(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// lwz r7,-140(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// b 0x8239bd38
	return;
loc_8261199C:
	// rlwinm r8,r5,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, ctx.r4.u32);
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// li r31,16
	r31.s64 = 16;
	// li r30,16
	r30.s64 = 16;
	// stw r8,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r8.u32);
	// stw r10,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r10.u32);
	// li r29,16
	r29.s64 = 16;
	// stw r9,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r9.u32);
	// li r28,16
	r28.s64 = 16;
	// stw r5,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r5.u32);
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v13,r4,r31
	temp.u32 = ctx.r4.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r27,16
	r27.s64 = 16;
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v11,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r26,16
	r26.s64 = 16;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// vor v11,v11,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r8,r28
	temp.u32 = ctx.r8.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vaddshs v13,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r9,r27
	temp.u32 = ctx.r9.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v9,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vaddshs v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r5,r26
	temp.u32 = ctx.r5.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vaddshs v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-144
	r11.s64 = ctx.r1.s64 + -144;
	// vaddshs v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r8,r1,-96
	ctx.r8.s64 = ctx.r1.s64 + -96;
	// vaddshs v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-160
	r11.s64 = ctx.r1.s64 + -160;
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// vaddshs v0,v8,v0
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-176
	r11.s64 = ctx.r1.s64 + -176;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v11,v11,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-112
	r11.s64 = ctx.r1.s64 + -112;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r8,r1,-128
	ctx.r8.s64 = ctx.r1.s64 + -128;
	// stvx v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-144
	ctx.r8.s64 = ctx.r1.s64 + -144;
	// vpkshus v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-160
	ctx.r8.s64 = ctx.r1.s64 + -160;
	// vpkshus v12,v9,v9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-176
	ctx.r8.s64 = ctx.r1.s64 + -176;
	// stvx v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-112
	ctx.r8.s64 = ctx.r1.s64 + -112;
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r30,r1,-96
	r30.s64 = ctx.r1.s64 + -96;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r4,-144(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// lwz r5,-128(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lwz r3,-160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r29,-176(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lwz r31,-124(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// lwz r28,-108(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r5,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r5.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// lwz r4,-112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// lwz r30,-140(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -140);
	// lwz r5,-156(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// lwz r3,-172(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// lwz r4,-96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r29,-92(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// stw r4,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r4.u32);
	// stw r31,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, r31.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r5,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r5.u32);
	// stw r3,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r3.u32);
	// stw r28,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r28.u32);
	// stw r29,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r29.u32);
loc_82611B5C:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82611B60"))) PPC_WEAK_FUNC(sub_82611B60);
PPC_FUNC_IMPL(__imp__sub_82611B60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82611B6C:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r10.u32);
	// stw r10,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r10.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82611b6c
	if (!cr6.eq) goto loc_82611B6C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82611B90"))) PPC_WEAK_FUNC(sub_82611B90);
PPC_FUNC_IMPL(__imp__sub_82611B90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,8
	r11.s64 = 8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82611B98:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82611b98
	if (!cr6.eq) goto loc_82611B98;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82611BB4"))) PPC_WEAK_FUNC(sub_82611BB4);
PPC_FUNC_IMPL(__imp__sub_82611BB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82611BB8"))) PPC_WEAK_FUNC(sub_82611BB8);
PPC_FUNC_IMPL(__imp__sub_82611BB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// li r9,4
	ctx.r9.s64 = 4;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82611BC4:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r10.u32);
	// stw r10,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r10.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82611bc4
	if (!cr6.eq) goto loc_82611BC4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82611BE8"))) PPC_WEAK_FUNC(sub_82611BE8);
PPC_FUNC_IMPL(__imp__sub_82611BE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82611C10"))) PPC_WEAK_FUNC(sub_82611C10);
PPC_FUNC_IMPL(__imp__sub_82611C10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lbz r11,1180(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 1180);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x82611cbc
	if (cr6.lt) goto loc_82611CBC;
	// lwz r11,1244(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1244);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82611c40
	if (cr6.eq) goto loc_82611C40;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x82611c4c
	if (!cr6.eq) goto loc_82611C4C;
loc_82611C40:
	// lwz r11,1104(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1104);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82611c58
	if (!cr6.eq) goto loc_82611C58;
loc_82611C4C:
	// lbz r11,27(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 27);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82611cbc
	if (!cr6.eq) goto loc_82611CBC;
loc_82611C58:
	// li r30,0
	r30.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_82611C60:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82611cac
	if (!cr6.eq) goto loc_82611CAC;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82611c94
	if (!cr0.lt) goto loc_82611C94;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611C94:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// mr r11,r31
	r11.u64 = r31.u64;
	// cmpwi cr6,r30,6
	cr6.compare<int32_t>(r30.s32, 6, xer);
	// blt cr6,0x82611c60
	if (cr6.lt) goto loc_82611C60;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82611cb4
	if (cr6.eq) goto loc_82611CB4;
loc_82611CAC:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// b 0x82611e70
	goto loc_82611E70;
loc_82611CB4:
	// li r11,8
	r11.s64 = 8;
	// b 0x82611e70
	goto loc_82611E70;
loc_82611CBC:
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82611d30
	if (!cr6.lt) goto loc_82611D30;
loc_82611CD8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82611d30
	if (cr6.eq) goto loc_82611D30;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82611d20
	if (!cr0.lt) goto loc_82611D20;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611D20:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82611cd8
	if (cr6.gt) goto loc_82611CD8;
loc_82611D30:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82611d6c
	if (!cr0.lt) goto loc_82611D6C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611D6C:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r11,1183(r28)
	PPC_STORE_U8(r28.u32 + 1183, r11.u8);
	// bne cr6,0x82611e74
	if (!cr6.eq) goto loc_82611E74;
	// lwz r11,1104(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1104);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82611e40
	if (cr6.lt) goto loc_82611E40;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82611dfc
	if (!cr6.lt) goto loc_82611DFC;
loc_82611DA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82611dfc
	if (cr6.eq) goto loc_82611DFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82611dec
	if (!cr0.lt) goto loc_82611DEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611DEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82611da4
	if (cr6.gt) goto loc_82611DA4;
loc_82611DFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82611e38
	if (!cr0.lt) goto loc_82611E38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611E38:
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// b 0x82611e70
	goto loc_82611E70;
loc_82611E40:
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82611e6c
	if (!cr0.lt) goto loc_82611E6C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611E6C:
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
loc_82611E70:
	// stb r11,1183(r28)
	PPC_STORE_U8(r28.u32 + 1183, r11.u8);
loc_82611E74:
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82611ee8
	if (!cr6.lt) goto loc_82611EE8;
loc_82611E90:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82611ee8
	if (cr6.eq) goto loc_82611EE8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82611ed8
	if (!cr0.lt) goto loc_82611ED8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611ED8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82611e90
	if (cr6.gt) goto loc_82611E90;
loc_82611EE8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82611f24
	if (!cr0.lt) goto loc_82611F24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82611F24:
	// addi r11,r30,3
	r11.s64 = r30.s64 + 3;
	// stb r11,1184(r28)
	PPC_STORE_U8(r28.u32 + 1184, r11.u8);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82611F34"))) PPC_WEAK_FUNC(sub_82611F34);
PPC_FUNC_IMPL(__imp__sub_82611F34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82611F38"))) PPC_WEAK_FUNC(sub_82611F38);
PPC_FUNC_IMPL(__imp__sub_82611F38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcec
	// lwz r9,24(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// li r11,0
	r11.s64 = 0;
	// lwz r8,0(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r10,260(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 260);
	// lbz r9,-1(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// stw r8,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, ctx.r8.u32);
	// lwz r8,4(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// stw r10,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, ctx.r10.u32);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// stw r9,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r9.u32);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// stw r8,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, ctx.r8.u32);
	// stw r9,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, ctx.r9.u32);
	// dcbzl r0,r10
	memset(base + ((ctx.r10.u32) & ~127), 0, 128);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r31,-96(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r29,r10,25
	r29.u64 = ctx.r10.u32 & 0x7F;
	// lwz r3,-92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x82612028
	if (!cr6.gt) goto loc_82612028;
	// lwz r28,-84(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// lwz r27,-80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r26,-76(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r6,-72(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
loc_82611FA4:
	// lhz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r8,r9,25,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
	// rlwinm r30,r9,0,25,25
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40;
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x82611fe8
	if (cr6.eq) goto loc_82611FE8;
	// lhz r30,0(r31)
	r30.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// rotlwi r30,r30,8
	r30.u64 = __builtin_rotateleft32(r30.u32, 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// or r10,r30,r10
	ctx.r10.u64 = r30.u64 | ctx.r10.u64;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
loc_82611FE8:
	// clrlwi r30,r10,16
	r30.u64 = ctx.r10.u32 & 0xFFFF;
	// clrlwi r10,r9,24
	ctx.r10.u64 = ctx.r9.u32 & 0xFF;
	// mullw r9,r30,r28
	ctx.r9.s64 = int64_t(r30.s32) * int64_t(r28.s32);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// xor r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// lbzx r9,r10,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// rotlwi r30,r9,1
	r30.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// lbzx r25,r9,r5
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// or r3,r25,r3
	ctx.r3.u64 = r25.u64 | ctx.r3.u64;
	// sthx r9,r30,r26
	PPC_STORE_U16(r30.u32 + r26.u32, ctx.r9.u16);
	// blt cr6,0x82611fa4
	if (cr6.lt) goto loc_82611FA4;
loc_82612028:
	// stw r31,20(r7)
	PPC_STORE_U32(ctx.r7.u32 + 20, r31.u32);
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82612030"))) PPC_WEAK_FUNC(sub_82612030);
PPC_FUNC_IMPL(__imp__sub_82612030) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcfc
	// lhz r30,50(r3)
	r30.u64 = PPC_LOAD_U16(ctx.r3.u32 + 50);
	// srawi r31,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	r31.s64 = ctx.r6.s32 >> 16;
	// lhz r29,52(r3)
	r29.u64 = PPC_LOAD_U16(ctx.r3.u32 + 52);
	// li r11,0
	r11.s64 = 0;
	// extsh r3,r6
	ctx.r3.s64 = ctx.r6.s16;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// bne cr6,0x82612060
	if (!cr6.eq) goto loc_82612060;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x82612068
	goto loc_82612068;
loc_82612060:
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x8261206c
	if (!cr6.eq) goto loc_8261206C;
loc_82612068:
	// li r11,1
	r11.s64 = 1;
loc_8261206C:
	// lhz r10,18(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 18);
	// clrlwi r9,r5,31
	ctx.r9.u64 = ctx.r5.u32 & 0x1;
	// lhz r8,16(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 16);
	// srawi r7,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r7,31
	ctx.r9.u64 = ctx.r7.u32 & 0x1;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r7,r10,r3
	ctx.r7.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r10,r9,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// add r6,r10,r31
	ctx.r6.u64 = ctx.r10.u64 + r31.u64;
	// subfic r10,r11,-15
	xer.ca = r11.u32 <= 4294967281;
	ctx.r10.s64 = -15 - r11.s64;
	// beq cr6,0x826120a8
	if (cr6.eq) goto loc_826120A8;
	// subfic r10,r11,-7
	xer.ca = r11.u32 <= 4294967289;
	ctx.r10.s64 = -7 - r11.s64;
loc_826120A8:
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r30,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r29,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// slw r5,r10,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// addi r8,r11,-4
	ctx.r8.s64 = r11.s64 + -4;
	// and r11,r10,r7
	r11.u64 = ctx.r10.u64 & ctx.r7.u64;
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826120f8
	if (!cr6.lt) goto loc_826120F8;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// b 0x82612104
	goto loc_82612104;
loc_826120F8:
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x82612108
	if (!cr6.gt) goto loc_82612108;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
loc_82612104:
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
loc_82612108:
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// bge cr6,0x82612118
	if (!cr6.lt) goto loc_82612118;
	// subf r11,r10,r5
	r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// b 0x82612124
	goto loc_82612124;
loc_82612118:
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x82612128
	if (!cr6.gt) goto loc_82612128;
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
loc_82612124:
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
loc_82612128:
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// bne cr6,0x82612138
	if (!cr6.eq) goto loc_82612138;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
loc_82612138:
	// rlwimi r3,r31,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(r31.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82612140"))) PPC_WEAK_FUNC(sub_82612140);
PPC_FUNC_IMPL(__imp__sub_82612140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// li r31,0
	r31.s64 = 0;
	// li r24,1
	r24.s64 = 1;
	// lis r25,256
	r25.s64 = 16777216;
	// lis r26,1
	r26.s64 = 65536;
	// lwz r11,19984(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 19984);
	// lwz r10,364(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 364);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r22,r11,8024
	r22.s64 = r11.s64 + 8024;
loc_82612180:
	// lwz r11,1972(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r28,76(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826121ac
	if (cr6.eq) goto loc_826121AC;
	// cmpwi cr6,r27,5
	cr6.compare<int32_t>(r27.s32, 5, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x826121b0
	if (cr6.eq) goto loc_826121B0;
loc_826121AC:
	// li r11,0
	r11.s64 = 0;
loc_826121B0:
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x826121c8
	if (!cr6.lt) goto loc_826121C8;
	// li r30,0
	r30.s64 = 0;
loc_826121C8:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263a9f8
	sub_8263A9F8(ctx, base);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x826121e8
	if (cr6.eq) goto loc_826121E8;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x826121ec
	if (cr6.eq) goto loc_826121EC;
loc_826121E8:
	// li r11,0
	r11.s64 = 0;
loc_826121EC:
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82612204
	if (!cr6.lt) goto loc_82612204;
	// li r11,0
	r11.s64 = 0;
loc_82612204:
	// slw r8,r24,r27
	ctx.r8.u64 = r27.u8 & 0x20 ? 0 : (r24.u32 << (r27.u8 & 0x3F));
	// rlwinm r10,r11,4,24,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xF0;
	// slw r11,r24,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r24.u32 << (r11.u8 & 0x3F));
	// clrlwi r9,r30,28
	ctx.r9.u64 = r30.u32 & 0xF;
	// rlwinm r7,r11,24,0,7
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF000000;
	// rlwinm r11,r8,8,0,23
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// subf r7,r25,r7
	ctx.r7.s64 = ctx.r7.s64 - r25.s64;
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// or r11,r7,r11
	r11.u64 = ctx.r7.u64 | r11.u64;
	// cmpwi cr6,r31,36
	cr6.compare<int32_t>(r31.s32, 36, xer);
	// slw r8,r24,r3
	ctx.r8.u64 = ctx.r3.u8 & 0x20 ? 0 : (r24.u32 << (ctx.r3.u8 & 0x3F));
	// rlwinm r8,r8,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 16) & 0xFFFF0000;
	// subf r8,r26,r8
	ctx.r8.s64 = ctx.r8.s64 - r26.s64;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// blt cr6,0x82612180
	if (cr6.lt) goto loc_82612180;
	// li r30,0
	r30.s64 = 0;
	// li r28,0
	r28.s64 = 0;
loc_8261225C:
	// lwz r11,1972(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 1972);
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r31,76(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82612284
	if (cr6.eq) goto loc_82612284;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x82612288
	if (cr6.eq) goto loc_82612288;
loc_82612284:
	// li r11,0
	r11.s64 = 0;
loc_82612288:
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826122a0
	if (!cr6.lt) goto loc_826122A0;
	// li r11,0
	r11.s64 = 0;
loc_826122A0:
	// rlwinm r10,r3,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// or r29,r10,r11
	r29.u64 = ctx.r10.u64 | r11.u64;
	// bl 0x8263a9f8
	sub_8263A9F8(ctx, base);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826122c8
	if (cr6.eq) goto loc_826122C8;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// mr r11,r24
	r11.u64 = r24.u64;
	// beq cr6,0x826122cc
	if (cr6.eq) goto loc_826122CC;
loc_826122C8:
	// li r11,0
	r11.s64 = 0;
loc_826122CC:
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826122e4
	if (!cr6.lt) goto loc_826122E4;
	// li r11,0
	r11.s64 = 0;
loc_826122E4:
	// rlwinm r10,r3,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r9,360(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + 360);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// sthx r11,r28,r9
	PPC_STORE_U16(r28.u32 + ctx.r9.u32, r11.u16);
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// cmpwi cr6,r28,72
	cr6.compare<int32_t>(r28.s32, 72, xer);
	// blt cr6,0x8261225c
	if (cr6.lt) goto loc_8261225C;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82612314"))) PPC_WEAK_FUNC(sub_82612314);
PPC_FUNC_IMPL(__imp__sub_82612314) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82612318"))) PPC_WEAK_FUNC(sub_82612318);
PPC_FUNC_IMPL(__imp__sub_82612318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// li r26,0
	r26.s64 = 0;
	// lwz r11,3916(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3916);
	// lwz r5,15656(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15656);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwz r31,15660(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15660);
	// lwz r4,15664(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15664);
	// lwz r30,15668(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15668);
	// ble cr6,0x826124ec
	if (!cr6.gt) goto loc_826124EC;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
loc_82612348:
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826124dc
	if (cr6.eq) goto loc_826124DC;
loc_82612354:
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r29,r11,2
	r29.s64 = r11.s64 + 2;
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r28,r11,5
	r28.s64 = r11.s64 + 5;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsb r7,r7
	ctx.r7.s64 = ctx.r7.s8;
	// lbz r25,4(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplw cr6,r26,r10
	cr6.compare<uint32_t>(r26.u32, ctx.r10.u32, xer);
	// lbz r10,3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// srawi r7,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// extsb r24,r10
	r24.s64 = ctx.r10.s8;
	// lbz r27,0(r28)
	r27.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// extsb r23,r9
	r23.s64 = ctx.r9.s8;
	// srawi r24,r24,6
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3F) != 0);
	r24.s64 = r24.s32 >> 6;
	// srawi r23,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	r23.s64 = r23.s32 >> 4;
	// rlwimi r7,r24,0,30,31
	ctx.r7.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0x3) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFFC);
	// extsb r27,r27
	r27.s64 = r27.s8;
	// rlwimi r7,r23,0,28,29
	ctx.r7.u64 = (__builtin_rotateleft32(r23.u32, 0) & 0xC) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFF3);
	// rlwimi r10,r9,2,22,27
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0x3F0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFC0F);
	// srawi r27,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	r27.s64 = r27.s32 >> 2;
	// rlwimi r7,r8,0,24,25
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0xC0) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwimi r25,r27,0,26,27
	r25.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0x30) | (r25.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r10,r10,2,20,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFF0;
	// clrlwi r9,r7,24
	ctx.r9.u64 = ctx.r7.u32 & 0xFF;
	// rlwinm r7,r25,0,24,27
	ctx.r7.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xF0;
	// clrlwi r8,r10,24
	ctx.r8.u64 = ctx.r10.u32 & 0xFF;
	// beq cr6,0x82612414
	if (cr6.eq) goto loc_82612414;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lbz r27,1(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r25,5(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// lbz r24,0(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r27,r27,0,28,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xC;
	// lbz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r25,r25,0,28,29
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xC;
	// srawi r27,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	r27.s64 = r27.s32 >> 2;
	// rlwinm r24,r24,0,28,29
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xC;
	// srawi r25,r25,2
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x3) != 0);
	r25.s64 = r25.s32 >> 2;
	// rlwinm r10,r10,0,28,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xC;
	// or r27,r27,r24
	r27.u64 = r27.u64 | r24.u64;
	// or r10,r25,r10
	ctx.r10.u64 = r25.u64 | ctx.r10.u64;
	// or r8,r27,r8
	ctx.r8.u64 = r27.u64 | ctx.r8.u64;
	// or r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 | ctx.r7.u64;
loc_82612414:
	// stb r9,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r9.u8);
	// addi r9,r5,1
	ctx.r9.s64 = ctx.r5.s64 + 1;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// addi r5,r9,1
	ctx.r5.s64 = ctx.r9.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// stb r7,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r7.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lbz r27,4(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 10);
	// lbz r24,11(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 11);
	// rlwimi r7,r27,0,24,27
	ctx.r7.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xF0) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFF0F);
	// lbz r27,0(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r28,0(r28)
	r28.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// rlwinm r27,r27,2,22,25
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0x3C0;
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r28,r28,0,26,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r25,-2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// andi. r7,r7,243
	ctx.r7.u64 = ctx.r7.u64 & 243;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// rlwimi r24,r7,2,0,29
	r24.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xFFFFFFFC) | (r24.u64 & 0xFFFFFFFF00000003);
	// lbz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// clrlwi r29,r27,24
	r29.u64 = r27.u32 & 0xFF;
	// rlwinm r27,r7,0,26,27
	r27.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x30;
	// or r7,r24,r28
	ctx.r7.u64 = r24.u64 | r28.u64;
	// rlwinm r28,r9,0,26,27
	r28.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x30;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// rlwimi r9,r8,2,22,29
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 2) & 0x3FC) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFC03);
	// srawi r8,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	ctx.r8.s64 = r28.s32 >> 2;
	// rlwinm r28,r24,0,26,27
	r28.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x30;
	// clrlwi r9,r9,22
	ctx.r9.u64 = ctx.r9.u32 & 0x3FF;
	// or r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 | r28.u64;
	// rlwimi r25,r9,2,0,29
	r25.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0xFFFFFFFC) | (r25.u64 & 0xFFFFFFFF00000003);
	// extsb r9,r8
	ctx.r9.s64 = ctx.r8.s8;
	// rlwimi r10,r25,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(r25.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// or r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 | r29.u64;
	// or r8,r9,r27
	ctx.r8.u64 = ctx.r9.u64 | r27.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r31,1
	ctx.r10.s64 = r31.s64 + 1;
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r10,1
	r31.s64 = ctx.r10.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// stb r7,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r7.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplw cr6,r6,r10
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r10.u32, xer);
	// blt cr6,0x82612354
	if (cr6.lt) goto loc_82612354;
loc_826124DC:
	// lwz r9,140(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmplw cr6,r26,r9
	cr6.compare<uint32_t>(r26.u32, ctx.r9.u32, xer);
	// blt cr6,0x82612348
	if (cr6.lt) goto loc_82612348;
loc_826124EC:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826124F0"))) PPC_WEAK_FUNC(sub_826124F0);
PPC_FUNC_IMPL(__imp__sub_826124F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r16,r7
	r16.u64 = ctx.r7.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r17,r6
	r17.u64 = ctx.r6.u64;
	// lhz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// lhz r11,50(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rlwinm r29,r10,31,1,31
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r15,r11,31,1,31
	r15.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// rotlwi r18,r10,3
	r18.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lwz r24,1292(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 1292);
	// rotlwi r10,r11,2
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 2);
	// stw r30,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r30.u32);
	// rotlwi r21,r11,3
	r21.u64 = __builtin_rotateleft32(r11.u32, 3);
	// stw r17,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r17.u32);
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// stw r16,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r16.u32);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// stw r15,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r15.u32);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// rotlwi r19,r11,4
	r19.u64 = __builtin_rotateleft32(r11.u32, 4);
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// stw r18,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r18.u32);
	// add r10,r21,r5
	ctx.r10.u64 = r21.u64 + ctx.r5.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r4,r8
	ctx.r4.s64 = -ctx.r8.s64;
	// dcbt r4,r10
	// neg r3,r11
	ctx.r3.s64 = -r11.s64;
	// dcbt r3,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r19,r5
	ctx.r10.u64 = r19.u64 + ctx.r5.u64;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r4,r10
	// dcbt r3,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// dcbt r0,r5
	// dcbt r11,r5
	// dcbt r8,r5
	// dcbt r9,r5
	// lwz r11,19976(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826125f8
	if (cr6.eq) goto loc_826125F8;
	// lwz r11,19980(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826125f8
	if (cr6.eq) goto loc_826125F8;
	// lwz r11,1520(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1520);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826125f8
	if (!cr6.eq) goto loc_826125F8;
	// lwz r11,21268(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21268);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x826125fc
	goto loc_826125FC;
loc_826125F8:
	// lwz r11,21268(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21268);
loc_826125FC:
	// stw r11,21264(r30)
	PPC_STORE_U32(r30.u32 + 21264, r11.u32);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// li r14,0
	r14.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// addi r20,r11,8240
	r20.s64 = r11.s64 + 8240;
	// beq cr6,0x82612a28
	if (cr6.eq) goto loc_82612A28;
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r14.u32);
loc_8261261C:
	// lwz r11,21236(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// beq cr6,0x8261265c
	if (cr6.eq) goto loc_8261265C;
	// cmplw cr6,r14,r11
	cr6.compare<uint32_t>(r14.u32, r11.u32, xer);
	// bge cr6,0x82612654
	if (!cr6.lt) goto loc_82612654;
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82612654
	if (!cr6.eq) goto loc_82612654;
	// li r15,0
	r15.s64 = 0;
	// b 0x82612668
	goto loc_82612668;
loc_82612654:
	// li r15,1
	r15.s64 = 1;
	// b 0x82612668
	goto loc_82612668;
loc_8261265C:
	// subfc r11,r11,r14
	xer.ca = r14.u32 >= r11.u32;
	r11.s64 = r14.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r15,r11,1
	r15.s64 = r11.s64 + 1;
loc_82612668:
	// lwz r30,-108(r20)
	r30.u64 = PPC_LOAD_U32(r20.u32 + -108);
	// add r28,r23,r21
	r28.u64 = r23.u64 + r21.u64;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// mr r16,r23
	r16.u64 = r23.u64;
	// lhz r27,74(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// rlwinm r10,r30,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lbz r6,-106(r20)
	ctx.r6.u64 = PPC_LOAD_U8(r20.u32 + -106);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826126bc
	if (cr6.eq) goto loc_826126BC;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826126BC:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x82612714
	if (!cr6.eq) goto loc_82612714;
	// lwz r30,-108(r20)
	r30.u64 = PPC_LOAD_U32(r20.u32 + -108);
	// add r27,r23,r19
	r27.u64 = r23.u64 + r19.u64;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// rlwinm r10,r30,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lbz r6,-106(r20)
	ctx.r6.u64 = PPC_LOAD_U8(r20.u32 + -106);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82612714
	if (cr6.eq) goto loc_82612714;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612714:
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x82612924
	if (!cr6.gt) goto loc_82612924;
	// addi r30,r23,16
	r30.s64 = r23.s64 + 16;
	// addi r22,r21,-16
	r22.s64 = r21.s64 + -16;
	// addi r18,r19,-16
	r18.s64 = r19.s64 + -16;
loc_82612730:
	// addic. r17,r11,1
	xer.ca = r11.u32 > 4294967294;
	r17.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r17.s32, 0, xer);
	// bne 0x826127c0
	if (!cr0.eq) goto loc_826127C0;
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r10,r30,r21
	ctx.r10.u64 = r30.u64 + r21.u64;
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r5,r8
	ctx.r5.s64 = -ctx.r8.s64;
	// dcbt r5,r10
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r30,r19
	ctx.r10.u64 = r30.u64 + r19.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r5,r10
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// addi r10,r30,16
	ctx.r10.s64 = r30.s64 + 16;
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
loc_826127C0:
	// lwz r29,-108(r20)
	r29.u64 = PPC_LOAD_U32(r20.u32 + -108);
	// addi r25,r30,16
	r25.s64 = r30.s64 + 16;
	// lbz r28,1180(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r27,74(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// rlwinm r10,r29,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFE;
	// add r26,r22,r25
	r26.u64 = r22.u64 + r25.u64;
	// lbz r6,-106(r20)
	ctx.r6.u64 = PPC_LOAD_U8(r20.u32 + -106);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r10,r26
	ctx.r3.u64 = ctx.r10.u64 + r26.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82612814
	if (cr6.eq) goto loc_82612814;
	// rlwinm r11,r29,16,16,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r26
	ctx.r3.u64 = ctx.r10.u64 + r26.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612814:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x8261286c
	if (!cr6.eq) goto loc_8261286C;
	// lwz r29,-108(r20)
	r29.u64 = PPC_LOAD_U32(r20.u32 + -108);
	// add r26,r18,r25
	r26.u64 = r18.u64 + r25.u64;
	// lbz r28,1180(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r27,74(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// rlwinm r10,r29,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFE;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lbz r6,-106(r20)
	ctx.r6.u64 = PPC_LOAD_U8(r20.u32 + -106);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r10,r26
	ctx.r3.u64 = ctx.r10.u64 + r26.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261286c
	if (cr6.eq) goto loc_8261286C;
	// rlwinm r11,r29,16,16,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 16) & 0xFFFF;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r26
	ctx.r3.u64 = ctx.r10.u64 + r26.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_8261286C:
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// addi r27,r30,-13
	r27.s64 = r30.s64 + -13;
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lbz r10,180(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 180);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,184(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 184);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,181(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 181);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826128b8
	if (cr6.lt) goto loc_826128B8;
	// lwz r11,188(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 188);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_826128B8:
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// addi r27,r30,-5
	r27.s64 = r30.s64 + -5;
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lbz r10,180(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 180);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,184(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 184);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,181(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 181);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x82612904
	if (cr6.lt) goto loc_82612904;
	// lwz r11,188(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 188);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82612904:
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mr r11,r17
	r11.u64 = r17.u64;
	// addi r16,r16,16
	r16.s64 = r16.s64 + 16;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82612730
	if (cr6.lt) goto loc_82612730;
	// lwz r18,92(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r17,300(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
loc_82612924:
	// lhz r11,82(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 82);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// add r23,r11,r23
	r23.u64 = r11.u64 + r23.u64;
	// bne cr6,0x826129b0
	if (!cr6.eq) goto loc_826129B0;
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r10,r23,r21
	ctx.r10.u64 = r23.u64 + r21.u64;
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r5,r8
	ctx.r5.s64 = -ctx.r8.s64;
	// dcbt r5,r10
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r23,r19
	ctx.r10.u64 = r23.u64 + r19.u64;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r5,r10
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// dcbt r0,r23
	// dcbt r11,r23
	// dcbt r8,r23
	// dcbt r9,r23
loc_826129B0:
	// lbz r30,1180(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// addi r28,r16,3
	r28.s64 = r16.s64 + 3;
	// lhz r29,74(r31)
	r29.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lbz r10,180(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 180);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r11,184(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 184);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,181(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 181);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826129fc
	if (cr6.lt) goto loc_826129FC;
	// lwz r11,188(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 188);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_826129FC:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r14,r14,1
	r14.s64 = r14.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r14,r11
	cr6.compare<uint32_t>(r14.u32, r11.u32, xer);
	// mr r29,r11
	r29.u64 = r11.u64;
	// lwz r30,276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// blt cr6,0x8261261c
	if (cr6.lt) goto loc_8261261C;
	// lwz r16,308(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r15,88(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82612A28:
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r18,r17
	r11.u64 = r18.u64 + r17.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r17
	// dcbt r10,r17
	// dcbt r8,r17
	// dcbt r9,r17
	// mr r22,r17
	r22.u64 = r17.u64;
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82612c60
	if (cr6.eq) goto loc_82612C60;
	// addi r24,r29,-1
	r24.s64 = r29.s64 + -1;
	// li r25,0
	r25.s64 = 0;
loc_82612A98:
	// lwz r11,21236(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82612ad0
	if (cr6.eq) goto loc_82612AD0;
	// cmplw cr6,r26,r24
	cr6.compare<uint32_t>(r26.u32, r24.u32, xer);
	// bge cr6,0x82612ac8
	if (!cr6.lt) goto loc_82612AC8;
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82612ac8
	if (!cr6.eq) goto loc_82612AC8;
	// li r27,0
	r27.s64 = 0;
	// b 0x82612ae4
	goto loc_82612AE4;
loc_82612AC8:
	// li r27,1
	r27.s64 = 1;
	// b 0x82612b04
	goto loc_82612B04;
loc_82612AD0:
	// subfc r11,r24,r26
	xer.ca = r26.u32 >= r24.u32;
	r11.s64 = r26.s64 - r24.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r27,r11,1
	r27.s64 = r11.s64 + 1;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82612b04
	if (!cr6.eq) goto loc_82612B04;
loc_82612AE4:
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r22
	r11.u64 = ctx.r10.u64 + r22.u64;
	// add r3,r11,r18
	ctx.r3.u64 = r11.u64 + r18.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612B04:
	// addi r28,r22,8
	r28.s64 = r22.s64 + 8;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r15,1
	cr6.compare<uint32_t>(r15.u32, 1, xer);
	// ble cr6,0x82612be4
	if (!cr6.gt) goto loc_82612BE4;
	// addi r30,r28,8
	r30.s64 = r28.s64 + 8;
loc_82612B18:
	// addic. r29,r11,1
	xer.ca = r11.u32 > 4294967294;
	r29.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// bne 0x82612b7c
	if (!cr0.eq) goto loc_82612B7C;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r28,r18
	r11.u64 = r28.u64 + r18.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r30
	// dcbt r10,r30
	// dcbt r8,r30
	// dcbt r9,r30
loc_82612B7C:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82612ba4
	if (!cr6.eq) goto loc_82612BA4;
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r28
	r11.u64 = ctx.r10.u64 + r28.u64;
	// add r3,r11,r18
	ctx.r3.u64 = r11.u64 + r18.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612BA4:
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r3,r11,-13
	ctx.r3.s64 = r11.s64 + -13;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// mr r11,r29
	r11.u64 = r29.u64;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplw cr6,r11,r15
	cr6.compare<uint32_t>(r11.u32, r15.u32, xer);
	// blt cr6,0x82612b18
	if (cr6.lt) goto loc_82612B18;
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r30,276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
loc_82612BE4:
	// lhz r11,84(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 84);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// bne cr6,0x82612c4c
	if (!cr6.eq) goto loc_82612C4C;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r22,r18
	r11.u64 = r22.u64 + r18.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r22
	// dcbt r10,r22
	// dcbt r8,r22
	// dcbt r9,r22
loc_82612C4C:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// cmplw cr6,r26,r29
	cr6.compare<uint32_t>(r26.u32, r29.u32, xer);
	// blt cr6,0x82612a98
	if (cr6.lt) goto loc_82612A98;
	// b 0x82612c64
	goto loc_82612C64;
loc_82612C60:
	// lwz r28,92(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_82612C64:
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r18,r16
	r11.u64 = r18.u64 + r16.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r17
	// dcbt r10,r17
	// dcbt r8,r17
	// dcbt r9,r17
	// mr r26,r16
	r26.u64 = r16.u64;
	// li r24,0
	r24.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82612ea0
	if (cr6.eq) goto loc_82612EA0;
	// addi r21,r29,-1
	r21.s64 = r29.s64 + -1;
	// li r23,0
	r23.s64 = 0;
	// b 0x82612cdc
	goto loc_82612CDC;
loc_82612CD8:
	// lwz r30,276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
loc_82612CDC:
	// lwz r11,21236(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82612d14
	if (cr6.eq) goto loc_82612D14;
	// cmplw cr6,r24,r21
	cr6.compare<uint32_t>(r24.u32, r21.u32, xer);
	// bge cr6,0x82612d0c
	if (!cr6.lt) goto loc_82612D0C;
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82612d0c
	if (!cr6.eq) goto loc_82612D0C;
	// li r25,0
	r25.s64 = 0;
	// b 0x82612d28
	goto loc_82612D28;
loc_82612D0C:
	// li r25,1
	r25.s64 = 1;
	// b 0x82612d48
	goto loc_82612D48;
loc_82612D14:
	// subfc r11,r21,r24
	xer.ca = r24.u32 >= r21.u32;
	r11.s64 = r24.s64 - r21.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x82612d48
	if (!cr6.eq) goto loc_82612D48;
loc_82612D28:
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r26
	r11.u64 = ctx.r10.u64 + r26.u64;
	// add r3,r11,r18
	ctx.r3.u64 = r11.u64 + r18.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612D48:
	// addi r30,r26,8
	r30.s64 = r26.s64 + 8;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r15,1
	cr6.compare<uint32_t>(r15.u32, 1, xer);
	// ble cr6,0x82612e28
	if (!cr6.gt) goto loc_82612E28;
	// addi r29,r30,-8
	r29.s64 = r30.s64 + -8;
loc_82612D5C:
	// addic. r27,r11,1
	xer.ca = r11.u32 > 4294967294;
	r27.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// bne 0x82612dc4
	if (!cr0.eq) goto loc_82612DC4;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r30,r18
	r11.u64 = r30.u64 + r18.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// addi r11,r28,8
	r11.s64 = r28.s64 + 8;
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
loc_82612DC4:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x82612dec
	if (!cr6.eq) goto loc_82612DEC;
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r30
	r11.u64 = ctx.r10.u64 + r30.u64;
	// add r3,r11,r18
	ctx.r3.u64 = r11.u64 + r18.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82612DEC:
	// lbz r11,3(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 3);
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// mr r11,r27
	r11.u64 = r27.u64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// cmplw cr6,r11,r15
	cr6.compare<uint32_t>(r11.u32, r15.u32, xer);
	// blt cr6,0x82612d5c
	if (cr6.lt) goto loc_82612D5C;
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82612E28:
	// lhz r11,84(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 84);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// bne cr6,0x82612e90
	if (!cr6.eq) goto loc_82612E90;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r26,r18
	r11.u64 = r26.u64 + r18.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r22
	// dcbt r10,r22
	// dcbt r8,r22
	// dcbt r9,r22
loc_82612E90:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// cmplw cr6,r24,r29
	cr6.compare<uint32_t>(r24.u32, r29.u32, xer);
	// blt cr6,0x82612cd8
	if (cr6.lt) goto loc_82612CD8;
loc_82612EA0:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82612EA8"))) PPC_WEAK_FUNC(sub_82612EA8);
PPC_FUNC_IMPL(__imp__sub_82612EA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r7,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r7.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r9,15656(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 15656);
	// lhz r11,50(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// stw r29,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r29.u32);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lhz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// stw r30,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r30.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// rlwinm r28,r10,31,1,31
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r9,15660(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 15660);
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lwz r18,1292(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + 1292);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// rotlwi r8,r11,2
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lwz r9,15664(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 15664);
	// rotlwi r16,r11,3
	r16.u64 = __builtin_rotateleft32(r11.u32, 3);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// rotlwi r15,r11,4
	r15.u64 = __builtin_rotateleft32(r11.u32, 4);
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// lwz r9,15668(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 15668);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stw r7,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r7.u32);
	// rotlwi r7,r10,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// add r10,r16,r5
	ctx.r10.u64 = r16.u64 + ctx.r5.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r4,r8
	ctx.r4.s64 = -ctx.r8.s64;
	// dcbt r4,r10
	// neg r3,r11
	ctx.r3.s64 = -r11.s64;
	// dcbt r3,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r15,r5
	ctx.r10.u64 = r15.u64 + ctx.r5.u64;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r4,r10
	// dcbt r3,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// dcbt r0,r5
	// dcbt r11,r5
	// dcbt r8,r5
	// dcbt r9,r5
	// lwz r11,19976(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82612fe8
	if (cr6.eq) goto loc_82612FE8;
	// lwz r11,19980(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82612fe8
	if (cr6.eq) goto loc_82612FE8;
	// lwz r11,1520(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1520);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82612fe8
	if (!cr6.eq) goto loc_82612FE8;
	// lwz r11,21268(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21268);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82612fec
	goto loc_82612FEC;
loc_82612FE8:
	// lwz r11,21268(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21268);
loc_82612FEC:
	// stw r11,21264(r30)
	PPC_STORE_U32(r30.u32 + 21264, r11.u32);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,8240
	r11.s64 = r11.s64 + 8240;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x826138c4
	if (cr6.eq) goto loc_826138C4;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
loc_82613014:
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r11,21236(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// beq cr6,0x82613060
	if (cr6.eq) goto loc_82613060;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82613058
	if (!cr6.lt) goto loc_82613058;
	// lwz r11,21264(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 21264);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82613058
	if (!cr6.eq) goto loc_82613058;
	// li r23,0
	r23.s64 = 0;
	// b 0x82613070
	goto loc_82613070;
loc_82613058:
	// li r23,1
	r23.s64 = 1;
	// b 0x82613070
	goto loc_82613070;
loc_82613060:
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// subfc r11,r11,r10
	xer.ca = ctx.r10.u32 >= r11.u32;
	r11.s64 = ctx.r10.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
loc_82613070:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r14,r17
	r14.u64 = r17.u64;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// add r27,r17,r16
	r27.u64 = r17.u64 + r16.u64;
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lwz r24,80(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r23,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r23.u32);
	// lbz r25,0(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// rlwinm r11,r26,28,4,31
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826130fc
	if (cr6.eq) goto loc_826130FC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r24,-168
	ctx.r10.s64 = r24.s64 + -168;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwzx r30,r11,r10
	r30.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826130fc
	if (cr6.eq) goto loc_826130FC;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826130FC:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82613168
	if (!cr6.eq) goto loc_82613168;
	// clrlwi r11,r26,28
	r11.u64 = r26.u32 & 0xF;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r27,74(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r28,r17,r15
	r28.u64 = r17.u64 + r15.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613168
	if (cr6.eq) goto loc_82613168;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r24,-168
	ctx.r10.s64 = r24.s64 + -168;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwzx r30,r11,r10
	r30.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613168
	if (cr6.eq) goto loc_82613168;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613168:
	// rlwinm r11,r25,28,28,31
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 28) & 0xF;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r27,r17,r10
	r27.u64 = r17.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826131d4
	if (cr6.eq) goto loc_826131D4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r24,-168
	ctx.r10.s64 = r24.s64 + -168;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwzx r30,r11,r10
	r30.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826131d4
	if (cr6.eq) goto loc_826131D4;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826131D4:
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// clrlwi r11,r25,28
	r11.u64 = r25.u32 & 0xF;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r27,r17,r10
	r27.u64 = r17.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261323c
	if (cr6.eq) goto loc_8261323C;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r24,-168
	ctx.r10.s64 = r24.s64 + -168;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwzx r30,r11,r10
	r30.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261323c
	if (cr6.eq) goto loc_8261323C;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r27
	ctx.r3.u64 = ctx.r10.u64 + r27.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_8261323C:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x826136ac
	if (!cr6.gt) goto loc_826136AC;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r29,r17,16
	r29.s64 = r17.s64 + 16;
	// addi r22,r16,-16
	r22.s64 = r16.s64 + -16;
	// addi r20,r11,-16
	r20.s64 = r11.s64 + -16;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r21,r15,-16
	r21.s64 = r15.s64 + -16;
	// addi r19,r11,-16
	r19.s64 = r11.s64 + -16;
loc_82613268:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// addic. r23,r10,1
	xer.ca = ctx.r10.u32 > 4294967294;
	r23.s64 = ctx.r10.s64 + 1;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// lbz r24,0(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r25,0(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bne 0x82613310
	if (!cr0.eq) goto loc_82613310;
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r10,r29,r16
	ctx.r10.u64 = r29.u64 + r16.u64;
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r5,r8
	ctx.r5.s64 = -ctx.r8.s64;
	// dcbt r5,r10
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r29,r15
	ctx.r10.u64 = r29.u64 + r15.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r5,r10
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// addi r10,r29,16
	ctx.r10.s64 = r29.s64 + 16;
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
loc_82613310:
	// rlwinm r11,r25,28,28,31
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 28) & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613380
	if (cr6.eq) goto loc_82613380;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r22,r29
	r11.u64 = r22.u64 + r29.u64;
	// addi r10,r10,-168
	ctx.r10.s64 = ctx.r10.s64 + -168;
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwzx r30,r9,r10
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613380
	if (cr6.eq) goto loc_82613380;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613380:
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826133f8
	if (!cr6.eq) goto loc_826133F8;
	// clrlwi r11,r25,28
	r11.u64 = r25.u32 & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826133f8
	if (cr6.eq) goto loc_826133F8;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r21,r29
	r11.u64 = r21.u64 + r29.u64;
	// addi r10,r10,-168
	ctx.r10.s64 = ctx.r10.s64 + -168;
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwzx r30,r9,r10
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826133f8
	if (cr6.eq) goto loc_826133F8;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826133F8:
	// rlwinm r11,r24,28,28,31
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 28) & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613468
	if (cr6.eq) goto loc_82613468;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r20,r29
	r11.u64 = r20.u64 + r29.u64;
	// addi r10,r10,-168
	ctx.r10.s64 = ctx.r10.s64 + -168;
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwzx r30,r9,r10
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613468
	if (cr6.eq) goto loc_82613468;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613468:
	// clrlwi r11,r24,28
	r11.u64 = r24.u32 & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826134d4
	if (cr6.eq) goto loc_826134D4;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r19,r29
	r11.u64 = r19.u64 + r29.u64;
	// addi r10,r10,-168
	ctx.r10.s64 = ctx.r10.s64 + -168;
	// addi r28,r11,16
	r28.s64 = r11.s64 + 16;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwzx r30,r9,r10
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r11,r30,0,24,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFE;
	// rlwinm r6,r30,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 24) & 0xFF;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826134d4
	if (cr6.eq) goto loc_826134D4;
	// rlwinm r11,r30,16,16,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r10,r11,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r6,r11,24,24,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// add r3,r10,r28
	ctx.r3.u64 = ctx.r10.u64 + r28.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826134D4:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lbz r24,0(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r25,0(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// rlwinm r11,r25,28,4,31
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613558
	if (cr6.eq) goto loc_82613558;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r28,r29,-13
	r28.s64 = r29.s64 + -13;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x82613558
	if (cr6.lt) goto loc_82613558;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613558:
	// clrlwi r11,r25,28
	r11.u64 = r25.u32 & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826135c0
	if (cr6.eq) goto loc_826135C0;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r28,r29,-5
	r28.s64 = r29.s64 + -5;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826135c0
	if (cr6.lt) goto loc_826135C0;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_826135C0:
	// rlwinm r11,r24,28,28,31
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 28) & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261362c
	if (cr6.eq) goto loc_8261362C;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r28,r29,-17
	r28.s64 = r29.s64 + -17;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x8261362c
	if (cr6.lt) goto loc_8261362C;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_8261362C:
	// clrlwi r11,r24,28
	r11.u64 = r24.u32 & 0xF;
	// lbz r27,1180(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r26,74(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613694
	if (cr6.eq) goto loc_82613694;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r28,r29,-9
	r28.s64 = r29.s64 + -9;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x82613694
	if (cr6.lt) goto loc_82613694;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613694:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// addi r14,r14,16
	r14.s64 = r14.s64 + 16;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82613268
	if (cr6.lt) goto loc_82613268;
loc_826136AC:
	// lhz r11,82(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 82);
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r17,r11,r17
	r17.u64 = r11.u64 + r17.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8261373c
	if (!cr6.eq) goto loc_8261373C;
	// lhz r11,74(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// add r10,r17,r16
	ctx.r10.u64 = r17.u64 + r16.u64;
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// dcbt r7,r10
	// neg r6,r9
	ctx.r6.s64 = -ctx.r9.s64;
	// dcbt r6,r10
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// neg r5,r8
	ctx.r5.s64 = -ctx.r8.s64;
	// dcbt r5,r10
	// neg r4,r11
	ctx.r4.s64 = -r11.s64;
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// add r10,r17,r15
	ctx.r10.u64 = r17.u64 + r15.u64;
	// dcbt r7,r10
	// dcbt r6,r10
	// dcbt r5,r10
	// dcbt r4,r10
	// dcbt r0,r10
	// dcbt r11,r10
	// dcbt r8,r10
	// dcbt r9,r10
	// dcbt r0,r17
	// dcbt r11,r17
	// dcbt r8,r17
	// dcbt r9,r17
loc_8261373C:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r28,r14,3
	r28.s64 = r14.s64 + 3;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r27,74(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// rlwinm r11,r10,28,4,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826137c0
	if (cr6.eq) goto loc_826137C0;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826137c0
	if (cr6.lt) goto loc_826137C0;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_826137C0:
	// rlwinm r11,r26,28,28,31
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 28) & 0xF;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// addi r27,r14,-1
	r27.s64 = r14.s64 + -1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261382c
	if (cr6.eq) goto loc_8261382C;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x8261382c
	if (cr6.lt) goto loc_8261382C;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_8261382C:
	// clrlwi r11,r26,28
	r11.u64 = r26.u32 & 0xF;
	// lbz r29,1180(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r28,74(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// addi r27,r14,7
	r27.s64 = r14.s64 + 7;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613894
	if (cr6.eq) goto loc_82613894;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r18
	r30.u64 = r11.u64 + r18.u64;
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// extsb r6,r10
	ctx.r6.s64 = ctx.r10.s8;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
	// lbz r11,1(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// extsb r6,r11
	ctx.r6.s64 = r11.s8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x82613894
	if (cr6.lt) goto loc_82613894;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613894:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82613014
	if (cr6.lt) goto loc_82613014;
	// lwz r30,308(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// rotlwi r28,r10,0
	r28.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r29,332(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
loc_826138C4:
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r29
	// dcbt r10,r29
	// dcbt r8,r29
	// dcbt r9,r29
	// mr r18,r29
	r18.u64 = r29.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82613c48
	if (cr6.eq) goto loc_82613C48;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r24,0
	r24.s64 = 0;
	// lwz r20,128(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r16,132(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r23,r11,-1
	r23.s64 = r11.s64 + -1;
	// lwz r14,308(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r15,108(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r17,116(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r22,80(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82613958:
	// lwz r11,21236(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82613990
	if (cr6.eq) goto loc_82613990;
	// cmplw cr6,r25,r23
	cr6.compare<uint32_t>(r25.u32, r23.u32, xer);
	// bge cr6,0x82613988
	if (!cr6.lt) goto loc_82613988;
	// lwz r11,21264(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 21264);
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82613988
	if (!cr6.eq) goto loc_82613988;
	// li r26,0
	r26.s64 = 0;
	// b 0x8261399c
	goto loc_8261399C;
loc_82613988:
	// li r26,1
	r26.s64 = 1;
	// b 0x8261399c
	goto loc_8261399C;
loc_82613990:
	// subfc r11,r23,r25
	xer.ca = r25.u32 >= r23.u32;
	r11.s64 = r25.s64 - r23.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r26,r11,1
	r26.s64 = r11.s64 + 1;
loc_8261399C:
	// lbz r30,0(r16)
	r30.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// mr r28,r18
	r28.u64 = r18.u64;
	// addi r16,r16,1
	r16.s64 = r16.s64 + 1;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x826139e0
	if (!cr6.eq) goto loc_826139E0;
	// rlwinm r11,r30,30,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826139e0
	if (cr6.eq) goto loc_826139E0;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r18
	r11.u64 = ctx.r10.u64 + r18.u64;
	// add r3,r11,r21
	ctx.r3.u64 = r11.u64 + r21.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_826139E0:
	// rlwinm r11,r30,26,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 26) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613a10
	if (cr6.eq) goto loc_82613A10;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r18
	r11.u64 = ctx.r10.u64 + r18.u64;
	// add r3,r11,r17
	ctx.r3.u64 = r11.u64 + r17.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613A10:
	// addi r19,r18,8
	r19.s64 = r18.s64 + 8;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r15,1
	cr6.compare<uint32_t>(r15.u32, 1, xer);
	// ble cr6,0x82613b80
	if (!cr6.gt) goto loc_82613B80;
	// addi r29,r19,8
	r29.s64 = r19.s64 + 8;
loc_82613A24:
	// lbz r30,0(r16)
	r30.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// addic. r27,r11,1
	xer.ca = r11.u32 > 4294967294;
	r27.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// addi r16,r16,1
	r16.s64 = r16.s64 + 1;
	// bne 0x82613a90
	if (!cr0.eq) goto loc_82613A90;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r19,r21
	r11.u64 = r19.u64 + r21.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r29
	// dcbt r10,r29
	// dcbt r8,r29
	// dcbt r9,r29
loc_82613A90:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x82613ac8
	if (!cr6.eq) goto loc_82613AC8;
	// rlwinm r11,r30,30,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613ac8
	if (cr6.eq) goto loc_82613AC8;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r19
	r11.u64 = ctx.r10.u64 + r19.u64;
	// add r3,r11,r21
	ctx.r3.u64 = r11.u64 + r21.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613AC8:
	// rlwinm r11,r30,26,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 26) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613af8
	if (cr6.eq) goto loc_82613AF8;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r19
	r11.u64 = ctx.r10.u64 + r19.u64;
	// add r3,r11,r17
	ctx.r3.u64 = r11.u64 + r17.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613AF8:
	// lbz r30,0(r20)
	r30.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r11,r30,30,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613b34
	if (cr6.eq) goto loc_82613B34;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613B34:
	// rlwinm r11,r30,26,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 26) & 0x3;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613b68
	if (cr6.eq) goto loc_82613B68;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613B68:
	// mr r11,r27
	r11.u64 = r27.u64;
	// addi r19,r19,8
	r19.s64 = r19.s64 + 8;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplw cr6,r11,r15
	cr6.compare<uint32_t>(r11.u32, r15.u32, xer);
	// blt cr6,0x82613a24
	if (cr6.lt) goto loc_82613A24;
loc_82613B80:
	// lhz r11,84(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 84);
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// add r18,r11,r18
	r18.u64 = r11.u64 + r18.u64;
	// bne cr6,0x82613be8
	if (!cr6.eq) goto loc_82613BE8;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r18,r21
	r11.u64 = r18.u64 + r21.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r18
	// dcbt r10,r18
	// dcbt r8,r18
	// dcbt r9,r18
loc_82613BE8:
	// lbz r11,0(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613c24
	if (cr6.eq) goto loc_82613C24;
	// lbzx r11,r11,r22
	r11.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613C24:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// blt cr6,0x82613958
	if (cr6.lt) goto loc_82613958;
	// lwz r30,308(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r29,332(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// b 0x82613c4c
	goto loc_82613C4C;
loc_82613C48:
	// lwz r19,132(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_82613C4C:
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r25,340(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// neg r8,r8
	ctx.r8.s64 = -ctx.r8.s64;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r29
	// dcbt r10,r29
	// dcbt r8,r29
	// dcbt r9,r29
	// li r22,0
	r22.s64 = 0;
	// lwz r23,15664(r30)
	r23.u64 = PPC_LOAD_U32(r30.u32 + 15664);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// lwz r26,15668(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 15668);
	// beq cr6,0x82613fbc
	if (cr6.eq) goto loc_82613FBC;
	// addi r20,r28,-1
	r20.s64 = r28.s64 + -1;
	// li r21,0
	r21.s64 = 0;
	// b 0x82613cd0
	goto loc_82613CD0;
loc_82613CCC:
	// lwz r30,308(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
loc_82613CD0:
	// lwz r11,21236(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82613d08
	if (cr6.eq) goto loc_82613D08;
	// cmplw cr6,r22,r20
	cr6.compare<uint32_t>(r22.u32, r20.u32, xer);
	// bge cr6,0x82613d00
	if (!cr6.lt) goto loc_82613D00;
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// add r11,r21,r11
	r11.u64 = r21.u64 + r11.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82613d00
	if (!cr6.eq) goto loc_82613D00;
	// li r24,0
	r24.s64 = 0;
	// b 0x82613d14
	goto loc_82613D14;
loc_82613D00:
	// li r24,1
	r24.s64 = 1;
	// b 0x82613d14
	goto loc_82613D14;
loc_82613D08:
	// subfc r11,r20,r22
	xer.ca = r22.u32 >= r20.u32;
	r11.s64 = r22.s64 - r20.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r24,r11,1
	r24.s64 = r11.s64 + 1;
loc_82613D14:
	// lbz r30,0(r23)
	r30.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// mr r28,r25
	r28.u64 = r25.u64;
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x82613d60
	if (!cr6.eq) goto loc_82613D60;
	// clrlwi r11,r30,30
	r11.u64 = r30.u32 & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613d60
	if (cr6.eq) goto loc_82613D60;
	// lwz r17,80(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r16,88(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r25
	r11.u64 = ctx.r10.u64 + r25.u64;
	// add r3,r11,r16
	ctx.r3.u64 = r11.u64 + r16.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
	// b 0x82613d68
	goto loc_82613D68;
loc_82613D60:
	// lwz r16,88(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r17,80(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82613D68:
	// rlwinm r11,r30,28,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 28) & 0x3;
	// lwz r15,116(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613d9c
	if (cr6.eq) goto loc_82613D9C;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r25
	r11.u64 = ctx.r10.u64 + r25.u64;
	// add r3,r11,r15
	ctx.r3.u64 = r11.u64 + r15.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613D9C:
	// lwz r14,108(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r14,1
	cr6.compare<uint32_t>(r14.u32, 1, xer);
	// ble cr6,0x82613f04
	if (!cr6.gt) goto loc_82613F04;
	// addi r29,r25,8
	r29.s64 = r25.s64 + 8;
loc_82613DB0:
	// lbz r30,0(r23)
	r30.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// addic. r27,r11,1
	xer.ca = r11.u32 > 4294967294;
	r27.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// bne 0x82613e20
	if (!cr0.eq) goto loc_82613E20;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r29,r16
	r11.u64 = r29.u64 + r16.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// addi r11,r19,8
	r11.s64 = r19.s64 + 8;
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
loc_82613E20:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x82613e54
	if (!cr6.eq) goto loc_82613E54;
	// clrlwi r11,r30,30
	r11.u64 = r30.u32 & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613e54
	if (cr6.eq) goto loc_82613E54;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	// add r3,r11,r16
	ctx.r3.u64 = r11.u64 + r16.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613E54:
	// rlwinm r11,r30,28,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 28) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613e84
	if (cr6.eq) goto loc_82613E84;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// add r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	// add r3,r11,r15
	ctx.r3.u64 = r11.u64 + r15.u64;
	// bl 0x826107f8
	sub_826107F8(ctx, base);
loc_82613E84:
	// lbz r30,0(r26)
	r30.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r11,r30,30
	r11.u64 = r30.u32 & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613ebc
	if (cr6.eq) goto loc_82613EBC;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,3
	ctx.r3.s64 = r11.s64 + 3;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613EBC:
	// rlwinm r11,r30,28,30,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 28) & 0x3;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613ef0
	if (cr6.eq) goto loc_82613EF0;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613EF0:
	// mr r11,r27
	r11.u64 = r27.u64;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// cmplw cr6,r11,r14
	cr6.compare<uint32_t>(r11.u32, r14.u32, xer);
	// blt cr6,0x82613db0
	if (cr6.lt) goto loc_82613DB0;
loc_82613F04:
	// lhz r11,84(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 84);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bne cr6,0x82613f6c
	if (!cr6.eq) goto loc_82613F6C;
	// lhz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// add r11,r25,r16
	r11.u64 = r25.u64 + r16.u64;
	// rotlwi r9,r10,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r8,r11
	// neg r8,r9
	ctx.r8.s64 = -ctx.r9.s64;
	// dcbt r8,r11
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// neg r7,r8
	ctx.r7.s64 = -ctx.r8.s64;
	// dcbt r7,r11
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// dcbt r7,r11
	// dcbt r0,r11
	// dcbt r10,r11
	// dcbt r8,r11
	// dcbt r9,r11
	// dcbt r0,r18
	// dcbt r10,r18
	// dcbt r8,r18
	// dcbt r9,r18
loc_82613F6C:
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lhz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// rlwinm r11,r11,28,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x3;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82613fa8
	if (cr6.eq) goto loc_82613FA8;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// lbz r5,1180(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 1180);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// clrlwi r6,r11,28
	ctx.r6.u64 = r11.u32 & 0xF;
	// mullw r11,r10,r4
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// bl 0x82610c68
	sub_82610C68(ctx, base);
loc_82613FA8:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r21,r21,4
	r21.s64 = r21.s64 + 4;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// blt cr6,0x82613ccc
	if (cr6.lt) goto loc_82613CCC;
loc_82613FBC:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82613FC4"))) PPC_WEAK_FUNC(sub_82613FC4);
PPC_FUNC_IMPL(__imp__sub_82613FC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82613FC8"))) PPC_WEAK_FUNC(sub_82613FC8);
PPC_FUNC_IMPL(__imp__sub_82613FC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf0
	// addi r29,r3,23264
	r29.s64 = ctx.r3.s64 + 23264;
	// li r11,0
	r11.s64 = 0;
loc_82613FD8:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// srawi r7,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r7.s64 = r11.s32 >> 2;
	// srawi r6,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r6.s64 = r11.s32 >> 3;
	// srawi r31,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r31.s64 = r11.s32 >> 5;
	// srawi r28,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r28.s64 = r11.s32 >> 6;
	// srawi r10,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r10.s64 = r11.s32 >> 7;
	// srawi r27,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r27.s64 = r11.s32 >> 8;
	// srawi r4,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r4.s64 = r11.s32 >> 4;
	// clrlwi r30,r6,31
	r30.u64 = ctx.r6.u32 & 0x1;
	// clrlwi r26,r4,31
	r26.u64 = ctx.r4.u32 & 0x1;
	// clrlwi r6,r28,31
	ctx.r6.u64 = r28.u32 & 0x1;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// clrlwi r5,r9,31
	ctx.r5.u64 = ctx.r9.u32 & 0x1;
	// clrlwi r4,r7,31
	ctx.r4.u64 = ctx.r7.u32 & 0x1;
	// clrlwi r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	// clrlwi r31,r31,31
	r31.u64 = r31.u32 & 0x1;
	// clrlwi r9,r27,31
	ctx.r9.u64 = r27.u32 & 0x1;
	// cmplw cr6,r26,r10
	cr6.compare<uint32_t>(r26.u32, ctx.r10.u32, xer);
	// clrlwi r7,r6,24
	ctx.r7.u64 = ctx.r6.u32 & 0xFF;
	// beq cr6,0x8261402c
	if (cr6.eq) goto loc_8261402C;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
loc_8261402C:
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// bne cr6,0x82614048
	if (!cr6.eq) goto loc_82614048;
	// clrlwi r9,r8,24
	ctx.r9.u64 = ctx.r8.u32 & 0xFF;
loc_82614048:
	// clrlwi r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r8,r5,24
	ctx.r8.u64 = ctx.r5.u32 & 0xFF;
	// clrlwi r7,r6,24
	ctx.r7.u64 = ctx.r6.u32 & 0xFF;
	// xor r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// clrlwi r8,r31,24
	ctx.r8.u64 = r31.u32 & 0xFF;
	// beq cr6,0x8261406c
	if (cr6.eq) goto loc_8261406C;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_8261406C:
	// clrlwi r7,r4,24
	ctx.r7.u64 = ctx.r4.u32 & 0xFF;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// xor r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 ^ ctx.r8.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// beq cr6,0x8261408c
	if (cr6.eq) goto loc_8261408C;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
loc_8261408C:
	// clrlwi r6,r30,24
	ctx.r6.u64 = r30.u32 & 0xFF;
	// clrlwi r5,r8,24
	ctx.r5.u64 = ctx.r8.u32 & 0xFF;
	// xor r8,r6,r7
	ctx.r8.u64 = ctx.r6.u64 ^ ctx.r7.u64;
	// cmpwi cr6,r11,256
	cr6.compare<int32_t>(r11.s32, 256, xer);
	// rlwinm r8,r8,1,23,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1FE;
	// or r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 | ctx.r5.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// blt cr6,0x826140d0
	if (cr6.lt) goto loc_826140D0;
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r8,-256(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + -256);
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// stb r10,-256(r9)
	PPC_STORE_U8(ctx.r9.u32 + -256, ctx.r10.u8);
	// b 0x826140d4
	goto loc_826140D4;
loc_826140D0:
	// stbx r10,r11,r29
	PPC_STORE_U8(r11.u32 + r29.u32, ctx.r10.u8);
loc_826140D4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,512
	cr6.compare<int32_t>(r11.s32, 512, xer);
	// blt cr6,0x82613fd8
	if (cr6.lt) goto loc_82613FD8;
	// addi r7,r3,23520
	ctx.r7.s64 = ctx.r3.s64 + 23520;
	// li r11,0
	r11.s64 = 0;
loc_826140E8:
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	// rlwinm r9,r10,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r10,r8,1,0,30
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 1) & 0xFFFFFFFE) | (ctx.r10.u64 & 0xFFFFFFFF00000001);
	// rlwinm r8,r9,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r9,r10,1,0,30
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0xFFFFFFFE) | (ctx.r9.u64 & 0xFFFFFFFF00000001);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r10,r9,1,0,30
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 1) & 0xFFFFFFFE) | (ctx.r10.u64 & 0xFFFFFFFF00000001);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r9,r10,1,0,30
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0xFFFFFFFE) | (ctx.r9.u64 & 0xFFFFFFFF00000001);
	// rlwinm r10,r8,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r8,r9,1,0,30
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 1) & 0xFFFFFFFE) | (ctx.r8.u64 & 0xFFFFFFFF00000001);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwimi r9,r8,1,0,30
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 1) & 0xFFFFFFFE) | (ctx.r9.u64 & 0xFFFFFFFF00000001);
	// rlwimi r10,r9,2,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// stbx r10,r11,r7
	PPC_STORE_U8(r11.u32 + ctx.r7.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x826140e8
	if (cr6.lt) goto loc_826140E8;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82614144"))) PPC_WEAK_FUNC(sub_82614144);
PPC_FUNC_IMPL(__imp__sub_82614144) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82614148"))) PPC_WEAK_FUNC(sub_82614148);
PPC_FUNC_IMPL(__imp__sub_82614148) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r3,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r3.u32);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lhz r10,52(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 52);
	// lhz r11,50(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 50);
	// rlwinm r27,r10,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r16,1248(r3)
	r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1248);
	// rlwinm r23,r11,31,1,31
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r28,1252(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1252);
	// stw r3,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, ctx.r3.u32);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r4,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r4.u32);
	// stw r27,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r27.u32);
	// stw r23,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r23.u32);
	// beq cr6,0x826147a0
	if (cr6.eq) goto loc_826147A0;
	// li r20,256
	r20.s64 = 256;
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// stw r20,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r20.u32);
	// rlwinm r8,r23,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r20,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r20.u32);
	// addi r10,r11,11208
	ctx.r10.s64 = r11.s64 + 11208;
	// b 0x826141cc
	goto loc_826141CC;
loc_826141C0:
	// lwz r20,124(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r6,108(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r7,104(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
loc_826141CC:
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826141f4
	if (cr6.eq) goto loc_826141F4;
	// lwz r11,1240(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1240);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826141f4
	if (!cr6.eq) goto loc_826141F4;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// b 0x826141fc
	goto loc_826141FC;
loc_826141F4:
	// li r11,1
	r11.s64 = 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_826141FC:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r30,r20,-128
	r30.s64 = r20.s64 + -128;
	// lwz r11,188(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// dcbt r30,r11
	// dcbt r20,r11
	// addi r30,r20,128
	r30.s64 = r20.s64 + 128;
	// dcbt r30,r11
	// addi r30,r20,256
	r30.s64 = r20.s64 + 256;
	// dcbt r30,r11
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r11,192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// addi r26,r30,-128
	r26.s64 = r30.s64 + -128;
	// dcbt r26,r11
	// dcbt r30,r11
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x8261474c
	if (cr6.eq) goto loc_8261474C;
	// and r11,r9,r23
	r11.u64 = ctx.r9.u64 & r23.u64;
	// stw r6,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r6.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// rlwinm r30,r5,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mr r15,r4
	r15.u64 = ctx.r4.u64;
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// add r9,r8,r7
	ctx.r9.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// add r9,r8,r6
	ctx.r9.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// add r9,r8,r31
	ctx.r9.u64 = ctx.r8.u64 + r31.u64;
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r14,r11,r16
	r14.s64 = r16.s64 - r11.s64;
	// add r11,r5,r23
	r11.u64 = ctx.r5.u64 + r23.u64;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
loc_82614298:
	// lis r12,-1
	r12.s64 = -65536;
	// ld r11,0(r16)
	r11.u64 = PPC_LOAD_U64(r16.u32 + 0);
	// cntlzw r9,r4
	ctx.r9.u64 = ctx.r4.u32 == 0 ? 32 : __builtin_clz(ctx.r4.u32);
	// lwz r29,132(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// ld r8,0(r14)
	ctx.r8.u64 = PPC_LOAD_U64(r14.u32 + 0);
	// rlwinm r27,r9,27,31,31
	r27.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// addi r9,r10,-192
	ctx.r9.s64 = ctx.r10.s64 + -192;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// addi r6,r10,-192
	ctx.r6.s64 = ctx.r10.s64 + -192;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// lis r12,-1
	r12.s64 = -65536;
	// addi r9,r10,-112
	ctx.r9.s64 = ctx.r10.s64 + -112;
	// stw r6,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r6.u32);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// addi r30,r9,16
	r30.s64 = ctx.r9.s64 + 16;
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r29,r29,3,0,28
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// srawi r7,r15,31
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = r15.s32 >> 31;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rlwinm r7,r7,3,28,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x8;
	// ldx r30,r29,r30
	r30.u64 = PPC_LOAD_U64(r29.u32 + r30.u32);
	// and r24,r8,r12
	r24.u64 = ctx.r8.u64 & r12.u64;
	// lis r12,-1
	r12.s64 = -65536;
	// subf r7,r7,r16
	ctx.r7.s64 = r16.s64 - ctx.r7.s64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rldicl r8,r11,56,56
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFF;
	// std r30,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r30.u64);
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// addi r22,r10,-192
	r22.s64 = ctx.r10.s64 + -192;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// ld r9,0(r7)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// rotlwi r25,r8,0
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// addi r19,r10,-192
	r19.s64 = ctx.r10.s64 + -192;
	// and r21,r9,r12
	r21.u64 = ctx.r9.u64 & r12.u64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r7,r11,48,56
	ctx.r7.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFF;
	// clrlwi r26,r9,24
	r26.u64 = ctx.r9.u32 & 0xFF;
	// lbzx r8,r25,r19
	ctx.r8.u64 = PPC_LOAD_U8(r25.u32 + r19.u32);
	// addi r18,r10,-192
	r18.s64 = ctx.r10.s64 + -192;
	// rotlwi r29,r7,0
	r29.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rldicl r6,r11,40,56
	ctx.r6.u64 = __builtin_rotateleft64(r11.u64, 40) & 0xFF;
	// addi r17,r10,-192
	r17.s64 = ctx.r10.s64 + -192;
	// lbzx r9,r26,r22
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + r22.u32);
	// rotlwi r30,r6,0
	r30.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// rldicl r5,r11,32,56
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFF;
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbzx r7,r29,r18
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + r18.u32);
	// rldicl r31,r11,24,56
	r31.u64 = __builtin_rotateleft64(r11.u64, 24) & 0xFF;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// rldicl r11,r11,16,48
	r11.u64 = __builtin_rotateleft64(r11.u64, 16) & 0xFFFF;
	// rldicr r9,r9,8,55
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// rotlwi r6,r5,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r5.u32, 0);
	// lbzx r5,r30,r17
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + r17.u32);
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// rlwinm r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// rldicr r9,r9,8,55
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// or r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 | ctx.r5.u64;
	// rotlwi r11,r31,0
	r11.u64 = __builtin_rotateleft32(r31.u32, 0);
	// rldicr r9,r9,8,55
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r22,136(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lbzx r8,r6,r22
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + r22.u32);
	// lwz r22,140(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// rldicr r9,r9,8,55
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lbzx r31,r11,r22
	r31.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// or r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 | r31.u64;
	// ld r22,160(r1)
	r22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// and r22,r9,r22
	r22.u64 = ctx.r9.u64 & r22.u64;
	// bne cr6,0x8261469c
	if (!cr6.eq) goto loc_8261469C;
	// lwz r8,188(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// li r17,255
	r17.s64 = 255;
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r18,255
	r18.s64 = 255;
	// li r19,255
	r19.s64 = 255;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// li r20,255
	r20.s64 = 255;
	// li r31,255
	r31.s64 = 255;
	// li r23,255
	r23.s64 = 255;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,16384
	cr6.compare<int32_t>(ctx.r7.s32, 16384, xer);
	// beq cr6,0x8261446c
	if (cr6.eq) goto loc_8261446C;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwzx r5,r5,r8
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// subf r5,r7,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r7.s64;
	// or r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 | ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82614434
	if (!cr6.eq) goto loc_82614434;
	// rldicl r5,r24,40,24
	ctx.r5.u64 = __builtin_rotateleft64(r24.u64, 40) & 0xFFFFFFFFFF;
	// addi r3,r10,160
	ctx.r3.s64 = ctx.r10.s64 + 160;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// addi r17,r10,80
	r17.s64 = ctx.r10.s64 + 80;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r3,r11,r17
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// or r17,r5,r3
	r17.u64 = ctx.r5.u64 | ctx.r3.u64;
loc_82614434:
	// lwz r5,-4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// subf r5,r7,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r7.s64;
	// or r5,r5,r27
	ctx.r5.u64 = ctx.r5.u64 | r27.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82614468
	if (!cr6.eq) goto loc_82614468;
	// rldicl r5,r21,32,32
	ctx.r5.u64 = __builtin_rotateleft64(r21.u64, 32) & 0xFFFFFFFF;
	// addi r3,r10,-80
	ctx.r3.s64 = ctx.r10.s64 + -80;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// clrlwi r17,r17,24
	r17.u64 = r17.u32 & 0xFF;
	// lbzx r3,r11,r3
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + ctx.r3.u32);
	// lbzx r5,r5,r10
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// or r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 | ctx.r3.u64;
	// and r17,r5,r17
	r17.u64 = ctx.r5.u64 & r17.u64;
loc_82614468:
	// lwz r3,348(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
loc_8261446C:
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpwi cr6,r5,16384
	cr6.compare<int32_t>(ctx.r5.s32, 16384, xer);
	// beq cr6,0x826144d8
	if (cr6.eq) goto loc_826144D8;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// or r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 | ctx.r3.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826144b4
	if (!cr6.eq) goto loc_826144B4;
	// rldicl r9,r24,48,16
	ctx.r9.u64 = __builtin_rotateleft64(r24.u64, 48) & 0xFFFFFFFFFFFF;
	// addi r3,r10,160
	ctx.r3.s64 = ctx.r10.s64 + 160;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// addi r18,r10,80
	r18.s64 = ctx.r10.s64 + 80;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// lbzx r3,r6,r18
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + r18.u32);
	// or r18,r9,r3
	r18.u64 = ctx.r9.u64 | ctx.r3.u64;
loc_826144B4:
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// bne cr6,0x826144d4
	if (!cr6.eq) goto loc_826144D4;
	// addi r9,r10,-80
	ctx.r9.s64 = ctx.r10.s64 + -80;
	// clrlwi r3,r18,24
	ctx.r3.u64 = r18.u32 & 0xFF;
	// lbzx r18,r11,r10
	r18.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lbzx r9,r6,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r9.u32);
	// or r9,r18,r9
	ctx.r9.u64 = r18.u64 | ctx.r9.u64;
	// and r18,r9,r3
	r18.u64 = ctx.r9.u64 & ctx.r3.u64;
loc_826144D4:
	// lwz r3,348(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
loc_826144D8:
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r8,16384
	cr6.compare<int32_t>(ctx.r8.s32, 16384, xer);
	// beq cr6,0x8261453c
	if (cr6.eq) goto loc_8261453C;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// bne cr6,0x82614508
	if (!cr6.eq) goto loc_82614508;
	// addi r7,r10,160
	ctx.r7.s64 = ctx.r10.s64 + 160;
	// addi r19,r10,80
	r19.s64 = ctx.r10.s64 + 80;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// lbzx r7,r30,r19
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + r19.u32);
	// or r19,r11,r7
	r19.u64 = r11.u64 | ctx.r7.u64;
loc_82614508:
	// lwz r11,-4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// or r11,r11,r27
	r11.u64 = r11.u64 | r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261453c
	if (!cr6.eq) goto loc_8261453C;
	// rldicl r11,r21,48,16
	r11.u64 = __builtin_rotateleft64(r21.u64, 48) & 0xFFFFFFFFFFFF;
	// addi r7,r10,-80
	ctx.r7.s64 = ctx.r10.s64 + -80;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// clrlwi r19,r19,24
	r19.u64 = r19.u32 & 0xFF;
	// lbzx r7,r30,r7
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + ctx.r7.u32);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// and r19,r11,r19
	r19.u64 = r11.u64 & r19.u64;
loc_8261453C:
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x82614584
	if (cr6.eq) goto loc_82614584;
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bne cr6,0x82614564
	if (!cr6.eq) goto loc_82614564;
	// addi r9,r10,160
	ctx.r9.s64 = ctx.r10.s64 + 160;
	// addi r7,r10,80
	ctx.r7.s64 = ctx.r10.s64 + 80;
	// lbzx r9,r6,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r9.u32);
	// lbzx r7,r29,r7
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + ctx.r7.u32);
	// or r20,r9,r7
	r20.u64 = ctx.r9.u64 | ctx.r7.u64;
loc_82614564:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bne cr6,0x82614584
	if (!cr6.eq) goto loc_82614584;
	// addi r11,r10,-80
	r11.s64 = ctx.r10.s64 + -80;
	// lbzx r8,r30,r10
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + ctx.r10.u32);
	// clrlwi r9,r20,24
	ctx.r9.u64 = r20.u32 & 0xFF;
	// lbzx r11,r29,r11
	r11.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// and r20,r11,r9
	r20.u64 = r11.u64 & ctx.r9.u64;
loc_82614584:
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// beq cr6,0x8261463c
	if (cr6.eq) goto loc_8261463C;
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwzx r11,r7,r11
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826145ec
	if (!cr6.eq) goto loc_826145EC;
	// rldicl r11,r24,56,8
	r11.u64 = __builtin_rotateleft64(r24.u64, 56) & 0xFFFFFFFFFFFFFF;
	// addi r7,r10,160
	ctx.r7.s64 = ctx.r10.s64 + 160;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r6,r10,80
	ctx.r6.s64 = ctx.r10.s64 + 80;
	// clrlwi r31,r24,24
	r31.u64 = r24.u32 & 0xFF;
	// addi r5,r10,160
	ctx.r5.s64 = ctx.r10.s64 + 160;
	// addi r30,r10,80
	r30.s64 = ctx.r10.s64 + 80;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// lbzx r7,r25,r6
	ctx.r7.u64 = PPC_LOAD_U8(r25.u32 + ctx.r6.u32);
	// lbzx r6,r31,r5
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + ctx.r5.u32);
	// or r31,r11,r7
	r31.u64 = r11.u64 | ctx.r7.u64;
	// lbzx r11,r26,r30
	r11.u64 = PPC_LOAD_U8(r26.u32 + r30.u32);
	// or r23,r6,r11
	r23.u64 = ctx.r6.u64 | r11.u64;
loc_826145EC:
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// or r11,r11,r27
	r11.u64 = r11.u64 | r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261463c
	if (!cr6.eq) goto loc_8261463C;
	// rldicl r11,r21,56,8
	r11.u64 = __builtin_rotateleft64(r21.u64, 56) & 0xFFFFFFFFFFFFFF;
	// addi r9,r10,-80
	ctx.r9.s64 = ctx.r10.s64 + -80;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r8,r10,-80
	ctx.r8.s64 = ctx.r10.s64 + -80;
	// clrlwi r7,r21,24
	ctx.r7.u64 = r21.u32 & 0xFF;
	// clrlwi r6,r31,24
	ctx.r6.u64 = r31.u32 & 0xFF;
	// lbzx r9,r25,r9
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + ctx.r9.u32);
	// clrlwi r5,r23,24
	ctx.r5.u64 = r23.u32 & 0xFF;
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// lbzx r7,r7,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// lbzx r9,r26,r8
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + ctx.r8.u32);
	// and r31,r11,r6
	r31.u64 = r11.u64 & ctx.r6.u64;
	// or r11,r7,r9
	r11.u64 = ctx.r7.u64 | ctx.r9.u64;
	// and r23,r11,r5
	r23.u64 = r11.u64 & ctx.r5.u64;
loc_8261463C:
	// clrlwi r9,r22,24
	ctx.r9.u64 = r22.u32 & 0xFF;
	// rldicl r11,r22,56,8
	r11.u64 = __builtin_rotateleft64(r22.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 & r17.u64;
	// stb r9,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 & r18.u64;
	// stb r9,1(r28)
	PPC_STORE_U8(r28.u32 + 1, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 & r19.u64;
	// stb r9,2(r28)
	PPC_STORE_U8(r28.u32 + 2, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r20
	ctx.r9.u64 = ctx.r9.u64 & r20.u64;
	// lwz r20,124(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stb r9,3(r28)
	PPC_STORE_U8(r28.u32 + 3, ctx.r9.u8);
	// rldicl r9,r11,56,8
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// and r11,r11,r31
	r11.u64 = r11.u64 & r31.u64;
	// and r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 & r23.u64;
	// lwz r23,144(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// b 0x826146cc
	goto loc_826146CC;
loc_8261469C:
	// rldicl r11,r22,56,8
	r11.u64 = __builtin_rotateleft64(r22.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r22,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r22.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,1(r28)
	PPC_STORE_U8(r28.u32 + 1, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,2(r28)
	PPC_STORE_U8(r28.u32 + 2, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,3(r28)
	PPC_STORE_U8(r28.u32 + 3, ctx.r9.u8);
	// rldicl r9,r11,56,8
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
loc_826146CC:
	// stb r11,4(r28)
	PPC_STORE_U8(r28.u32 + 4, r11.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r15,r15,-1
	r15.s64 = r15.s64 + -1;
	// stb r9,5(r28)
	PPC_STORE_U8(r28.u32 + 5, ctx.r9.u8);
	// addi r16,r16,8
	r16.s64 = r16.s64 + 8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r28,r28,6
	r28.s64 = r28.s64 + 6;
	// addi r14,r14,8
	r14.s64 = r14.s64 + 8;
	// cmplw cr6,r4,r23
	cr6.compare<uint32_t>(ctx.r4.u32, r23.u32, xer);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt cr6,0x82614298
	if (cr6.lt) goto loc_82614298;
	// lwz r6,108(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r7,104(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r31,112(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r5,148(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r27,152(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
loc_8261474C:
	// rlwinm r8,r23,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r11,r8,r7
	r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// add r11,r8,r6
	r11.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// beq cr6,0x82614774
	if (cr6.eq) goto loc_82614774;
	// add r11,r8,r31
	r11.u64 = ctx.r8.u64 + r31.u64;
	// rotlwi r31,r11,0
	r31.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_82614774:
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// addi r9,r20,512
	ctx.r9.s64 = r20.s64 + 512;
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// addi r9,r30,256
	ctx.r9.s64 = r30.s64 + 256;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// blt cr6,0x826141c0
	if (cr6.lt) goto loc_826141C0;
loc_826147A0:
	// lwz r3,340(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// bl 0x82612318
	sub_82612318(ctx, base);
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826147B0"))) PPC_WEAK_FUNC(sub_826147B0);
PPC_FUNC_IMPL(__imp__sub_826147B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// stw r3,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r3.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// lhz r11,50(r21)
	r11.u64 = PPC_LOAD_U16(r21.u32 + 50);
	// lhz r10,52(r21)
	ctx.r10.u64 = PPC_LOAD_U16(r21.u32 + 52);
	// rlwinm r24,r11,31,1,31
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r16,1248(r21)
	r16.u64 = PPC_LOAD_U32(r21.u32 + 1248);
	// rlwinm r4,r10,31,1,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r29,1252(r21)
	r29.u64 = PPC_LOAD_U32(r21.u32 + 1252);
	// stw r21,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r21.u32);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r24,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r24.u32);
	// stw r4,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r4.u32);
	// beq cr6,0x82614d84
	if (cr6.eq) goto loc_82614D84;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r9,r24,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r11,10744
	ctx.r10.s64 = r11.s64 + 10744;
	// b 0x8261481c
	goto loc_8261481C;
loc_82614814:
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_8261481C:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// beq cr6,0x82614d48
	if (cr6.eq) goto loc_82614D48;
	// rlwinm r4,r8,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r24,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r24.u32);
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// rlwinm r31,r5,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// and r11,r11,r24
	r11.u64 = r11.u64 & r24.u64;
	// li r15,0
	r15.s64 = 0;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r4,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r4.u32);
	// rlwinm r4,r7,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// add r7,r9,r6
	ctx.r7.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// stw r7,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r7.u32);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r14,r11,r16
	r14.s64 = r16.s64 - r11.s64;
	// add r11,r8,r24
	r11.u64 = ctx.r8.u64 + r24.u64;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
loc_82614894:
	// lis r12,-1
	r12.s64 = -65536;
	// ld r11,0(r16)
	r11.u64 = PPC_LOAD_U64(r16.u32 + 0);
	// addi r7,r10,272
	ctx.r7.s64 = ctx.r10.s64 + 272;
	// ld r8,0(r14)
	ctx.r8.u64 = PPC_LOAD_U64(r14.u32 + 0);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// addi r6,r10,272
	ctx.r6.s64 = ctx.r10.s64 + 272;
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// addi r5,r10,352
	ctx.r5.s64 = ctx.r10.s64 + 352;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
	// srawi r9,r15,31
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r15.s32 >> 31;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stw r6,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r6.u32);
	// addi r27,r9,1
	r27.s64 = ctx.r9.s64 + 1;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// stw r5,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r5.u32);
	// lis r12,-1
	r12.s64 = -65536;
	// rlwinm r30,r9,3,28,28
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0x8;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// rldicl r7,r11,48,56
	ctx.r7.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFF;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// rldicl r6,r11,40,56
	ctx.r6.u64 = __builtin_rotateleft64(r11.u64, 40) & 0xFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rldicl r5,r11,32,56
	ctx.r5.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFF;
	// and r23,r8,r12
	r23.u64 = ctx.r8.u64 & r12.u64;
	// rldicl r8,r11,56,56
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFF;
	// rldicl r31,r11,24,56
	r31.u64 = __builtin_rotateleft64(r11.u64, 24) & 0xFF;
	// rldicl r11,r11,16,48
	r11.u64 = __builtin_rotateleft64(r11.u64, 16) & 0xFFFF;
	// clrlwi r26,r9,24
	r26.u64 = ctx.r9.u32 & 0xFF;
	// rlwinm r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// lis r12,-1
	r12.s64 = -65536;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r25,r8,0
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// addi r22,r10,272
	r22.s64 = ctx.r10.s64 + 272;
	// subf r30,r30,r16
	r30.s64 = r16.s64 - r30.s64;
	// rldicr r12,r12,32,31
	r12.u64 = __builtin_rotateleft64(r12.u64, 32) & 0xFFFFFFFF00000000;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// addi r20,r10,272
	r20.s64 = ctx.r10.s64 + 272;
	// oris r12,r12,32639
	r12.u64 = r12.u64 | 2139029504;
	// rotlwi r28,r7,0
	r28.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// ld r30,0(r30)
	r30.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// addi r19,r10,272
	r19.s64 = ctx.r10.s64 + 272;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// lbzx r7,r25,r20
	ctx.r7.u64 = PPC_LOAD_U8(r25.u32 + r20.u32);
	// addi r18,r10,272
	r18.s64 = ctx.r10.s64 + 272;
	// and r17,r30,r12
	r17.u64 = r30.u64 & r12.u64;
	// rotlwi r30,r6,0
	r30.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// rotlwi r6,r5,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r5.u32, 0);
	// lbzx r5,r28,r19
	ctx.r5.u64 = PPC_LOAD_U8(r28.u32 + r19.u32);
	// rotlwi r11,r31,0
	r11.u64 = __builtin_rotateleft32(r31.u32, 0);
	// lbzx r31,r30,r18
	r31.u64 = PPC_LOAD_U8(r30.u32 + r18.u32);
	// lwz r8,124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r9,r9,r8
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r9.u32 + ctx.r8.u32);
	// lbzx r8,r26,r22
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + r22.u32);
	// lwz r22,128(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lbzx r7,r6,r22
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + r22.u32);
	// lwz r22,132(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// or r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 | ctx.r5.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | r31.u64;
	// lbzx r22,r11,r22
	r22.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// or r8,r8,r22
	ctx.r8.u64 = ctx.r8.u64 | r22.u64;
	// and r22,r8,r9
	r22.u64 = ctx.r8.u64 & ctx.r9.u64;
	// beq cr6,0x82614a0c
	if (cr6.eq) goto loc_82614A0C;
	// lwz r9,188(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 188);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x82614a0c
	if (!cr6.eq) goto loc_82614A0C;
	// rldicl r11,r22,56,8
	r11.u64 = __builtin_rotateleft64(r22.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r22,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r22.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// rldicl r9,r11,56,8
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// b 0x82614cd4
	goto loc_82614CD4;
loc_82614A0C:
	// lwz r8,188(r21)
	ctx.r8.u64 = PPC_LOAD_U32(r21.u32 + 188);
	// li r18,255
	r18.s64 = 255;
	// li r19,255
	r19.s64 = 255;
	// add r9,r4,r8
	ctx.r9.u64 = ctx.r4.u64 + ctx.r8.u64;
	// li r20,255
	r20.s64 = 255;
	// li r21,255
	r21.s64 = 255;
	// li r31,255
	r31.s64 = 255;
	// li r24,255
	r24.s64 = 255;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,16384
	cr6.compare<int32_t>(ctx.r7.s32, 16384, xer);
	// beq cr6,0x82614aa8
	if (cr6.eq) goto loc_82614AA8;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwzx r5,r5,r8
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// subf r5,r7,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r7.s64;
	// or r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 | ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82614a70
	if (!cr6.eq) goto loc_82614A70;
	// rldicl r5,r23,40,24
	ctx.r5.u64 = __builtin_rotateleft64(r23.u64, 40) & 0xFFFFFFFFFF;
	// addi r3,r10,192
	ctx.r3.s64 = ctx.r10.s64 + 192;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// addi r18,r10,128
	r18.s64 = ctx.r10.s64 + 128;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r3,r11,r18
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + r18.u32);
	// or r18,r5,r3
	r18.u64 = ctx.r5.u64 | ctx.r3.u64;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82614A70:
	// lwz r5,-4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// subf r5,r7,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r7.s64;
	// or r5,r5,r27
	ctx.r5.u64 = ctx.r5.u64 | r27.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82614aa8
	if (!cr6.eq) goto loc_82614AA8;
	// rldicl r5,r17,32,32
	ctx.r5.u64 = __builtin_rotateleft64(r17.u64, 32) & 0xFFFFFFFF;
	// addi r3,r10,64
	ctx.r3.s64 = ctx.r10.s64 + 64;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// clrlwi r18,r18,24
	r18.u64 = r18.u32 & 0xFF;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// or r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 | ctx.r3.u64;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// and r18,r5,r18
	r18.u64 = ctx.r5.u64 & r18.u64;
loc_82614AA8:
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpwi cr6,r5,16384
	cr6.compare<int32_t>(ctx.r5.s32, 16384, xer);
	// beq cr6,0x82614b14
	if (cr6.eq) goto loc_82614B14;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// or r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 | ctx.r3.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82614af0
	if (!cr6.eq) goto loc_82614AF0;
	// rldicl r9,r23,48,16
	ctx.r9.u64 = __builtin_rotateleft64(r23.u64, 48) & 0xFFFFFFFFFFFF;
	// addi r3,r10,192
	ctx.r3.s64 = ctx.r10.s64 + 192;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// addi r19,r10,128
	r19.s64 = ctx.r10.s64 + 128;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// lbzx r3,r6,r19
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + r19.u32);
	// or r19,r9,r3
	r19.u64 = ctx.r9.u64 | ctx.r3.u64;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82614AF0:
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// bne cr6,0x82614b14
	if (!cr6.eq) goto loc_82614B14;
	// addi r9,r10,64
	ctx.r9.s64 = ctx.r10.s64 + 64;
	// clrlwi r3,r19,24
	ctx.r3.u64 = r19.u32 & 0xFF;
	// lbzx r19,r6,r10
	r19.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// lbzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// or r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 | r19.u64;
	// and r19,r9,r3
	r19.u64 = ctx.r9.u64 & ctx.r3.u64;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82614B14:
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r8,16384
	cr6.compare<int32_t>(ctx.r8.s32, 16384, xer);
	// beq cr6,0x82614b78
	if (cr6.eq) goto loc_82614B78;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// bne cr6,0x82614b44
	if (!cr6.eq) goto loc_82614B44;
	// addi r7,r10,192
	ctx.r7.s64 = ctx.r10.s64 + 192;
	// addi r20,r10,128
	r20.s64 = ctx.r10.s64 + 128;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// lbzx r7,r30,r20
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// or r20,r11,r7
	r20.u64 = r11.u64 | ctx.r7.u64;
loc_82614B44:
	// lwz r11,-4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// or r11,r11,r27
	r11.u64 = r11.u64 | r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82614b78
	if (!cr6.eq) goto loc_82614B78;
	// rldicl r11,r17,48,16
	r11.u64 = __builtin_rotateleft64(r17.u64, 48) & 0xFFFFFFFFFFFF;
	// addi r7,r10,64
	ctx.r7.s64 = ctx.r10.s64 + 64;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// clrlwi r20,r20,24
	r20.u64 = r20.u32 & 0xFF;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// lbzx r7,r30,r10
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + ctx.r10.u32);
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// and r20,r11,r20
	r20.u64 = r11.u64 & r20.u64;
loc_82614B78:
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x82614bc0
	if (cr6.eq) goto loc_82614BC0;
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bne cr6,0x82614ba0
	if (!cr6.eq) goto loc_82614BA0;
	// addi r9,r10,192
	ctx.r9.s64 = ctx.r10.s64 + 192;
	// addi r7,r10,128
	ctx.r7.s64 = ctx.r10.s64 + 128;
	// lbzx r9,r6,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r9.u32);
	// lbzx r7,r28,r7
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + ctx.r7.u32);
	// or r21,r9,r7
	r21.u64 = ctx.r9.u64 | ctx.r7.u64;
loc_82614BA0:
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bne cr6,0x82614bc0
	if (!cr6.eq) goto loc_82614BC0;
	// addi r11,r10,64
	r11.s64 = ctx.r10.s64 + 64;
	// lbzx r8,r28,r10
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// clrlwi r9,r21,24
	ctx.r9.u64 = r21.u32 & 0xFF;
	// lbzx r11,r6,r11
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// and r21,r11,r9
	r21.u64 = r11.u64 & ctx.r9.u64;
loc_82614BC0:
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,192(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 192);
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// beq cr6,0x82614c78
	if (cr6.eq) goto loc_82614C78;
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwzx r11,r7,r11
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// or r11,r11,r3
	r11.u64 = r11.u64 | ctx.r3.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82614c28
	if (!cr6.eq) goto loc_82614C28;
	// rldicl r11,r23,56,8
	r11.u64 = __builtin_rotateleft64(r23.u64, 56) & 0xFFFFFFFFFFFFFF;
	// addi r7,r10,192
	ctx.r7.s64 = ctx.r10.s64 + 192;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r6,r10,128
	ctx.r6.s64 = ctx.r10.s64 + 128;
	// addi r31,r10,192
	r31.s64 = ctx.r10.s64 + 192;
	// clrlwi r5,r23,24
	ctx.r5.u64 = r23.u32 & 0xFF;
	// addi r30,r10,128
	r30.s64 = ctx.r10.s64 + 128;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// lbzx r7,r25,r6
	ctx.r7.u64 = PPC_LOAD_U8(r25.u32 + ctx.r6.u32);
	// lbzx r6,r5,r31
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r5.u32 + r31.u32);
	// or r31,r11,r7
	r31.u64 = r11.u64 | ctx.r7.u64;
	// lbzx r11,r26,r30
	r11.u64 = PPC_LOAD_U8(r26.u32 + r30.u32);
	// or r24,r6,r11
	r24.u64 = ctx.r6.u64 | r11.u64;
loc_82614C28:
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// or r11,r11,r27
	r11.u64 = r11.u64 | r27.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82614c78
	if (!cr6.eq) goto loc_82614C78;
	// rldicl r11,r17,56,8
	r11.u64 = __builtin_rotateleft64(r17.u64, 56) & 0xFFFFFFFFFFFFFF;
	// lbzx r6,r25,r10
	ctx.r6.u64 = PPC_LOAD_U8(r25.u32 + ctx.r10.u32);
	// addi r9,r10,64
	ctx.r9.s64 = ctx.r10.s64 + 64;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r7,r10,64
	ctx.r7.s64 = ctx.r10.s64 + 64;
	// clrlwi r8,r17,24
	ctx.r8.u64 = r17.u32 & 0xFF;
	// clrlwi r5,r24,24
	ctx.r5.u64 = r24.u32 & 0xFF;
	// lbzx r11,r11,r9
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// clrlwi r9,r31,24
	ctx.r9.u64 = r31.u32 & 0xFF;
	// lbzx r8,r8,r7
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r7.u32);
	// or r11,r11,r6
	r11.u64 = r11.u64 | ctx.r6.u64;
	// lbzx r7,r26,r10
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// and r31,r11,r9
	r31.u64 = r11.u64 & ctx.r9.u64;
	// or r11,r8,r7
	r11.u64 = ctx.r8.u64 | ctx.r7.u64;
	// and r24,r11,r5
	r24.u64 = r11.u64 & ctx.r5.u64;
loc_82614C78:
	// clrlwi r9,r22,24
	ctx.r9.u64 = r22.u32 & 0xFF;
	// rldicl r11,r22,56,8
	r11.u64 = __builtin_rotateleft64(r22.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 & r18.u64;
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 & r19.u64;
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r20
	ctx.r9.u64 = ctx.r9.u64 & r20.u64;
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rldicl r11,r11,56,8
	r11.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// and r9,r9,r21
	ctx.r9.u64 = ctx.r9.u64 & r21.u64;
	// lwz r21,332(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// rldicl r9,r11,56,8
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 56) & 0xFFFFFFFFFFFFFF;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// and r11,r11,r31
	r11.u64 = r11.u64 & r31.u64;
	// and r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 & r24.u64;
	// lwz r24,136(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
loc_82614CD4:
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// addi r15,r15,-1
	r15.s64 = r15.s64 + -1;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// stb r11,4(r29)
	PPC_STORE_U8(r29.u32 + 4, r11.u8);
	// addi r16,r16,8
	r16.s64 = r16.s64 + 8;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r29,r29,6
	r29.s64 = r29.s64 + 6;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r14,r14,8
	r14.s64 = r14.s64 + 8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// bne cr6,0x82614894
	if (!cr6.eq) goto loc_82614894;
	// lwz r5,108(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r8,140(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r4,144(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_82614D48:
	// rlwinm r9,r24,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// add r11,r9,r7
	r11.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// add r11,r9,r6
	r11.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// bne cr6,0x82614d70
	if (!cr6.eq) goto loc_82614D70;
	// add r11,r9,r5
	r11.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
loc_82614D70:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// blt cr6,0x82614814
	if (cr6.lt) goto loc_82614814;
loc_82614D84:
	// lwz r3,324(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// bl 0x82612318
	sub_82612318(ctx, base);
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82614D94"))) PPC_WEAK_FUNC(sub_82614D94);
PPC_FUNC_IMPL(__imp__sub_82614D94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82614D98"))) PPC_WEAK_FUNC(sub_82614D98);
PPC_FUNC_IMPL(__imp__sub_82614D98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82613fc8
	sub_82613FC8(ctx, base);
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// addi r11,r31,104
	r11.s64 = r31.s64 + 104;
	// li r26,0
	r26.s64 = 0;
	// li r25,1
	r25.s64 = 1;
	// stw r10,1104(r31)
	PPC_STORE_U32(r31.u32 + 1104, ctx.r10.u32);
	// lwz r10,21480(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 21480);
	// stw r10,1108(r31)
	PPC_STORE_U32(r31.u32 + 1108, ctx.r10.u32);
	// lwz r10,356(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 356);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r10,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stw r10,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// stw r10,124(r31)
	PPC_STORE_U32(r31.u32 + 124, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// stw r10,128(r31)
	PPC_STORE_U32(r31.u32 + 128, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// stw r10,132(r31)
	PPC_STORE_U32(r31.u32 + 132, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// stw r10,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// stw r10,140(r31)
	PPC_STORE_U32(r31.u32 + 140, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// stw r10,144(r31)
	PPC_STORE_U32(r31.u32 + 144, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// stw r10,148(r31)
	PPC_STORE_U32(r31.u32 + 148, ctx.r10.u32);
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,152(r31)
	PPC_STORE_U32(r31.u32 + 152, ctx.r10.u32);
	// lwz r11,376(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 376);
	// stw r11,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r11.u32);
	// lwz r11,380(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 380);
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
	// lwz r11,136(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,50(r31)
	PPC_STORE_U16(r31.u32 + 50, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lwz r10,140(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 140);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// sth r26,36(r31)
	PPC_STORE_U16(r31.u32 + 36, r26.u16);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r25,38(r31)
	PPC_STORE_U16(r31.u32 + 38, r25.u16);
	// rotlwi r8,r11,3
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 3);
	// sth r11,40(r31)
	PPC_STORE_U16(r31.u32 + 40, r11.u16);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// sth r9,42(r31)
	PPC_STORE_U16(r31.u32 + 42, ctx.r9.u16);
	// sth r10,52(r31)
	PPC_STORE_U16(r31.u32 + 52, ctx.r10.u16);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r8,54(r31)
	PPC_STORE_U16(r31.u32 + 54, ctx.r8.u16);
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// rotlwi r9,r10,3
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// sth r11,58(r31)
	PPC_STORE_U16(r31.u32 + 58, r11.u16);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// sth r9,56(r31)
	PPC_STORE_U16(r31.u32 + 56, ctx.r9.u16);
	// sth r10,60(r31)
	PPC_STORE_U16(r31.u32 + 60, ctx.r10.u16);
	// lwz r11,204(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 204);
	// sth r11,74(r31)
	PPC_STORE_U16(r31.u32 + 74, r11.u16);
	// lwz r11,208(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 208);
	// lhz r10,74(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// sth r11,76(r31)
	PPC_STORE_U16(r31.u32 + 76, r11.u16);
	// lwz r11,212(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 212);
	// lhz r9,76(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// sth r11,78(r31)
	PPC_STORE_U16(r31.u32 + 78, r11.u16);
	// lwz r11,216(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 216);
	// sth r11,80(r31)
	PPC_STORE_U16(r31.u32 + 80, r11.u16);
	// lwz r11,228(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 228);
	// sth r11,82(r31)
	PPC_STORE_U16(r31.u32 + 82, r11.u16);
	// lwz r11,232(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 232);
	// sth r11,84(r31)
	PPC_STORE_U16(r31.u32 + 84, r11.u16);
	// lwz r11,172(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 172);
	// sth r11,86(r31)
	PPC_STORE_U16(r31.u32 + 86, r11.u16);
	// lwz r11,176(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// sth r10,90(r31)
	PPC_STORE_U16(r31.u32 + 90, ctx.r10.u16);
	// sth r9,92(r31)
	PPC_STORE_U16(r31.u32 + 92, ctx.r9.u16);
	// sth r11,88(r31)
	PPC_STORE_U16(r31.u32 + 88, r11.u16);
	// lwz r11,1876(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1876);
	// stw r11,1100(r31)
	PPC_STORE_U32(r31.u32 + 1100, r11.u32);
	// lwz r11,1768(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1768);
	// stw r11,260(r31)
	PPC_STORE_U32(r31.u32 + 260, r11.u32);
	// lwz r11,460(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 460);
	// stw r11,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r11.u32);
	// lwz r11,464(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 464);
	// stw r11,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r11.u32);
	// lwz r11,468(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 468);
	// stw r11,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r11.u32);
	// lwz r11,3052(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3052);
	// stw r11,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r11.u32);
loc_82614F58:
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
loc_82614F5C:
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
loc_82614F60:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne cr6,0x82614fd4
	if (!cr6.eq) goto loc_82614FD4;
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// add. r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82614fb4
	if (cr0.eq) goto loc_82614FB4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82614fa4
	if (cr6.eq) goto loc_82614FA4;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// beq cr6,0x82614fa4
	if (cr6.eq) goto loc_82614FA4;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bne cr6,0x82614f94
	if (!cr6.eq) goto loc_82614F94;
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// beq cr6,0x82614fa4
	if (cr6.eq) goto loc_82614FA4;
loc_82614F94:
	// mr r11,r26
	r11.u64 = r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// b 0x82614fb8
	goto loc_82614FB8;
loc_82614FA4:
	// mr r11,r25
	r11.u64 = r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r11,r11,1
	xer.ca = r11.u32 <= 1;
	r11.s64 = 1 - r11.s64;
	// b 0x82614fb8
	goto loc_82614FB8;
loc_82614FB4:
	// mr r11,r25
	r11.u64 = r25.u64;
loc_82614FB8:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// stb r10,1376(r11)
	PPC_STORE_U8(r11.u32 + 1376, ctx.r10.u8);
	// b 0x82615000
	goto loc_82615000;
loc_82614FD4:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// blt cr6,0x82614fe4
	if (cr6.lt) goto loc_82614FE4;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_82614FE4:
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r8,276
	r11.s64 = ctx.r8.s64 + 276;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
loc_82615000:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpwi cr6,r7,4
	cr6.compare<int32_t>(ctx.r7.s32, 4, xer);
	// blt cr6,0x82614f60
	if (cr6.lt) goto loc_82614F60;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// blt cr6,0x82614f5c
	if (cr6.lt) goto loc_82614F5C;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// blt cr6,0x82614f58
	if (cr6.lt) goto loc_82614F58;
	// lwz r11,21436(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 21436);
	// lhz r10,50(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lhz r11,52(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// bne cr6,0x82615070
	if (!cr6.eq) goto loc_82615070;
	// rotlwi r9,r11,5
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 5);
	// rotlwi r11,r11,16
	r11.u64 = __builtin_rotateleft32(r11.u32, 16);
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r9,r9,11,0,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 11) & 0xFFFFF800;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// stw r9,1412(r31)
	PPC_STORE_U32(r31.u32 + 1412, ctx.r9.u32);
	// stw r11,1428(r31)
	PPC_STORE_U32(r31.u32 + 1428, r11.u32);
	// stw r10,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, ctx.r10.u32);
	// b 0x826150cc
	goto loc_826150CC;
loc_82615070:
	// rotlwi r9,r11,16
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 16);
	// rotlwi r8,r11,6
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 6);
	// rotlwi r11,r11,4
	r11.u64 = __builtin_rotateleft32(r11.u32, 4);
	// addi r8,r8,-8
	ctx.r8.s64 = ctx.r8.s64 + -8;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// rlwinm r8,r8,11,0,20
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 11) & 0xFFFFF800;
	// rlwinm r11,r11,12,0,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFFFF000;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r10,r8,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r11,r11,-12
	r11.s64 = r11.s64 + -12;
	// addi r10,r10,-12
	ctx.r10.s64 = ctx.r10.s64 + -12;
	// stw r11,1428(r31)
	PPC_STORE_U32(r31.u32 + 1428, r11.u32);
	// rlwinm r11,r9,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// stw r10,1412(r31)
	PPC_STORE_U32(r31.u32 + 1412, ctx.r10.u32);
	// rlwinm r10,r9,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addis r11,r11,51
	r11.s64 = r11.s64 + 3342336;
	// addis r10,r10,27
	ctx.r10.s64 = ctx.r10.s64 + 1769472;
	// addi r11,r11,51
	r11.s64 = r11.s64 + 51;
	// addi r10,r10,27
	ctx.r10.s64 = ctx.r10.s64 + 27;
	// stw r11,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, r11.u32);
loc_826150CC:
	// stw r10,1404(r31)
	PPC_STORE_U32(r31.u32 + 1404, ctx.r10.u32);
	// lis r4,-32143
	ctx.r4.s64 = -2106523648;
	// lwz r11,1764(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 1764);
	// lis r8,-32143
	ctx.r8.s64 = -2106523648;
	// addi r4,r4,15112
	ctx.r4.s64 = ctx.r4.s64 + 15112;
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// lis r5,-32143
	ctx.r5.s64 = -2106523648;
	// lis r6,-32143
	ctx.r6.s64 = -2106523648;
	// stw r11,448(r31)
	PPC_STORE_U32(r31.u32 + 448, r11.u32);
	// lis r7,-32143
	ctx.r7.s64 = -2106523648;
	// lis r9,-32143
	ctx.r9.s64 = -2106523648;
	// lwz r3,15840(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 15840);
	// lis r11,-32143
	r11.s64 = -2106523648;
	// stw r4,468(r31)
	PPC_STORE_U32(r31.u32 + 468, ctx.r4.u32);
	// addi r8,r8,17536
	ctx.r8.s64 = ctx.r8.s64 + 17536;
	// addi r4,r10,18864
	ctx.r4.s64 = ctx.r10.s64 + 18864;
	// addi r5,r5,15536
	ctx.r5.s64 = ctx.r5.s64 + 15536;
	// addi r6,r6,16312
	ctx.r6.s64 = ctx.r6.s64 + 16312;
	// stw r3,792(r31)
	PPC_STORE_U32(r31.u32 + 792, ctx.r3.u32);
	// addi r7,r7,16744
	ctx.r7.s64 = ctx.r7.s64 + 16744;
	// addi r9,r9,17992
	ctx.r9.s64 = ctx.r9.s64 + 17992;
	// stw r8,484(r31)
	PPC_STORE_U32(r31.u32 + 484, ctx.r8.u32);
	// addi r11,r11,20152
	r11.s64 = r11.s64 + 20152;
	// stw r4,492(r31)
	PPC_STORE_U32(r31.u32 + 492, ctx.r4.u32);
	// stw r5,472(r31)
	PPC_STORE_U32(r31.u32 + 472, ctx.r5.u32);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stw r6,476(r31)
	PPC_STORE_U32(r31.u32 + 476, ctx.r6.u32);
	// addi r8,r31,500
	ctx.r8.s64 = r31.s64 + 500;
	// stw r7,480(r31)
	PPC_STORE_U32(r31.u32 + 480, ctx.r7.u32);
	// li r27,4
	r27.s64 = 4;
	// stw r9,488(r31)
	PPC_STORE_U32(r31.u32 + 488, ctx.r9.u32);
	// stw r11,496(r31)
	PPC_STORE_U32(r31.u32 + 496, r11.u32);
loc_8261514C:
	// rlwinm r7,r10,0,28,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82615164
	if (cr6.eq) goto loc_82615164;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_82615164:
	// rlwinm r7,r10,0,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82615178
	if (cr6.eq) goto loc_82615178;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82615178:
	// rlwinm r7,r10,0,30,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82615190
	if (cr6.eq) goto loc_82615190;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// ori r9,r9,8
	ctx.r9.u64 = ctx.r9.u64 | 8;
loc_82615190:
	// clrlwi r7,r10,31
	ctx.r7.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826151a8
	if (cr6.eq) goto loc_826151A8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// ori r9,r9,12
	ctx.r9.u64 = ctx.r9.u64 | 12;
loc_826151A8:
	// subfic r11,r11,4
	xer.ca = r11.u32 <= 4;
	r11.s64 = 4 - r11.s64;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rlwinm r7,r11,30,28,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0xC;
	// rlwimi r9,r11,4,0,27
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0xFFFFFFF0) | (ctx.r9.u64 & 0xFFFFFFFF0000000F);
	// rlwinm r11,r11,26,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3;
	// rlwinm r9,r9,2,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFF0;
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// blt cr6,0x8261514c
	if (cr6.lt) goto loc_8261514C;
	// li r29,2
	r29.s64 = 2;
	// stb r26,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r26.u8);
	// li r30,3
	r30.s64 = 3;
	// stb r25,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r25.u8);
	// addi r3,r31,160
	ctx.r3.s64 = r31.s64 + 160;
	// stb r25,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, r25.u8);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stb r25,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r25.u8);
	// li r5,16
	ctx.r5.s64 = 16;
	// stb r25,88(r1)
	PPC_STORE_U8(ctx.r1.u32 + 88, r25.u8);
	// stb r29,83(r1)
	PPC_STORE_U8(ctx.r1.u32 + 83, r29.u8);
	// stb r29,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r29.u8);
	// stb r29,86(r1)
	PPC_STORE_U8(ctx.r1.u32 + 86, r29.u8);
	// stb r30,87(r1)
	PPC_STORE_U8(ctx.r1.u32 + 87, r30.u8);
	// stb r29,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, r29.u8);
	// stb r29,90(r1)
	PPC_STORE_U8(ctx.r1.u32 + 90, r29.u8);
	// stb r30,91(r1)
	PPC_STORE_U8(ctx.r1.u32 + 91, r30.u8);
	// stb r29,92(r1)
	PPC_STORE_U8(ctx.r1.u32 + 92, r29.u8);
	// stb r30,93(r1)
	PPC_STORE_U8(ctx.r1.u32 + 93, r30.u8);
	// stb r30,94(r1)
	PPC_STORE_U8(ctx.r1.u32 + 94, r30.u8);
	// stb r27,95(r1)
	PPC_STORE_U8(ctx.r1.u32 + 95, r27.u8);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lis r11,28
	r11.s64 = 1835008;
	// lwz r9,1412(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1412);
	// lwz r8,1428(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1428);
	// ori r7,r11,28
	ctx.r7.u64 = r11.u64 | 28;
	// lis r11,60
	r11.s64 = 3932160;
	// ori r6,r11,60
	ctx.r6.u64 = r11.u64 | 60;
	// stw r9,1436(r31)
	PPC_STORE_U32(r31.u32 + 1436, ctx.r9.u32);
	// lis r11,64
	r11.s64 = 4194304;
	// stw r8,1444(r31)
	PPC_STORE_U32(r31.u32 + 1444, ctx.r8.u32);
	// stw r7,1392(r31)
	PPC_STORE_U32(r31.u32 + 1392, ctx.r7.u32);
	// ori r10,r11,64
	ctx.r10.u64 = r11.u64 | 64;
	// lis r11,32
	r11.s64 = 2097152;
	// stw r6,1400(r31)
	PPC_STORE_U32(r31.u32 + 1400, ctx.r6.u32);
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// stw r10,1408(r31)
	PPC_STORE_U32(r31.u32 + 1408, ctx.r10.u32);
	// stw r10,1432(r31)
	PPC_STORE_U32(r31.u32 + 1432, ctx.r10.u32);
	// stw r11,1424(r31)
	PPC_STORE_U32(r31.u32 + 1424, r11.u32);
	// stw r11,1440(r31)
	PPC_STORE_U32(r31.u32 + 1440, r11.u32);
	// lwz r9,15472(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r9,7
	cr6.compare<int32_t>(ctx.r9.s32, 7, xer);
	// bne cr6,0x826152c0
	if (!cr6.eq) goto loc_826152C0;
	// lhz r8,52(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// lhz r9,50(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// rotlwi r10,r8,16
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 16);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,1432(r31)
	PPC_STORE_U32(r31.u32 + 1432, ctx.r7.u32);
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,1440(r31)
	PPC_STORE_U32(r31.u32 + 1440, ctx.r8.u32);
	// stw r10,1436(r31)
	PPC_STORE_U32(r31.u32 + 1436, ctx.r10.u32);
	// stw r11,1444(r31)
	PPC_STORE_U32(r31.u32 + 1444, r11.u32);
loc_826152C0:
	// li r11,96
	r11.s64 = 96;
	// stb r26,1420(r31)
	PPC_STORE_U8(r31.u32 + 1420, r26.u8);
	// li r10,100
	ctx.r10.s64 = 100;
	// stb r26,1421(r31)
	PPC_STORE_U8(r31.u32 + 1421, r26.u8);
	// li r9,32
	ctx.r9.s64 = 32;
	// stb r26,1422(r31)
	PPC_STORE_U8(r31.u32 + 1422, r26.u8);
	// stb r25,1423(r31)
	PPC_STORE_U8(r31.u32 + 1423, r25.u8);
	// li r8,16
	ctx.r8.s64 = 16;
	// stb r26,516(r31)
	PPC_STORE_U8(r31.u32 + 516, r26.u8);
	// stb r11,1450(r31)
	PPC_STORE_U8(r31.u32 + 1450, r11.u8);
	// li r11,64
	r11.s64 = 64;
	// stb r10,1451(r31)
	PPC_STORE_U8(r31.u32 + 1451, ctx.r10.u8);
	// addi r10,r31,932
	ctx.r10.s64 = r31.s64 + 932;
	// stb r25,519(r31)
	PPC_STORE_U8(r31.u32 + 519, r25.u8);
	// stb r25,518(r31)
	PPC_STORE_U8(r31.u32 + 518, r25.u8);
	// stb r25,517(r31)
	PPC_STORE_U8(r31.u32 + 517, r25.u8);
	// stb r29,522(r31)
	PPC_STORE_U8(r31.u32 + 522, r29.u8);
	// stb r29,521(r31)
	PPC_STORE_U8(r31.u32 + 521, r29.u8);
	// stb r29,520(r31)
	PPC_STORE_U8(r31.u32 + 520, r29.u8);
	// stb r27,523(r31)
	PPC_STORE_U8(r31.u32 + 523, r27.u8);
	// stb r26,524(r31)
	PPC_STORE_U8(r31.u32 + 524, r26.u8);
	// stb r25,525(r31)
	PPC_STORE_U8(r31.u32 + 525, r25.u8);
	// stb r29,526(r31)
	PPC_STORE_U8(r31.u32 + 526, r29.u8);
	// stb r30,527(r31)
	PPC_STORE_U8(r31.u32 + 527, r30.u8);
	// stb r25,528(r31)
	PPC_STORE_U8(r31.u32 + 528, r25.u8);
	// stb r29,529(r31)
	PPC_STORE_U8(r31.u32 + 529, r29.u8);
	// stb r30,530(r31)
	PPC_STORE_U8(r31.u32 + 530, r30.u8);
	// stb r26,531(r31)
	PPC_STORE_U8(r31.u32 + 531, r26.u8);
	// stb r26,1448(r31)
	PPC_STORE_U8(r31.u32 + 1448, r26.u8);
	// stb r27,1449(r31)
	PPC_STORE_U8(r31.u32 + 1449, r27.u8);
	// stb r26,1452(r31)
	PPC_STORE_U8(r31.u32 + 1452, r26.u8);
	// stb r26,1453(r31)
	PPC_STORE_U8(r31.u32 + 1453, r26.u8);
	// stb r26,1454(r31)
	PPC_STORE_U8(r31.u32 + 1454, r26.u8);
	// stb r26,1455(r31)
	PPC_STORE_U8(r31.u32 + 1455, r26.u8);
	// stb r26,1456(r31)
	PPC_STORE_U8(r31.u32 + 1456, r26.u8);
	// stb r26,1457(r31)
	PPC_STORE_U8(r31.u32 + 1457, r26.u8);
	// stb r26,1458(r31)
	PPC_STORE_U8(r31.u32 + 1458, r26.u8);
	// stb r26,1459(r31)
	PPC_STORE_U8(r31.u32 + 1459, r26.u8);
	// stb r11,924(r31)
	PPC_STORE_U8(r31.u32 + 924, r11.u8);
	// mr r11,r26
	r11.u64 = r26.u64;
	// stb r9,925(r31)
	PPC_STORE_U8(r31.u32 + 925, ctx.r9.u8);
	// stb r9,926(r31)
	PPC_STORE_U8(r31.u32 + 926, ctx.r9.u8);
	// stb r26,927(r31)
	PPC_STORE_U8(r31.u32 + 927, r26.u8);
	// stb r8,928(r31)
	PPC_STORE_U8(r31.u32 + 928, ctx.r8.u8);
loc_82615370:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bge cr6,0x82615380
	if (!cr6.lt) goto loc_82615380;
	// stbx r26,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r26.u8);
	// b 0x8261538c
	goto loc_8261538C;
loc_82615380:
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// slw r9,r25,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r25.u32 << (ctx.r9.u8 & 0x3F));
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
loc_8261538C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x82615370
	if (cr6.lt) goto loc_82615370;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_8261539C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826153ac
	if (!cr6.eq) goto loc_826153AC;
	// stb r26,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r26.u8);
	// b 0x826153d4
	goto loc_826153D4;
loc_826153AC:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bge cr6,0x826153bc
	if (!cr6.lt) goto loc_826153BC;
	// stbx r25,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r25.u8);
	// b 0x826153d4
	goto loc_826153D4;
loc_826153BC:
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826153d0
	if (!cr6.eq) goto loc_826153D0;
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// b 0x826153d4
	goto loc_826153D4;
loc_826153D0:
	// stbx r27,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r27.u8);
loc_826153D4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x8261539c
	if (cr6.lt) goto loc_8261539C;
	// mr r11,r26
	r11.u64 = r26.u64;
	// addi r10,r31,996
	ctx.r10.s64 = r31.s64 + 996;
loc_826153E8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826153f8
	if (!cr6.eq) goto loc_826153F8;
	// stb r26,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r26.u8);
	// b 0x82615420
	goto loc_82615420;
loc_826153F8:
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bge cr6,0x82615408
	if (!cr6.lt) goto loc_82615408;
	// stbx r25,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r25.u8);
	// b 0x82615420
	goto loc_82615420;
loc_82615408:
	// clrlwi r9,r11,30
	ctx.r9.u64 = r11.u32 & 0x3;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8261541c
	if (!cr6.eq) goto loc_8261541C;
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// b 0x82615420
	goto loc_82615420;
loc_8261541C:
	// stbx r27,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r27.u8);
loc_82615420:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// blt cr6,0x826153e8
	if (cr6.lt) goto loc_826153E8;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r8,r31,280
	ctx.r8.s64 = r31.s64 + 280;
	// addi r9,r11,30952
	ctx.r9.s64 = r11.s64 + 30952;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82615440:
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r6,r9,32
	ctx.r6.s64 = ctx.r9.s64 + 32;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x82615440
	if (cr6.lt) goto loc_82615440;
	// lis r11,-32768
	r11.s64 = -2147483648;
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82615470"))) PPC_WEAK_FUNC(sub_82615470);
PPC_FUNC_IMPL(__imp__sub_82615470) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82614d98
	sub_82614D98(ctx, base);
	// lwz r11,360(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 360);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r10,364(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r10,368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 368);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r9,r31,2916
	ctx.r9.s64 = r31.s64 + 2916;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r10,372(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 372);
	// mulli r11,r11,504
	r11.s64 = r11.s64 * 504;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r10,252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,24(r30)
	PPC_STORE_U8(r30.u32 + 24, r11.u8);
	// lwz r11,348(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// stb r11,25(r30)
	PPC_STORE_U8(r30.u32 + 25, r11.u8);
	// lwz r11,344(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 344);
	// stb r11,26(r30)
	PPC_STORE_U8(r30.u32 + 26, r11.u8);
	// lwz r11,2376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2376);
	// stw r11,176(r30)
	PPC_STORE_U32(r30.u32 + 176, r11.u32);
	// lwz r11,20264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20264);
	// stw r11,180(r30)
	PPC_STORE_U32(r30.u32 + 180, r11.u32);
	// lwz r11,20284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20284);
	// stw r11,184(r30)
	PPC_STORE_U32(r30.u32 + 184, r11.u32);
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// stb r11,27(r30)
	PPC_STORE_U8(r30.u32 + 27, r11.u8);
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// stb r11,28(r30)
	PPC_STORE_U8(r30.u32 + 28, r11.u8);
	// lwz r11,328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// stb r11,29(r30)
	PPC_STORE_U8(r30.u32 + 29, r11.u8);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// addi r10,r11,-3
	ctx.r10.s64 = r11.s64 + -3;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stb r11,30(r30)
	PPC_STORE_U8(r30.u32 + 30, r11.u8);
	// lwz r11,2140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2140);
	// stw r11,196(r30)
	PPC_STORE_U32(r30.u32 + 196, r11.u32);
	// lwz r11,2516(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2516);
	// sth r8,46(r30)
	PPC_STORE_U16(r30.u32 + 46, ctx.r8.u16);
	// sth r8,44(r30)
	PPC_STORE_U16(r30.u32 + 44, ctx.r8.u16);
	// stw r11,200(r30)
	PPC_STORE_U32(r30.u32 + 200, r11.u32);
	// lwz r11,416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 416);
	// sth r11,62(r30)
	PPC_STORE_U16(r30.u32 + 62, r11.u16);
	// lwz r11,420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 420);
	// sth r11,64(r30)
	PPC_STORE_U16(r30.u32 + 64, r11.u16);
	// addi r11,r31,2904
	r11.s64 = r31.s64 + 2904;
	// lwz r10,424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// sth r10,66(r30)
	PPC_STORE_U16(r30.u32 + 66, ctx.r10.u16);
	// lwz r10,428(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 428);
	// sth r10,68(r30)
	PPC_STORE_U16(r30.u32 + 68, ctx.r10.u16);
	// lwz r10,408(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 408);
	// sth r10,70(r30)
	PPC_STORE_U16(r30.u32 + 70, ctx.r10.u16);
	// lwz r10,412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// sth r10,72(r30)
	PPC_STORE_U16(r30.u32 + 72, ctx.r10.u16);
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14772);
	// stb r10,32(r30)
	PPC_STORE_U8(r30.u32 + 32, ctx.r10.u8);
	// lwz r10,1792(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1792);
	// stb r10,31(r30)
	PPC_STORE_U8(r30.u32 + 31, ctx.r10.u8);
	// lwz r10,336(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// stb r10,34(r30)
	PPC_STORE_U8(r30.u32 + 34, ctx.r10.u8);
	// lwz r10,6548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// stw r10,220(r30)
	PPC_STORE_U32(r30.u32 + 220, ctx.r10.u32);
	// lwz r10,14752(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14752);
	// stw r11,228(r30)
	PPC_STORE_U32(r30.u32 + 228, r11.u32);
	// stw r9,232(r30)
	PPC_STORE_U32(r30.u32 + 232, ctx.r9.u32);
	// stw r10,224(r30)
	PPC_STORE_U32(r30.u32 + 224, ctx.r10.u32);
	// lwz r11,2880(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2880);
	// stw r11,236(r30)
	PPC_STORE_U32(r30.u32 + 236, r11.u32);
	// lwz r11,2884(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2884);
	// stw r11,240(r30)
	PPC_STORE_U32(r30.u32 + 240, r11.u32);
	// lwz r11,2888(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2888);
	// stw r11,244(r30)
	PPC_STORE_U32(r30.u32 + 244, r11.u32);
	// lwz r11,2892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2892);
	// stw r11,248(r30)
	PPC_STORE_U32(r30.u32 + 248, r11.u32);
	// lwz r11,2896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2896);
	// stw r11,252(r30)
	PPC_STORE_U32(r30.u32 + 252, r11.u32);
	// lwz r11,2900(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2900);
	// stw r11,256(r30)
	PPC_STORE_U32(r30.u32 + 256, r11.u32);
	// lwz r11,1940(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// stw r11,96(r30)
	PPC_STORE_U32(r30.u32 + 96, r11.u32);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// stb r11,33(r30)
	PPC_STORE_U8(r30.u32 + 33, r11.u8);
	// lwz r11,1832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// stw r11,276(r30)
	PPC_STORE_U32(r30.u32 + 276, r11.u32);
	// lwz r11,456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// stb r8,49(r30)
	PPC_STORE_U8(r30.u32 + 49, ctx.r8.u8);
	// stb r11,48(r30)
	PPC_STORE_U8(r30.u32 + 48, r11.u8);
	// lwz r11,3904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// stb r11,35(r30)
	PPC_STORE_U8(r30.u32 + 35, r11.u8);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mulli r11,r11,-6
	r11.s64 = r11.s64 * -6;
	// sth r11,208(r30)
	PPC_STORE_U16(r30.u32 + 208, r11.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,210(r30)
	PPC_STORE_U16(r30.u32 + 210, r11.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,212(r30)
	PPC_STORE_U16(r30.u32 + 212, r11.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,214(r30)
	PPC_STORE_U16(r30.u32 + 214, r11.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sth r11,204(r30)
	PPC_STORE_U16(r30.u32 + 204, r11.u16);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// sth r11,206(r30)
	PPC_STORE_U16(r30.u32 + 206, r11.u16);
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// lwz r10,3756(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,296(r30)
	PPC_STORE_U32(r30.u32 + 296, r11.u32);
	// lwz r9,296(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// stw r10,300(r30)
	PPC_STORE_U32(r30.u32 + 300, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,304(r30)
	PPC_STORE_U32(r30.u32 + 304, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,308(r30)
	PPC_STORE_U32(r30.u32 + 308, r11.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r7,19984(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r9,3736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,312(r30)
	PPC_STORE_U32(r30.u32 + 312, r11.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r7,19984(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r9,224(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,3740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,316(r30)
	PPC_STORE_U32(r30.u32 + 316, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,3772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3772);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,344(r30)
	PPC_STORE_U32(r30.u32 + 344, r11.u32);
	// lwz r9,344(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 344);
	// stw r10,348(r30)
	PPC_STORE_U32(r30.u32 + 348, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,352(r30)
	PPC_STORE_U32(r30.u32 + 352, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,356(r30)
	PPC_STORE_U32(r30.u32 + 356, r11.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r7,19984(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r9,3764(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3764);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,360(r30)
	PPC_STORE_U32(r30.u32 + 360, r11.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,3768(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3768);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,364(r30)
	PPC_STORE_U32(r30.u32 + 364, r11.u32);
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261584c
	if (!cr6.eq) goto loc_8261584C;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,3756(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,320(r30)
	PPC_STORE_U32(r30.u32 + 320, r11.u32);
	// lwz r9,320(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// stw r10,324(r30)
	PPC_STORE_U32(r30.u32 + 324, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,328(r30)
	PPC_STORE_U32(r30.u32 + 328, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,332(r30)
	PPC_STORE_U32(r30.u32 + 332, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r9,3736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,336(r30)
	PPC_STORE_U32(r30.u32 + 336, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r9,224(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r10,3740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// b 0x82615978
	goto loc_82615978;
loc_8261584C:
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826158e8
	if (!cr6.eq) goto loc_826158E8;
	// lwz r11,20024(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20024);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826158e8
	if (cr6.eq) goto loc_826158E8;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,3756(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,320(r30)
	PPC_STORE_U32(r30.u32 + 320, r11.u32);
	// lwz r9,320(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// stw r10,324(r30)
	PPC_STORE_U32(r30.u32 + 324, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,328(r30)
	PPC_STORE_U32(r30.u32 + 328, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,332(r30)
	PPC_STORE_U32(r30.u32 + 332, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r9,3736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,336(r30)
	PPC_STORE_U32(r30.u32 + 336, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r9,224(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r10,3740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// b 0x82615978
	goto loc_82615978;
loc_826158E8:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r7,19984(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r9,3720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,320(r30)
	PPC_STORE_U32(r30.u32 + 320, r11.u32);
	// lwz r9,320(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// stw r10,324(r30)
	PPC_STORE_U32(r30.u32 + 324, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,328(r30)
	PPC_STORE_U32(r30.u32 + 328, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,332(r30)
	PPC_STORE_U32(r30.u32 + 332, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,336(r30)
	PPC_STORE_U32(r30.u32 + 336, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// xori r7,r10,1
	ctx.r7.u64 = ctx.r10.u64 ^ 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
loc_82615978:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r7,2
	ctx.r7.s64 = 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,340(r30)
	PPC_STORE_U32(r30.u32 + 340, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,3772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3772);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,368(r30)
	PPC_STORE_U32(r30.u32 + 368, r11.u32);
	// lwz r9,368(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 368);
	// stw r10,372(r30)
	PPC_STORE_U32(r30.u32 + 372, ctx.r10.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,376(r30)
	PPC_STORE_U32(r30.u32 + 376, r11.u32);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,380(r30)
	PPC_STORE_U32(r30.u32 + 380, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// xori r6,r10,1
	ctx.r6.u64 = ctx.r10.u64 ^ 1;
	// lwz r9,3764(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3764);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r6
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r6.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,384(r30)
	PPC_STORE_U32(r30.u32 + 384, r11.u32);
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// xori r6,r10,1
	ctx.r6.u64 = ctx.r10.u64 ^ 1;
	// lwz r9,3768(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3768);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r6
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r6.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,388(r30)
	PPC_STORE_U32(r30.u32 + 388, r11.u32);
	// sth r7,1112(r30)
	PPC_STORE_U16(r30.u32 + 1112, ctx.r7.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1114(r30)
	PPC_STORE_U16(r30.u32 + 1114, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1116(r30)
	PPC_STORE_U16(r30.u32 + 1116, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1118(r30)
	PPC_STORE_U16(r30.u32 + 1118, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1120(r30)
	PPC_STORE_U16(r30.u32 + 1120, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1122(r30)
	PPC_STORE_U16(r30.u32 + 1122, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r30,1128
	ctx.r10.s64 = r30.s64 + 1128;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1124(r30)
	PPC_STORE_U16(r30.u32 + 1124, r11.u16);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mulli r11,r11,28
	r11.s64 = r11.s64 * 28;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1126(r30)
	PPC_STORE_U16(r30.u32 + 1126, r11.u16);
loc_82615AE4:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpwi cr6,r9,16
	cr6.compare<int32_t>(ctx.r9.s32, 16, xer);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// blt cr6,0x82615ae4
	if (cr6.lt) goto loc_82615AE4;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// vspltish v13,8
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r7,3720(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lbz r11,35(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 35);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addis r6,r11,31
	ctx.r6.s64 = r11.s64 + 2031616;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addis r5,r11,15
	ctx.r5.s64 = r11.s64 + 983040;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// addi r6,r6,31
	ctx.r6.s64 = ctx.r6.s64 + 31;
	// addi r5,r5,15
	ctx.r5.s64 = ctx.r5.s64 + 15;
	// addis r4,r11,7
	ctx.r4.s64 = r11.s64 + 458752;
	// stw r10,392(r30)
	PPC_STORE_U32(r30.u32 + 392, ctx.r10.u32);
	// lwz r7,392(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 392);
	// stw r9,396(r30)
	PPC_STORE_U32(r30.u32 + 396, ctx.r9.u32);
	// addi r4,r4,7
	ctx.r4.s64 = ctx.r4.s64 + 7;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r10,400(r30)
	PPC_STORE_U32(r30.u32 + 400, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,404(r30)
	PPC_STORE_U32(r30.u32 + 404, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r3,19984(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// lwz r7,3724(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r9,224(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,408(r30)
	PPC_STORE_U32(r30.u32 + 408, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r3,19984(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// lwz r7,3728(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r9,224(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lis r7,64
	ctx.r7.s64 = 4194304;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lis r9,32
	ctx.r9.s64 = 2097152;
	// ori r7,r7,64
	ctx.r7.u64 = ctx.r7.u64 | 64;
	// ori r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 | 32;
	// stw r10,412(r30)
	PPC_STORE_U32(r30.u32 + 412, ctx.r10.u32);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// lwz r10,2980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2980);
	// stw r10,420(r30)
	PPC_STORE_U32(r30.u32 + 420, ctx.r10.u32);
	// stw r10,416(r30)
	PPC_STORE_U32(r30.u32 + 416, ctx.r10.u32);
	// lwz r10,2988(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2988);
	// stw r10,428(r30)
	PPC_STORE_U32(r30.u32 + 428, ctx.r10.u32);
	// stw r10,424(r30)
	PPC_STORE_U32(r30.u32 + 424, ctx.r10.u32);
	// lwz r10,2992(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// stw r10,432(r30)
	PPC_STORE_U32(r30.u32 + 432, ctx.r10.u32);
	// lwz r10,3000(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// stw r10,436(r30)
	PPC_STORE_U32(r30.u32 + 436, ctx.r10.u32);
	// lwz r10,2556(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// stw r10,440(r30)
	PPC_STORE_U32(r30.u32 + 440, ctx.r10.u32);
	// lwz r10,2476(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// stw r10,444(r30)
	PPC_STORE_U32(r30.u32 + 444, ctx.r10.u32);
	// lwz r10,1852(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// stw r10,452(r30)
	PPC_STORE_U32(r30.u32 + 452, ctx.r10.u32);
	// lwz r10,1856(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// stw r10,456(r30)
	PPC_STORE_U32(r30.u32 + 456, ctx.r10.u32);
	// lwz r10,1860(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// stw r6,1072(r30)
	PPC_STORE_U32(r30.u32 + 1072, ctx.r6.u32);
	// lis r6,8
	ctx.r6.s64 = 524288;
	// stw r5,1076(r30)
	PPC_STORE_U32(r30.u32 + 1076, ctx.r5.u32);
	// addis r5,r11,3
	ctx.r5.s64 = r11.s64 + 196608;
	// stw r4,1080(r30)
	PPC_STORE_U32(r30.u32 + 1080, ctx.r4.u32);
	// ori r6,r6,8
	ctx.r6.u64 = ctx.r6.u64 | 8;
	// addi r5,r5,3
	ctx.r5.s64 = ctx.r5.s64 + 3;
	// stw r10,464(r30)
	PPC_STORE_U32(r30.u32 + 464, ctx.r10.u32);
	// addi r10,r30,1056
	ctx.r10.s64 = r30.s64 + 1056;
	// subf r7,r11,r7
	ctx.r7.s64 = ctx.r7.s64 - r11.s64;
	// stw r5,1084(r30)
	PPC_STORE_U32(r30.u32 + 1084, ctx.r5.u32);
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// stw r9,1088(r30)
	PPC_STORE_U32(r30.u32 + 1088, ctx.r9.u32);
	// addi r6,r30,1040
	ctx.r6.s64 = r30.s64 + 1040;
	// addi r5,r31,23264
	ctx.r5.s64 = r31.s64 + 23264;
	// addi r9,r31,2116
	ctx.r9.s64 = r31.s64 + 2116;
	// stw r7,1092(r30)
	PPC_STORE_U32(r30.u32 + 1092, ctx.r7.u32);
	// addi r7,r31,23520
	ctx.r7.s64 = r31.s64 + 23520;
	// stw r11,1096(r30)
	PPC_STORE_U32(r30.u32 + 1096, r11.u32);
	// lbz r11,35(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 35);
	// sth r11,1070(r30)
	PPC_STORE_U16(r30.u32 + 1070, r11.u16);
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v0,v0,7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0x100))));
	// vsubshs v13,v13,v0
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,2092(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// stw r11,1160(r30)
	PPC_STORE_U32(r30.u32 + 1160, r11.u32);
	// lwz r11,2096(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// stw r11,1164(r30)
	PPC_STORE_U32(r30.u32 + 1164, r11.u32);
	// lwz r11,21008(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21008);
	// stw r11,1172(r30)
	PPC_STORE_U32(r30.u32 + 1172, r11.u32);
	// lwz r11,21012(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21012);
	// stw r11,1176(r30)
	PPC_STORE_U32(r30.u32 + 1176, r11.u32);
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// stb r11,1180(r30)
	PPC_STORE_U8(r30.u32 + 1180, r11.u8);
	// lwz r11,3976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3976);
	// stb r11,1181(r30)
	PPC_STORE_U8(r30.u32 + 1181, r11.u8);
	// lwz r11,3984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3984);
	// stb r11,1182(r30)
	PPC_STORE_U8(r30.u32 + 1182, r11.u8);
	// lwz r11,252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// stb r11,1185(r30)
	PPC_STORE_U8(r30.u32 + 1185, r11.u8);
	// lwz r11,472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 472);
	// stb r11,1186(r30)
	PPC_STORE_U8(r30.u32 + 1186, r11.u8);
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// stw r11,1240(r30)
	PPC_STORE_U32(r30.u32 + 1240, r11.u32);
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// stw r11,1244(r30)
	PPC_STORE_U32(r30.u32 + 1244, r11.u32);
	// lwz r11,1948(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1948);
	// stb r11,1183(r30)
	PPC_STORE_U8(r30.u32 + 1183, r11.u8);
	// lwz r11,1952(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1952);
	// stb r11,1184(r30)
	PPC_STORE_U8(r30.u32 + 1184, r11.u8);
	// lwz r11,1944(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1944);
	// stb r11,1187(r30)
	PPC_STORE_U8(r30.u32 + 1187, r11.u8);
	// stw r5,1196(r30)
	PPC_STORE_U32(r30.u32 + 1196, ctx.r5.u32);
	// stw r9,1168(r30)
	PPC_STORE_U32(r30.u32 + 1168, ctx.r9.u32);
	// lwz r11,20004(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20004);
	// stb r11,1190(r30)
	PPC_STORE_U8(r30.u32 + 1190, r11.u8);
	// lwz r11,20940(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20940);
	// stb r11,1191(r30)
	PPC_STORE_U8(r30.u32 + 1191, r11.u8);
	// stw r7,1200(r30)
	PPC_STORE_U32(r30.u32 + 1200, ctx.r7.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82615d50
	if (cr6.eq) goto loc_82615D50;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82615d50
	if (!cr6.eq) goto loc_82615D50;
	// lwz r11,1832(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// stw r11,1204(r30)
	PPC_STORE_U32(r30.u32 + 1204, r11.u32);
	// lwz r11,20048(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20048);
	// stw r11,1208(r30)
	PPC_STORE_U32(r30.u32 + 1208, r11.u32);
	// lwz r11,20052(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20052);
	// stw r11,1236(r30)
	PPC_STORE_U32(r30.u32 + 1236, r11.u32);
	// b 0x82615dd0
	goto loc_82615DD0;
loc_82615D50:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82615da0
	if (!cr6.eq) goto loc_82615DA0;
	// lwz r11,1812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1812);
	// stw r11,1204(r30)
	PPC_STORE_U32(r30.u32 + 1204, r11.u32);
	// lwz r11,1812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1812);
	// stw r11,1208(r30)
	PPC_STORE_U32(r30.u32 + 1208, r11.u32);
	// lwz r11,1820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// stw r11,1212(r30)
	PPC_STORE_U32(r30.u32 + 1212, r11.u32);
	// lwz r11,1816(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// stw r11,1216(r30)
	PPC_STORE_U32(r30.u32 + 1216, r11.u32);
	// lwz r11,1820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// stw r11,1220(r30)
	PPC_STORE_U32(r30.u32 + 1220, r11.u32);
	// lwz r11,1816(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// stw r11,1224(r30)
	PPC_STORE_U32(r30.u32 + 1224, r11.u32);
	// lwz r11,1820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// stw r11,1228(r30)
	PPC_STORE_U32(r30.u32 + 1228, r11.u32);
	// lwz r11,1816(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// stw r11,1232(r30)
	PPC_STORE_U32(r30.u32 + 1232, r11.u32);
	// b 0x82615dd0
	goto loc_82615DD0;
loc_82615DA0:
	// lwz r11,1808(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1808);
	// stw r11,1204(r30)
	PPC_STORE_U32(r30.u32 + 1204, r11.u32);
	// lwz r11,1820(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// stw r11,1208(r30)
	PPC_STORE_U32(r30.u32 + 1208, r11.u32);
	// lwz r11,1804(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1804);
	// stw r11,1212(r30)
	PPC_STORE_U32(r30.u32 + 1212, r11.u32);
	// lwz r11,1816(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// stw r11,1216(r30)
	PPC_STORE_U32(r30.u32 + 1216, r11.u32);
	// lwz r11,1800(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1800);
	// stw r11,1220(r30)
	PPC_STORE_U32(r30.u32 + 1220, r11.u32);
	// lwz r11,1812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1812);
	// stw r11,1224(r30)
	PPC_STORE_U32(r30.u32 + 1224, r11.u32);
loc_82615DD0:
	// sth r8,1192(r30)
	PPC_STORE_U16(r30.u32 + 1192, ctx.r8.u16);
	// addi r10,r31,23584
	ctx.r10.s64 = r31.s64 + 23584;
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// addi r9,r31,23776
	ctx.r9.s64 = r31.s64 + 23776;
	// stb r11,1188(r30)
	PPC_STORE_U8(r30.u32 + 1188, r11.u8);
	// lwz r11,1936(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1936);
	// stb r11,1189(r30)
	PPC_STORE_U8(r30.u32 + 1189, r11.u8);
	// lwz r11,21576(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21576);
	// stw r11,1248(r30)
	PPC_STORE_U32(r30.u32 + 1248, r11.u32);
	// lwz r11,3916(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3916);
	// stw r11,1252(r30)
	PPC_STORE_U32(r30.u32 + 1252, r11.u32);
	// lwz r11,3920(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3920);
	// stw r11,1256(r30)
	PPC_STORE_U32(r30.u32 + 1256, r11.u32);
	// lwz r11,19988(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19988);
	// stw r11,1308(r30)
	PPC_STORE_U32(r30.u32 + 1308, r11.u32);
	// lwz r11,15220(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15220);
	// stw r11,1312(r30)
	PPC_STORE_U32(r30.u32 + 1312, r11.u32);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,29,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x4;
	// stb r11,1260(r30)
	PPC_STORE_U8(r30.u32 + 1260, r11.u8);
	// lwz r11,2980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2980);
	// stw r11,1264(r30)
	PPC_STORE_U32(r30.u32 + 1264, r11.u32);
	// lwz r11,2984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2984);
	// stw r11,1268(r30)
	PPC_STORE_U32(r30.u32 + 1268, r11.u32);
	// lwz r11,2988(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2988);
	// stw r11,1272(r30)
	PPC_STORE_U32(r30.u32 + 1272, r11.u32);
	// lwz r11,2992(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// stw r11,1276(r30)
	PPC_STORE_U32(r30.u32 + 1276, r11.u32);
	// lwz r11,2996(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2996);
	// stw r11,1280(r30)
	PPC_STORE_U32(r30.u32 + 1280, r11.u32);
	// lwz r11,3000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// stw r11,1284(r30)
	PPC_STORE_U32(r30.u32 + 1284, r11.u32);
	// lwz r11,3004(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3004);
	// stw r11,1288(r30)
	PPC_STORE_U32(r30.u32 + 1288, r11.u32);
	// stw r10,1292(r30)
	PPC_STORE_U32(r30.u32 + 1292, ctx.r10.u32);
	// stw r9,1296(r30)
	PPC_STORE_U32(r30.u32 + 1296, ctx.r9.u32);
	// lwz r11,20064(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20064);
	// stw r11,1316(r30)
	PPC_STORE_U32(r30.u32 + 1316, r11.u32);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// stw r11,1320(r30)
	PPC_STORE_U32(r30.u32 + 1320, r11.u32);
	// lwz r11,14804(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14804);
	// stw r11,1460(r30)
	PPC_STORE_U32(r30.u32 + 1460, r11.u32);
	// lwz r11,14780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14780);
	// stw r11,1464(r30)
	PPC_STORE_U32(r30.u32 + 1464, r11.u32);
	// lwz r11,14784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14784);
	// stw r11,1468(r30)
	PPC_STORE_U32(r30.u32 + 1468, r11.u32);
	// lwz r11,14776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14776);
	// lwz r10,3392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3392);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,1472(r30)
	PPC_STORE_U32(r30.u32 + 1472, r11.u32);
	// lwz r11,15276(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15276);
	// stw r11,1492(r30)
	PPC_STORE_U32(r30.u32 + 1492, r11.u32);
	// lwz r11,15280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15280);
	// stw r11,1496(r30)
	PPC_STORE_U32(r30.u32 + 1496, r11.u32);
	// lwz r11,15284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15284);
	// stw r11,1500(r30)
	PPC_STORE_U32(r30.u32 + 1500, r11.u32);
	// lwz r11,15288(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15288);
	// stw r11,1504(r30)
	PPC_STORE_U32(r30.u32 + 1504, r11.u32);
	// lwz r11,20196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20196);
	// stw r11,1324(r30)
	PPC_STORE_U32(r30.u32 + 1324, r11.u32);
	// lwz r11,20996(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20996);
	// stw r11,1508(r30)
	PPC_STORE_U32(r30.u32 + 1508, r11.u32);
	// lwz r11,20992(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20992);
	// stw r11,1512(r30)
	PPC_STORE_U32(r30.u32 + 1512, r11.u32);
	// lwz r11,3964(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3964);
	// stw r11,1532(r30)
	PPC_STORE_U32(r30.u32 + 1532, r11.u32);
	// lwz r11,20024(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20024);
	// stw r11,1536(r30)
	PPC_STORE_U32(r30.u32 + 1536, r11.u32);
	// lwz r11,20028(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20028);
	// stw r11,1540(r30)
	PPC_STORE_U32(r30.u32 + 1540, r11.u32);
	// lwz r11,20032(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20032);
	// stw r11,1544(r30)
	PPC_STORE_U32(r30.u32 + 1544, r11.u32);
	// lwz r11,20036(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20036);
	// stw r11,1548(r30)
	PPC_STORE_U32(r30.u32 + 1548, r11.u32);
	// lwz r11,20040(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20040);
	// stw r11,1552(r30)
	PPC_STORE_U32(r30.u32 + 1552, r11.u32);
	// lwz r11,20044(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20044);
	// stw r11,1556(r30)
	PPC_STORE_U32(r30.u32 + 1556, r11.u32);
	// lwz r11,19984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// stw r11,1516(r30)
	PPC_STORE_U32(r30.u32 + 1516, r11.u32);
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// stw r11,1520(r30)
	PPC_STORE_U32(r30.u32 + 1520, r11.u32);
	// lwz r11,21436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21436);
	// stw r11,1528(r30)
	PPC_STORE_U32(r30.u32 + 1528, r11.u32);
	// lwz r11,21088(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21088);
	// stw r11,1560(r30)
	PPC_STORE_U32(r30.u32 + 1560, r11.u32);
	// lwz r11,21092(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21092);
	// stw r11,1564(r30)
	PPC_STORE_U32(r30.u32 + 1564, r11.u32);
	// lwz r11,21096(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21096);
	// stw r11,1568(r30)
	PPC_STORE_U32(r30.u32 + 1568, r11.u32);
	// lwz r11,21100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21100);
	// stw r11,1572(r30)
	PPC_STORE_U32(r30.u32 + 1572, r11.u32);
	// lwz r11,21104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21104);
	// stw r11,1576(r30)
	PPC_STORE_U32(r30.u32 + 1576, r11.u32);
	// lwz r11,21108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21108);
	// stw r11,1580(r30)
	PPC_STORE_U32(r30.u32 + 1580, r11.u32);
	// lwz r11,21112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21112);
	// stw r11,1584(r30)
	PPC_STORE_U32(r30.u32 + 1584, r11.u32);
	// lwz r11,21468(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21468);
	// stw r11,1588(r30)
	PPC_STORE_U32(r30.u32 + 1588, r11.u32);
	// lwz r11,21116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21116);
	// stw r11,1592(r30)
	PPC_STORE_U32(r30.u32 + 1592, r11.u32);
	// lwz r11,21120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21120);
	// stw r11,1596(r30)
	PPC_STORE_U32(r30.u32 + 1596, r11.u32);
	// lwz r11,21124(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21124);
	// stw r11,1600(r30)
	PPC_STORE_U32(r30.u32 + 1600, r11.u32);
	// lwz r11,21128(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21128);
	// stw r11,1604(r30)
	PPC_STORE_U32(r30.u32 + 1604, r11.u32);
	// lwz r11,21132(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21132);
	// stw r11,1608(r30)
	PPC_STORE_U32(r30.u32 + 1608, r11.u32);
	// lwz r11,21136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21136);
	// stw r11,1612(r30)
	PPC_STORE_U32(r30.u32 + 1612, r11.u32);
	// lwz r11,21140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21140);
	// stw r11,1616(r30)
	PPC_STORE_U32(r30.u32 + 1616, r11.u32);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82615fc8
	if (!cr6.eq) goto loc_82615FC8;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82615fc8
	if (!cr6.eq) goto loc_82615FC8;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r11,3372(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3372);
	// b 0x82615fd0
	goto loc_82615FD0;
loc_82615FC8:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r11,3364(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3364);
loc_82615FD0:
	// li r10,-1
	ctx.r10.s64 = -1;
	// stw r11,1300(r30)
	PPC_STORE_U32(r30.u32 + 1300, r11.u32);
	// lwz r11,1292(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1292);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r6,-1
	ctx.r6.s64 = -1;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// li r10,1036
	ctx.r10.s64 = 1036;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// li r10,1032
	ctx.r10.s64 = 1032;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// li r10,2056
	ctx.r10.s64 = 2056;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// li r10,1028
	ctx.r10.s64 = 1028;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lis r10,1036
	ctx.r10.s64 = 67895296;
	// ori r10,r10,1029
	ctx.r10.u64 = ctx.r10.u64 | 1029;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// li r10,2052
	ctx.r10.s64 = 2052;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// li r10,3076
	ctx.r10.s64 = 3076;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// li r10,1024
	ctx.r10.s64 = 1024;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lis r10,1036
	ctx.r10.s64 = 67895296;
	// ori r10,r10,1025
	ctx.r10.u64 = ctx.r10.u64 | 1025;
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// lis r10,1032
	ctx.r10.s64 = 67633152;
	// ori r10,r10,1025
	ctx.r10.u64 = ctx.r10.u64 | 1025;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lis r10,2056
	ctx.r10.s64 = 134742016;
	// ori r10,r10,1025
	ctx.r10.u64 = ctx.r10.u64 | 1025;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// li r10,2048
	ctx.r10.s64 = 2048;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lis r10,1036
	ctx.r10.s64 = 67895296;
	// ori r10,r10,2049
	ctx.r10.u64 = ctx.r10.u64 | 2049;
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// li r10,3072
	ctx.r10.s64 = 3072;
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// li r10,4096
	ctx.r10.s64 = 4096;
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
loc_82616080:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r4,r10,0,24,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r4,r4,r8
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// stw r4,3(r11)
	PPC_STORE_U32(r11.u32 + 3, ctx.r4.u32);
	// clrlwi r5,r10,31
	ctx.r5.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x826160c0
	if (cr6.eq) goto loc_826160C0;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r10,r10,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// stw r10,7(r11)
	PPC_STORE_U32(r11.u32 + 7, ctx.r10.u32);
loc_826160C0:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82616080
	if (!cr6.eq) goto loc_82616080;
	// lwz r11,1296(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1296);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826160E8:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r4,r10,0,24,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r4,r4,r8
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// stw r4,3(r11)
	PPC_STORE_U32(r11.u32 + 3, ctx.r4.u32);
	// clrlwi r5,r10,31
	ctx.r5.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82616128
	if (cr6.eq) goto loc_82616128;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r10,r10,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// stw r10,7(r11)
	PPC_STORE_U32(r11.u32 + 7, ctx.r10.u32);
loc_82616128:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x826160e8
	if (!cr6.eq) goto loc_826160E8;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82616154"))) PPC_WEAK_FUNC(sub_82616154);
PPC_FUNC_IMPL(__imp__sub_82616154) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82616158"))) PPC_WEAK_FUNC(sub_82616158);
PPC_FUNC_IMPL(__imp__sub_82616158) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x826e5cd8
	sub_826E5CD8(ctx, base);
	// addi r29,r31,17248
	r29.s64 = r31.s64 + 17248;
	// addi r30,r31,15920
	r30.s64 = r31.s64 + 15920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x826e5db8
	sub_826E5DB8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82616258
	if (!cr6.eq) goto loc_82616258;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82715600
	sub_82715600(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82616258
	if (!cr6.eq) goto loc_82616258;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826fd650
	sub_826FD650(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82616258
	if (!cr6.eq) goto loc_82616258;
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82616214
	if (cr6.eq) goto loc_82616214;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826161e4
	if (!cr6.eq) goto loc_826161E4;
	// bl 0x82614148
	sub_82614148(ctx, base);
	// b 0x826161e8
	goto loc_826161E8;
loc_826161E4:
	// bl 0x826147b0
	sub_826147B0(ctx, base);
loc_826161E8:
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// add r5,r8,r7
	ctx.r5.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r10,3724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x82612ea8
	sub_82612EA8(ctx, base);
loc_82616214:
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261623c
	if (!cr6.eq) goto loc_8261623C;
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261623c
	if (!cr6.eq) goto loc_8261623C;
	// lwz r11,15196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15196);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x82616240
	if (cr6.eq) goto loc_82616240;
loc_8261623C:
	// li r11,1
	r11.s64 = 1;
loc_82616240:
	// stw r11,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// stw r11,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r11.u32);
loc_82616258:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82616260"))) PPC_WEAK_FUNC(sub_82616260);
PPC_FUNC_IMPL(__imp__sub_82616260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// lhz r9,50(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 50);
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// lhz r7,52(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 52);
	// addi r11,r4,-2
	r11.s64 = ctx.r4.s64 + -2;
	// extsh r3,r5
	ctx.r3.s64 = ctx.r5.s16;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// rlwinm r8,r11,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srw r9,r9,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// srw r7,r7,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// bne cr6,0x826162a0
	if (!cr6.eq) goto loc_826162A0;
	// li r11,1
	r11.s64 = 1;
loc_826162A0:
	// clrlwi r29,r10,16
	r29.u64 = ctx.r10.u32 & 0xFFFF;
	// lhz r31,18(r6)
	r31.u64 = PPC_LOAD_U16(ctx.r6.u32 + 18);
	// clrlwi r4,r10,16
	ctx.r4.u64 = ctx.r10.u32 & 0xFFFF;
	// lhz r30,16(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 16);
	// subfic r10,r11,-7
	xer.ca = r11.u32 <= 4294967289;
	ctx.r10.s64 = -7 - r11.s64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r28,r10,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r7,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-4
	ctx.r7.s64 = ctx.r10.s64 + -4;
	// addi r6,r9,-4
	ctx.r6.s64 = ctx.r9.s64 + -4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// slw r8,r28,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 << (r11.u8 & 0x3F));
	// srw r10,r31,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (r31.u32 >> (ctx.r4.u8 & 0x3F));
	// srw r9,r30,r29
	ctx.r9.u64 = r29.u8 & 0x20 ? 0 : (r30.u32 >> (r29.u8 & 0x3F));
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r9,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// ble cr6,0x8261630c
	if (!cr6.gt) goto loc_8261630C;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// b 0x82616320
	goto loc_82616320;
loc_8261630C:
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// not r11,r11
	r11.u64 = ~r11.u64;
	// and r10,r11,r10
	ctx.r10.u64 = r11.u64 & ctx.r10.u64;
	// and r9,r11,r9
	ctx.r9.u64 = r11.u64 & ctx.r9.u64;
loc_82616320:
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82616330
	if (!cr6.lt) goto loc_82616330;
	// subf r11,r10,r8
	r11.s64 = ctx.r8.s64 - ctx.r10.s64;
	// b 0x8261633c
	goto loc_8261633C;
loc_82616330:
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// ble cr6,0x82616340
	if (!cr6.gt) goto loc_82616340;
	// subf r11,r10,r7
	r11.s64 = ctx.r7.s64 - ctx.r10.s64;
loc_8261633C:
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
loc_82616340:
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x82616358
	if (!cr6.lt) goto loc_82616358;
	// subf r11,r9,r8
	r11.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// rlwimi r3,r5,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r5.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// b 0x8239bd48
	return;
loc_82616358:
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// ble cr6,0x82616368
	if (!cr6.gt) goto loc_82616368;
	// subf r11,r9,r6
	r11.s64 = ctx.r6.s64 - ctx.r9.s64;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
loc_82616368:
	// rlwimi r3,r5,16,0,15
	ctx.r3.u64 = (__builtin_rotateleft32(ctx.r5.u32, 16) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82616370"))) PPC_WEAK_FUNC(sub_82616370);
PPC_FUNC_IMPL(__imp__sub_82616370) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r26,0
	r26.s64 = 0;
	// addi r11,r31,104
	r11.s64 = r31.s64 + 104;
	// li r27,1
	r27.s64 = 1;
	// lwz r10,356(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 356);
	// addi r3,r31,160
	ctx.r3.s64 = r31.s64 + 160;
	// li r29,2
	r29.s64 = 2;
	// li r28,3
	r28.s64 = 3;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// ld r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r10,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stw r10,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// stw r10,124(r31)
	PPC_STORE_U32(r31.u32 + 124, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// stw r10,128(r31)
	PPC_STORE_U32(r31.u32 + 128, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// stw r10,132(r31)
	PPC_STORE_U32(r31.u32 + 132, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// stw r10,136(r31)
	PPC_STORE_U32(r31.u32 + 136, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// stw r10,140(r31)
	PPC_STORE_U32(r31.u32 + 140, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// stw r10,144(r31)
	PPC_STORE_U32(r31.u32 + 144, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// stw r10,148(r31)
	PPC_STORE_U32(r31.u32 + 148, ctx.r10.u32);
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,152(r31)
	PPC_STORE_U32(r31.u32 + 152, ctx.r10.u32);
	// lwz r11,376(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 376);
	// stw r11,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r11.u32);
	// lwz r11,380(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 380);
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
	// lwz r11,376(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 376);
	// stw r11,1440(r31)
	PPC_STORE_U32(r31.u32 + 1440, r11.u32);
	// lwz r11,380(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 380);
	// stw r11,1444(r31)
	PPC_STORE_U32(r31.u32 + 1444, r11.u32);
	// lwz r11,3052(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3052);
	// stw r11,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r11.u32);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,50(r31)
	PPC_STORE_U16(r31.u32 + 50, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// sth r26,36(r31)
	PPC_STORE_U16(r31.u32 + 36, r26.u16);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r27,38(r31)
	PPC_STORE_U16(r31.u32 + 38, r27.u16);
	// rotlwi r8,r11,3
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 3);
	// sth r11,40(r31)
	PPC_STORE_U16(r31.u32 + 40, r11.u16);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// sth r9,42(r31)
	PPC_STORE_U16(r31.u32 + 42, ctx.r9.u16);
	// sth r10,52(r31)
	PPC_STORE_U16(r31.u32 + 52, ctx.r10.u16);
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r8,54(r31)
	PPC_STORE_U16(r31.u32 + 54, ctx.r8.u16);
	// addi r8,r31,1456
	ctx.r8.s64 = r31.s64 + 1456;
	// rotlwi r9,r10,3
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// sth r11,58(r31)
	PPC_STORE_U16(r31.u32 + 58, r11.u16);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// sth r9,56(r31)
	PPC_STORE_U16(r31.u32 + 56, ctx.r9.u16);
	// sth r10,60(r31)
	PPC_STORE_U16(r31.u32 + 60, ctx.r10.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// li r25,4
	r25.s64 = 4;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stb r26,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r26.u8);
	// li r5,16
	ctx.r5.s64 = 16;
	// stb r27,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r27.u8);
	// stb r27,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, r27.u8);
	// stb r29,83(r1)
	PPC_STORE_U8(ctx.r1.u32 + 83, r29.u8);
	// sth r11,74(r31)
	PPC_STORE_U16(r31.u32 + 74, r11.u16);
	// lwz r11,208(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 208);
	// lhz r10,74(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 74);
	// stb r27,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r27.u8);
	// stb r29,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r29.u8);
	// stb r29,86(r1)
	PPC_STORE_U8(ctx.r1.u32 + 86, r29.u8);
	// sth r11,76(r31)
	PPC_STORE_U16(r31.u32 + 76, r11.u16);
	// lwz r11,212(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 212);
	// lhz r9,76(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 76);
	// stb r28,87(r1)
	PPC_STORE_U8(ctx.r1.u32 + 87, r28.u8);
	// stb r27,88(r1)
	PPC_STORE_U8(ctx.r1.u32 + 88, r27.u8);
	// stb r29,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, r29.u8);
	// sth r11,78(r31)
	PPC_STORE_U16(r31.u32 + 78, r11.u16);
	// lwz r11,216(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 216);
	// stb r29,90(r1)
	PPC_STORE_U8(ctx.r1.u32 + 90, r29.u8);
	// stb r28,91(r1)
	PPC_STORE_U8(ctx.r1.u32 + 91, r28.u8);
	// stb r29,92(r1)
	PPC_STORE_U8(ctx.r1.u32 + 92, r29.u8);
	// stb r28,93(r1)
	PPC_STORE_U8(ctx.r1.u32 + 93, r28.u8);
	// sth r11,80(r31)
	PPC_STORE_U16(r31.u32 + 80, r11.u16);
	// lwz r11,228(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 228);
	// stb r28,94(r1)
	PPC_STORE_U8(ctx.r1.u32 + 94, r28.u8);
	// stb r25,95(r1)
	PPC_STORE_U8(ctx.r1.u32 + 95, r25.u8);
	// sth r11,82(r31)
	PPC_STORE_U16(r31.u32 + 82, r11.u16);
	// lwz r11,232(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 232);
	// sth r11,84(r31)
	PPC_STORE_U16(r31.u32 + 84, r11.u16);
	// lwz r11,172(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 172);
	// sth r11,86(r31)
	PPC_STORE_U16(r31.u32 + 86, r11.u16);
	// lwz r11,176(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 176);
	// sth r10,90(r31)
	PPC_STORE_U16(r31.u32 + 90, ctx.r10.u16);
	// sth r9,92(r31)
	PPC_STORE_U16(r31.u32 + 92, ctx.r9.u16);
	// sth r11,88(r31)
	PPC_STORE_U16(r31.u32 + 88, r11.u16);
	// lwz r11,1876(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1876);
	// stw r11,1100(r31)
	PPC_STORE_U32(r31.u32 + 1100, r11.u32);
	// lwz r11,1768(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1768);
	// stw r11,260(r31)
	PPC_STORE_U32(r31.u32 + 260, r11.u32);
	// lwz r11,460(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 460);
	// stw r11,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r11.u32);
	// lwz r11,464(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 464);
	// stw r11,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r11.u32);
	// lwz r11,468(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 468);
	// stw r8,1840(r31)
	PPC_STORE_U32(r31.u32 + 1840, ctx.r8.u32);
	// stb r27,1411(r31)
	PPC_STORE_U8(r31.u32 + 1411, r27.u8);
	// stb r29,1410(r31)
	PPC_STORE_U8(r31.u32 + 1410, r29.u8);
	// stb r28,1409(r31)
	PPC_STORE_U8(r31.u32 + 1409, r28.u8);
	// stw r11,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r11.u32);
	// stb r28,1408(r31)
	PPC_STORE_U8(r31.u32 + 1408, r28.u8);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,1764(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1764);
	// li r10,100
	ctx.r10.s64 = 100;
	// li r9,192
	ctx.r9.s64 = 192;
	// stb r26,1404(r31)
	PPC_STORE_U8(r31.u32 + 1404, r26.u8);
	// li r8,196
	ctx.r8.s64 = 196;
	// stb r26,1405(r31)
	PPC_STORE_U8(r31.u32 + 1405, r26.u8);
	// stb r26,1406(r31)
	PPC_STORE_U8(r31.u32 + 1406, r26.u8);
	// stb r27,1407(r31)
	PPC_STORE_U8(r31.u32 + 1407, r27.u8);
	// stw r11,448(r31)
	PPC_STORE_U32(r31.u32 + 448, r11.u32);
	// li r11,96
	r11.s64 = 96;
	// stb r26,1380(r31)
	PPC_STORE_U8(r31.u32 + 1380, r26.u8);
	// stb r25,1381(r31)
	PPC_STORE_U8(r31.u32 + 1381, r25.u8);
	// stb r10,1383(r31)
	PPC_STORE_U8(r31.u32 + 1383, ctx.r10.u8);
	// stb r9,1384(r31)
	PPC_STORE_U8(r31.u32 + 1384, ctx.r9.u8);
	// stb r11,1382(r31)
	PPC_STORE_U8(r31.u32 + 1382, r11.u8);
	// stb r8,1385(r31)
	PPC_STORE_U8(r31.u32 + 1385, ctx.r8.u8);
	// stb r26,516(r31)
	PPC_STORE_U8(r31.u32 + 516, r26.u8);
	// stb r27,519(r31)
	PPC_STORE_U8(r31.u32 + 519, r27.u8);
	// stb r27,518(r31)
	PPC_STORE_U8(r31.u32 + 518, r27.u8);
	// stb r27,517(r31)
	PPC_STORE_U8(r31.u32 + 517, r27.u8);
	// stb r29,522(r31)
	PPC_STORE_U8(r31.u32 + 522, r29.u8);
	// stb r29,521(r31)
	PPC_STORE_U8(r31.u32 + 521, r29.u8);
	// stb r29,520(r31)
	PPC_STORE_U8(r31.u32 + 520, r29.u8);
	// stb r25,523(r31)
	PPC_STORE_U8(r31.u32 + 523, r25.u8);
	// stb r26,524(r31)
	PPC_STORE_U8(r31.u32 + 524, r26.u8);
	// stb r27,525(r31)
	PPC_STORE_U8(r31.u32 + 525, r27.u8);
	// li r11,64
	r11.s64 = 64;
	// stb r29,526(r31)
	PPC_STORE_U8(r31.u32 + 526, r29.u8);
	// li r9,32
	ctx.r9.s64 = 32;
	// stb r28,527(r31)
	PPC_STORE_U8(r31.u32 + 527, r28.u8);
	// li r8,16
	ctx.r8.s64 = 16;
	// stb r27,528(r31)
	PPC_STORE_U8(r31.u32 + 528, r27.u8);
	// stb r29,529(r31)
	PPC_STORE_U8(r31.u32 + 529, r29.u8);
	// addi r10,r31,932
	ctx.r10.s64 = r31.s64 + 932;
	// stb r28,530(r31)
	PPC_STORE_U8(r31.u32 + 530, r28.u8);
	// stb r11,924(r31)
	PPC_STORE_U8(r31.u32 + 924, r11.u8);
	// mr r11,r26
	r11.u64 = r26.u64;
	// stb r26,531(r31)
	PPC_STORE_U8(r31.u32 + 531, r26.u8);
	// stb r9,925(r31)
	PPC_STORE_U8(r31.u32 + 925, ctx.r9.u8);
	// stb r9,926(r31)
	PPC_STORE_U8(r31.u32 + 926, ctx.r9.u8);
	// stb r26,927(r31)
	PPC_STORE_U8(r31.u32 + 927, r26.u8);
	// stb r8,928(r31)
	PPC_STORE_U8(r31.u32 + 928, ctx.r8.u8);
loc_82616644:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bge cr6,0x82616654
	if (!cr6.lt) goto loc_82616654;
	// stbx r26,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r26.u8);
	// b 0x82616660
	goto loc_82616660;
loc_82616654:
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// slw r9,r27,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r9.u8 & 0x3F));
	// stbx r9,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r9.u8);
loc_82616660:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x82616644
	if (cr6.lt) goto loc_82616644;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_82616670:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82616680
	if (!cr6.eq) goto loc_82616680;
	// stb r26,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r26.u8);
	// b 0x826166a8
	goto loc_826166A8;
loc_82616680:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bge cr6,0x82616690
	if (!cr6.lt) goto loc_82616690;
	// stbx r27,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r27.u8);
	// b 0x826166a8
	goto loc_826166A8;
loc_82616690:
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826166a4
	if (!cr6.eq) goto loc_826166A4;
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// b 0x826166a8
	goto loc_826166A8;
loc_826166A4:
	// stbx r25,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r25.u8);
loc_826166A8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x82616670
	if (cr6.lt) goto loc_82616670;
	// mr r11,r26
	r11.u64 = r26.u64;
	// addi r10,r31,996
	ctx.r10.s64 = r31.s64 + 996;
loc_826166BC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826166cc
	if (!cr6.eq) goto loc_826166CC;
	// stb r26,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r26.u8);
	// b 0x826166f4
	goto loc_826166F4;
loc_826166CC:
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bge cr6,0x826166dc
	if (!cr6.lt) goto loc_826166DC;
	// stbx r27,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r27.u8);
	// b 0x826166f4
	goto loc_826166F4;
loc_826166DC:
	// clrlwi r9,r11,30
	ctx.r9.u64 = r11.u32 & 0x3;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826166f0
	if (!cr6.eq) goto loc_826166F0;
	// stbx r29,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r29.u8);
	// b 0x826166f4
	goto loc_826166F4;
loc_826166F0:
	// stbx r25,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, r25.u8);
loc_826166F4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// blt cr6,0x826166bc
	if (cr6.lt) goto loc_826166BC;
	// lis r11,4
	r11.s64 = 262144;
	// lhz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 52);
	// lis r4,-32143
	ctx.r4.s64 = -2106523648;
	// lhz r9,50(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 50);
	// ori r11,r11,4
	r11.u64 = r11.u64 | 4;
	// rotlwi r10,r10,16
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 16);
	// addi r4,r4,15536
	ctx.r4.s64 = ctx.r4.s64 + 15536;
	// mr r30,r11
	r30.u64 = r11.u64;
	// mr r29,r11
	r29.u64 = r11.u64;
	// lis r3,-32143
	ctx.r3.s64 = -2106523648;
	// lis r6,-32143
	ctx.r6.s64 = -2106523648;
	// lis r7,-32143
	ctx.r7.s64 = -2106523648;
	// stw r4,472(r31)
	PPC_STORE_U32(r31.u32 + 472, ctx.r4.u32);
	// lis r28,64
	r28.s64 = 4194304;
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// ori r28,r28,64
	r28.u64 = r28.u64 | 64;
	// addi r3,r3,15112
	ctx.r3.s64 = ctx.r3.s64 + 15112;
	// addi r6,r6,16744
	ctx.r6.s64 = ctx.r6.s64 + 16744;
	// addi r7,r7,17536
	ctx.r7.s64 = ctx.r7.s64 + 17536;
	// addi r4,r10,20152
	ctx.r4.s64 = ctx.r10.s64 + 20152;
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// stw r28,1412(r31)
	PPC_STORE_U32(r31.u32 + 1412, r28.u32);
	// lis r8,-32143
	ctx.r8.s64 = -2106523648;
	// stw r3,468(r31)
	PPC_STORE_U32(r31.u32 + 468, ctx.r3.u32);
	// lis r9,-32143
	ctx.r9.s64 = -2106523648;
	// stw r6,480(r31)
	PPC_STORE_U32(r31.u32 + 480, ctx.r6.u32);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r7,484(r31)
	PPC_STORE_U32(r31.u32 + 484, ctx.r7.u32);
	// lis r5,-32143
	ctx.r5.s64 = -2106523648;
	// lis r24,60
	r24.s64 = 3932160;
	// stw r10,1424(r31)
	PPC_STORE_U32(r31.u32 + 1424, ctx.r10.u32);
	// lis r23,28
	r23.s64 = 1835008;
	// lis r22,32
	r22.s64 = 2097152;
	// addi r28,r8,17992
	r28.s64 = ctx.r8.s64 + 17992;
	// stw r11,1876(r31)
	PPC_STORE_U32(r31.u32 + 1876, r11.u32);
	// addi r3,r9,18864
	ctx.r3.s64 = ctx.r9.s64 + 18864;
	// ori r24,r24,60
	r24.u64 = r24.u64 | 60;
	// ori r23,r23,28
	r23.u64 = r23.u64 | 28;
	// ori r22,r22,32
	r22.u64 = r22.u64 | 32;
	// addi r5,r5,16312
	ctx.r5.s64 = ctx.r5.s64 + 16312;
	// stw r28,488(r31)
	PPC_STORE_U32(r31.u32 + 488, r28.u32);
	// subf r7,r30,r10
	ctx.r7.s64 = ctx.r10.s64 - r30.s64;
	// stw r3,492(r31)
	PPC_STORE_U32(r31.u32 + 492, ctx.r3.u32);
	// subf r6,r29,r11
	ctx.r6.s64 = r11.s64 - r29.s64;
	// stw r24,1420(r31)
	PPC_STORE_U32(r31.u32 + 1420, r24.u32);
	// stw r23,1416(r31)
	PPC_STORE_U32(r31.u32 + 1416, r23.u32);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// stw r22,1872(r31)
	PPC_STORE_U32(r31.u32 + 1872, r22.u32);
	// addi r8,r31,500
	ctx.r8.s64 = r31.s64 + 500;
	// stw r5,476(r31)
	PPC_STORE_U32(r31.u32 + 476, ctx.r5.u32);
	// stw r7,1432(r31)
	PPC_STORE_U32(r31.u32 + 1432, ctx.r7.u32);
	// stw r6,1428(r31)
	PPC_STORE_U32(r31.u32 + 1428, ctx.r6.u32);
	// stw r4,496(r31)
	PPC_STORE_U32(r31.u32 + 496, ctx.r4.u32);
loc_826167D8:
	// rlwinm r7,r9,0,28,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x8;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826167f0
	if (cr6.eq) goto loc_826167F0;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_826167F0:
	// rlwinm r7,r9,0,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82616804
	if (cr6.eq) goto loc_82616804;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82616804:
	// rlwinm r7,r9,0,30,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8261681c
	if (cr6.eq) goto loc_8261681C;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// ori r10,r10,8
	ctx.r10.u64 = ctx.r10.u64 | 8;
loc_8261681C:
	// clrlwi r7,r9,31
	ctx.r7.u64 = ctx.r9.u32 & 0x1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82616834
	if (cr6.eq) goto loc_82616834;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// ori r10,r10,12
	ctx.r10.u64 = ctx.r10.u64 | 12;
loc_82616834:
	// subfic r11,r11,4
	xer.ca = r11.u32 <= 4;
	r11.s64 = 4 - r11.s64;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r7,r11,30,28,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0xC;
	// rlwimi r10,r11,4,0,27
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0xFFFFFFF0) | (ctx.r10.u64 & 0xFFFFFFFF0000000F);
	// rlwinm r11,r11,26,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3;
	// rlwinm r10,r10,2,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFF0;
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stbx r11,r9,r8
	PPC_STORE_U8(ctx.r9.u32 + ctx.r8.u32, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r9,16
	cr6.compare<int32_t>(ctx.r9.s32, 16, xer);
	// blt cr6,0x826167d8
	if (cr6.lt) goto loc_826167D8;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82616878"))) PPC_WEAK_FUNC(sub_82616878);
PPC_FUNC_IMPL(__imp__sub_82616878) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82616370
	sub_82616370(ctx, base);
	// lwz r11,19984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19984);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r11,1880(r31)
	PPC_STORE_U32(r31.u32 + 1880, r11.u32);
	// lwz r11,19984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19984);
	// lwz r10,364(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 364);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// lwz r11,19984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19984);
	// lwz r10,368(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 368);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r9,r30,2916
	ctx.r9.s64 = r30.s64 + 2916;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lwz r11,360(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 360);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,248(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 248);
	// lwz r10,252(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,24(r31)
	PPC_STORE_U8(r31.u32 + 24, r11.u8);
	// lwz r11,344(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 344);
	// stb r11,26(r31)
	PPC_STORE_U8(r31.u32 + 26, r11.u8);
	// lwz r11,2376(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2376);
	// stw r11,176(r31)
	PPC_STORE_U32(r31.u32 + 176, r11.u32);
	// lwz r11,280(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 280);
	// stb r11,27(r31)
	PPC_STORE_U8(r31.u32 + 27, r11.u8);
	// lwz r11,392(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 392);
	// stb r11,28(r31)
	PPC_STORE_U8(r31.u32 + 28, r11.u8);
	// lwz r11,328(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 328);
	// stb r11,29(r31)
	PPC_STORE_U8(r31.u32 + 29, r11.u8);
	// lwz r11,3960(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3960);
	// addi r10,r11,-3
	ctx.r10.s64 = r11.s64 + -3;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// addi r10,r30,2904
	ctx.r10.s64 = r30.s64 + 2904;
	// stb r11,30(r31)
	PPC_STORE_U8(r31.u32 + 30, r11.u8);
	// lwz r11,2140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2140);
	// stw r11,196(r31)
	PPC_STORE_U32(r31.u32 + 196, r11.u32);
	// lwz r11,2516(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2516);
	// sth r8,46(r31)
	PPC_STORE_U16(r31.u32 + 46, ctx.r8.u16);
	// sth r8,44(r31)
	PPC_STORE_U16(r31.u32 + 44, ctx.r8.u16);
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
	// lwz r11,416(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 416);
	// sth r11,62(r31)
	PPC_STORE_U16(r31.u32 + 62, r11.u16);
	// lwz r11,420(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 420);
	// sth r11,64(r31)
	PPC_STORE_U16(r31.u32 + 64, r11.u16);
	// lwz r11,424(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 424);
	// sth r11,66(r31)
	PPC_STORE_U16(r31.u32 + 66, r11.u16);
	// lwz r11,428(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 428);
	// sth r11,68(r31)
	PPC_STORE_U16(r31.u32 + 68, r11.u16);
	// lwz r11,408(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 408);
	// sth r11,70(r31)
	PPC_STORE_U16(r31.u32 + 70, r11.u16);
	// lwz r11,412(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 412);
	// sth r11,72(r31)
	PPC_STORE_U16(r31.u32 + 72, r11.u16);
	// lwz r11,14772(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14772);
	// stb r11,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r11.u8);
	// lwz r11,1792(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1792);
	// stb r11,31(r31)
	PPC_STORE_U8(r31.u32 + 31, r11.u8);
	// lwz r11,336(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 336);
	// stb r11,34(r31)
	PPC_STORE_U8(r31.u32 + 34, r11.u8);
	// lwz r11,6548(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 6548);
	// stw r11,220(r31)
	PPC_STORE_U32(r31.u32 + 220, r11.u32);
	// lwz r11,14752(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14752);
	// stw r10,228(r31)
	PPC_STORE_U32(r31.u32 + 228, ctx.r10.u32);
	// stw r9,232(r31)
	PPC_STORE_U32(r31.u32 + 232, ctx.r9.u32);
	// stw r11,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r11.u32);
	// lwz r11,2880(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2880);
	// stw r11,236(r31)
	PPC_STORE_U32(r31.u32 + 236, r11.u32);
	// lwz r11,2884(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2884);
	// stw r11,240(r31)
	PPC_STORE_U32(r31.u32 + 240, r11.u32);
	// lwz r11,2888(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2888);
	// stw r11,244(r31)
	PPC_STORE_U32(r31.u32 + 244, r11.u32);
	// lwz r11,2892(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2892);
	// stw r11,248(r31)
	PPC_STORE_U32(r31.u32 + 248, r11.u32);
	// lwz r11,2896(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2896);
	// stw r11,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r11.u32);
	// lwz r11,2900(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2900);
	// stw r11,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r11.u32);
	// lwz r11,1940(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1940);
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// lwz r11,2968(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2968);
	// stb r11,33(r31)
	PPC_STORE_U8(r31.u32 + 33, r11.u8);
	// lwz r11,1832(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1832);
	// stw r11,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r11.u32);
	// lwz r11,456(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 456);
	// stb r8,49(r31)
	PPC_STORE_U8(r31.u32 + 49, ctx.r8.u8);
	// stb r11,48(r31)
	PPC_STORE_U8(r31.u32 + 48, r11.u8);
	// lwz r11,3904(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3904);
	// stb r11,35(r31)
	PPC_STORE_U8(r31.u32 + 35, r11.u8);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// mulli r11,r11,-6
	r11.s64 = r11.s64 * -6;
	// sth r11,208(r31)
	PPC_STORE_U16(r31.u32 + 208, r11.u16);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,210(r31)
	PPC_STORE_U16(r31.u32 + 210, r11.u16);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,212(r31)
	PPC_STORE_U16(r31.u32 + 212, r11.u16);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r11,214(r31)
	PPC_STORE_U16(r31.u32 + 214, r11.u16);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sth r11,204(r31)
	PPC_STORE_U16(r31.u32 + 204, r11.u16);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// sth r11,206(r31)
	PPC_STORE_U16(r31.u32 + 206, r11.u16);
	// lwz r11,3756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3756);
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// lwz r10,220(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// lwz r11,3760(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3760);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r11.u32);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,3736(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3736);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r11.u32);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,3764(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3764);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,308(r31)
	PPC_STORE_U32(r31.u32 + 308, r11.u32);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,3740(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3740);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,312(r31)
	PPC_STORE_U32(r31.u32 + 312, r11.u32);
	// lwz r11,3768(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3768);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,316(r31)
	PPC_STORE_U32(r31.u32 + 316, r11.u32);
	// lwz r11,15900(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15900);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82616b18
	if (cr6.eq) goto loc_82616B18;
	// lwz r11,15904(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15904);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82616b18
	if (cr6.eq) goto loc_82616B18;
	// lwz r11,15908(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15908);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82616b18
	if (!cr6.eq) goto loc_82616B18;
	// lwz r11,15912(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15912);
	// lwz r10,220(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// lwz r11,15912(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15912);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r11.u32);
	// lwz r11,15912(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15912);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,312(r31)
	PPC_STORE_U32(r31.u32 + 312, r11.u32);
loc_82616B18:
	// lwz r10,220(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 220);
	// lwz r11,3720(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3720);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r11,392(r31)
	PPC_STORE_U32(r31.u32 + 392, r11.u32);
	// lwz r9,392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// stw r10,396(r31)
	PPC_STORE_U32(r31.u32 + 396, ctx.r10.u32);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r11,400(r31)
	PPC_STORE_U32(r31.u32 + 400, r11.u32);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,404(r31)
	PPC_STORE_U32(r31.u32 + 404, r11.u32);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,3724(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3724);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,408(r31)
	PPC_STORE_U32(r31.u32 + 408, r11.u32);
	// lwz r10,224(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 224);
	// lwz r11,3728(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3728);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,412(r31)
	PPC_STORE_U32(r31.u32 + 412, r11.u32);
	// lwz r11,3960(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3960);
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,1864(r31)
	PPC_STORE_U32(r31.u32 + 1864, r11.u32);
	// lwz r11,14804(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14804);
	// stw r11,1852(r31)
	PPC_STORE_U32(r31.u32 + 1852, r11.u32);
	// lwz r11,14780(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14780);
	// stw r11,1856(r31)
	PPC_STORE_U32(r31.u32 + 1856, r11.u32);
	// lwz r11,14784(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14784);
	// stw r11,1860(r31)
	PPC_STORE_U32(r31.u32 + 1860, r11.u32);
	// lwz r11,14776(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14776);
	// lwz r10,3392(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 3392);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,1376(r31)
	PPC_STORE_U32(r31.u32 + 1376, r11.u32);
	// lwz r11,2980(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2980);
	// stw r11,420(r31)
	PPC_STORE_U32(r31.u32 + 420, r11.u32);
	// stw r11,416(r31)
	PPC_STORE_U32(r31.u32 + 416, r11.u32);
	// lwz r11,2988(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2988);
	// stw r11,428(r31)
	PPC_STORE_U32(r31.u32 + 428, r11.u32);
	// stw r11,424(r31)
	PPC_STORE_U32(r31.u32 + 424, r11.u32);
	// lwz r11,2992(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2992);
	// stw r11,432(r31)
	PPC_STORE_U32(r31.u32 + 432, r11.u32);
	// lwz r11,3000(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3000);
	// stw r11,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r11.u32);
	// lwz r11,2556(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2556);
	// stw r11,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r11.u32);
	// lwz r11,2476(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2476);
	// stw r11,444(r31)
	PPC_STORE_U32(r31.u32 + 444, r11.u32);
	// lwz r11,15472(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15472);
	// stw r11,1104(r31)
	PPC_STORE_U32(r31.u32 + 1104, r11.u32);
	// lwz r11,15472(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bge cr6,0x82616c0c
	if (!cr6.lt) goto loc_82616C0C;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
loc_82616C0C:
	// stw r9,1868(r31)
	PPC_STORE_U32(r31.u32 + 1868, ctx.r9.u32);
	// addi r10,r31,1056
	ctx.r10.s64 = r31.s64 + 1056;
	// lwz r7,21480(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 21480);
	// lis r9,32
	ctx.r9.s64 = 2097152;
	// lbz r11,35(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// lis r6,64
	ctx.r6.s64 = 4194304;
	// lis r5,8
	ctx.r5.s64 = 524288;
	// vspltish v13,8
	// ori r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 | 32;
	// ori r6,r6,64
	ctx.r6.u64 = ctx.r6.u64 | 64;
	// stw r7,1108(r31)
	PPC_STORE_U32(r31.u32 + 1108, ctx.r7.u32);
	// addis r29,r11,15
	r29.s64 = r11.s64 + 983040;
	// lwz r7,1852(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 1852);
	// addis r28,r11,7
	r28.s64 = r11.s64 + 458752;
	// addis r27,r11,3
	r27.s64 = r11.s64 + 196608;
	// ori r5,r5,8
	ctx.r5.u64 = ctx.r5.u64 | 8;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - r11.s64;
	// stw r7,452(r31)
	PPC_STORE_U32(r31.u32 + 452, ctx.r7.u32);
	// addi r4,r31,1040
	ctx.r4.s64 = r31.s64 + 1040;
	// lwz r7,1856(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 1856);
	// addi r29,r29,15
	r29.s64 = r29.s64 + 15;
	// addi r28,r28,7
	r28.s64 = r28.s64 + 7;
	// addi r27,r27,3
	r27.s64 = r27.s64 + 3;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r7,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r7.u32);
	// lwz r7,1860(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 1860);
	// stw r9,1088(r31)
	PPC_STORE_U32(r31.u32 + 1088, ctx.r9.u32);
	// addi r9,r31,1128
	ctx.r9.s64 = r31.s64 + 1128;
	// stw r29,1076(r31)
	PPC_STORE_U32(r31.u32 + 1076, r29.u32);
	// stw r28,1080(r31)
	PPC_STORE_U32(r31.u32 + 1080, r28.u32);
	// stw r27,1084(r31)
	PPC_STORE_U32(r31.u32 + 1084, r27.u32);
	// stw r7,464(r31)
	PPC_STORE_U32(r31.u32 + 464, ctx.r7.u32);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// stw r6,1092(r31)
	PPC_STORE_U32(r31.u32 + 1092, ctx.r6.u32);
	// sth r7,1070(r31)
	PPC_STORE_U16(r31.u32 + 1070, ctx.r7.u16);
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v0,v0,7
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0x100))));
	// addis r7,r11,31
	ctx.r7.s64 = r11.s64 + 2031616;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// addi r7,r7,31
	ctx.r7.s64 = ctx.r7.s64 + 31;
	// vsubshs v13,v13,v0
	// stw r11,1096(r31)
	PPC_STORE_U32(r31.u32 + 1096, r11.u32);
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r7,1072(r31)
	PPC_STORE_U32(r31.u32 + 1072, ctx.r7.u32);
	// stvx v13,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// sth r3,1112(r31)
	PPC_STORE_U16(r31.u32 + 1112, ctx.r3.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1114(r31)
	PPC_STORE_U16(r31.u32 + 1114, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1116(r31)
	PPC_STORE_U16(r31.u32 + 1116, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1118(r31)
	PPC_STORE_U16(r31.u32 + 1118, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1120(r31)
	PPC_STORE_U16(r31.u32 + 1120, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1122(r31)
	PPC_STORE_U16(r31.u32 + 1122, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1124(r31)
	PPC_STORE_U16(r31.u32 + 1124, r11.u16);
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// mulli r11,r11,28
	r11.s64 = r11.s64 * 28;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// sth r11,1126(r31)
	PPC_STORE_U16(r31.u32 + 1126, r11.u16);
loc_82616D70:
	// lwz r11,208(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 208);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// sth r11,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r11.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// blt cr6,0x82616d70
	if (cr6.lt) goto loc_82616D70;
	// lwz r11,2092(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2092);
	// li r9,1036
	ctx.r9.s64 = 1036;
	// addi r10,r30,23584
	ctx.r10.s64 = r30.s64 + 23584;
	// addi r8,r30,23776
	ctx.r8.s64 = r30.s64 + 23776;
	// li r7,16
	ctx.r7.s64 = 16;
	// li r6,-1
	ctx.r6.s64 = -1;
	// stw r11,1160(r31)
	PPC_STORE_U32(r31.u32 + 1160, r11.u32);
	// li r11,-1
	r11.s64 = -1;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// li r9,1032
	ctx.r9.s64 = 1032;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r11,2096(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 2096);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// li r9,2056
	ctx.r9.s64 = 2056;
	// stw r11,1164(r31)
	PPC_STORE_U32(r31.u32 + 1164, r11.u32);
	// li r11,2052
	r11.s64 = 2052;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// li r9,1028
	ctx.r9.s64 = 1028;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// lwz r11,21008(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21008);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lis r9,1036
	ctx.r9.s64 = 67895296;
	// ori r9,r9,1029
	ctx.r9.u64 = ctx.r9.u64 | 1029;
	// stw r11,1172(r31)
	PPC_STORE_U32(r31.u32 + 1172, r11.u32);
	// li r11,2048
	r11.s64 = 2048;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// li r9,3076
	ctx.r9.s64 = 3076;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// lwz r11,21012(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21012);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// li r9,1024
	ctx.r9.s64 = 1024;
	// stw r11,1176(r31)
	PPC_STORE_U32(r31.u32 + 1176, r11.u32);
	// lwz r11,248(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 248);
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// lis r9,1036
	ctx.r9.s64 = 67895296;
	// ori r9,r9,1025
	ctx.r9.u64 = ctx.r9.u64 | 1025;
	// stb r11,1180(r31)
	PPC_STORE_U8(r31.u32 + 1180, r11.u8);
	// lwz r11,3976(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3976);
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
	// lis r9,1032
	ctx.r9.s64 = 67633152;
	// ori r9,r9,1025
	ctx.r9.u64 = ctx.r9.u64 | 1025;
	// stb r11,1181(r31)
	PPC_STORE_U8(r31.u32 + 1181, r11.u8);
	// lwz r11,3984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3984);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// lis r9,2056
	ctx.r9.s64 = 134742016;
	// stb r11,1182(r31)
	PPC_STORE_U8(r31.u32 + 1182, r11.u8);
	// ori r9,r9,1025
	ctx.r9.u64 = ctx.r9.u64 | 1025;
	// lwz r11,252(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 252);
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// lis r9,1036
	ctx.r9.s64 = 67895296;
	// stb r11,1185(r31)
	PPC_STORE_U8(r31.u32 + 1185, r11.u8);
	// lwz r11,472(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 472);
	// ori r9,r9,2049
	ctx.r9.u64 = ctx.r9.u64 | 2049;
	// stb r11,1186(r31)
	PPC_STORE_U8(r31.u32 + 1186, r11.u8);
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// li r9,3072
	ctx.r9.s64 = 3072;
	// stw r11,1240(r31)
	PPC_STORE_U32(r31.u32 + 1240, r11.u32);
	// lwz r11,284(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 284);
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// li r9,4096
	ctx.r9.s64 = 4096;
	// stw r11,1244(r31)
	PPC_STORE_U32(r31.u32 + 1244, r11.u32);
	// lwz r11,1948(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1948);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stb r11,1183(r31)
	PPC_STORE_U8(r31.u32 + 1183, r11.u8);
	// lwz r11,1952(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1952);
	// stb r11,1184(r31)
	PPC_STORE_U8(r31.u32 + 1184, r11.u8);
	// lwz r11,1944(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1944);
	// stb r11,1187(r31)
	PPC_STORE_U8(r31.u32 + 1187, r11.u8);
	// lwz r11,21576(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21576);
	// stw r11,1248(r31)
	PPC_STORE_U32(r31.u32 + 1248, r11.u32);
	// lwz r11,3916(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3916);
	// stw r11,1252(r31)
	PPC_STORE_U32(r31.u32 + 1252, r11.u32);
	// stw r10,1292(r31)
	PPC_STORE_U32(r31.u32 + 1292, ctx.r10.u32);
	// stw r8,1296(r31)
	PPC_STORE_U32(r31.u32 + 1296, ctx.r8.u32);
	// lwz r11,1292(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1292);
	// lwz r8,204(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82616ED0:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r4,r10,0,24,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r4,r4,r8
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// stw r4,3(r11)
	PPC_STORE_U32(r11.u32 + 3, ctx.r4.u32);
	// clrlwi r5,r10,31
	ctx.r5.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82616f10
	if (cr6.eq) goto loc_82616F10;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r10,r10,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// stw r10,7(r11)
	PPC_STORE_U32(r11.u32 + 7, ctx.r10.u32);
loc_82616F10:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82616ed0
	if (!cr6.eq) goto loc_82616ED0;
	// lwz r11,1296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1296);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r8,208(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 208);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82616F38:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r4,r10,0,24,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r4,r4,r8
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// stb r5,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r5.u8);
	// stw r4,3(r11)
	PPC_STORE_U32(r11.u32 + 3, ctx.r4.u32);
	// clrlwi r5,r10,31
	ctx.r5.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82616f78
	if (cr6.eq) goto loc_82616F78;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// rlwinm r5,r10,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r10,r10,0,24,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFE;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// stw r10,7(r11)
	PPC_STORE_U32(r11.u32 + 7, ctx.r10.u32);
loc_82616F78:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82616f38
	if (!cr6.eq) goto loc_82616F38;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82616F94"))) PPC_WEAK_FUNC(sub_82616F94);
PPC_FUNC_IMPL(__imp__sub_82616F94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82616F98"))) PPC_WEAK_FUNC(sub_82616F98);
PPC_FUNC_IMPL(__imp__sub_82616F98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce0
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82617118
	if (!cr6.gt) goto loc_82617118;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r24,r10,r3
	r24.s64 = ctx.r3.s64 - ctx.r10.s64;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r23,r4,r3
	r23.s64 = ctx.r3.s64 - ctx.r4.s64;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// add r31,r11,r3
	r31.u64 = r11.u64 + ctx.r3.u64;
	// subf r26,r11,r3
	r26.s64 = ctx.r3.s64 - r11.s64;
	// subf r25,r10,r3
	r25.s64 = ctx.r3.s64 - ctx.r10.s64;
loc_82616FD0:
	// lbz r29,0(r3)
	r29.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r28,0(r23)
	r28.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// subf r9,r29,r28
	ctx.r9.s64 = r28.s64 - r29.s64;
	// srawi r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	// addze r27,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r27.s64 = temp.s64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826170f4
	if (cr6.eq) goto loc_826170F4;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbzx r10,r3,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r4.u32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r8,0(r25)
	ctx.r8.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// subf r30,r10,r11
	r30.s64 = r11.s64 - ctx.r10.s64;
	// lbz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// rlwinm r6,r30,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// srawi r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// xor r6,r9,r27
	ctx.r6.u64 = ctx.r9.u64 ^ r27.u64;
	// rlwinm r6,r6,0,0,0
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826170f4
	if (cr6.eq) goto loc_826170F4;
	// srawi r6,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r6.u64;
	// subf r30,r6,r9
	r30.s64 = ctx.r9.s64 - ctx.r6.s64;
	// cmpw cr6,r30,r5
	cr6.compare<int32_t>(r30.s32, ctx.r5.s32, xer);
	// bge cr6,0x826170f4
	if (!cr6.lt) goto loc_826170F4;
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// lbzx r6,r31,r4
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + ctx.r4.u32);
	// subf r8,r28,r7
	ctx.r8.s64 = ctx.r7.s64 - r28.s64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r9,r6,r29
	ctx.r9.s64 = r29.s64 - ctx.r6.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r6
	r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826170a8
	if (!cr6.lt) goto loc_826170A8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826170A8:
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x826170f4
	if (!cr6.lt) goto loc_826170F4;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// srawi r10,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r27.s32 >> 31;
	// xor r9,r27,r10
	ctx.r9.u64 = r27.u64 ^ ctx.r10.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x826170d8
	if (cr6.lt) goto loc_826170D8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_826170D8:
	// cmpw cr6,r28,r29
	cr6.compare<int32_t>(r28.s32, r29.s32, xer);
	// bge cr6,0x826170e4
	if (!cr6.lt) goto loc_826170E4;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_826170E4:
	// subf r10,r11,r28
	ctx.r10.s64 = r28.s64 - r11.s64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stb r10,0(r23)
	PPC_STORE_U8(r23.u32 + 0, ctx.r10.u8);
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
loc_826170F4:
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82616fd0
	if (!cr6.eq) goto loc_82616FD0;
loc_82617118:
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8261711C"))) PPC_WEAK_FUNC(sub_8261711C);
PPC_FUNC_IMPL(__imp__sub_8261711C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617120"))) PPC_WEAK_FUNC(sub_82617120);
PPC_FUNC_IMPL(__imp__sub_82617120) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd8
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x826172a4
	if (!cr6.gt) goto loc_826172A4;
	// addi r26,r3,5
	r26.s64 = ctx.r3.s64 + 5;
	// addi r27,r3,4
	r27.s64 = ctx.r3.s64 + 4;
	// addi r28,r3,7
	r28.s64 = ctx.r3.s64 + 7;
	// addi r29,r3,8
	r29.s64 = ctx.r3.s64 + 8;
	// addi r30,r3,1
	r30.s64 = ctx.r3.s64 + 1;
	// addi r31,r3,2
	r31.s64 = ctx.r3.s64 + 2;
	// addi r25,r3,6
	r25.s64 = ctx.r3.s64 + 6;
	// addi r3,r3,3
	ctx.r3.s64 = ctx.r3.s64 + 3;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
loc_82617154:
	// lbz r24,0(r27)
	r24.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lbz r23,0(r26)
	r23.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// subf r9,r23,r24
	ctx.r9.s64 = r24.s64 - r23.s64;
	// srawi r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	// addze r22,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r22.s64 = temp.s64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x82617278
	if (cr6.eq) goto loc_82617278;
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// srawi r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// xor r8,r9,r22
	ctx.r8.u64 = ctx.r9.u64 ^ r22.u64;
	// rlwinm r8,r8,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82617278
	if (cr6.eq) goto loc_82617278;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r21,r8,r9
	r21.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpw cr6,r21,r5
	cr6.compare<int32_t>(r21.s32, ctx.r5.s32, xer);
	// bge cr6,0x82617278
	if (!cr6.lt) goto loc_82617278;
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// lbz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lbz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lbz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r8,r24,r9
	ctx.r8.s64 = ctx.r9.s64 - r24.s64;
	// subf r9,r7,r23
	ctx.r9.s64 = r23.s64 - ctx.r7.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r6
	r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8261722c
	if (!cr6.lt) goto loc_8261722C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8261722C:
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x82617278
	if (!cr6.lt) goto loc_82617278;
	// subf r11,r11,r21
	r11.s64 = r21.s64 - r11.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// srawi r10,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r22.s32 >> 31;
	// xor r9,r22,r10
	ctx.r9.u64 = r22.u64 ^ ctx.r10.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8261725c
	if (cr6.lt) goto loc_8261725C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8261725C:
	// cmpw cr6,r24,r23
	cr6.compare<int32_t>(r24.s32, r23.s32, xer);
	// bge cr6,0x82617268
	if (!cr6.lt) goto loc_82617268;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_82617268:
	// subf r10,r11,r24
	ctx.r10.s64 = r24.s64 - r11.s64;
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// stb r10,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r10.u8);
	// stb r11,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r11.u8);
loc_82617278:
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r25,r25,r4
	r25.u64 = r25.u64 + ctx.r4.u64;
	// add r31,r31,r4
	r31.u64 = r31.u64 + ctx.r4.u64;
	// add r30,r30,r4
	r30.u64 = r30.u64 + ctx.r4.u64;
	// add r29,r29,r4
	r29.u64 = r29.u64 + ctx.r4.u64;
	// add r28,r28,r4
	r28.u64 = r28.u64 + ctx.r4.u64;
	// add r27,r27,r4
	r27.u64 = r27.u64 + ctx.r4.u64;
	// add r26,r26,r4
	r26.u64 = r26.u64 + ctx.r4.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x82617154
	if (!cr6.eq) goto loc_82617154;
loc_826172A4:
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_826172A8"))) PPC_WEAK_FUNC(sub_826172A8);
PPC_FUNC_IMPL(__imp__sub_826172A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// stw r10,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r10.u32);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// addi r29,r28,3
	r29.s64 = r28.s64 + 3;
	// lwz r11,132(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 132);
	// lwz r23,128(r15)
	r23.u64 = PPC_LOAD_U32(r15.u32 + 128);
	// lwz r10,228(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 228);
	// addi r20,r11,-1
	r20.s64 = r11.s64 + -1;
	// lwz r24,112(r15)
	r24.u64 = PPC_LOAD_U32(r15.u32 + 112);
	// rlwinm r11,r23,1,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r21,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r21.s64 = ctx.r10.s32 >> 1;
	// lwz r31,204(r15)
	r31.u64 = PPC_LOAD_U32(r15.u32 + 204);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r30,208(r15)
	r30.u64 = PPC_LOAD_U32(r15.u32 + 208);
	// lwz r5,248(r15)
	ctx.r5.u64 = PPC_LOAD_U32(r15.u32 + 248);
	// stw r23,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r23.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r20,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r20.u32);
	// stw r21,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r21.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// ble cr6,0x82617334
	if (!cr6.gt) goto loc_82617334;
	// mr r27,r11
	r27.u64 = r11.u64;
loc_82617314:
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82617314
	if (!cr6.eq) goto loc_82617314;
loc_82617334:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r17,r28
	r17.u64 = r28.u64;
	// subfic r14,r11,-1
	xer.ca = r11.u32 <= 4294967295;
	r14.s64 = -1 - r11.s64;
	// li r18,0
	r18.s64 = 0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x8261750c
	if (!cr6.gt) goto loc_8261750C;
	// add r11,r24,r26
	r11.u64 = r24.u64 + r26.u64;
	// rlwinm r10,r31,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r19,r11,4
	r19.s64 = r11.s64 + 4;
	// add r10,r10,r21
	ctx.r10.u64 = ctx.r10.u64 + r21.u64;
	// subf r11,r26,r25
	r11.s64 = r25.s64 - r26.s64;
	// add r16,r24,r25
	r16.u64 = r24.u64 + r25.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_8261736C:
	// add r29,r17,r21
	r29.u64 = r17.u64 + r21.u64;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r28,r11,r17
	r28.u64 = r11.u64 + r17.u64;
	// li r22,8
	r22.s64 = 8;
	// li r25,-4
	r25.s64 = -4;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r27,r29,4
	r27.s64 = r29.s64 + 4;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// addi r26,r28,4
	r26.s64 = r28.s64 + 4;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x826173b4
	if (!cr6.eq) goto loc_826173B4;
	// li r25,-8
	r25.s64 = -8;
	// b 0x826173c4
	goto loc_826173C4;
loc_826173B4:
	// addi r11,r20,-1
	r11.s64 = r20.s64 + -1;
	// cmpw cr6,r18,r11
	cr6.compare<int32_t>(r18.s32, r11.s32, xer);
	// bne cr6,0x826173c8
	if (!cr6.eq) goto loc_826173C8;
	// li r25,-4
	r25.s64 = -4;
loc_826173C4:
	// li r22,12
	r22.s64 = 12;
loc_826173C8:
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r19,-4
	ctx.r3.s64 = r19.s64 + -4;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// mullw r11,r25,r30
	r11.s64 = int64_t(r25.s32) * int64_t(r30.s32);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// mr r29,r19
	r29.u64 = r19.u64;
	// addi r23,r11,-1
	r23.s64 = r11.s64 + -1;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r28,r19,r11
	r28.u64 = r19.u64 + r11.u64;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// addi r25,r11,-1
	r25.s64 = r11.s64 + -1;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x826174a8
	if (!cr6.gt) goto loc_826174A8;
loc_82617408:
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r24,r27,r14
	r24.u64 = r27.u64 + r14.u64;
	// add r21,r23,r29
	r21.u64 = r23.u64 + r29.u64;
	// add r20,r28,r23
	r20.u64 = r28.u64 + r23.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// addi r26,r26,16
	r26.s64 = r26.s64 + 16;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r24,8
	ctx.r3.s64 = r24.s64 + 8;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x82617408
	if (!cr6.eq) goto loc_82617408;
	// lwz r20,348(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_826174A8:
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// add r3,r27,r14
	ctx.r3.u64 = r27.u64 + r14.u64;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// lwz r11,100(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 100);
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// add r19,r19,r24
	r19.u64 = r19.u64 + r24.u64;
	// add r16,r16,r24
	r16.u64 = r16.u64 + r24.u64;
	// add r17,r17,r11
	r17.u64 = r17.u64 + r11.u64;
	// cmpw cr6,r18,r20
	cr6.compare<int32_t>(r18.s32, r20.s32, xer);
	// blt cr6,0x8261736c
	if (cr6.lt) goto loc_8261736C;
	// lwz r23,332(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
loc_8261750C:
	// add r30,r17,r21
	r30.u64 = r17.u64 + r21.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x82617584
	if (!cr6.gt) goto loc_82617584;
	// addi r27,r23,-1
	r27.s64 = r23.s64 + -1;
loc_82617534:
	// add r28,r30,r14
	r28.u64 = r30.u64 + r14.u64;
	// li r6,16
	ctx.r6.s64 = 16;
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// bne cr6,0x82617548
	if (!cr6.eq) goto loc_82617548;
	// li r6,12
	ctx.r6.s64 = 12;
loc_82617548:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82616f98
	sub_82616F98(ctx, base);
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bl 0x82617120
	sub_82617120(ctx, base);
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// bge cr6,0x82617578
	if (!cr6.lt) goto loc_82617578;
	// li r6,12
	ctx.r6.s64 = 12;
	// addi r3,r28,8
	ctx.r3.s64 = r28.s64 + 8;
	// bl 0x82617120
	sub_82617120(ctx, base);
loc_82617578:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmpw cr6,r29,r23
	cr6.compare<int32_t>(r29.s32, r23.s32, xer);
	// blt cr6,0x82617534
	if (cr6.lt) goto loc_82617534;
loc_82617584:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8261758C"))) PPC_WEAK_FUNC(sub_8261758C);
PPC_FUNC_IMPL(__imp__sub_8261758C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617590"))) PPC_WEAK_FUNC(sub_82617590);
PPC_FUNC_IMPL(__imp__sub_82617590) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// mr r22,r9
	r22.u64 = ctx.r9.u64;
	// li r25,16
	r25.s64 = 16;
	// li r26,4
	r26.s64 = 4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r24,r11,r28
	r24.u64 = r11.u64 + r28.u64;
	// beq cr6,0x826175d8
	if (cr6.eq) goto loc_826175D8;
	// li r25,20
	r25.s64 = 20;
	// li r26,0
	r26.s64 = 0;
loc_826175D8:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x826175e8
	if (cr6.eq) goto loc_826175E8;
	// addi r25,r25,-4
	r25.s64 = r25.s64 + -4;
	// b 0x82617608
	goto loc_82617608;
loc_826175E8:
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
loc_82617608:
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mullw r11,r26,r31
	r11.s64 = int64_t(r26.s32) * int64_t(r31.s32);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// addi r27,r28,4
	r27.s64 = r28.s64 + 4;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// addi r28,r11,3
	r28.s64 = r11.s64 + 3;
	// addi r26,r23,-1
	r26.s64 = r23.s64 + -1;
	// bne cr6,0x826176d4
	if (!cr6.eq) goto loc_826176D4;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82617744
	if (!cr6.gt) goto loc_82617744;
loc_82617648:
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r28,8
	ctx.r3.s64 = r28.s64 + 8;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// addi r24,r24,16
	r24.s64 = r24.s64 + 16;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x82617648
	if (!cr6.eq) goto loc_82617648;
	// b 0x82617744
	goto loc_82617744;
loc_826176D4:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82617744
	if (!cr6.gt) goto loc_82617744;
loc_826176DC:
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r28,8
	ctx.r3.s64 = r28.s64 + 8;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x826176dc
	if (!cr6.eq) goto loc_826176DC;
loc_82617744:
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// bne cr6,0x82617784
	if (!cr6.eq) goto loc_82617784;
	// lwz r11,15864(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15864);
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82617784:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_826177A8"))) PPC_WEAK_FUNC(sub_826177A8);
PPC_FUNC_IMPL(__imp__sub_826177A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// rlwinm r11,r30,3,0,28
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// li r25,8
	r25.s64 = 8;
	// add r29,r11,r31
	r29.u64 = r11.u64 + r31.u64;
	// li r28,4
	r28.s64 = 4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x826177ec
	if (cr6.eq) goto loc_826177EC;
	// li r25,12
	r25.s64 = 12;
	// li r28,0
	r28.s64 = 0;
loc_826177EC:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x826177fc
	if (cr6.eq) goto loc_826177FC;
	// addi r25,r25,-4
	r25.s64 = r25.s64 + -4;
	// b 0x8261781c
	goto loc_8261781C;
loc_826177FC:
	// lwz r11,15864(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
loc_8261781C:
	// mullw r11,r28,r30
	r11.s64 = int64_t(r28.s32) * int64_t(r30.s32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// addi r31,r11,3
	r31.s64 = r11.s64 + 3;
	// bne cr6,0x826178ac
	if (!cr6.eq) goto loc_826178AC;
	// addi r28,r23,-1
	r28.s64 = r23.s64 + -1;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82617888
	if (!cr6.gt) goto loc_82617888;
loc_8261783C:
	// lwz r11,15864(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15864);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,15868(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8261783c
	if (!cr6.eq) goto loc_8261783C;
loc_82617888:
	// lwz r11,15864(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826178A4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_826178AC:
	// addi r29,r23,-1
	r29.s64 = r23.s64 + -1;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826178a4
	if (!cr6.gt) goto loc_826178A4;
loc_826178B8:
	// lwz r11,15868(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15868);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x826178b8
	if (!cr6.eq) goto loc_826178B8;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826178EC"))) PPC_WEAK_FUNC(sub_826178EC);
PPC_FUNC_IMPL(__imp__sub_826178EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826178F0"))) PPC_WEAK_FUNC(sub_826178F0);
PPC_FUNC_IMPL(__imp__sub_826178F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r3,6
	r11.s64 = ctx.r3.s64 + 6;
	// li r10,8
	ctx.r10.s64 = 8;
loc_82617900:
	// lbz r7,-5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -5);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,-6(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + -6);
	// lbz r6,-4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + -4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,-3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -3);
	// lbz r31,-2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lbz r3,-1(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// bne cr6,0x82617900
	if (!cr6.eq) goto loc_82617900;
	// addi r11,r9,4
	r11.s64 = ctx.r9.s64 + 4;
	// twllei r5,0
	// srawi r10,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r10.s64 = r11.s32 >> 3;
	// rotlwi r11,r10,1
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// divw r3,r10,r5
	ctx.r3.s32 = ctx.r10.s32 / ctx.r5.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r5,r11
	r11.u64 = ctx.r5.u64 & ~r11.u64;
	// twlgei r11,-1
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82617978"))) PPC_WEAK_FUNC(sub_82617978);
PPC_FUNC_IMPL(__imp__sub_82617978) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// lwz r11,-25032(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -25032);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// stw r11,-25032(r10)
	PPC_STORE_U32(ctx.r10.u32 + -25032, r11.u32);
	// beq cr6,0x8261799c
	if (cr6.eq) goto loc_8261799C;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r3,3384(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 3384);
	// blr 
	return;
loc_8261799C:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lis r8,-32126
	ctx.r8.s64 = -2105409536;
	// addi r10,r11,3392
	ctx.r10.s64 = r11.s64 + 3392;
	// li r11,-5120
	r11.s64 = -5120;
	// addi r9,r10,5120
	ctx.r9.s64 = ctx.r10.s64 + 5120;
	// stw r9,3384(r8)
	PPC_STORE_U32(ctx.r8.u32 + 3384, ctx.r9.u32);
	// b 0x826179bc
	goto loc_826179BC;
loc_826179B8:
	// lwz r9,3384(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 3384);
loc_826179BC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826179cc
	if (!cr6.lt) goto loc_826179CC;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826179dc
	goto loc_826179DC;
loc_826179CC:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// li r10,255
	ctx.r10.s64 = 255;
	// bgt cr6,0x826179dc
	if (cr6.gt) goto loc_826179DC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_826179DC:
	// stbx r10,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,5120
	cr6.compare<int32_t>(r11.s32, 5120, xer);
	// blt cr6,0x826179b8
	if (cr6.lt) goto loc_826179B8;
	// lwz r3,3384(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 3384);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826179F4"))) PPC_WEAK_FUNC(sub_826179F4);
PPC_FUNC_IMPL(__imp__sub_826179F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826179F8"))) PPC_WEAK_FUNC(sub_826179F8);
PPC_FUNC_IMPL(__imp__sub_826179F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,248(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 248);
	// li r11,0
	r11.s64 = 0;
	// stw r11,280(r3)
	PPC_STORE_U32(ctx.r3.u32 + 280, r11.u32);
	// stw r11,472(r3)
	PPC_STORE_U32(ctx.r3.u32 + 472, r11.u32);
	// stw r11,3976(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3976, r11.u32);
	// stw r10,3984(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3984, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82617A14"))) PPC_WEAK_FUNC(sub_82617A14);
PPC_FUNC_IMPL(__imp__sub_82617A14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617A18"))) PPC_WEAK_FUNC(sub_82617A18);
PPC_FUNC_IMPL(__imp__sub_82617A18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce0
	// srawi r11,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	r11.s64 = ctx.r6.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82617bb8
	if (!cr6.gt) goto loc_82617BB8;
	// mr r23,r11
	r23.u64 = r11.u64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r24,r11,30096
	r24.s64 = r11.s64 + 30096;
loc_82617A38:
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// li r25,0
	r25.s64 = 0;
loc_82617A40:
	// subf r27,r4,r3
	r27.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lbz r31,0(r3)
	r31.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// subf r10,r4,r27
	ctx.r10.s64 = r27.s64 - ctx.r4.s64;
	// lbz r30,0(r27)
	r30.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// subf r6,r4,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r4.s64;
	// srawi r8,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r8.s64 = r11.s32 >> 1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addze r28,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r28.s64 = temp.s64;
	// lbz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// beq cr6,0x82617b84
	if (cr6.eq) goto loc_82617B84;
	// subf r29,r8,r9
	r29.s64 = ctx.r9.s64 - ctx.r8.s64;
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// rlwinm r26,r29,1,0,30
	r26.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r11,2,0,29
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// subf r11,r11,r26
	r11.s64 = r26.s64 - r11.s64;
	// srawi r26,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r26.s64 = r11.s32 >> 3;
	// mr r11,r26
	r11.u64 = r26.u64;
	// srawi r29,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r11.s32 >> 31;
	// xor r11,r11,r29
	r11.u64 = r11.u64 ^ r29.u64;
	// subf r29,r29,r11
	r29.s64 = r11.s64 - r29.s64;
	// cmpw cr6,r29,r5
	cr6.compare<int32_t>(r29.s32, ctx.r5.s32, xer);
	// bge cr6,0x82617b84
	if (!cr6.lt) goto loc_82617B84;
	// lbz r22,0(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r11,r7,r9
	r11.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lbzx r9,r10,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// subf r10,r8,r22
	ctx.r10.s64 = r22.s64 - ctx.r8.s64;
	// subf r8,r30,r6
	ctx.r8.s64 = ctx.r6.s64 - r30.s64;
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r6
	r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x82617b24
	if (!cr6.lt) goto loc_82617B24;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82617B24:
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// bge cr6,0x82617b84
	if (!cr6.lt) goto loc_82617B84;
	// xor r10,r26,r28
	ctx.r10.u64 = r26.u64 ^ r28.u64;
	// rlwinm r10,r10,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82617b8c
	if (cr6.eq) goto loc_82617B8C;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// srawi r10,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r28.s32 >> 31;
	// xor r9,r28,r10
	ctx.r9.u64 = r28.u64 ^ ctx.r10.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82617b64
	if (cr6.lt) goto loc_82617B64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82617B64:
	// cmpw cr6,r30,r31
	cr6.compare<int32_t>(r30.s32, r31.s32, xer);
	// bge cr6,0x82617b70
	if (!cr6.lt) goto loc_82617B70;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_82617B70:
	// subf r10,r11,r30
	ctx.r10.s64 = r30.s64 - r11.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// stb r10,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r10.u8);
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// b 0x82617b8c
	goto loc_82617B8C;
loc_82617B84:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x82617ba8
	if (cr6.eq) goto loc_82617BA8;
loc_82617B8C:
	// lbzx r11,r25,r24
	r11.u64 = PPC_LOAD_U8(r25.u32 + r24.u32);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// blt cr6,0x82617a40
	if (cr6.lt) goto loc_82617A40;
	// b 0x82617bac
	goto loc_82617BAC;
loc_82617BA8:
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
loc_82617BAC:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x82617a38
	if (!cr6.eq) goto loc_82617A38;
loc_82617BB8:
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82617BBC"))) PPC_WEAK_FUNC(sub_82617BBC);
PPC_FUNC_IMPL(__imp__sub_82617BBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617BC0"))) PPC_WEAK_FUNC(sub_82617BC0);
PPC_FUNC_IMPL(__imp__sub_82617BC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce4
	// srawi r11,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	r11.s64 = ctx.r6.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82617d50
	if (!cr6.gt) goto loc_82617D50;
	// mr r23,r11
	r23.u64 = r11.u64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r24,r4,1,0,30
	r24.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r25,r11,30096
	r25.s64 = r11.s64 + 30096;
loc_82617BE4:
	// add r3,r24,r3
	ctx.r3.u64 = r24.u64 + ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
loc_82617BEC:
	// lbz r31,4(r3)
	r31.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// lbz r30,5(r3)
	r30.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// lbz r11,3(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// subf r9,r30,r31
	ctx.r9.s64 = r31.s64 - r30.s64;
	// lbz r10,6(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 6);
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// addze r27,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r27.s64 = temp.s64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x82617d18
	if (cr6.eq) goto loc_82617D18;
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// srawi r28,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r28.s64 = ctx.r9.s32 >> 3;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r29,r8,r9
	r29.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpw cr6,r29,r5
	cr6.compare<int32_t>(r29.s32, ctx.r5.s32, xer);
	// bge cr6,0x82617d18
	if (!cr6.lt) goto loc_82617D18;
	// lbz r9,2(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// lbz r8,7(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 7);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lbz r9,1(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lbz r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// subf r8,r31,r9
	ctx.r8.s64 = ctx.r9.s64 - r31.s64;
	// subf r9,r7,r30
	ctx.r9.s64 = r30.s64 - ctx.r7.s64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r10,r6
	r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x82617cb8
	if (!cr6.lt) goto loc_82617CB8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82617CB8:
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// bge cr6,0x82617d18
	if (!cr6.lt) goto loc_82617D18;
	// xor r10,r28,r27
	ctx.r10.u64 = r28.u64 ^ r27.u64;
	// rlwinm r10,r10,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82617d20
	if (cr6.eq) goto loc_82617D20;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// srawi r10,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r27.s32 >> 31;
	// xor r9,r27,r10
	ctx.r9.u64 = r27.u64 ^ ctx.r10.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82617cf8
	if (cr6.lt) goto loc_82617CF8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82617CF8:
	// cmpw cr6,r31,r30
	cr6.compare<int32_t>(r31.s32, r30.s32, xer);
	// bge cr6,0x82617d04
	if (!cr6.lt) goto loc_82617D04;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_82617D04:
	// subf r10,r11,r31
	ctx.r10.s64 = r31.s64 - r11.s64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, r11.u8);
	// b 0x82617d20
	goto loc_82617D20;
loc_82617D18:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x82617d40
	if (cr6.eq) goto loc_82617D40;
loc_82617D20:
	// lbzx r11,r26,r25
	r11.u64 = PPC_LOAD_U8(r26.u32 + r25.u32);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// mullw r11,r11,r4
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// blt cr6,0x82617bec
	if (cr6.lt) goto loc_82617BEC;
	// b 0x82617d44
	goto loc_82617D44;
loc_82617D40:
	// add r3,r24,r3
	ctx.r3.u64 = r24.u64 + ctx.r3.u64;
loc_82617D44:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x82617be4
	if (!cr6.eq) goto loc_82617BE4;
loc_82617D50:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82617D54"))) PPC_WEAK_FUNC(sub_82617D54);
PPC_FUNC_IMPL(__imp__sub_82617D54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617D58"))) PPC_WEAK_FUNC(sub_82617D58);
PPC_FUNC_IMPL(__imp__sub_82617D58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r5,24
	r11.u64 = ctx.r5.u32 & 0xFF;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bgt cr6,0x82617f04
	if (cr6.gt) goto loc_82617F04;
	// lis r12,-32159
	r12.s64 = -2107572224;
	// addi r12,r12,32156
	r12.s64 = r12.s64 + 32156;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_82617DD8;
	case 1:
		goto loc_82617E7C;
	case 2:
		goto loc_82617EA4;
	case 3:
		goto loc_82617DE4;
	case 4:
		goto loc_82617DF0;
	case 5:
		goto loc_82617E18;
	case 6:
		goto loc_82617E24;
	case 7:
		goto loc_82617E30;
	case 8:
		goto loc_82617E38;
	case 9:
		goto loc_82617E60;
	case 10:
		goto loc_82617E88;
	case 11:
		goto loc_82617EB0;
	case 12:
		goto loc_82617EB8;
	case 13:
		goto loc_82617EE0;
	case 14:
		goto loc_82617EE8;
	default:
		__builtin_unreachable();
	}
	// lwz r19,32216(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32216);
	// lwz r19,32380(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32380);
	// lwz r19,32420(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32420);
	// lwz r19,32228(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32228);
	// lwz r19,32240(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32240);
	// lwz r19,32280(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32280);
	// lwz r19,32292(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32292);
	// lwz r19,32304(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32304);
	// lwz r19,32312(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32312);
	// lwz r19,32352(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32352);
	// lwz r19,32392(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32392);
	// lwz r19,32432(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32432);
	// lwz r19,32440(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32440);
	// lwz r19,32480(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32480);
	// lwz r19,32488(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32488);
loc_82617DD8:
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617DE4:
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617DF0:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617E18:
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617E24:
	// li r6,12
	ctx.r6.s64 = 12;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617E30:
	// li r6,4
	ctx.r6.s64 = 4;
	// b 0x82617eec
	goto loc_82617EEC;
loc_82617E38:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617E60:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82617E7C:
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617E88:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82617EA4:
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617EB0:
	// li r6,8
	ctx.r6.s64 = 8;
	// b 0x82617eec
	goto loc_82617EEC;
loc_82617EB8:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// b 0x82617ef0
	goto loc_82617EF0;
loc_82617EE0:
	// li r6,12
	ctx.r6.s64 = 12;
	// b 0x82617eec
	goto loc_82617EEC;
loc_82617EE8:
	// li r6,16
	ctx.r6.s64 = 16;
loc_82617EEC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_82617EF0:
	// lwz r11,15864(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15864);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82617F04:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82617F0C"))) PPC_WEAK_FUNC(sub_82617F0C);
PPC_FUNC_IMPL(__imp__sub_82617F0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82617F10"))) PPC_WEAK_FUNC(sub_82617F10);
PPC_FUNC_IMPL(__imp__sub_82617F10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r5,24
	r11.u64 = ctx.r5.u32 & 0xFF;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bgt cr6,0x826181dc
	if (cr6.gt) goto loc_826181DC;
	// lis r12,-32159
	r12.s64 = -2107572224;
	// addi r12,r12,32596
	r12.s64 = r12.s64 + 32596;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_82617F90;
	case 1:
		goto loc_826180F8;
	case 2:
		goto loc_8261813C;
	case 3:
		goto loc_82617FC0;
	case 4:
		goto loc_82617FE8;
	case 5:
		goto loc_82618038;
	case 6:
		goto loc_82618060;
	case 7:
		goto loc_82618088;
	case 8:
		goto loc_82618090;
	case 9:
		goto loc_826180DC;
	case 10:
		goto loc_82618120;
	case 11:
		goto loc_82618164;
	case 12:
		goto loc_8261816C;
	case 13:
		goto loc_826181B8;
	case 14:
		goto loc_826181C0;
	default:
		__builtin_unreachable();
	}
	// lwz r19,32656(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32656);
	// lwz r19,-32520(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32520);
	// lwz r19,-32452(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32452);
	// lwz r19,32704(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32704);
	// lwz r19,32744(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 32744);
	// lwz r19,-32712(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32712);
	// lwz r19,-32672(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32672);
	// lwz r19,-32632(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32632);
	// lwz r19,-32624(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32624);
	// lwz r19,-32548(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32548);
	// lwz r19,-32480(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32480);
	// lwz r19,-32412(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32412);
	// lwz r19,-32404(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32404);
	// lwz r19,-32328(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32328);
	// lwz r19,-32320(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32320);
loc_82617F90:
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82617FC0:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82617FE8:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// li r6,4
	ctx.r6.s64 = 4;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82618038:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82618060:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82618088:
	// li r6,4
	ctx.r6.s64 = 4;
	// b 0x826181c4
	goto loc_826181C4;
loc_82618090:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_826180DC:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826180F8:
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82618120:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8261813C:
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_82618164:
	// li r6,8
	ctx.r6.s64 = 8;
	// b 0x826181c4
	goto loc_826181C4;
loc_8261816C:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,15868(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_826181B8:
	// li r6,12
	ctx.r6.s64 = 12;
	// b 0x826181c4
	goto loc_826181C4;
loc_826181C0:
	// li r6,16
	ctx.r6.s64 = 16;
loc_826181C4:
	// lwz r11,15868(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 15868);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826181DC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826181E4"))) PPC_WEAK_FUNC(sub_826181E4);
PPC_FUNC_IMPL(__imp__sub_826181E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826181E8"))) PPC_WEAK_FUNC(sub_826181E8);
PPC_FUNC_IMPL(__imp__sub_826181E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd4
	// lwz r31,284(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmpwi cr6,r31,2
	cr6.compare<int32_t>(r31.s32, 2, xer);
	// lwz r29,3916(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3916);
	// mullw r31,r11,r5
	r31.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// add r30,r31,r4
	r30.u64 = r31.u64 + ctx.r4.u64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r4
	r31.u64 = r31.u64 + ctx.r4.u64;
	// rlwinm r22,r31,1,0,30
	r22.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r30,1,0,30
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r11,r22
	r20.u64 = r11.u64 + r22.u64;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// subf r21,r11,r22
	r21.s64 = r22.s64 - r11.s64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// lis r31,-32138
	r31.s64 = -2106195968;
	// addi r31,r31,11648
	r31.s64 = r31.s64 + 11648;
	// bne cr6,0x8261825c
	if (!cr6.eq) goto loc_8261825C;
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r29,15
	r29.s64 = 15;
	// li r28,6
	r28.s64 = 6;
	// mtctr r28
	ctr.u64 = r28.u64;
loc_8261824C:
	// stb r29,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r29.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// bdnz 0x8261824c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8261824C;
	// b 0x826182dc
	goto loc_826182DC;
loc_8261825C:
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// subf r26,r7,r10
	r26.s64 = ctx.r10.s64 - ctx.r7.s64;
	// subf r29,r7,r11
	r29.s64 = r11.s64 - ctx.r7.s64;
	// li r27,6
	r27.s64 = 6;
	// li r23,15
	r23.s64 = 15;
	// li r24,-49
	r24.s64 = -49;
	// li r25,63
	r25.s64 = 63;
loc_82618278:
	// lbz r28,0(r30)
	r28.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// extsb r28,r28
	r28.s64 = r28.s8;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82618290
	if (!cr6.eq) goto loc_82618290;
	// stbx r23,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r23.u8);
	// b 0x826182cc
	goto loc_826182CC;
loc_82618290:
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bne cr6,0x826182a0
	if (!cr6.eq) goto loc_826182A0;
	// stbx r24,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r24.u8);
	// b 0x826182cc
	goto loc_826182CC;
loc_826182A0:
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// bne cr6,0x826182b0
	if (!cr6.eq) goto loc_826182B0;
	// stbx r25,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r25.u8);
	// b 0x826182cc
	goto loc_826182CC;
loc_826182B0:
	// cmpwi cr6,r28,4
	cr6.compare<int32_t>(r28.s32, 4, xer);
	// bne cr6,0x826182cc
	if (!cr6.eq) goto loc_826182CC;
	// lbzx r28,r26,r30
	r28.u64 = PPC_LOAD_U8(r26.u32 + r30.u32);
	// addi r19,r31,1024
	r19.s64 = r31.s64 + 1024;
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lwzx r28,r28,r19
	r28.u64 = PPC_LOAD_U32(r28.u32 + r19.u32);
	// stbx r28,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r28.u8);
loc_826182CC:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82618278
	if (!cr6.eq) goto loc_82618278;
loc_826182DC:
	// lwz r23,100(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r19,108(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8261837c
	if (cr6.eq) goto loc_8261837C;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// beq cr6,0x82618348
	if (cr6.eq) goto loc_82618348;
	// rlwinm r30,r30,0,0,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFF0;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r28,4(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// extsb r27,r27
	r27.s64 = r27.s8;
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rlwinm r28,r28,0,0,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r27,r27,0,0,27
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFF0;
	// extsb r30,r30
	r30.s64 = r30.s8;
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r29,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r29.u8);
	// stb r28,4(r11)
	PPC_STORE_U8(r11.u32 + 4, r28.u8);
	// stb r27,5(r11)
	PPC_STORE_U8(r11.u32 + 5, r27.u8);
	// stb r30,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r30.u8);
	// b 0x826183c8
	goto loc_826183C8;
loc_82618348:
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// lbz r29,4(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// rlwinm r29,r29,0,30,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// rlwinm r28,r28,0,30,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r30,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r30.u8);
	// b 0x826183c0
	goto loc_826183C0;
loc_8261837C:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x826183c8
	if (cr6.eq) goto loc_826183C8;
	// lwz r30,3364(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3364);
	// lbzx r29,r30,r11
	r29.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// stbx r29,r30,r11
	PPC_STORE_U8(r30.u32 + r11.u32, r29.u8);
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r29,4(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// rlwinm r30,r30,0,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFC;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r28,r28,0,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFC;
	// stb r30,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r30.u8);
loc_826183C0:
	// stb r28,5(r11)
	PPC_STORE_U8(r11.u32 + 5, r28.u8);
	// stb r29,4(r11)
	PPC_STORE_U8(r11.u32 + 4, r29.u8);
loc_826183C8:
	// lwz r30,284(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x82618a8c
	if (cr6.eq) goto loc_82618A8C;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x826183ec
	if (!cr6.eq) goto loc_826183EC;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618a8c
	if (cr6.eq) goto loc_82618A8C;
loc_826183EC:
	// lwz r25,84(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82618488
	if (!cr6.eq) goto loc_82618488;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r30,r22,1,0,30
	r30.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r30,r6
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + ctx.r6.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618488
	if (cr6.eq) goto loc_82618488;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r29,r21,1,0,30
	r29.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r28,r6,r30
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// lhzx r6,r29,r6
	ctx.r6.u64 = PPC_LOAD_U16(r29.u32 + ctx.r6.u32);
	// cmplw cr6,r6,r28
	cr6.compare<uint32_t>(ctx.r6.u32, r28.u32, xer);
	// bne cr6,0x82618488
	if (!cr6.eq) goto loc_82618488;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// lhzx r29,r6,r29
	r29.u64 = PPC_LOAD_U16(ctx.r6.u32 + r29.u32);
	// lhzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// cmplw cr6,r29,r6
	cr6.compare<uint32_t>(r29.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618488
	if (!cr6.eq) goto loc_82618488;
	// lbz r6,2(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r25)
	r28.u64 = PPC_LOAD_U8(r25.u32 + 2);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,768
	r29.s64 = r31.s64 + 768;
	// addi r28,r31,512
	r28.s64 = r31.s64 + 512;
	// lwzx r30,r30,r29
	r30.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// lwzx r6,r6,r28
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r28.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// and r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & r26.u64;
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
loc_82618488:
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82618524
	if (!cr6.eq) goto loc_82618524;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r6,r22,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r6,r30
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// cmplwi cr6,r30,16384
	cr6.compare<uint32_t>(r30.u32, 16384, xer);
	// beq cr6,0x82618524
	if (cr6.eq) goto loc_82618524;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r30,r6,r30
	r30.u64 = ctx.r6.u64 + r30.u64;
	// lhz r29,-2(r30)
	r29.u64 = PPC_LOAD_U16(r30.u32 + -2);
	// lhz r30,0(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// bne cr6,0x82618524
	if (!cr6.eq) goto loc_82618524;
	// lwz r30,1776(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618524
	if (!cr6.eq) goto loc_82618524;
	// lbz r6,1(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r24)
	r28.u64 = PPC_LOAD_U8(r24.u32 + 1);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,256
	r29.s64 = r31.s64 + 256;
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// lwzx r30,r30,r29
	r30.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// and r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & r26.u64;
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
loc_82618524:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x826185d0
	if (!cr6.eq) goto loc_826185D0;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r6,r22,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r6,r30
	r30.u64 = ctx.r6.u64 + r30.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// cmplwi cr6,r30,16384
	cr6.compare<uint32_t>(r30.u32, 16384, xer);
	// beq cr6,0x826185d0
	if (cr6.eq) goto loc_826185D0;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r29,r21,1,0,30
	r29.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r30,r6
	r28.u64 = r30.u64 + ctx.r6.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lhz r28,2(r28)
	r28.u64 = PPC_LOAD_U16(r28.u32 + 2);
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bne cr6,0x826185d0
	if (!cr6.eq) goto loc_826185D0;
	// lwz r30,1776(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// add r29,r30,r29
	r29.u64 = r30.u64 + r29.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r30,2(r29)
	r30.u64 = PPC_LOAD_U16(r29.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x826185d0
	if (!cr6.eq) goto loc_826185D0;
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r30,3(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 3);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,3(r25)
	r30.u64 = PPC_LOAD_U8(r25.u32 + 3);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,512
	r29.s64 = r31.s64 + 512;
	// addi r28,r31,768
	r28.s64 = r31.s64 + 768;
	// lwzx r30,r30,r29
	r30.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// lwzx r6,r6,r28
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r28.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r6.u8);
loc_826185D0:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r26,r22,1,0,30
	r26.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r26,r6
	ctx.r6.u64 = r26.u64 + ctx.r6.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618664
	if (cr6.eq) goto loc_82618664;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r26,r6
	ctx.r6.u64 = r26.u64 + ctx.r6.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82618664
	if (!cr6.eq) goto loc_82618664;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82618664
	if (!cr6.eq) goto loc_82618664;
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,256
	r29.s64 = r31.s64 + 256;
	// lwzx r30,r30,r31
	r30.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// lwzx r6,r6,r29
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r29.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r6.u8);
loc_82618664:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r27,r20,1,0,30
	r27.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r27,r6
	ctx.r6.u64 = PPC_LOAD_U16(r27.u32 + ctx.r6.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x826186f0
	if (cr6.eq) goto loc_826186F0;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// lhzx r30,r6,r26
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + r26.u32);
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x826186f0
	if (!cr6.eq) goto loc_826186F0;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// lhzx r30,r6,r26
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + r26.u32);
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x826186f0
	if (!cr6.eq) goto loc_826186F0;
	// lbz r6,2(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r22,r30
	r22.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r22,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,512
	r29.s64 = r31.s64 + 512;
	// addi r28,r31,768
	r28.s64 = r31.s64 + 768;
	// lwzx r30,r30,r29
	r30.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// lwzx r6,r6,r28
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r28.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r6.u8);
loc_826186F0:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82618784
	if (!cr6.eq) goto loc_82618784;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// lhzx r6,r27,r6
	ctx.r6.u64 = PPC_LOAD_U16(r27.u32 + ctx.r6.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618784
	if (cr6.eq) goto loc_82618784;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r27,r6
	ctx.r6.u64 = r27.u64 + ctx.r6.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618784
	if (!cr6.eq) goto loc_82618784;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618784
	if (!cr6.eq) goto loc_82618784;
	// lbz r6,2(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// lbz r30,3(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r22,r30
	r22.s64 = r30.s8;
	// lbz r30,3(r24)
	r30.u64 = PPC_LOAD_U8(r24.u32 + 3);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r22,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,256
	r29.s64 = r31.s64 + 256;
	// lwzx r30,r30,r31
	r30.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// lwzx r6,r6,r29
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r29.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r6.u8);
loc_82618784:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r27,r6
	ctx.r6.u64 = r27.u64 + ctx.r6.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618820
	if (cr6.eq) goto loc_82618820;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r30,r6,r26
	r30.u64 = ctx.r6.u64 + r26.u64;
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618820
	if (!cr6.eq) goto loc_82618820;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r30,r6,r26
	r30.u64 = ctx.r6.u64 + r26.u64;
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618820
	if (!cr6.eq) goto loc_82618820;
	// lbz r6,3(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// lbz r30,1(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r26,r30
	r26.s64 = r30.s8;
	// lbz r30,1(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r26,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,512
	r29.s64 = r31.s64 + 512;
	// addi r28,r31,768
	r28.s64 = r31.s64 + 768;
	// lwzx r30,r30,r29
	r30.u64 = PPC_LOAD_U32(r30.u32 + r29.u32);
	// lwzx r6,r6,r28
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r28.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r6.u8);
loc_82618820:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r27,r6
	ctx.r6.u64 = r27.u64 + ctx.r6.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x826188b0
	if (cr6.eq) goto loc_826188B0;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r27,r6
	ctx.r6.u64 = r27.u64 + ctx.r6.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x826188b0
	if (!cr6.eq) goto loc_826188B0;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x826188b0
	if (!cr6.eq) goto loc_826188B0;
	// lbz r6,3(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// lbz r30,1(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,1(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r31,256
	r29.s64 = r31.s64 + 256;
	// lwzx r30,r30,r31
	r30.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// lwzx r6,r6,r29
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r29.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r6.u8);
loc_826188B0:
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// mullw r5,r6,r5
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// add r29,r5,r4
	r29.u64 = ctx.r5.u64 + ctx.r4.u64;
	// bne cr6,0x826189a8
	if (!cr6.eq) goto loc_826189A8;
	// lwz r5,1780(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r4,r29,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r5,r4,r5
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r4.u32 + ctx.r5.u32);
	// cmplwi cr6,r5,16384
	cr6.compare<uint32_t>(ctx.r5.u32, 16384, xer);
	// beq cr6,0x826189a8
	if (cr6.eq) goto loc_826189A8;
	// subf r5,r6,r29
	ctx.r5.s64 = r29.s64 - ctx.r6.s64;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r6,r4
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r4.u32);
	// lhzx r6,r6,r5
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r5.u32);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x826189a8
	if (!cr6.eq) goto loc_826189A8;
	// lwz r6,1784(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1784);
	// lhzx r5,r6,r5
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r5.u32);
	// lhzx r6,r6,r4
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r4.u32);
	// cmplw cr6,r5,r6
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// bne cr6,0x826189a8
	if (!cr6.eq) goto loc_826189A8;
	// lbz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lbz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r28,r5
	r28.s64 = ctx.r5.s8;
	// lbz r5,4(r25)
	ctx.r5.u64 = PPC_LOAD_U8(r25.u32 + 4);
	// rlwinm r4,r6,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rlwinm r6,r28,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r31,512
	ctx.r4.s64 = r31.s64 + 512;
	// addi r30,r31,768
	r30.s64 = r31.s64 + 768;
	// addi r28,r31,768
	r28.s64 = r31.s64 + 768;
	// lwzx r5,r5,r4
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// addi r30,r31,512
	r30.s64 = r31.s64 + 512;
	// or r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 | ctx.r6.u64;
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// and r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 & ctx.r5.u64;
	// stb r6,4(r11)
	PPC_STORE_U8(r11.u32 + 4, ctx.r6.u8);
	// lbz r8,5(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// lbz r6,5(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r4,5(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r5,r6
	ctx.r5.s64 = ctx.r6.s8;
	// lbz r6,5(r25)
	ctx.r6.u64 = PPC_LOAD_U8(r25.u32 + 5);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r5,r5,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r28
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r28.u32);
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 & r27.u64;
	// stb r8,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r8.u8);
loc_826189A8:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82618a8c
	if (!cr6.eq) goto loc_82618A8C;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r8,r29,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r8,r6
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r6.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618a8c
	if (cr6.eq) goto loc_82618A8C;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lhz r5,-2(r6)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r5,r6
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618a8c
	if (!cr6.eq) goto loc_82618A8C;
	// lwz r6,1784(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1784);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lhz r6,-2(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// lhz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// cmplw cr6,r6,r8
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r8.u32, xer);
	// bne cr6,0x82618a8c
	if (!cr6.eq) goto loc_82618A8C;
	// lbz r8,4(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lbz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r3,r6
	ctx.r3.s64 = ctx.r6.s8;
	// lbz r6,4(r24)
	ctx.r6.u64 = PPC_LOAD_U8(r24.u32 + 4);
	// rlwinm r5,r8,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r3,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r4,5(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r31,256
	ctx.r5.s64 = r31.s64 + 256;
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// lwzx r8,r8,r5
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r5.u32);
	// addi r5,r31,256
	ctx.r5.s64 = r31.s64 + 256;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// and r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 & ctx.r6.u64;
	// stb r8,4(r11)
	PPC_STORE_U8(r11.u32 + 4, ctx.r8.u8);
	// lbz r9,5(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// lbz r8,5(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// lbz r7,5(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r10,r9
	ctx.r10.s64 = ctx.r9.s8;
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r9,5(r24)
	ctx.r9.u64 = PPC_LOAD_U8(r24.u32 + 5);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r5
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// and r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 & ctx.r4.u64;
	// stb r10,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r10.u8);
loc_82618A8C:
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_82618A90"))) PPC_WEAK_FUNC(sub_82618A90);
PPC_FUNC_IMPL(__imp__sub_82618A90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd4
	// lwz r11,19976(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// lwz r29,3916(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3916);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mullw r31,r11,r5
	r31.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// add r30,r31,r4
	r30.u64 = r31.u64 + ctx.r4.u64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r4
	r31.u64 = r31.u64 + ctx.r4.u64;
	// rlwinm r22,r31,1,0,30
	r22.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r30,1,0,30
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r11,r22
	r20.u64 = r11.u64 + r22.u64;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// subf r21,r11,r22
	r21.s64 = r22.s64 - r11.s64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
	// lis r31,-32138
	r31.s64 = -2106195968;
	// addi r31,r31,13056
	r31.s64 = r31.s64 + 13056;
	// bne cr6,0x82618b10
	if (!cr6.eq) goto loc_82618B10;
	// lwz r30,284(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x82618b10
	if (!cr6.eq) goto loc_82618B10;
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r29,15
	r29.s64 = 15;
	// li r28,6
	r28.s64 = 6;
	// mtctr r28
	ctr.u64 = r28.u64;
loc_82618B00:
	// stb r29,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r29.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// bdnz 0x82618b00
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82618B00;
	// b 0x82618b90
	goto loc_82618B90;
loc_82618B10:
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// subf r26,r7,r10
	r26.s64 = ctx.r10.s64 - ctx.r7.s64;
	// subf r29,r7,r11
	r29.s64 = r11.s64 - ctx.r7.s64;
	// li r27,6
	r27.s64 = 6;
	// li r23,15
	r23.s64 = 15;
	// li r24,-49
	r24.s64 = -49;
	// li r25,63
	r25.s64 = 63;
loc_82618B2C:
	// lbz r28,0(r30)
	r28.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// extsb r28,r28
	r28.s64 = r28.s8;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82618b44
	if (!cr6.eq) goto loc_82618B44;
	// stbx r23,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r23.u8);
	// b 0x82618b80
	goto loc_82618B80;
loc_82618B44:
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bne cr6,0x82618b54
	if (!cr6.eq) goto loc_82618B54;
	// stbx r24,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r24.u8);
	// b 0x82618b80
	goto loc_82618B80;
loc_82618B54:
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// bne cr6,0x82618b64
	if (!cr6.eq) goto loc_82618B64;
	// stbx r25,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r25.u8);
	// b 0x82618b80
	goto loc_82618B80;
loc_82618B64:
	// cmpwi cr6,r28,4
	cr6.compare<int32_t>(r28.s32, 4, xer);
	// bne cr6,0x82618b80
	if (!cr6.eq) goto loc_82618B80;
	// lbzx r28,r26,r30
	r28.u64 = PPC_LOAD_U8(r26.u32 + r30.u32);
	// addi r19,r31,960
	r19.s64 = r31.s64 + 960;
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lwzx r28,r28,r19
	r28.u64 = PPC_LOAD_U32(r28.u32 + r19.u32);
	// stbx r28,r29,r30
	PPC_STORE_U8(r29.u32 + r30.u32, r28.u8);
loc_82618B80:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82618b2c
	if (!cr6.eq) goto loc_82618B2C;
loc_82618B90:
	// lwz r23,100(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r19,108(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x82618c30
	if (cr6.eq) goto loc_82618C30;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// beq cr6,0x82618bfc
	if (cr6.eq) goto loc_82618BFC;
	// rlwinm r30,r30,0,0,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFF0;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r28,4(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// extsb r27,r27
	r27.s64 = r27.s8;
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rlwinm r28,r28,0,0,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r27,r27,0,0,27
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFF0;
	// extsb r30,r30
	r30.s64 = r30.s8;
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r29,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r29.u8);
	// stb r28,4(r11)
	PPC_STORE_U8(r11.u32 + 4, r28.u8);
	// stb r27,5(r11)
	PPC_STORE_U8(r11.u32 + 5, r27.u8);
	// stb r30,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r30.u8);
	// b 0x82618c7c
	goto loc_82618C7C;
loc_82618BFC:
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// lbz r29,4(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// rlwinm r29,r29,0,30,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// rlwinm r28,r28,0,30,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// rlwinm r30,r30,0,30,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// stb r30,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r30.u8);
	// b 0x82618c74
	goto loc_82618C74;
loc_82618C30:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x82618c7c
	if (cr6.eq) goto loc_82618C7C;
	// lwz r30,3364(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3364);
	// lbzx r29,r30,r11
	r29.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// stbx r29,r30,r11
	PPC_STORE_U8(r30.u32 + r11.u32, r29.u8);
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r29,4(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsb r29,r29
	r29.s64 = r29.s8;
	// rlwinm r30,r30,0,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFC;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// rlwinm r29,r29,0,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r28,r28,0,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xFFFFFFFC;
	// stb r30,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r30.u8);
loc_82618C74:
	// stb r28,5(r11)
	PPC_STORE_U8(r11.u32 + 5, r28.u8);
	// stb r29,4(r11)
	PPC_STORE_U8(r11.u32 + 4, r29.u8);
loc_82618C7C:
	// lwz r30,284(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// beq cr6,0x82619330
	if (cr6.eq) goto loc_82619330;
	// cmplwi cr6,r6,4
	cr6.compare<uint32_t>(ctx.r6.u32, 4, xer);
	// beq cr6,0x82619330
	if (cr6.eq) goto loc_82619330;
	// lwz r25,84(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82618d2c
	if (!cr6.eq) goto loc_82618D2C;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r30,r22,1,0,30
	r30.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618d2c
	if (cr6.eq) goto loc_82618D2C;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r29,r21,1,0,30
	r29.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r28,r6,r30
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// lhzx r6,r29,r6
	ctx.r6.u64 = PPC_LOAD_U16(r29.u32 + ctx.r6.u32);
	// cmplw cr6,r6,r28
	cr6.compare<uint32_t>(ctx.r6.u32, r28.u32, xer);
	// bne cr6,0x82618d2c
	if (!cr6.eq) goto loc_82618D2C;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// lhzx r29,r6,r29
	r29.u64 = PPC_LOAD_U16(ctx.r6.u32 + r29.u32);
	// lhzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r30.u32);
	// cmplw cr6,r29,r6
	cr6.compare<uint32_t>(r29.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618d2c
	if (!cr6.eq) goto loc_82618D2C;
	// lbz r6,2(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// addi r27,r31,640
	r27.s64 = r31.s64 + 640;
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r25)
	r28.u64 = PPC_LOAD_U8(r25.u32 + 2);
	// extsb r26,r30
	r26.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r24,0(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r6,r26,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// addi r30,r31,320
	r30.s64 = r31.s64 + 320;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// lwzx r29,r29,r27
	r29.u64 = PPC_LOAD_U32(r29.u32 + r27.u32);
	// or r6,r29,r6
	ctx.r6.u64 = r29.u64 | ctx.r6.u64;
	// and r6,r6,r24
	ctx.r6.u64 = ctx.r6.u64 & r24.u64;
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
loc_82618D2C:
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82618dc8
	if (!cr6.eq) goto loc_82618DC8;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r6,r22,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r30,r6
	r30.u64 = PPC_LOAD_U16(r30.u32 + ctx.r6.u32);
	// cmplwi cr6,r30,16384
	cr6.compare<uint32_t>(r30.u32, 16384, xer);
	// beq cr6,0x82618dc8
	if (cr6.eq) goto loc_82618DC8;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r30,r30,r6
	r30.u64 = r30.u64 + ctx.r6.u64;
	// lhz r29,-2(r30)
	r29.u64 = PPC_LOAD_U16(r30.u32 + -2);
	// lhz r30,0(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// bne cr6,0x82618dc8
	if (!cr6.eq) goto loc_82618DC8;
	// lwz r30,1776(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618dc8
	if (!cr6.eq) goto loc_82618DC8;
	// lbz r6,1(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r24)
	r28.u64 = PPC_LOAD_U8(r24.u32 + 1);
	// extsb r27,r30
	r27.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// addi r30,r31,-320
	r30.s64 = r31.s64 + -320;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// lwzx r29,r29,r31
	r29.u64 = PPC_LOAD_U32(r29.u32 + r31.u32);
	// or r6,r29,r6
	ctx.r6.u64 = r29.u64 | ctx.r6.u64;
	// and r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & r26.u64;
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
loc_82618DC8:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82618e74
	if (!cr6.eq) goto loc_82618E74;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r6,r22,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r30,r6
	r30.u64 = r30.u64 + ctx.r6.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// cmplwi cr6,r30,16384
	cr6.compare<uint32_t>(r30.u32, 16384, xer);
	// beq cr6,0x82618e74
	if (cr6.eq) goto loc_82618E74;
	// lwz r30,1772(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r29,r21,1,0,30
	r29.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r30,r6
	r28.u64 = r30.u64 + ctx.r6.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lhz r28,2(r28)
	r28.u64 = PPC_LOAD_U16(r28.u32 + 2);
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bne cr6,0x82618e74
	if (!cr6.eq) goto loc_82618E74;
	// lwz r30,1776(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// add r29,r30,r29
	r29.u64 = r30.u64 + r29.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r30,2(r29)
	r30.u64 = PPC_LOAD_U16(r29.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618e74
	if (!cr6.eq) goto loc_82618E74;
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r27,r31,320
	r27.s64 = r31.s64 + 320;
	// lbz r30,3(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 3);
	// addi r26,r31,640
	r26.s64 = r31.s64 + 640;
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r21,r30
	r21.s64 = r30.s8;
	// lbz r30,3(r25)
	r30.u64 = PPC_LOAD_U8(r25.u32 + 3);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r21,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r27
	r30.u64 = PPC_LOAD_U32(r30.u32 + r27.u32);
	// lwzx r6,r6,r26
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r26.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r6.u8);
loc_82618E74:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r26,r22,1,0,30
	r26.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618f08
	if (cr6.eq) goto loc_82618F08;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82618f08
	if (!cr6.eq) goto loc_82618F08;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + r26.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82618f08
	if (!cr6.eq) goto loc_82618F08;
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r27,r31,-320
	r27.s64 = r31.s64 + -320;
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r22,r30
	r22.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r22,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r27
	r30.u64 = PPC_LOAD_U32(r30.u32 + r27.u32);
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,1(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r6.u8);
loc_82618F08:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r27,r20,1,0,30
	r27.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82618f94
	if (cr6.eq) goto loc_82618F94;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// lhzx r30,r6,r26
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + r26.u32);
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618f94
	if (!cr6.eq) goto loc_82618F94;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// lhzx r30,r6,r26
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + r26.u32);
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82618f94
	if (!cr6.eq) goto loc_82618F94;
	// lbz r6,2(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// addi r22,r31,320
	r22.s64 = r31.s64 + 320;
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// addi r21,r31,640
	r21.s64 = r31.s64 + 640;
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r20,r30
	r20.s64 = r30.s8;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r20,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r22
	r30.u64 = PPC_LOAD_U32(r30.u32 + r22.u32);
	// lwzx r6,r6,r21
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r21.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r6.u8);
loc_82618F94:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82619028
	if (!cr6.eq) goto loc_82619028;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// lhzx r6,r6,r27
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r27.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82619028
	if (cr6.eq) goto loc_82619028;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82619028
	if (!cr6.eq) goto loc_82619028;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,-2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x82619028
	if (!cr6.eq) goto loc_82619028;
	// lbz r6,2(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// addi r22,r31,-320
	r22.s64 = r31.s64 + -320;
	// lbz r30,3(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r21,r30
	r21.s64 = r30.s8;
	// lbz r30,3(r24)
	r30.u64 = PPC_LOAD_U8(r24.u32 + 3);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r21,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r22
	r30.u64 = PPC_LOAD_U32(r30.u32 + r22.u32);
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r6.u8);
loc_82619028:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x826190c4
	if (cr6.eq) goto loc_826190C4;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r30,r6,r26
	r30.u64 = ctx.r6.u64 + r26.u64;
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x826190c4
	if (!cr6.eq) goto loc_826190C4;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r30,r6,r26
	r30.u64 = ctx.r6.u64 + r26.u64;
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r30)
	r30.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// bne cr6,0x826190c4
	if (!cr6.eq) goto loc_826190C4;
	// lbz r6,3(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// addi r26,r31,320
	r26.s64 = r31.s64 + 320;
	// lbz r30,1(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r22,r31,640
	r22.s64 = r31.s64 + 640;
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r21,r30
	r21.s64 = r30.s8;
	// lbz r30,1(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r21,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r26
	r30.u64 = PPC_LOAD_U32(r30.u32 + r26.u32);
	// lwzx r6,r6,r22
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r22.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r6.u8);
loc_826190C4:
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r6,2(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82619154
	if (cr6.eq) goto loc_82619154;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82619154
	if (!cr6.eq) goto loc_82619154;
	// lwz r6,1776(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lhz r30,2(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x82619154
	if (!cr6.eq) goto loc_82619154;
	// lbz r6,3(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// addi r27,r31,-320
	r27.s64 = r31.s64 + -320;
	// lbz r30,2(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r26,r30
	r26.s64 = r30.s8;
	// lbz r30,2(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r29,r6,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r26,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r30,r27
	r30.u64 = PPC_LOAD_U32(r30.u32 + r27.u32);
	// lwzx r6,r6,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// or r6,r30,r6
	ctx.r6.u64 = r30.u64 | ctx.r6.u64;
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// and r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 & r30.u64;
	// stb r6,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r6.u8);
loc_82619154:
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// mullw r5,r6,r5
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// add r29,r5,r4
	r29.u64 = ctx.r5.u64 + ctx.r4.u64;
	// bne cr6,0x8261924c
	if (!cr6.eq) goto loc_8261924C;
	// lwz r5,1780(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r4,r29,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r5,r4,r5
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r4.u32 + ctx.r5.u32);
	// cmplwi cr6,r5,16384
	cr6.compare<uint32_t>(ctx.r5.u32, 16384, xer);
	// beq cr6,0x8261924c
	if (cr6.eq) goto loc_8261924C;
	// subf r5,r6,r29
	ctx.r5.s64 = r29.s64 - ctx.r6.s64;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r6,r4
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r4.u32);
	// lhzx r6,r6,r5
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r5.u32);
	// cmplw cr6,r6,r30
	cr6.compare<uint32_t>(ctx.r6.u32, r30.u32, xer);
	// bne cr6,0x8261924c
	if (!cr6.eq) goto loc_8261924C;
	// lwz r6,1784(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1784);
	// lhzx r5,r6,r5
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r5.u32);
	// lhzx r6,r6,r4
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r4.u32);
	// cmplw cr6,r5,r6
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// bne cr6,0x8261924c
	if (!cr6.eq) goto loc_8261924C;
	// lbz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// addi r28,r31,320
	r28.s64 = r31.s64 + 320;
	// lbz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r27,r5
	r27.s64 = ctx.r5.s8;
	// lbz r5,4(r25)
	ctx.r5.u64 = PPC_LOAD_U8(r25.u32 + 4);
	// rlwinm r4,r6,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r6,r27,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addi r5,r31,640
	ctx.r5.s64 = r31.s64 + 640;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r31,320
	r30.s64 = r31.s64 + 320;
	// addi r27,r31,640
	r27.s64 = r31.s64 + 640;
	// lwzx r6,r6,r5
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r5.u32);
	// lwzx r4,r4,r28
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r28.u32);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// or r6,r4,r6
	ctx.r6.u64 = ctx.r4.u64 | ctx.r6.u64;
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// and r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 & ctx.r5.u64;
	// stb r6,4(r11)
	PPC_STORE_U8(r11.u32 + 4, ctx.r6.u8);
	// lbz r8,5(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// lbz r6,5(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r4,5(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r5,r6
	ctx.r5.s64 = ctx.r6.s8;
	// lbz r6,5(r25)
	ctx.r6.u64 = PPC_LOAD_U8(r25.u32 + 5);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r5,r5,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r27
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r27.u32);
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 & r28.u64;
	// stb r8,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r8.u8);
loc_8261924C:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x82619330
	if (!cr6.eq) goto loc_82619330;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// rlwinm r8,r29,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r6,r8,r6
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r6.u32);
	// cmplwi cr6,r6,16384
	cr6.compare<uint32_t>(ctx.r6.u32, 16384, xer);
	// beq cr6,0x82619330
	if (cr6.eq) goto loc_82619330;
	// lwz r6,1780(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lhz r5,-2(r6)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// lhz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// cmplw cr6,r5,r6
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// bne cr6,0x82619330
	if (!cr6.eq) goto loc_82619330;
	// lwz r6,1784(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1784);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lhz r6,-2(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// lhz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// cmplw cr6,r6,r8
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r8.u32, xer);
	// bne cr6,0x82619330
	if (!cr6.eq) goto loc_82619330;
	// lbz r8,4(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// addi r3,r31,-320
	ctx.r3.s64 = r31.s64 + -320;
	// lbz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r30,r6
	r30.s64 = ctx.r6.s8;
	// lbz r6,4(r24)
	ctx.r6.u64 = PPC_LOAD_U8(r24.u32 + 4);
	// rlwinm r5,r8,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r30,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r4,5(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r31,-320
	ctx.r5.s64 = r31.s64 + -320;
	// lwzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
	// lwzx r8,r8,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// and r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 & ctx.r6.u64;
	// stb r8,4(r11)
	PPC_STORE_U8(r11.u32 + 4, ctx.r8.u8);
	// lbz r9,5(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// lbz r8,5(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// lbz r7,5(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r10,r9
	ctx.r10.s64 = ctx.r9.s8;
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// lbz r9,5(r24)
	ctx.r9.u64 = PPC_LOAD_U8(r24.u32 + 5);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// lwzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// and r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 & ctx.r4.u64;
	// stb r10,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r10.u8);
loc_82619330:
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_82619334"))) PPC_WEAK_FUNC(sub_82619334);
PPC_FUNC_IMPL(__imp__sub_82619334) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82619338"))) PPC_WEAK_FUNC(sub_82619338);
PPC_FUNC_IMPL(__imp__sub_82619338) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd4
	// lis r27,-32126
	r27.s64 = -2105409536;
	// lwz r21,92(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r11,r3,r6
	r11.u64 = ctx.r3.u64 + ctx.r6.u64;
	// lwz r19,84(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// neg r3,r9
	ctx.r3.s64 = -ctx.r9.s64;
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// clrlwi r31,r3,28
	r31.u64 = ctx.r3.u32 & 0xF;
	// lwz r10,13632(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 13632);
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r24,r10,r11
	r24.s64 = r11.s64 - ctx.r10.s64;
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// srawi r20,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	r20.s64 = ctx.r6.s32 >> 3;
	// addi r3,r9,-1
	ctx.r3.s64 = ctx.r9.s64 + -1;
	// subfic r9,r21,0
	xer.ca = r21.u32 <= 0;
	ctx.r9.s64 = 0 - r21.s64;
	// subf r26,r10,r6
	r26.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// andi. r9,r9,20
	ctx.r9.u64 = ctx.r9.u64 & 20;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r22,r9,20
	r22.s64 = ctx.r9.s64 + 20;
	// bge cr6,0x82619444
	if (!cr6.lt) goto loc_82619444;
	// subf r25,r24,r11
	r25.s64 = r11.s64 - r24.s64;
	// subf r23,r4,r5
	r23.s64 = ctx.r5.s64 - ctx.r4.s64;
loc_826193A0:
	// lbzx r9,r25,r28
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + r28.u32);
	// add r11,r26,r28
	r11.u64 = r26.u64 + r28.u64;
	// lbz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// rldicr r30,r9,8,63
	r30.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// rldicr r29,r6,8,63
	r29.u64 = __builtin_rotateleft64(ctx.r6.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// or r9,r30,r9
	ctx.r9.u64 = r30.u64 | ctx.r9.u64;
	// or r6,r29,r6
	ctx.r6.u64 = r29.u64 | ctx.r6.u64;
	// rldicr r30,r9,16,47
	r30.u64 = __builtin_rotateleft64(ctx.r9.u64, 16) & 0xFFFFFFFFFFFF0000;
	// rldicr r29,r6,16,47
	r29.u64 = __builtin_rotateleft64(ctx.r6.u64, 16) & 0xFFFFFFFFFFFF0000;
	// or r9,r30,r9
	ctx.r9.u64 = r30.u64 | ctx.r9.u64;
	// or r6,r29,r6
	ctx.r6.u64 = r29.u64 | ctx.r6.u64;
	// rldicr r30,r9,32,31
	r30.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r29,r6,32,31
	r29.u64 = __builtin_rotateleft64(ctx.r6.u64, 32) & 0xFFFFFFFF00000000;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// or r30,r30,r9
	r30.u64 = r30.u64 | ctx.r9.u64;
	// or r29,r29,r6
	r29.u64 = r29.u64 | ctx.r6.u64;
	// ble cr6,0x82619404
	if (!cr6.gt) goto loc_82619404;
	// addi r10,r3,1
	ctx.r10.s64 = ctx.r3.s64 + 1;
loc_826193EC:
	// lbz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// stbx r9,r10,r4
	PPC_STORE_U8(ctx.r10.u32 + ctx.r4.u32, ctx.r9.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// blt cr6,0x826193ec
	if (cr6.lt) goto loc_826193EC;
	// lwz r10,13632(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 13632);
loc_82619404:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82619430
	if (!cr6.gt) goto loc_82619430;
	// subf r6,r11,r28
	ctx.r6.s64 = r28.s64 - r11.s64;
loc_82619414:
	// stdx r30,r6,r11
	PPC_STORE_U64(ctx.r6.u32 + r11.u32, r30.u64);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// std r29,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r29.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r10,13632(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 13632);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x82619414
	if (cr6.lt) goto loc_82619414;
loc_82619430:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// add r28,r28,r19
	r28.u64 = r28.u64 + r19.u64;
	// add r3,r3,r19
	ctx.r3.u64 = ctx.r3.u64 + r19.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x826193a0
	if (!cr6.eq) goto loc_826193A0;
loc_82619444:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8261949c
	if (cr6.eq) goto loc_8261949C;
	// mullw r11,r22,r19
	r11.s64 = int64_t(r22.s32) * int64_t(r19.s32);
	// subf r11,r11,r24
	r11.s64 = r24.s64 - r11.s64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// ble cr6,0x8261949c
	if (!cr6.gt) goto loc_8261949C;
	// subf r9,r24,r11
	ctx.r9.s64 = r11.s64 - r24.s64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
loc_82619464:
	// mr r11,r24
	r11.u64 = r24.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x8261948c
	if (!cr6.gt) goto loc_8261948C;
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
loc_82619474:
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stdx r6,r9,r11
	PPC_STORE_U64(ctx.r9.u32 + r11.u32, ctx.r6.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82619474
	if (!cr6.eq) goto loc_82619474;
loc_8261948C:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 + r19.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82619464
	if (!cr6.eq) goto loc_82619464;
loc_8261949C:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8261950c
	if (cr6.eq) goto loc_8261950C;
	// subf r7,r19,r28
	ctx.r7.s64 = r28.s64 - r19.s64;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// neg r11,r5
	r11.s64 = -ctx.r5.s64;
	// beq cr6,0x826194bc
	if (cr6.eq) goto loc_826194BC;
	// clrlwi r11,r11,27
	r11.u64 = r11.u32 & 0x1F;
	// b 0x826194c0
	goto loc_826194C0;
loc_826194BC:
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
loc_826194C0:
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261950c
	if (!cr6.gt) goto loc_8261950C;
	// subf r9,r7,r28
	ctx.r9.s64 = r28.s64 - ctx.r7.s64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_826194D4:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x826194fc
	if (!cr6.gt) goto loc_826194FC;
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
loc_826194E4:
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stdx r6,r9,r11
	PPC_STORE_U64(ctx.r9.u32 + r11.u32, ctx.r6.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x826194e4
	if (!cr6.eq) goto loc_826194E4;
loc_826194FC:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// add r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 + r19.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826194d4
	if (!cr6.eq) goto loc_826194D4;
loc_8261950C:
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_82619510"))) PPC_WEAK_FUNC(sub_82619510);
PPC_FUNC_IMPL(__imp__sub_82619510) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// lis r21,-32126
	r21.s64 = -2105409536;
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// add r31,r3,r7
	r31.u64 = ctx.r3.u64 + ctx.r7.u64;
	// lwz r14,92(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r11,r4,r7
	r11.u64 = ctx.r4.u64 + ctx.r7.u64;
	// neg r9,r10
	ctx.r9.s64 = -ctx.r10.s64;
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// lwz r7,3380(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 3380);
	// clrlwi r22,r9,29
	r22.u64 = ctx.r9.u32 & 0x7;
	// rlwinm r30,r7,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r16,r7,r31
	r16.s64 = r31.s64 - ctx.r7.s64;
	// add r29,r30,r22
	r29.u64 = r30.u64 + r22.u64;
	// add r30,r31,r10
	r30.u64 = r31.u64 + ctx.r10.u64;
	// add r31,r29,r10
	r31.u64 = r29.u64 + ctx.r10.u64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r29,r30,-1
	r29.s64 = r30.s64 + -1;
	// addi r30,r10,-1
	r30.s64 = ctx.r10.s64 + -1;
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// srawi r9,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r9.s64 = r31.s32 >> 2;
	// subfic r10,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// subf r17,r7,r11
	r17.s64 = r11.s64 - ctx.r7.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// mr r15,r16
	r15.u64 = r16.u64;
	// subf r20,r7,r31
	r20.s64 = r31.s64 - ctx.r7.s64;
	// stw r9,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r9.u32);
	// mr r23,r17
	r23.u64 = r17.u64;
	// andi. r10,r10,10
	ctx.r10.u64 = ctx.r10.u64 & 10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r28,r10,10
	r28.s64 = ctx.r10.s64 + 10;
	// stw r28,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r28.u32);
	// bge cr6,0x826196b8
	if (!cr6.lt) goto loc_826196B8;
	// mr r31,r11
	r31.u64 = r11.u64;
	// subf r19,r4,r3
	r19.s64 = ctx.r3.s64 - ctx.r4.s64;
	// subf r18,r17,r16
	r18.s64 = r16.s64 - r17.s64;
	// subf r9,r5,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r5.s64;
loc_826195A0:
	// lbzx r11,r19,r31
	r11.u64 = PPC_LOAD_U8(r19.u32 + r31.u32);
	// add r3,r20,r23
	ctx.r3.u64 = r20.u64 + r23.u64;
	// lbz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// lbz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// rldicr r28,r11,8,63
	r28.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// rldicr r27,r10,8,63
	r27.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// rldicr r26,r5,8,63
	r26.u64 = __builtin_rotateleft64(ctx.r5.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// rldicr r25,r4,8,63
	r25.u64 = __builtin_rotateleft64(ctx.r4.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r28,r11
	r11.u64 = r28.u64 | r11.u64;
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// or r5,r26,r5
	ctx.r5.u64 = r26.u64 | ctx.r5.u64;
	// or r4,r25,r4
	ctx.r4.u64 = r25.u64 | ctx.r4.u64;
	// rldicr r28,r11,16,47
	r28.u64 = __builtin_rotateleft64(r11.u64, 16) & 0xFFFFFFFFFFFF0000;
	// rldicr r27,r10,16,47
	r27.u64 = __builtin_rotateleft64(ctx.r10.u64, 16) & 0xFFFFFFFFFFFF0000;
	// rldicr r26,r5,16,47
	r26.u64 = __builtin_rotateleft64(ctx.r5.u64, 16) & 0xFFFFFFFFFFFF0000;
	// rldicr r25,r4,16,47
	r25.u64 = __builtin_rotateleft64(ctx.r4.u64, 16) & 0xFFFFFFFFFFFF0000;
	// or r11,r28,r11
	r11.u64 = r28.u64 | r11.u64;
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// or r5,r26,r5
	ctx.r5.u64 = r26.u64 | ctx.r5.u64;
	// or r4,r25,r4
	ctx.r4.u64 = r25.u64 | ctx.r4.u64;
	// rldicr r28,r11,32,31
	r28.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r27,r10,32,31
	r27.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r26,r5,32,31
	r26.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
	// rldicr r25,r4,32,31
	r25.u64 = __builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000;
	// add r24,r18,r3
	r24.u64 = r18.u64 + ctx.r3.u64;
	// or r28,r28,r11
	r28.u64 = r28.u64 | r11.u64;
	// or r27,r27,r10
	r27.u64 = r27.u64 | ctx.r10.u64;
	// or r26,r26,r5
	r26.u64 = r26.u64 | ctx.r5.u64;
	// or r25,r25,r4
	r25.u64 = r25.u64 | ctx.r4.u64;
	// ble cr6,0x82619650
	if (!cr6.gt) goto loc_82619650;
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// subf r7,r30,r29
	ctx.r7.s64 = r29.s64 - r30.s64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_8261962C:
	// lbz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r5,r7,r11
	PPC_STORE_U8(ctx.r7.u32 + r11.u32, ctx.r5.u8);
	// lbz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// stb r5,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r5.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8261962c
	if (!cr6.eq) goto loc_8261962C;
	// lwz r7,3380(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 3380);
loc_82619650:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82619690
	if (!cr6.gt) goto loc_82619690;
	// mr r11,r23
	r11.u64 = r23.u64;
	// subf r5,r23,r15
	ctx.r5.s64 = r15.s64 - r23.s64;
	// subf r4,r23,r24
	ctx.r4.s64 = r24.s64 - r23.s64;
	// subf r3,r23,r3
	ctx.r3.s64 = ctx.r3.s64 - r23.s64;
loc_8261966C:
	// stdx r28,r5,r11
	PPC_STORE_U64(ctx.r5.u32 + r11.u32, r28.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// std r27,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r27.u64);
	// stdx r26,r4,r11
	PPC_STORE_U64(ctx.r4.u32 + r11.u32, r26.u64);
	// stdx r25,r3,r11
	PPC_STORE_U64(ctx.r3.u32 + r11.u32, r25.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r7,3380(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 3380);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x8261966c
	if (cr6.lt) goto loc_8261966C;
loc_82619690:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r15,r15,r14
	r15.u64 = r15.u64 + r14.u64;
	// add r23,r23,r14
	r23.u64 = r23.u64 + r14.u64;
	// add r31,r31,r14
	r31.u64 = r31.u64 + r14.u64;
	// add r29,r29,r14
	r29.u64 = r29.u64 + r14.u64;
	// add r30,r30,r14
	r30.u64 = r30.u64 + r14.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x826195a0
	if (!cr6.eq) goto loc_826195A0;
	// lwz r9,-160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r28,-156(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -156);
loc_826196B8:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82619738
	if (cr6.eq) goto loc_82619738;
	// mullw r11,r28,r14
	r11.s64 = int64_t(r28.s32) * int64_t(r14.s32);
	// subf r7,r11,r16
	ctx.r7.s64 = r16.s64 - r11.s64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82619738
	if (!cr6.gt) goto loc_82619738;
	// subf r11,r16,r7
	r11.s64 = ctx.r7.s64 - r16.s64;
	// neg r29,r14
	r29.s64 = -r14.s64;
	// add r3,r11,r17
	ctx.r3.u64 = r11.u64 + r17.u64;
	// subf r8,r7,r17
	ctx.r8.s64 = r17.s64 - ctx.r7.s64;
	// subf r30,r17,r16
	r30.s64 = r16.s64 - r17.s64;
	// mr r31,r28
	r31.u64 = r28.u64;
loc_826196E8:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82619720
	if (!cr6.gt) goto loc_82619720;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// add r5,r8,r30
	ctx.r5.u64 = ctx.r8.u64 + r30.u64;
	// subf r4,r7,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r7.s64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82619700:
	// lwzx r27,r11,r5
	r27.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwzx r27,r8,r11
	r27.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// stwx r27,r4,r11
	PPC_STORE_U32(ctx.r4.u32 + r11.u32, r27.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x82619700
	if (!cr6.eq) goto loc_82619700;
loc_82619720:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r7,r7,r14
	ctx.r7.u64 = ctx.r7.u64 + r14.u64;
	// add r3,r3,r14
	ctx.r3.u64 = ctx.r3.u64 + r14.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x826196e8
	if (!cr6.eq) goto loc_826196E8;
loc_82619738:
	// lwz r11,68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826197d4
	if (cr6.eq) goto loc_826197D4;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// subf r8,r14,r15
	ctx.r8.s64 = r15.s64 - r14.s64;
	// subf r10,r14,r23
	ctx.r10.s64 = r23.s64 - r14.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// neg r11,r6
	r11.s64 = -ctx.r6.s64;
	// beq cr6,0x82619764
	if (cr6.eq) goto loc_82619764;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// b 0x82619768
	goto loc_82619768;
loc_82619764:
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
loc_82619768:
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826197d4
	if (!cr6.gt) goto loc_826197D4;
	// neg r4,r14
	ctx.r4.s64 = -r14.s64;
	// subf r9,r15,r10
	ctx.r9.s64 = ctx.r10.s64 - r15.s64;
	// subf r5,r10,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r10.s64;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82619784:
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826197bc
	if (!cr6.gt) goto loc_826197BC;
	// mr r11,r15
	r11.u64 = r15.u64;
	// add r8,r9,r5
	ctx.r8.u64 = ctx.r9.u64 + ctx.r5.u64;
	// subf r7,r15,r23
	ctx.r7.s64 = r23.s64 - r15.s64;
loc_8261979C:
	// lwzx r3,r8,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// lwzx r3,r9,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// stwx r3,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x8261979c
	if (!cr6.eq) goto loc_8261979C;
loc_826197BC:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r15,r15,r14
	r15.u64 = r15.u64 + r14.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r23,r23,r14
	r23.u64 = r23.u64 + r14.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x82619784
	if (!cr6.eq) goto loc_82619784;
loc_826197D4:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826197D8"))) PPC_WEAK_FUNC(sub_826197D8);
PPC_FUNC_IMPL(__imp__sub_826197D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r20,136(r3)
	r20.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r11,r20,1
	r11.s64 = r20.s64 + 1;
	// rlwinm r19,r11,31,1,31
	r19.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r20,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r20.u32);
	// add r11,r19,r7
	r11.u64 = r19.u64 + ctx.r7.u64;
	// add r7,r19,r4
	ctx.r7.u64 = r19.u64 + ctx.r4.u64;
	// stw r19,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, r19.u32);
	// beq cr6,0x82619ba8
	if (cr6.eq) goto loc_82619BA8;
	// srawi r7,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	ctx.r7.s64 = r20.s32 >> 2;
	// li r23,1
	r23.s64 = 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x826199fc
	if (!cr6.gt) goto loc_826199FC;
loc_82619820:
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r27,6(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// extsb r26,r28
	r26.s64 = r28.s8;
	// lbz r25,7(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// extsb r27,r27
	r27.s64 = r27.s8;
	// lbz r22,0(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// srawi r24,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r24.s64 = r26.s32 >> 2;
	// lbz r31,9(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// srawi r21,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r21.s64 = r27.s32 >> 4;
	// lbz r30,8(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// extsb r25,r25
	r25.s64 = r25.s8;
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwimi r24,r21,0,28,29
	r24.u64 = (__builtin_rotateleft32(r21.u32, 0) & 0xC) | (r24.u64 & 0xFFFFFFFFFFFFFFF3);
	// lbz r29,3(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// srawi r25,r25,6
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x3F) != 0);
	r25.s64 = r25.s32 >> 6;
	// lbz r21,13(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 13);
	// mr r17,r24
	r17.u64 = r24.u64;
	// lbz r18,18(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// extsb r14,r29
	r14.s64 = r29.s8;
	// lbz r16,19(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// rlwimi r17,r25,0,30,31
	r17.u64 = (__builtin_rotateleft32(r25.u32, 0) & 0x3) | (r17.u64 & 0xFFFFFFFFFFFFFFFC);
	// lbz r27,21(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// extsb r21,r21
	r21.s64 = r21.s8;
	// lbz r15,12(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 12);
	// rlwimi r17,r22,0,24,25
	r17.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0xC0) | (r17.u64 & 0xFFFFFFFFFFFFFF3F);
	// lbz r26,20(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 20);
	// mr r22,r31
	r22.u64 = r31.u64;
	// lbz r25,15(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 15);
	// extsb r31,r31
	r31.s64 = r31.s8;
	// lbz r24,14(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 14);
	// rlwinm r22,r22,0,28,29
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xC;
	// srawi r31,r31,6
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3F) != 0);
	r31.s64 = r31.s32 >> 6;
	// stb r17,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r17.u8);
	// extsb r17,r30
	r17.s64 = r30.s8;
	// extsb r18,r18
	r18.s64 = r18.s8;
	// srawi r17,r17,4
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0xF) != 0);
	r17.s64 = r17.s32 >> 4;
	// srawi r14,r14,2
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x3) != 0);
	r14.s64 = r14.s32 >> 2;
	// rlwimi r31,r17,0,28,29
	r31.u64 = (__builtin_rotateleft32(r17.u32, 0) & 0xC) | (r31.u64 & 0xFFFFFFFFFFFFFFF3);
	// mr r17,r28
	r17.u64 = r28.u64;
	// srawi r22,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r22.s64 = r22.s32 >> 2;
	// rlwimi r29,r17,2,22,27
	r29.u64 = (__builtin_rotateleft32(r17.u32, 2) & 0x3F0) | (r29.u64 & 0xFFFFFFFFFFFFFC0F);
	// lbz r17,17(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// srawi r21,r21,2
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3) != 0);
	r21.s64 = r21.s32 >> 2;
	// rlwinm r29,r29,2,20,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFF0;
	// srawi r18,r18,4
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0xF) != 0);
	r18.s64 = r18.s32 >> 4;
	// clrlwi r29,r29,24
	r29.u64 = r29.u32 & 0xFF;
	// extsb r16,r16
	r16.s64 = r16.s8;
	// rlwimi r31,r14,0,26,27
	r31.u64 = (__builtin_rotateleft32(r14.u32, 0) & 0x30) | (r31.u64 & 0xFFFFFFFFFFFFFFCF);
	// lbz r14,5(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// rlwimi r21,r18,0,28,29
	r21.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0xC) | (r21.u64 & 0xFFFFFFFFFFFFFFF3);
	// or r29,r29,r22
	r29.u64 = r29.u64 | r22.u64;
	// lbz r22,22(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 22);
	// rlwinm r30,r30,0,28,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// srawi r18,r16,6
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3F) != 0);
	r18.s64 = r16.s32 >> 6;
	// lbz r16,23(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + 23);
	// rlwimi r31,r28,0,24,25
	r31.u64 = (__builtin_rotateleft32(r28.u32, 0) & 0xC0) | (r31.u64 & 0xFFFFFFFFFFFFFF3F);
	// or r30,r29,r30
	r30.u64 = r29.u64 | r30.u64;
	// lbz r29,10(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// rlwimi r21,r18,0,30,31
	r21.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0x3) | (r21.u64 & 0xFFFFFFFFFFFFFFFC);
	// extsb r28,r27
	r28.s64 = r27.s8;
	// rlwimi r21,r15,0,24,25
	r21.u64 = (__builtin_rotateleft32(r15.u32, 0) & 0xC0) | (r21.u64 & 0xFFFFFFFFFFFFFF3F);
	// lbz r15,4(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// srawi r31,r28,6
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3F) != 0);
	r31.s64 = r28.s32 >> 6;
	// mr r28,r27
	r28.u64 = r27.u64;
	// lbz r27,16(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// extsb r30,r26
	r30.s64 = r26.s8;
	// extsb r18,r25
	r18.s64 = r25.s8;
	// stb r21,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r21.u8);
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbz r21,11(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// rlwinm r28,r28,0,28,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xC;
	// extsb r29,r29
	r29.s64 = r29.s8;
	// srawi r18,r18,2
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x3) != 0);
	r18.s64 = r18.s32 >> 2;
	// extsb r27,r27
	r27.s64 = r27.s8;
	// srawi r28,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	r28.s64 = r28.s32 >> 2;
	// srawi r29,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	r29.s64 = r29.s32 >> 2;
	// extsb r22,r22
	r22.s64 = r22.s8;
	// extsb r21,r21
	r21.s64 = r21.s8;
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// rlwimi r31,r30,0,28,29
	r31.u64 = (__builtin_rotateleft32(r30.u32, 0) & 0xC) | (r31.u64 & 0xFFFFFFFFFFFFFFF3);
	// extsb r17,r17
	r17.s64 = r17.s8;
	// srawi r22,r22,6
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3F) != 0);
	r22.s64 = r22.s32 >> 6;
	// mr r30,r24
	r30.u64 = r24.u64;
	// srawi r21,r21,2
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3) != 0);
	r21.s64 = r21.s32 >> 2;
	// rlwimi r29,r27,0,28,29
	r29.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xC) | (r29.u64 & 0xFFFFFFFFFFFFFFF3);
	// srawi r17,r17,4
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0xF) != 0);
	r17.s64 = r17.s32 >> 4;
	// extsb r27,r16
	r27.s64 = r16.s8;
	// rlwimi r25,r30,2,22,27
	r25.u64 = (__builtin_rotateleft32(r30.u32, 2) & 0x3F0) | (r25.u64 & 0xFFFFFFFFFFFFFC0F);
	// srawi r30,r27,6
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3F) != 0);
	r30.s64 = r27.s32 >> 6;
	// rlwimi r21,r17,0,28,29
	r21.u64 = (__builtin_rotateleft32(r17.u32, 0) & 0xC) | (r21.u64 & 0xFFFFFFFFFFFFFFF3);
	// rlwinm r27,r25,2,20,27
	r27.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFF0;
	// rlwimi r21,r30,0,30,31
	r21.u64 = (__builtin_rotateleft32(r30.u32, 0) & 0x3) | (r21.u64 & 0xFFFFFFFFFFFFFFFC);
	// clrlwi r30,r27,24
	r30.u64 = r27.u32 & 0xFF;
	// rlwimi r31,r18,0,26,27
	r31.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0x30) | (r31.u64 & 0xFFFFFFFFFFFFFFCF);
	// or r30,r30,r28
	r30.u64 = r30.u64 | r28.u64;
	// rlwimi r29,r22,0,30,31
	r29.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0x3) | (r29.u64 & 0xFFFFFFFFFFFFFFFC);
	// rlwinm r28,r26,0,28,29
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xC;
	// rlwimi r31,r24,0,24,25
	r31.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0xC0) | (r31.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwimi r29,r15,0,24,25
	r29.u64 = (__builtin_rotateleft32(r15.u32, 0) & 0xC0) | (r29.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwimi r21,r14,0,24,25
	r21.u64 = (__builtin_rotateleft32(r14.u32, 0) & 0xC0) | (r21.u64 & 0xFFFFFFFFFFFFFF3F);
	// or r30,r30,r28
	r30.u64 = r30.u64 | r28.u64;
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r29,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r29.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r21,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r21.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x82619820
	if (!cr6.eq) goto loc_82619820;
loc_826199FC:
	// clrlwi r25,r20,30
	r25.u64 = r20.u32 & 0x3;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x82619b98
	if (cr6.eq) goto loc_82619B98;
	// rlwinm r31,r20,0,30,30
	r31.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x2;
	// lbz r30,1(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplwi cr6,r31,2
	cr6.compare<uint32_t>(r31.u32, 2, xer);
	// lbz r27,5(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r31,r30
	r31.s64 = r30.s8;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// srawi r26,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r26.s64 = r31.s32 >> 2;
	// lbz r7,2(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r31,r30,0,0,25
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFC0;
	// rlwimi r26,r29,0,24,25
	r26.u64 = (__builtin_rotateleft32(r29.u32, 0) & 0xC0) | (r26.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwinm r30,r27,0,0,25
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFC0;
	// rlwinm r29,r26,0,24,27
	r29.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xF0;
	// extsb r26,r28
	r26.s64 = r28.s8;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// rlwimi r28,r27,2,22,27
	r28.u64 = (__builtin_rotateleft32(r27.u32, 2) & 0x3F0) | (r28.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwimi r7,r26,0,26,27
	ctx.r7.u64 = (__builtin_rotateleft32(r26.u32, 0) & 0x30) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r28,r28,2,20,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFF0;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// rlwinm r7,r7,0,24,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xF0;
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
	// bne cr6,0x82619af0
	if (!cr6.eq) goto loc_82619AF0;
	// lbz r24,1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r22,0(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r24,r24
	r24.s64 = r24.s8;
	// lbz r26,2(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r22,r22
	r22.s64 = r22.s8;
	// lbz r27,3(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// srawi r24,r24,6
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3F) != 0);
	r24.s64 = r24.s32 >> 6;
	// lbz r21,4(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// srawi r22,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	r22.s64 = r22.s32 >> 4;
	// lbz r18,5(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r21,r21
	r21.s64 = r21.s8;
	// rlwimi r24,r22,0,28,29
	r24.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0xC) | (r24.u64 & 0xFFFFFFFFFFFFFFF3);
	// extsb r22,r26
	r22.s64 = r26.s8;
	// clrlwi r24,r24,28
	r24.u64 = r24.u32 & 0xF;
	// srawi r22,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	r22.s64 = r22.s32 >> 4;
	// or r29,r24,r29
	r29.u64 = r24.u64 | r29.u64;
	// rlwinm r24,r27,0,28,29
	r24.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xC;
	// extsb r27,r27
	r27.s64 = r27.s8;
	// extsb r18,r18
	r18.s64 = r18.s8;
	// srawi r27,r27,6
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3F) != 0);
	r27.s64 = r27.s32 >> 6;
	// srawi r24,r24,2
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3) != 0);
	r24.s64 = r24.s32 >> 2;
	// rlwimi r22,r27,0,30,31
	r22.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0x3) | (r22.u64 & 0xFFFFFFFFFFFFFFFC);
	// rlwinm r27,r26,0,28,29
	r27.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xC;
	// srawi r21,r21,2
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3) != 0);
	r21.s64 = r21.s32 >> 2;
	// srawi r26,r18,2
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x3) != 0);
	r26.s64 = r18.s32 >> 2;
	// or r27,r24,r27
	r27.u64 = r24.u64 | r27.u64;
	// clrlwi r22,r22,28
	r22.u64 = r22.u32 & 0xF;
	// rlwinm r24,r21,0,26,27
	r24.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0x30;
	// rlwinm r26,r26,0,26,27
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x30;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// or r7,r22,r7
	ctx.r7.u64 = r22.u64 | ctx.r7.u64;
	// or r28,r27,r28
	r28.u64 = r27.u64 | r28.u64;
	// or r31,r24,r31
	r31.u64 = r24.u64 | r31.u64;
	// or r30,r26,r30
	r30.u64 = r26.u64 | r30.u64;
loc_82619AF0:
	// stb r29,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r29.u8);
	// cmplwi cr6,r25,3
	cr6.compare<uint32_t>(r25.u32, 3, xer);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r28,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r28.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x82619b88
	if (!cr6.eq) goto loc_82619B88;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r29,3(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r28,r28
	r28.s64 = r28.s8;
	// lbz r27,0(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r25,5(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// extsb r24,r29
	r24.s64 = r29.s8;
	// srawi r28,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	r28.s64 = r28.s32 >> 2;
	// lbz r7,2(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r26,4(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// srawi r24,r24,2
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3) != 0);
	r24.s64 = r24.s32 >> 2;
	// rlwimi r28,r27,0,24,25
	r28.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xC0) | (r28.u64 & 0xFFFFFFFFFFFFFF3F);
	// extsb r27,r25
	r27.s64 = r25.s8;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// extsb r26,r26
	r26.s64 = r26.s8;
	// rlwimi r29,r25,2,22,27
	r29.u64 = (__builtin_rotateleft32(r25.u32, 2) & 0x3F0) | (r29.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwimi r7,r24,0,26,27
	ctx.r7.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0x30) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r28,r28,0,24,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0xF0;
	// srawi r26,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r26.s64 = r26.s32 >> 4;
	// rlwinm r29,r29,2,20,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFF0;
	// rlwinm r7,r7,0,24,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xF0;
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// rlwinm r26,r26,0,28,29
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xC;
	// stb r28,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r28.u8);
	// rlwinm r27,r27,0,28,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xC;
	// stb r29,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r29.u8);
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// or r31,r26,r31
	r31.u64 = r26.u64 | r31.u64;
	// or r30,r27,r30
	r30.u64 = r27.u64 | r30.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
loc_82619B88:
	// stb r31,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r31.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r30,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r30.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_82619B98:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// add r7,r19,r4
	ctx.r7.u64 = r19.u64 + ctx.r4.u64;
	// add r11,r19,r11
	r11.u64 = r19.u64 + r11.u64;
	// b 0x82619bac
	goto loc_82619BAC;
loc_82619BA8:
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_82619BAC:
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpw cr6,r23,r31
	cr6.compare<int32_t>(r23.s32, r31.s32, xer);
	// bge cr6,0x8261a0e4
	if (!cr6.lt) goto loc_8261A0E4;
	// srawi r30,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r30.s64 = r20.s32 >> 2;
	// subf r31,r23,r31
	r31.s64 = r31.s64 - r23.s64;
	// stw r30,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r30.u32);
	// stw r31,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r31.u32);
	// b 0x82619bd0
	goto loc_82619BD0;
loc_82619BCC:
	// lwz r30,-160(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
loc_82619BD0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82619e78
	if (!cr6.gt) goto loc_82619E78;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_82619BDC:
	// lbz r30,7(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// lbz r29,6(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// extsb r6,r30
	ctx.r6.s64 = r30.s8;
	// lbz r27,9(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// extsb r18,r29
	r18.s64 = r29.s8;
	// lbz r26,8(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// extsb r17,r28
	r17.s64 = r28.s8;
	// srawi r6,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 6;
	// lbz r24,0(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r16,r27
	r16.s64 = r27.s8;
	// lbz r23,2(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// srawi r18,r18,4
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0xF) != 0);
	r18.s64 = r18.s32 >> 4;
	// lbz r25,3(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r15,r26
	r15.s64 = r26.s8;
	// lbz r22,19(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// srawi r17,r17,2
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x3) != 0);
	r17.s64 = r17.s32 >> 2;
	// lbz r21,18(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// srawi r16,r16,6
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3F) != 0);
	r16.s64 = r16.s32 >> 6;
	// lbz r20,13(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 13);
	// rlwimi r6,r18,0,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// lbz r19,12(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + 12);
	// srawi r15,r15,4
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0xF) != 0);
	r15.s64 = r15.s32 >> 4;
	// mr r18,r24
	r18.u64 = r24.u64;
	// mr r14,r23
	r14.u64 = r23.u64;
	// rlwimi r16,r15,0,28,29
	r16.u64 = (__builtin_rotateleft32(r15.u32, 0) & 0xC) | (r16.u64 & 0xFFFFFFFFFFFFFFF3);
	// rlwimi r28,r18,2,22,27
	r28.u64 = (__builtin_rotateleft32(r18.u32, 2) & 0x3F0) | (r28.u64 & 0xFFFFFFFFFFFFFC0F);
	// extsb r15,r25
	r15.s64 = r25.s8;
	// rlwimi r25,r14,2,22,27
	r25.u64 = (__builtin_rotateleft32(r14.u32, 2) & 0x3F0) | (r25.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwinm r28,r28,2,20,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFF0;
	// rlwinm r30,r30,0,28,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// rlwinm r27,r27,0,28,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xC;
	// rlwinm r25,r25,2,20,27
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFF0;
	// srawi r18,r15,2
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x3) != 0);
	r18.s64 = r15.s32 >> 2;
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
	// srawi r30,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r30.s64 = r30.s32 >> 2;
	// srawi r27,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	r27.s64 = r27.s32 >> 2;
	// clrlwi r25,r25,24
	r25.u64 = r25.u32 & 0xFF;
	// or r30,r28,r30
	r30.u64 = r28.u64 | r30.u64;
	// rlwimi r6,r17,0,26,27
	ctx.r6.u64 = (__builtin_rotateleft32(r17.u32, 0) & 0x30) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFCF);
	// or r28,r25,r27
	r28.u64 = r25.u64 | r27.u64;
	// rlwinm r29,r29,0,28,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xC;
	// rlwinm r27,r26,0,28,29
	r27.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0xC;
	// lbz r26,4(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwimi r6,r24,0,24,25
	ctx.r6.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0xC0) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFF3F);
	// or r30,r30,r29
	r30.u64 = r30.u64 | r29.u64;
	// lbz r29,20(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 20);
	// or r28,r28,r27
	r28.u64 = r28.u64 | r27.u64;
	// lbz r27,14(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 14);
	// extsb r25,r20
	r25.s64 = r20.s8;
	// rlwimi r16,r18,0,26,27
	r16.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0x30) | (r16.u64 & 0xFFFFFFFFFFFFFFCF);
	// stb r6,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r6.u8);
	// extsb r6,r22
	ctx.r6.s64 = r22.s8;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// mr r17,r19
	r17.u64 = r19.u64;
	// stb r28,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r28.u8);
	// extsb r28,r21
	r28.s64 = r21.s8;
	// srawi r6,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 6;
	// lbz r30,21(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// srawi r24,r28,4
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xF) != 0);
	r24.s64 = r28.s32 >> 4;
	// lbz r28,15(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 15);
	// srawi r18,r25,2
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x3) != 0);
	r18.s64 = r25.s32 >> 2;
	// lbz r25,10(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// rlwimi r6,r24,0,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// lbz r24,22(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 22);
	// rlwimi r16,r23,0,24,25
	r16.u64 = (__builtin_rotateleft32(r23.u32, 0) & 0xC0) | (r16.u64 & 0xFFFFFFFFFFFFFF3F);
	// lbz r23,16(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// rlwimi r6,r18,0,26,27
	ctx.r6.u64 = (__builtin_rotateleft32(r18.u32, 0) & 0x30) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFCF);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// rlwimi r6,r19,0,24,25
	ctx.r6.u64 = (__builtin_rotateleft32(r19.u32, 0) & 0xC0) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwimi r20,r17,2,22,27
	r20.u64 = (__builtin_rotateleft32(r17.u32, 2) & 0x3F0) | (r20.u64 & 0xFFFFFFFFFFFFFC0F);
	// stb r16,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r16.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// rlwinm r20,r20,2,20,27
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFF0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r6,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r6.u8);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r22,r22,0,28,29
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xC;
	// clrlwi r20,r20,24
	r20.u64 = r20.u32 & 0xFF;
	// rlwinm r21,r21,0,28,29
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xC;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// extsb r6,r30
	ctx.r6.s64 = r30.s8;
	// extsb r19,r29
	r19.s64 = r29.s8;
	// srawi r6,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 6;
	// srawi r19,r19,4
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0xF) != 0);
	r19.s64 = r19.s32 >> 4;
	// mr r18,r27
	r18.u64 = r27.u64;
	// rlwimi r6,r19,0,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(r19.u32, 0) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// extsb r19,r28
	r19.s64 = r28.s8;
	// rlwimi r28,r18,2,22,27
	r28.u64 = (__builtin_rotateleft32(r18.u32, 2) & 0x3F0) | (r28.u64 & 0xFFFFFFFFFFFFFC0F);
	// mr r16,r25
	r16.u64 = r25.u64;
	// mr r17,r26
	r17.u64 = r26.u64;
	// rlwinm r30,r30,0,28,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// rlwinm r28,r28,2,20,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFF0;
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// srawi r22,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r22.s64 = r22.s32 >> 2;
	// rlwimi r16,r17,2,22,27
	r16.u64 = (__builtin_rotateleft32(r17.u32, 2) & 0x3F0) | (r16.u64 & 0xFFFFFFFFFFFFFC0F);
	// srawi r30,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r30.s64 = r30.s32 >> 2;
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
	// rlwimi r6,r19,0,26,27
	ctx.r6.u64 = (__builtin_rotateleft32(r19.u32, 0) & 0x30) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFCF);
	// or r22,r20,r22
	r22.u64 = r20.u64 | r22.u64;
	// rlwinm r18,r24,0,28,29
	r18.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xC;
	// rlwinm r19,r16,2,20,27
	r19.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 2) & 0xFF0;
	// or r30,r28,r30
	r30.u64 = r28.u64 | r30.u64;
	// rlwinm r29,r29,0,28,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xC;
	// or r22,r22,r21
	r22.u64 = r22.u64 | r21.u64;
	// srawi r18,r18,2
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x3) != 0);
	r18.s64 = r18.s32 >> 2;
	// clrlwi r19,r19,24
	r19.u64 = r19.u32 & 0xFF;
	// or r30,r30,r29
	r30.u64 = r30.u64 | r29.u64;
	// rlwimi r6,r27,0,24,25
	ctx.r6.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xC0) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFF3F);
	// rlwinm r27,r23,0,28,29
	r27.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xC;
	// stb r22,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r22.u8);
	// or r28,r19,r18
	r28.u64 = r19.u64 | r18.u64;
	// extsb r25,r25
	r25.s64 = r25.s8;
	// or r29,r28,r27
	r29.u64 = r28.u64 | r27.u64;
	// stb r30,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r30.u8);
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// extsb r6,r24
	ctx.r6.s64 = r24.s8;
	// lbz r30,23(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 23);
	// extsb r24,r23
	r24.s64 = r23.s8;
	// lbz r28,11(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// rlwinm r23,r30,0,28,29
	r23.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// lbz r27,5(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// stb r29,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r29.u8);
	// extsb r30,r30
	r30.s64 = r30.s8;
	// lbz r29,17(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// srawi r23,r23,2
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3) != 0);
	r23.s64 = r23.s32 >> 2;
	// srawi r6,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 6;
	// srawi r24,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	r24.s64 = r24.s32 >> 4;
	// extsb r22,r29
	r22.s64 = r29.s8;
	// mr r20,r28
	r20.u64 = r28.u64;
	// srawi r25,r25,2
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x3) != 0);
	r25.s64 = r25.s32 >> 2;
	// mr r21,r27
	r21.u64 = r27.u64;
	// srawi r30,r30,6
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3F) != 0);
	r30.s64 = r30.s32 >> 6;
	// extsb r28,r28
	r28.s64 = r28.s8;
	// srawi r22,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	r22.s64 = r22.s32 >> 4;
	// rlwimi r20,r21,2,22,27
	r20.u64 = (__builtin_rotateleft32(r21.u32, 2) & 0x3F0) | (r20.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwimi r6,r24,0,28,29
	ctx.r6.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0xC) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFF3);
	// srawi r28,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	r28.s64 = r28.s32 >> 2;
	// rlwimi r30,r22,0,28,29
	r30.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0xC) | (r30.u64 & 0xFFFFFFFFFFFFFFF3);
	// rlwinm r24,r20,2,20,27
	r24.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFF0;
	// rlwimi r30,r28,0,26,27
	r30.u64 = (__builtin_rotateleft32(r28.u32, 0) & 0x30) | (r30.u64 & 0xFFFFFFFFFFFFFFCF);
	// clrlwi r28,r24,24
	r28.u64 = r24.u32 & 0xFF;
	// rlwimi r6,r25,0,26,27
	ctx.r6.u64 = (__builtin_rotateleft32(r25.u32, 0) & 0x30) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFFCF);
	// or r28,r28,r23
	r28.u64 = r28.u64 | r23.u64;
	// rlwinm r29,r29,0,28,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xC;
	// rlwimi r6,r26,0,24,25
	ctx.r6.u64 = (__builtin_rotateleft32(r26.u32, 0) & 0xC0) | (ctx.r6.u64 & 0xFFFFFFFFFFFFFF3F);
	// or r29,r28,r29
	r29.u64 = r28.u64 | r29.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwz r6,44(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r29,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r29.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r28,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r28.u8);
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// rlwimi r30,r27,0,24,25
	r30.u64 = (__builtin_rotateleft32(r27.u32, 0) & 0xC0) | (r30.u64 & 0xFFFFFFFFFFFFFF3F);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// stb r30,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r30.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x82619bdc
	if (!cr6.eq) goto loc_82619BDC;
	// lwz r20,-172(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// lwz r19,-168(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
loc_82619E78:
	// clrlwi r31,r20,30
	r31.u64 = r20.u32 & 0x3;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8261a0c0
	if (cr6.eq) goto loc_8261A0C0;
	// rlwinm r31,r20,0,30,30
	r31.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x2;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r27,3(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// cmplwi cr6,r31,2
	cr6.compare<uint32_t>(r31.u32, 2, xer);
	// lbz r29,2(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r31,r28
	r31.s64 = r28.s8;
	// mr r26,r30
	r26.u64 = r30.u64;
	// lbz r22,4(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r25,r27
	r25.s64 = r27.s8;
	// lbz r21,5(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// mr r24,r29
	r24.u64 = r29.u64;
	// srawi r31,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r31.s64 = r31.s32 >> 2;
	// rlwimi r28,r26,2,22,27
	r28.u64 = (__builtin_rotateleft32(r26.u32, 2) & 0x3F0) | (r28.u64 & 0xFFFFFFFFFFFFFC0F);
	// srawi r26,r25,2
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x3) != 0);
	r26.s64 = r25.s32 >> 2;
	// rlwimi r27,r24,2,22,27
	r27.u64 = (__builtin_rotateleft32(r24.u32, 2) & 0x3F0) | (r27.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwimi r30,r31,0,26,27
	r30.u64 = (__builtin_rotateleft32(r31.u32, 0) & 0x30) | (r30.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r31,r28,2,20,27
	r31.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFF0;
	// rlwimi r29,r26,0,26,27
	r29.u64 = (__builtin_rotateleft32(r26.u32, 0) & 0x30) | (r29.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r28,r27,2,20,27
	r28.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFF0;
	// rlwinm r18,r22,4,20,25
	r18.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 4) & 0xFC0;
	// rlwinm r17,r21,4,20,25
	r17.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 4) & 0xFC0;
	// rlwinm r26,r30,0,24,27
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xF0;
	// rlwinm r24,r29,0,24,27
	r24.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xF0;
	// clrlwi r23,r28,24
	r23.u64 = r28.u32 & 0xFF;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// clrlwi r25,r31,24
	r25.u64 = r31.u32 & 0xFF;
	// rlwinm r28,r22,0,0,25
	r28.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xFFFFFFC0;
	// rlwinm r27,r21,0,0,25
	r27.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xFFFFFFC0;
	// clrlwi r30,r18,24
	r30.u64 = r18.u32 & 0xFF;
	// clrlwi r29,r17,24
	r29.u64 = r17.u32 & 0xFF;
	// bne cr6,0x82619fd4
	if (!cr6.eq) goto loc_82619FD4;
	// lbz r6,5(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// clrlwi r31,r30,24
	r31.u64 = r30.u32 & 0xFF;
	// lbz r21,0(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// clrlwi r19,r29,24
	r19.u64 = r29.u32 & 0xFF;
	// lbz r22,1(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r29,2(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// extsb r16,r21
	r16.s64 = r21.s8;
	// lbz r30,3(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// rlwinm r18,r22,0,28,29
	r18.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xC;
	// stb r6,-176(r1)
	PPC_STORE_U8(ctx.r1.u32 + -176, ctx.r6.u8);
	// extsb r22,r22
	r22.s64 = r22.s8;
	// rlwinm r17,r30,0,28,29
	r17.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// lbz r20,4(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r15,r29
	r15.s64 = r29.s8;
	// srawi r16,r16,4
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0xF) != 0);
	r16.s64 = r16.s32 >> 4;
	// srawi r22,r22,6
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3F) != 0);
	r22.s64 = r22.s32 >> 6;
	// extsb r30,r30
	r30.s64 = r30.s8;
	// srawi r18,r18,2
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x3) != 0);
	r18.s64 = r18.s32 >> 2;
	// srawi r15,r15,4
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0xF) != 0);
	r15.s64 = r15.s32 >> 4;
	// srawi r30,r30,6
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3F) != 0);
	r30.s64 = r30.s32 >> 6;
	// extsb r14,r20
	r14.s64 = r20.s8;
	// rlwimi r16,r22,0,30,31
	r16.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0x3) | (r16.u64 & 0xFFFFFFFFFFFFFFFC);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// rlwinm r22,r21,0,28,29
	r22.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xC;
	// rlwimi r15,r30,0,30,31
	r15.u64 = (__builtin_rotateleft32(r30.u32, 0) & 0x3) | (r15.u64 & 0xFFFFFFFFFFFFFFFC);
	// srawi r17,r17,2
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x3) != 0);
	r17.s64 = r17.s32 >> 2;
	// rlwinm r30,r29,0,28,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xC;
	// srawi r14,r14,2
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x3) != 0);
	r14.s64 = r14.s32 >> 2;
	// rlwinm r29,r20,2,26,27
	r29.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0x30;
	// srawi r6,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 2;
	// or r30,r17,r30
	r30.u64 = r17.u64 | r30.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// clrlwi r20,r16,28
	r20.u64 = r16.u32 & 0xF;
	// or r22,r18,r22
	r22.u64 = r18.u64 | r22.u64;
	// rlwinm r6,r6,0,26,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x30;
	// clrlwi r18,r15,28
	r18.u64 = r15.u32 & 0xF;
	// rlwinm r17,r14,0,26,27
	r17.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 0) & 0x30;
	// or r26,r20,r26
	r26.u64 = r20.u64 | r26.u64;
	// lwz r20,-172(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// or r23,r30,r23
	r23.u64 = r30.u64 | r23.u64;
	// or r27,r6,r27
	r27.u64 = ctx.r6.u64 | r27.u64;
	// lwz r6,44(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// or r25,r22,r25
	r25.u64 = r22.u64 | r25.u64;
	// or r24,r18,r24
	r24.u64 = r18.u64 | r24.u64;
	// or r28,r17,r28
	r28.u64 = r17.u64 | r28.u64;
	// clrlwi r30,r31,24
	r30.u64 = r31.u32 & 0xFF;
	// lbz r21,-176(r1)
	r21.u64 = PPC_LOAD_U8(ctx.r1.u32 + -176);
	// rlwinm r21,r21,2,26,27
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0x30;
	// or r29,r21,r19
	r29.u64 = r21.u64 | r19.u64;
	// lwz r19,-168(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// clrlwi r29,r29,24
	r29.u64 = r29.u32 & 0xFF;
loc_82619FD4:
	// clrlwi r31,r20,30
	r31.u64 = r20.u32 & 0x3;
	// stb r26,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r26.u8);
	// stb r25,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r25.u8);
	// stb r24,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r24.u8);
	// cmplwi cr6,r31,3
	cr6.compare<uint32_t>(r31.u32, 3, xer);
	// stb r23,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r23.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// bne cr6,0x8261a09c
	if (!cr6.eq) goto loc_8261A09C;
	// clrlwi r21,r30,24
	r21.u64 = r30.u32 & 0xFF;
	// lbz r26,1(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// clrlwi r18,r29,24
	r18.u64 = r29.u32 & 0xFF;
	// lbz r25,3(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// extsb r31,r26
	r31.s64 = r26.s8;
	// lbz r29,2(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// mr r22,r30
	r22.u64 = r30.u64;
	// extsb r17,r25
	r17.s64 = r25.s8;
	// lbz r24,4(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// mr r16,r29
	r16.u64 = r29.u64;
	// lbz r23,5(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// srawi r31,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r31.s64 = r31.s32 >> 2;
	// rlwimi r26,r22,2,22,27
	r26.u64 = (__builtin_rotateleft32(r22.u32, 2) & 0x3F0) | (r26.u64 & 0xFFFFFFFFFFFFFC0F);
	// srawi r22,r17,2
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x3) != 0);
	r22.s64 = r17.s32 >> 2;
	// rlwimi r30,r31,0,26,27
	r30.u64 = (__builtin_rotateleft32(r31.u32, 0) & 0x30) | (r30.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwimi r25,r16,2,22,27
	r25.u64 = (__builtin_rotateleft32(r16.u32, 2) & 0x3F0) | (r25.u64 & 0xFFFFFFFFFFFFFC0F);
	// rlwimi r29,r22,0,26,27
	r29.u64 = (__builtin_rotateleft32(r22.u32, 0) & 0x30) | (r29.u64 & 0xFFFFFFFFFFFFFFCF);
	// rlwinm r31,r30,0,24,27
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xF0;
	// rlwinm r22,r25,2,20,27
	r22.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFF0;
	// rlwinm r25,r29,0,24,27
	r25.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xF0;
	// extsb r30,r24
	r30.s64 = r24.s8;
	// extsb r29,r23
	r29.s64 = r23.s8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// stb r31,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r31.u8);
	// rlwinm r26,r26,2,20,27
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFF0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// stb r25,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r25.u8);
	// rlwinm r30,r30,0,28,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xC;
	// rlwinm r29,r29,0,28,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xC;
	// rlwinm r24,r24,0,28,29
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0xC;
	// rlwinm r23,r23,0,28,29
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xC;
	// stb r26,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, r26.u8);
	// or r28,r30,r28
	r28.u64 = r30.u64 | r28.u64;
	// stb r22,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r22.u8);
	// or r27,r29,r27
	r27.u64 = r29.u64 | r27.u64;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// or r30,r24,r21
	r30.u64 = r24.u64 | r21.u64;
	// or r29,r23,r18
	r29.u64 = r23.u64 | r18.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
loc_8261A09C:
	// stb r29,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r29.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r30,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r30.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r28,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r28.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r27,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r27.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
loc_8261A0C0:
	// lwz r4,-164(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// add r11,r19,r11
	r11.u64 = r19.u64 + r11.u64;
	// addi r31,r4,-1
	r31.s64 = ctx.r4.s64 + -1;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// add r7,r19,r7
	ctx.r7.u64 = r19.u64 + ctx.r7.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r31.u32);
	// bne cr6,0x82619bcc
	if (!cr6.eq) goto loc_82619BCC;
loc_8261A0E4:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8261A0E8"))) PPC_WEAK_FUNC(sub_8261A0E8);
PPC_FUNC_IMPL(__imp__sub_8261A0E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r20,84(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// addi r11,r20,1
	r11.s64 = r20.s64 + 1;
	// srawi r18,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r18.s64 = r11.s32 >> 1;
	// srawi r31,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r31.s64 = r20.s32 >> 2;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// add r24,r18,r7
	r24.u64 = r18.u64 + ctx.r7.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// stw r18,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, r18.u32);
	// stw r31,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r31.u32);
	// ble cr6,0x8261a32c
	if (!cr6.gt) goto loc_8261A32C;
	// lwz r7,136(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r30,r31
	r30.u64 = r31.u64;
	// rlwinm r29,r7,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
loc_8261A130:
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r25,2(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r21,r28
	r21.u64 = r28.u64;
	// lbz r23,4(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// mr r19,r28
	r19.u64 = r28.u64;
	// lbz r22,5(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// rlwimi r25,r26,2,22,25
	r25.u64 = (__builtin_rotateleft32(r26.u32, 2) & 0x3C0) | (r25.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r23,r23,2,22,25
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0x3C0;
	// rlwinm r22,r22,2,22,25
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0x3C0;
	// rlwinm r26,r25,0,24,27
	r26.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0xF0;
	// lbz r16,2(r11)
	r16.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r25,r23
	r25.u64 = r23.u64;
	// lbz r15,0(r11)
	r15.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r23,r22
	r23.u64 = r22.u64;
	// rlwinm r16,r16,0,26,27
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 0) & 0x30;
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r15,r15,0,26,27
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0x30;
	// lbz r17,4(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// mr r22,r29
	r22.u64 = r29.u64;
	// or r16,r16,r15
	r16.u64 = r16.u64 | r15.u64;
	// rlwimi r21,r22,2,22,25
	r21.u64 = (__builtin_rotateleft32(r22.u32, 2) & 0x3C0) | (r21.u64 & 0xFFFFFFFFFFFFFC3F);
	// extsb r16,r16
	r16.s64 = r16.s8;
	// rlwimi r19,r29,2,22,29
	r19.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0x3FC) | (r19.u64 & 0xFFFFFFFFFFFFFC03);
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// mr r22,r21
	r22.u64 = r21.u64;
	// mr r21,r19
	r21.u64 = r19.u64;
	// lbz r19,5(r11)
	r19.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// or r26,r16,r26
	r26.u64 = r16.u64 | r26.u64;
	// rlwinm r19,r19,0,26,27
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0x30;
	// clrlwi r23,r23,24
	r23.u64 = r23.u32 & 0xFF;
	// rlwinm r17,r17,0,26,27
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x30;
	// or r23,r19,r23
	r23.u64 = r19.u64 | r23.u64;
	// stb r26,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r26.u8);
	// mr r19,r29
	r19.u64 = r29.u64;
	// rlwinm r26,r28,0,26,27
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// rlwinm r19,r19,0,26,27
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0x30;
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// clrlwi r25,r25,24
	r25.u64 = r25.u32 & 0xFF;
	// or r26,r26,r19
	r26.u64 = r26.u64 | r19.u64;
	// or r25,r17,r25
	r25.u64 = r17.u64 | r25.u64;
	// mr r17,r29
	r17.u64 = r29.u64;
	// mr r16,r28
	r16.u64 = r28.u64;
	// extsb r26,r26
	r26.s64 = r26.s8;
	// rlwimi r16,r17,2,28,29
	r16.u64 = (__builtin_rotateleft32(r17.u32, 2) & 0xC) | (r16.u64 & 0xFFFFFFFFFFFFFFF3);
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// rlwinm r22,r22,0,24,27
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xF0;
	// rlwinm r21,r21,4,24,27
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 4) & 0xF0;
	// clrlwi r16,r16,28
	r16.u64 = r16.u32 & 0xF;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// or r26,r26,r22
	r26.u64 = r26.u64 | r22.u64;
	// or r21,r16,r21
	r21.u64 = r16.u64 | r21.u64;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// stb r26,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r26.u8);
	// addi r26,r24,1
	r26.s64 = r24.s64 + 1;
	// stb r21,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r21.u8);
	// mr r24,r29
	r24.u64 = r29.u64;
	// mr r21,r28
	r21.u64 = r28.u64;
	// lbz r19,5(r11)
	r19.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rlwimi r28,r29,2,22,29
	r28.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0x3FC) | (r28.u64 & 0xFFFFFFFFFFFFFC03);
	// lbz r17,0(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwimi r21,r24,2,22,25
	r21.u64 = (__builtin_rotateleft32(r24.u32, 2) & 0x3C0) | (r21.u64 & 0xFFFFFFFFFFFFFC3F);
	// lbz r24,4(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r15,2(r11)
	r15.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r29,r19
	r29.u64 = r19.u64;
	// rlwinm r19,r24,0,26,27
	r19.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x30;
	// rlwimi r15,r17,2,22,25
	r15.u64 = (__builtin_rotateleft32(r17.u32, 2) & 0x3C0) | (r15.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r29,r29,0,26,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// rlwinm r24,r28,4,18,27
	r24.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0x3FF0;
	// srawi r28,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r28.s64 = r19.s32 >> 2;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// rlwinm r22,r15,0,24,27
	r22.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0xF0;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// srawi r29,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	r29.s64 = r29.s32 >> 2;
	// rlwinm r21,r21,0,24,27
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xF0;
	// lbz r19,2(r11)
	r19.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// or r23,r29,r23
	r23.u64 = r29.u64 | r23.u64;
	// lbz r17,0(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// or r25,r28,r25
	r25.u64 = r28.u64 | r25.u64;
	// rlwinm r19,r19,0,26,27
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0x30;
	// lbz r16,4(r11)
	r16.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rlwinm r17,r17,0,26,27
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x30;
	// lbz r15,5(r11)
	r15.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// lbz r29,3(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r28,1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// or r19,r19,r17
	r19.u64 = r19.u64 | r17.u64;
	// rlwinm r17,r16,0,26,27
	r17.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 0) & 0x30;
	// extsb r19,r19
	r19.s64 = r19.s8;
	// rlwinm r16,r15,0,26,27
	r16.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0x30;
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// clrlwi r15,r24,24
	r15.u64 = r24.u32 & 0xFF;
	// or r24,r19,r22
	r24.u64 = r19.u64 | r22.u64;
	// rlwinm r22,r29,0,26,27
	r22.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// rlwinm r19,r28,0,26,27
	r19.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// srawi r22,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r22.s64 = r22.s32 >> 2;
	// rlwimi r29,r28,2,28,29
	r29.u64 = (__builtin_rotateleft32(r28.u32, 2) & 0xC) | (r29.u64 & 0xFFFFFFFFFFFFFFF3);
	// or r22,r22,r19
	r22.u64 = r22.u64 | r19.u64;
	// stb r24,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r24.u8);
	// clrlwi r29,r29,28
	r29.u64 = r29.u32 & 0xF;
	// extsb r28,r22
	r28.s64 = r22.s8;
	// or r19,r29,r15
	r19.u64 = r29.u64 | r15.u64;
	// srawi r28,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	r28.s64 = r28.s32 >> 2;
	// srawi r22,r17,4
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0xF) != 0);
	r22.s64 = r17.s32 >> 4;
	// srawi r17,r16,4
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0xF) != 0);
	r17.s64 = r16.s32 >> 4;
	// or r25,r22,r25
	r25.u64 = r22.u64 | r25.u64;
	// or r23,r17,r23
	r23.u64 = r17.u64 | r23.u64;
	// stb r19,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r19.u8);
	// or r29,r28,r21
	r29.u64 = r28.u64 | r21.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stb r25,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r25.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// stb r23,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r23.u8);
	// addi r24,r26,1
	r24.s64 = r26.s64 + 1;
	// stb r29,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r29.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8261a130
	if (!cr6.eq) goto loc_8261A130;
loc_8261A32C:
	// clrlwi r21,r20,30
	r21.u64 = r20.u32 & 0x3;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x8261a4b4
	if (cr6.eq) goto loc_8261A4B4;
	// rlwinm r7,r20,0,30,30
	ctx.r7.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x2;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r25,4(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r23,5(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rlwimi r7,r30,2,22,25
	ctx.r7.u64 = (__builtin_rotateleft32(r30.u32, 2) & 0x3C0) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFC3F);
	// lwz r29,136(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r30,r25,2,22,25
	r30.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0x3C0;
	// lbz r26,3(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r23,r23,2,22,25
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0x3C0;
	// lbz r28,1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rlwinm r25,r7,0,24,27
	r25.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xF0;
	// clrlwi r7,r30,24
	ctx.r7.u64 = r30.u32 & 0xFF;
	// clrlwi r30,r23,24
	r30.u64 = r23.u32 & 0xFF;
	// rlwinm r23,r29,1,0,30
	r23.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r19,r26
	r19.u64 = r26.u64;
	// mr r22,r28
	r22.u64 = r28.u64;
	// add r29,r29,r23
	r29.u64 = r29.u64 + r23.u64;
	// rlwimi r26,r28,2,22,29
	r26.u64 = (__builtin_rotateleft32(r28.u32, 2) & 0x3FC) | (r26.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwinm r23,r29,1,0,30
	r23.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwimi r19,r22,2,22,25
	r19.u64 = (__builtin_rotateleft32(r22.u32, 2) & 0x3C0) | (r19.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r28,r26,4,18,27
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0x3FF0;
	// rlwinm r29,r19,0,24,27
	r29.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0xF0;
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
	// add r11,r23,r11
	r11.u64 = r23.u64 + r11.u64;
	// bne cr6,0x8261a424
	if (!cr6.eq) goto loc_8261A424;
	// lbz r19,2(r11)
	r19.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
	// lbz r17,0(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// rlwinm r19,r19,0,26,27
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 0) & 0x30;
	// lbz r26,3(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r17,r17,0,26,27
	r17.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x30;
	// lbz r22,1(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// lbz r16,4(r11)
	r16.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r15,5(r11)
	r15.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r11,r23,r11
	r11.u64 = r23.u64 + r11.u64;
	// or r19,r19,r17
	r19.u64 = r19.u64 | r17.u64;
	// rlwinm r23,r26,0,26,27
	r23.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x30;
	// extsb r19,r19
	r19.s64 = r19.s8;
	// rlwimi r26,r22,2,28,29
	r26.u64 = (__builtin_rotateleft32(r22.u32, 2) & 0xC) | (r26.u64 & 0xFFFFFFFFFFFFFFF3);
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// srawi r23,r23,2
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3) != 0);
	r23.s64 = r23.s32 >> 2;
	// or r25,r19,r25
	r25.u64 = r19.u64 | r25.u64;
	// rlwinm r19,r22,0,26,27
	r19.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0x30;
	// clrlwi r26,r26,28
	r26.u64 = r26.u32 & 0xF;
	// or r23,r23,r19
	r23.u64 = r23.u64 | r19.u64;
	// clrlwi r30,r30,24
	r30.u64 = r30.u32 & 0xFF;
	// extsb r23,r23
	r23.s64 = r23.s8;
	// rlwinm r16,r16,0,26,27
	r16.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 0) & 0x30;
	// rlwinm r15,r15,0,26,27
	r15.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0x30;
	// srawi r23,r23,2
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3) != 0);
	r23.s64 = r23.s32 >> 2;
	// or r28,r26,r28
	r28.u64 = r26.u64 | r28.u64;
	// or r7,r16,r7
	ctx.r7.u64 = r16.u64 | ctx.r7.u64;
	// or r30,r15,r30
	r30.u64 = r15.u64 | r30.u64;
	// or r29,r23,r29
	r29.u64 = r23.u64 | r29.u64;
	// clrlwi r28,r28,24
	r28.u64 = r28.u32 & 0xFF;
loc_8261A424:
	// stb r25,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r25.u8);
	// cmpwi cr6,r21,3
	cr6.compare<int32_t>(r21.s32, 3, xer);
	// stb r29,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r29.u8);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// stb r28,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r28.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x8261a4a4
	if (!cr6.eq) goto loc_8261A4A4;
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r25,2(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwimi r25,r26,2,22,25
	r25.u64 = (__builtin_rotateleft32(r26.u32, 2) & 0x3C0) | (r25.u64 & 0xFFFFFFFFFFFFFC3F);
	// lbz r23,4(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r11,5(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rlwinm r25,r25,0,22,27
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x3F0;
	// rlwinm r26,r23,0,26,27
	r26.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x30;
	// rlwinm r11,r11,0,26,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x30;
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// stb r25,1(r27)
	PPC_STORE_U8(r27.u32 + 1, r25.u8);
	// mr r27,r28
	r27.u64 = r28.u64;
	// or r30,r11,r30
	r30.u64 = r11.u64 | r30.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// rlwimi r28,r29,2,22,29
	r28.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0x3FC) | (r28.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwimi r27,r11,2,22,25
	r27.u64 = (__builtin_rotateleft32(r11.u32, 2) & 0x3C0) | (r27.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r29,r28,4,18,27
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0x3FF0;
	// rlwinm r11,r27,0,22,27
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x3F0;
	// or r7,r26,r7
	ctx.r7.u64 = r26.u64 | ctx.r7.u64;
	// stb r29,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r29.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stb r11,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r11.u8);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
loc_8261A4A4:
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r30,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r30.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_8261A4B4:
	// lwz r19,136(r3)
	r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// addi r11,r10,6
	r11.s64 = ctx.r10.s64 + 6;
	// add r7,r18,r24
	ctx.r7.u64 = r18.u64 + r24.u64;
	// add r3,r18,r4
	ctx.r3.u64 = r18.u64 + ctx.r4.u64;
	// cmpwi cr6,r19,1
	cr6.compare<int32_t>(r19.s32, 1, xer);
	// stw r19,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r19.u32);
	// stw r11,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, r11.u32);
	// ble cr6,0x8261aa20
	if (!cr6.gt) goto loc_8261AA20;
	// addi r10,r19,-1
	ctx.r10.s64 = r19.s64 + -1;
	// stw r10,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r10.u32);
loc_8261A4DC:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x8261a7b8
	if (!cr6.gt) goto loc_8261A7B8;
	// rlwinm r10,r19,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r19,r10
	ctx.r10.u64 = r19.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
loc_8261A4F0:
	// lbz r28,1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r27,3(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r14,r28
	r14.u64 = r28.u64;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r15,r27
	r15.u64 = r27.u64;
	// mr r16,r30
	r16.u64 = r30.u64;
	// lbz r26,4(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// mr r17,r29
	r17.u64 = r29.u64;
	// lbz r25,5(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rlwimi r15,r14,2,22,29
	r15.u64 = (__builtin_rotateleft32(r14.u32, 2) & 0x3FC) | (r15.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwimi r17,r16,2,22,29
	r17.u64 = (__builtin_rotateleft32(r16.u32, 2) & 0x3FC) | (r17.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwinm r16,r15,4,18,27
	r16.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 4) & 0x3FF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// mr r23,r30
	r23.u64 = r30.u64;
	// mr r22,r29
	r22.u64 = r29.u64;
	// mr r20,r28
	r20.u64 = r28.u64;
	// stb r16,-176(r1)
	PPC_STORE_U8(ctx.r1.u32 + -176, r16.u8);
	// rlwimi r22,r23,2,22,25
	r22.u64 = (__builtin_rotateleft32(r23.u32, 2) & 0x3C0) | (r22.u64 & 0xFFFFFFFFFFFFFC3F);
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r21,r27
	r21.u64 = r27.u64;
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mr r23,r22
	r23.u64 = r22.u64;
	// rlwinm r16,r30,0,26,27
	r16.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x30;
	// lbz r27,3(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r15,r29,0,26,27
	r15.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// lbz r28,1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// rlwinm r23,r23,0,24,27
	r23.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xF0;
	// or r16,r16,r15
	r16.u64 = r16.u64 | r15.u64;
	// rlwimi r21,r20,2,22,25
	r21.u64 = (__builtin_rotateleft32(r20.u32, 2) & 0x3C0) | (r21.u64 & 0xFFFFFFFFFFFFFC3F);
	// extsb r16,r16
	r16.s64 = r16.s8;
	// rlwinm r14,r27,0,26,27
	r14.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x30;
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// mr r22,r21
	r22.u64 = r21.u64;
	// or r23,r16,r23
	r23.u64 = r16.u64 | r23.u64;
	// mr r21,r26
	r21.u64 = r26.u64;
	// mr r19,r26
	r19.u64 = r26.u64;
	// lbz r26,4(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// srawi r15,r14,2
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x3) != 0);
	r15.s64 = r14.s32 >> 2;
	// rlwinm r14,r28,0,26,27
	r14.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// stb r23,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r23.u8);
	// rlwinm r23,r26,4,26,27
	r23.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0x30;
	// or r16,r15,r14
	r16.u64 = r15.u64 | r14.u64;
	// rlwinm r21,r21,6,24,25
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 6) & 0xC0;
	// rlwinm r26,r26,0,26,27
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x30;
	// rlwinm r19,r19,2,24,25
	r19.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xC0;
	// rlwimi r30,r29,2,28,29
	r30.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0xC) | (r30.u64 & 0xFFFFFFFFFFFFFFF3);
	// extsb r16,r16
	r16.s64 = r16.s8;
	// or r21,r23,r21
	r21.u64 = r23.u64 | r21.u64;
	// or r23,r26,r19
	r23.u64 = r26.u64 | r19.u64;
	// rlwimi r27,r28,2,28,29
	r27.u64 = (__builtin_rotateleft32(r28.u32, 2) & 0xC) | (r27.u64 & 0xFFFFFFFFFFFFFFF3);
	// addi r26,r24,1
	r26.s64 = r24.s64 + 1;
	// rlwinm r22,r22,0,24,27
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0xF0;
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// clrlwi r30,r30,28
	r30.u64 = r30.u32 & 0xF;
	// rlwinm r24,r17,4,24,27
	r24.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 4) & 0xF0;
	// mr r20,r25
	r20.u64 = r25.u64;
	// mr r18,r25
	r18.u64 = r25.u64;
	// lbz r25,5(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// clrlwi r28,r27,28
	r28.u64 = r27.u32 & 0xF;
	// or r22,r16,r22
	r22.u64 = r16.u64 | r22.u64;
	// or r30,r30,r24
	r30.u64 = r30.u64 | r24.u64;
	// rlwinm r16,r25,4,26,27
	r16.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 4) & 0x30;
	// rlwinm r20,r20,6,24,25
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 6) & 0xC0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r18,r18,2,24,25
	r18.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xC0;
	// stb r22,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r22.u8);
	// rlwinm r25,r25,0,26,27
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x30;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// or r20,r16,r20
	r20.u64 = r16.u64 | r20.u64;
	// or r22,r25,r18
	r22.u64 = r25.u64 | r18.u64;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// clrlwi r19,r21,24
	r19.u64 = r21.u32 & 0xFF;
	// clrlwi r18,r20,24
	r18.u64 = r20.u32 & 0xFF;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lbz r29,-176(r1)
	r29.u64 = PPC_LOAD_U8(ctx.r1.u32 + -176);
	// or r29,r28,r29
	r29.u64 = r28.u64 | r29.u64;
	// stb r29,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r29.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r21,r30
	r21.u64 = r30.u64;
	// lbz r27,3(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbz r28,1(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// mr r20,r29
	r20.u64 = r29.u64;
	// lbz r25,4(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// mr r17,r29
	r17.u64 = r29.u64;
	// lbz r24,5(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// mr r14,r27
	r14.u64 = r27.u64;
	// rlwimi r27,r28,2,22,29
	r27.u64 = (__builtin_rotateleft32(r28.u32, 2) & 0x3FC) | (r27.u64 & 0xFFFFFFFFFFFFFC03);
	// mr r15,r28
	r15.u64 = r28.u64;
	// rlwimi r20,r21,2,22,25
	r20.u64 = (__builtin_rotateleft32(r21.u32, 2) & 0x3C0) | (r20.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r28,r25,2,28,29
	r28.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xC;
	// lbz r29,2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rlwimi r17,r30,2,22,29
	r17.u64 = (__builtin_rotateleft32(r30.u32, 2) & 0x3FC) | (r17.u64 & 0xFFFFFFFFFFFFFC03);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r27,r27,4,18,27
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0x3FF0;
	// or r19,r28,r19
	r19.u64 = r28.u64 | r19.u64;
	// mr r21,r20
	r21.u64 = r20.u64;
	// mr r20,r17
	r20.u64 = r17.u64;
	// mr r28,r27
	r28.u64 = r27.u64;
	// rlwinm r17,r25,0,26,27
	r17.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x30;
	// mr r27,r19
	r27.u64 = r19.u64;
	// rlwinm r16,r24,0,26,27
	r16.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x30;
	// rlwinm r25,r24,2,28,29
	r25.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xC;
	// rlwinm r19,r29,0,26,27
	r19.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// srawi r17,r17,2
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x3) != 0);
	r17.s64 = r17.s32 >> 2;
	// or r24,r25,r18
	r24.u64 = r25.u64 | r18.u64;
	// srawi r16,r16,2
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x3) != 0);
	r16.s64 = r16.s32 >> 2;
	// rlwinm r18,r30,0,26,27
	r18.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x30;
	// srawi r19,r19,2
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x3) != 0);
	r19.s64 = r19.s32 >> 2;
	// rlwimi r29,r30,2,28,29
	r29.u64 = (__builtin_rotateleft32(r30.u32, 2) & 0xC) | (r29.u64 & 0xFFFFFFFFFFFFFFF3);
	// or r19,r19,r18
	r19.u64 = r19.u64 | r18.u64;
	// rlwinm r30,r20,4,24,27
	r30.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 4) & 0xF0;
	// extsb r20,r19
	r20.s64 = r19.s8;
	// rlwinm r21,r21,0,24,27
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0xF0;
	// srawi r20,r20,2
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3) != 0);
	r20.s64 = r20.s32 >> 2;
	// clrlwi r29,r29,28
	r29.u64 = r29.u32 & 0xF;
	// or r21,r20,r21
	r21.u64 = r20.u64 | r21.u64;
	// or r30,r29,r30
	r30.u64 = r29.u64 | r30.u64;
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// or r23,r17,r23
	r23.u64 = r17.u64 | r23.u64;
	// mr r20,r30
	r20.u64 = r30.u64;
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// clrlwi r17,r24,24
	r17.u64 = r24.u32 & 0xFF;
	// stb r21,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r21.u8);
	// addi r24,r26,1
	r24.s64 = r26.s64 + 1;
	// rlwinm r26,r30,0,26,27
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x30;
	// rlwinm r21,r29,0,26,27
	r21.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// stb r20,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r20.u8);
	// rlwimi r30,r29,2,28,29
	r30.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0xC) | (r30.u64 & 0xFFFFFFFFFFFFFFF3);
	// or r26,r26,r21
	r26.u64 = r26.u64 | r21.u64;
	// rlwimi r14,r15,2,22,25
	r14.u64 = (__builtin_rotateleft32(r15.u32, 2) & 0x3C0) | (r14.u64 & 0xFFFFFFFFFFFFFC3F);
	// extsb r26,r26
	r26.s64 = r26.s8;
	// clrlwi r19,r28,24
	r19.u64 = r28.u32 & 0xFF;
	// lbz r28,4(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// clrlwi r18,r27,24
	r18.u64 = r27.u32 & 0xFF;
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// clrlwi r30,r30,28
	r30.u64 = r30.u32 & 0xF;
	// srawi r26,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r26.s64 = r26.s32 >> 2;
	// rlwinm r25,r14,0,24,27
	r25.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 0) & 0xF0;
	// rlwinm r21,r28,0,26,27
	r21.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// or r20,r30,r19
	r20.u64 = r30.u64 | r19.u64;
	// or r30,r26,r25
	r30.u64 = r26.u64 | r25.u64;
	// rlwinm r29,r27,0,26,27
	r29.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x30;
	// or r22,r16,r22
	r22.u64 = r16.u64 | r22.u64;
	// srawi r21,r21,4
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xF) != 0);
	r21.s64 = r21.s32 >> 4;
	// clrlwi r16,r28,30
	r16.u64 = r28.u32 & 0x3;
	// clrlwi r15,r27,30
	r15.u64 = r27.u32 & 0x3;
	// stb r30,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r30.u8);
	// srawi r19,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r19.s64 = r29.s32 >> 4;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// or r28,r21,r23
	r28.u64 = r21.u64 | r23.u64;
	// or r27,r19,r22
	r27.u64 = r19.u64 | r22.u64;
	// or r26,r16,r18
	r26.u64 = r16.u64 | r18.u64;
	// or r25,r15,r17
	r25.u64 = r15.u64 | r17.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stb r20,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r20.u8);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r28,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r28.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// stb r27,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r27.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r26,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r26.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r25,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r25.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// bne cr6,0x8261a4f0
	if (!cr6.eq) goto loc_8261A4F0;
	// lwz r20,84(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r18,-168(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// lwz r31,-172(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// lwz r19,-160(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
loc_8261A7B8:
	// clrlwi r10,r20,30
	ctx.r10.u64 = r20.u32 & 0x3;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261a9f0
	if (cr6.eq) goto loc_8261A9F0;
	// rlwinm r26,r19,1,0,30
	r26.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rlwinm r25,r20,0,30,30
	r25.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x2;
	// add r23,r19,r26
	r23.u64 = r19.u64 + r26.u64;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r30,2(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mr r21,r29
	r21.u64 = r29.u64;
	// mr r17,r28
	r17.u64 = r28.u64;
	// lbz r27,4(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rlwinm r22,r23,1,0,30
	r22.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r26,5(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// cmpwi cr6,r25,2
	cr6.compare<int32_t>(r25.s32, 2, xer);
	// mr r23,r30
	r23.u64 = r30.u64;
	// mr r25,r10
	r25.u64 = ctx.r10.u64;
	// rlwimi r28,r29,2,22,29
	r28.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0x3FC) | (r28.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwimi r30,r10,2,22,29
	r30.u64 = (__builtin_rotateleft32(ctx.r10.u32, 2) & 0x3FC) | (r30.u64 & 0xFFFFFFFFFFFFFC03);
	// rlwimi r17,r21,2,22,25
	r17.u64 = (__builtin_rotateleft32(r21.u32, 2) & 0x3C0) | (r17.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwimi r23,r25,2,22,25
	r23.u64 = (__builtin_rotateleft32(r25.u32, 2) & 0x3C0) | (r23.u64 & 0xFFFFFFFFFFFFFC3F);
	// mr r29,r17
	r29.u64 = r17.u64;
	// rlwinm r30,r30,4,18,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0x3FF0;
	// rlwinm r28,r28,4,18,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0x3FF0;
	// rlwinm r21,r27,2,22,25
	r21.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0x3C0;
	// rlwinm r17,r26,2,22,25
	r17.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0x3C0;
	// rotlwi r16,r27,6
	r16.u64 = __builtin_rotateleft32(r27.u32, 6);
	// rotlwi r15,r26,6
	r15.u64 = __builtin_rotateleft32(r26.u32, 6);
	// rlwinm r27,r23,0,24,27
	r27.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0xF0;
	// clrlwi r26,r30,24
	r26.u64 = r30.u32 & 0xFF;
	// rlwinm r25,r29,0,24,27
	r25.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0xF0;
	// clrlwi r23,r28,24
	r23.u64 = r28.u32 & 0xFF;
	// clrlwi r10,r21,24
	ctx.r10.u64 = r21.u32 & 0xFF;
	// clrlwi r30,r17,24
	r30.u64 = r17.u32 & 0xFF;
	// clrlwi r29,r16,24
	r29.u64 = r16.u32 & 0xFF;
	// clrlwi r28,r15,24
	r28.u64 = r15.u32 & 0xFF;
	// add r11,r22,r11
	r11.u64 = r22.u64 + r11.u64;
	// bne cr6,0x8261a914
	if (!cr6.eq) goto loc_8261A914;
	// lbz r21,2(r11)
	r21.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// clrlwi r18,r23,24
	r18.u64 = r23.u32 & 0xFF;
	// clrlwi r31,r26,24
	r31.u64 = r26.u32 & 0xFF;
	// lbz r20,0(r11)
	r20.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// clrlwi r17,r29,24
	r17.u64 = r29.u32 & 0xFF;
	// lbz r23,1(r11)
	r23.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// clrlwi r16,r28,24
	r16.u64 = r28.u32 & 0xFF;
	// lbz r26,3(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r29,4(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rlwinm r15,r20,0,26,27
	r15.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x30;
	// lbz r28,5(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r11,r22,r11
	r11.u64 = r22.u64 + r11.u64;
	// rlwinm r22,r21,0,26,27
	r22.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0x30;
	// rlwinm r14,r26,0,26,27
	r14.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 0) & 0x30;
	// srawi r22,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r22.s64 = r22.s32 >> 2;
	// rlwimi r21,r20,2,28,29
	r21.u64 = (__builtin_rotateleft32(r20.u32, 2) & 0xC) | (r21.u64 & 0xFFFFFFFFFFFFFFF3);
	// or r22,r22,r15
	r22.u64 = r22.u64 | r15.u64;
	// rlwimi r26,r23,2,28,29
	r26.u64 = (__builtin_rotateleft32(r23.u32, 2) & 0xC) | (r26.u64 & 0xFFFFFFFFFFFFFFF3);
	// extsb r22,r22
	r22.s64 = r22.s8;
	// clrlwi r26,r26,28
	r26.u64 = r26.u32 & 0xF;
	// srawi r22,r22,2
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3) != 0);
	r22.s64 = r22.s32 >> 2;
	// srawi r15,r14,2
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x3) != 0);
	r15.s64 = r14.s32 >> 2;
	// rlwinm r14,r23,0,26,27
	r14.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x30;
	// clrlwi r23,r21,28
	r23.u64 = r21.u32 & 0xF;
	// or r20,r15,r14
	r20.u64 = r15.u64 | r14.u64;
	// or r31,r23,r31
	r31.u64 = r23.u64 | r31.u64;
	// extsb r21,r20
	r21.s64 = r20.s8;
	// rlwinm r20,r29,4,26,27
	r20.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 4) & 0x30;
	// rlwinm r15,r28,4,26,27
	r15.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0x30;
	// srawi r23,r21,2
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3) != 0);
	r23.s64 = r21.s32 >> 2;
	// or r21,r26,r18
	r21.u64 = r26.u64 | r18.u64;
	// rlwinm r29,r29,0,26,27
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x30;
	// rlwinm r28,r28,0,26,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x30;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// clrlwi r30,r30,24
	r30.u64 = r30.u32 & 0xFF;
	// or r20,r20,r17
	r20.u64 = r20.u64 | r17.u64;
	// or r18,r15,r16
	r18.u64 = r15.u64 | r16.u64;
	// or r10,r29,r10
	ctx.r10.u64 = r29.u64 | ctx.r10.u64;
	// or r30,r28,r30
	r30.u64 = r28.u64 | r30.u64;
	// clrlwi r26,r31,24
	r26.u64 = r31.u32 & 0xFF;
	// lwz r31,-172(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// or r25,r23,r25
	r25.u64 = r23.u64 | r25.u64;
	// clrlwi r29,r20,24
	r29.u64 = r20.u32 & 0xFF;
	// lwz r20,84(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// clrlwi r28,r18,24
	r28.u64 = r18.u32 & 0xFF;
	// lwz r18,-168(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// or r27,r22,r27
	r27.u64 = r22.u64 | r27.u64;
	// clrlwi r23,r21,24
	r23.u64 = r21.u32 & 0xFF;
loc_8261A914:
	// clrlwi r22,r20,30
	r22.u64 = r20.u32 & 0x3;
	// stb r27,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r27.u8);
	// stb r26,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r26.u8);
	// stb r25,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r25.u8);
	// cmpwi cr6,r22,3
	cr6.compare<int32_t>(r22.s32, 3, xer);
	// stb r23,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r23.u8);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x8261a9d0
	if (!cr6.eq) goto loc_8261A9D0;
	// lbz r26,2(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// clrlwi r22,r28,24
	r22.u64 = r28.u32 & 0xFF;
	// lbz r27,0(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// clrlwi r23,r29,24
	r23.u64 = r29.u32 & 0xFF;
	// lbz r28,3(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// mr r17,r26
	r17.u64 = r26.u64;
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rlwimi r26,r27,2,22,29
	r26.u64 = (__builtin_rotateleft32(r27.u32, 2) & 0x3FC) | (r26.u64 & 0xFFFFFFFFFFFFFC03);
	// mr r15,r28
	r15.u64 = r28.u64;
	// lbz r25,4(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rlwimi r28,r29,2,22,29
	r28.u64 = (__builtin_rotateleft32(r29.u32, 2) & 0x3FC) | (r28.u64 & 0xFFFFFFFFFFFFFC03);
	// lbz r11,5(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// mr r21,r27
	r21.u64 = r27.u64;
	// mr r16,r29
	r16.u64 = r29.u64;
	// rlwinm r26,r26,4,18,27
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 4) & 0x3FF0;
	// rlwinm r28,r28,4,18,27
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0x3FF0;
	// rlwimi r17,r21,2,22,25
	r17.u64 = (__builtin_rotateleft32(r21.u32, 2) & 0x3C0) | (r17.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwimi r15,r16,2,22,25
	r15.u64 = (__builtin_rotateleft32(r16.u32, 2) & 0x3C0) | (r15.u64 & 0xFFFFFFFFFFFFFC3F);
	// rlwinm r27,r25,0,26,27
	r27.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x30;
	// rlwinm r29,r11,0,26,27
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x30;
	// stb r26,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, r26.u8);
	// rlwinm r21,r17,0,22,27
	r21.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x3F0;
	// stb r28,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r28.u8);
	// rlwinm r17,r15,0,22,27
	r17.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 0) & 0x3F0;
	// rlwinm r25,r25,2,28,29
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xC;
	// srawi r4,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	ctx.r4.s64 = r27.s32 >> 2;
	// rlwinm r11,r11,2,28,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xC;
	// srawi r29,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	r29.s64 = r29.s32 >> 2;
	// stb r21,1(r24)
	PPC_STORE_U8(r24.u32 + 1, r21.u8);
	// or r28,r25,r23
	r28.u64 = r25.u64 | r23.u64;
	// stb r17,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, r17.u8);
	// or r11,r11,r22
	r11.u64 = r11.u64 | r22.u64;
	// or r30,r29,r30
	r30.u64 = r29.u64 | r30.u64;
	// clrlwi r29,r28,24
	r29.u64 = r28.u32 & 0xFF;
	// or r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 | ctx.r10.u64;
	// clrlwi r28,r11,24
	r28.u64 = r11.u32 & 0xFF;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
loc_8261A9D0:
	// stb r29,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r29.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r28,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r28.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r10,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r10.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r30,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r30.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_8261A9F0:
	// lwz r11,-164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// lwz r11,76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// add r7,r18,r7
	ctx.r7.u64 = r18.u64 + ctx.r7.u64;
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// add r3,r18,r3
	ctx.r3.u64 = r18.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r10,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r10.u32);
	// stw r11,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, r11.u32);
	// bne cr6,0x8261a4dc
	if (!cr6.eq) goto loc_8261A4DC;
loc_8261AA20:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8261AA24"))) PPC_WEAK_FUNC(sub_8261AA24);
PPC_FUNC_IMPL(__imp__sub_8261AA24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261AA28"))) PPC_WEAK_FUNC(sub_8261AA28);
PPC_FUNC_IMPL(__imp__sub_8261AA28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8261aa58
	if (!cr6.gt) goto loc_8261AA58;
loc_8261AA34:
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// blt cr6,0x8261aa34
	if (cr6.lt) goto loc_8261AA34;
loc_8261AA58:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8261aa88
	if (!cr6.gt) goto loc_8261AA88;
loc_8261AA64:
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// blt cr6,0x8261aa64
	if (cr6.lt) goto loc_8261AA64;
loc_8261AA88:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
loc_8261AA94:
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// blt cr6,0x8261aa94
	if (cr6.lt) goto loc_8261AA94;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261AABC"))) PPC_WEAK_FUNC(sub_8261AABC);
PPC_FUNC_IMPL(__imp__sub_8261AABC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261AAC0"))) PPC_WEAK_FUNC(sub_8261AAC0);
PPC_FUNC_IMPL(__imp__sub_8261AAC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// ble cr6,0x8261aafc
	if (!cr6.gt) goto loc_8261AAFC;
loc_8261AAD4:
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r3,r3,-64
	ctx.r3.s64 = ctx.r3.s64 + -64;
	// rlwinm r31,r3,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,3384(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r3,r31,r3
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + ctx.r3.u32);
	// stb r3,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r3.u8);
	// blt cr6,0x8261aad4
	if (cr6.lt) goto loc_8261AAD4;
loc_8261AAFC:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8261ab30
	if (!cr6.gt) goto loc_8261AB30;
loc_8261AB08:
	// add r9,r11,r5
	ctx.r9.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r7,r7,-64
	ctx.r7.s64 = ctx.r7.s64 + -64;
	// rlwinm r4,r7,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,3384(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r7,r4,r7
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r7.u32);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// blt cr6,0x8261ab08
	if (cr6.lt) goto loc_8261AB08;
loc_8261AB30:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8261ab64
	if (!cr6.gt) goto loc_8261AB64;
loc_8261AB3C:
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r7,r7,-64
	ctx.r7.s64 = ctx.r7.s64 + -64;
	// rlwinm r5,r7,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,3384(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r7,r5,r7
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// blt cr6,0x8261ab3c
	if (cr6.lt) goto loc_8261AB3C;
loc_8261AB64:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261AB6C"))) PPC_WEAK_FUNC(sub_8261AB6C);
PPC_FUNC_IMPL(__imp__sub_8261AB6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261AB70"))) PPC_WEAK_FUNC(sub_8261AB70);
PPC_FUNC_IMPL(__imp__sub_8261AB70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r9,216(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// mullw r7,r11,r10
	ctx.r7.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r11,14800(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14800);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mullw r8,r9,r8
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261abc8
	if (!cr6.eq) goto loc_8261ABC8;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261abec
	if (!cr6.eq) goto loc_8261ABEC;
	// bl 0x8261aa28
	sub_8261AA28(ctx, base);
	// b 0x8261abe4
	goto loc_8261ABE4;
loc_8261ABC8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261abec
	if (!cr6.eq) goto loc_8261ABEC;
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261abec
	if (!cr6.eq) goto loc_8261ABEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261aac0
	sub_8261AAC0(ctx, base);
loc_8261ABE4:
	// lwz r11,14796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14796);
	// stw r11,14800(r31)
	PPC_STORE_U32(r31.u32 + 14800, r11.u32);
loc_8261ABEC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261AC00"))) PPC_WEAK_FUNC(sub_8261AC00);
PPC_FUNC_IMPL(__imp__sub_8261AC00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// ble cr6,0x8261ac3c
	if (!cr6.gt) goto loc_8261AC3C;
loc_8261AC14:
	// add r9,r11,r3
	ctx.r9.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r8,r8,-64
	ctx.r8.s64 = ctx.r8.s64 + -64;
	// rlwinm r31,r8,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,3384(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r8,r31,r8
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + ctx.r8.u32);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// blt cr6,0x8261ac14
	if (cr6.lt) goto loc_8261AC14;
loc_8261AC3C:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8261ac70
	if (!cr6.gt) goto loc_8261AC70;
loc_8261AC48:
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r8,r8,-64
	ctx.r8.s64 = ctx.r8.s64 + -64;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,3384(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r8,r6,r8
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// blt cr6,0x8261ac48
	if (cr6.lt) goto loc_8261AC48;
loc_8261AC70:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8261aca4
	if (!cr6.gt) goto loc_8261ACA4;
loc_8261AC7C:
	// add r9,r11,r5
	ctx.r9.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r8,r8,-64
	ctx.r8.s64 = ctx.r8.s64 + -64;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,3384(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 3384);
	// lbzx r8,r6,r8
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// blt cr6,0x8261ac7c
	if (cr6.lt) goto loc_8261AC7C;
loc_8261ACA4:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261ACAC"))) PPC_WEAK_FUNC(sub_8261ACAC);
PPC_FUNC_IMPL(__imp__sub_8261ACAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261ACB0"))) PPC_WEAK_FUNC(sub_8261ACB0);
PPC_FUNC_IMPL(__imp__sub_8261ACB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,14796(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14796);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r11,216(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// lwz r10,208(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// lwz r9,212(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// lwz r8,204(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// mullw r7,r11,r10
	ctx.r7.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r5,3784(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3784);
	// mullw r6,r9,r8
	ctx.r6.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// lwz r4,3780(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3780);
	// lwz r3,3776(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3776);
	// b 0x8261ac00
	sub_8261AC00(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8261ACE4"))) PPC_WEAK_FUNC(sub_8261ACE4);
PPC_FUNC_IMPL(__imp__sub_8261ACE4) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261ACE8"))) PPC_WEAK_FUNC(sub_8261ACE8);
PPC_FUNC_IMPL(__imp__sub_8261ACE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcdc
	// lis r29,-32126
	r29.s64 = -2105409536;
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r11,r3,r6
	r11.u64 = ctx.r3.u64 + ctx.r6.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// mr r22,r23
	r22.u64 = r23.u64;
	// lwz r6,13632(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 13632);
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// addi r30,r9,-1
	r30.s64 = ctx.r9.s64 + -1;
	// subf r24,r6,r11
	r24.s64 = r11.s64 - ctx.r6.s64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// bge cr6,0x8261ada0
	if (!cr6.lt) goto loc_8261ADA0;
	// add r27,r24,r10
	r27.u64 = r24.u64 + ctx.r10.u64;
	// subf r26,r24,r11
	r26.s64 = r11.s64 - r24.s64;
	// subf r25,r4,r5
	r25.s64 = ctx.r5.s64 - ctx.r4.s64;
loc_8261AD30:
	// lbzx r11,r26,r28
	r11.u64 = PPC_LOAD_U8(r26.u32 + r28.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rotlwi r4,r11,8
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 8);
	// rotlwi r31,r5,8
	r31.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// or r11,r4,r11
	r11.u64 = ctx.r4.u64 | r11.u64;
	// or r5,r31,r5
	ctx.r5.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r4,r11,16,0,15
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r31,r5,16,0,15
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 16) & 0xFFFF0000;
	// or r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 | r11.u64;
	// or r5,r31,r5
	ctx.r5.u64 = r31.u64 | ctx.r5.u64;
	// ble cr6,0x8261ad88
	if (!cr6.gt) goto loc_8261AD88;
	// mr r11,r27
	r11.u64 = r27.u64;
	// subf r31,r27,r28
	r31.s64 = r28.s64 - r27.s64;
loc_8261AD6C:
	// stwx r4,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, ctx.r4.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,13632(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 13632);
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// blt cr6,0x8261ad6c
	if (cr6.lt) goto loc_8261AD6C;
loc_8261AD88:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// add r27,r27,r23
	r27.u64 = r27.u64 + r23.u64;
	// add r30,r30,r23
	r30.u64 = r30.u64 + r23.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x8261ad30
	if (!cr6.eq) goto loc_8261AD30;
loc_8261ADA0:
	// subf r11,r10,r23
	r11.s64 = r23.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// beq cr6,0x8261adc0
	if (cr6.eq) goto loc_8261ADC0;
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// rlwinm r10,r23,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// srawi r22,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r22.s64 = r23.s32 >> 1;
	// subf r3,r10,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r10.s64;
loc_8261ADC0:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// srawi r6,r22,3
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7) != 0);
	ctx.r6.s64 = r22.s32 >> 3;
	// beq cr6,0x8261ae14
	if (cr6.eq) goto loc_8261AE14;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8261ae14
	if (!cr6.gt) goto loc_8261AE14;
	// subf r9,r24,r3
	ctx.r9.s64 = ctx.r3.s64 - r24.s64;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_8261ADDC:
	// mr r11,r24
	r11.u64 = r24.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x8261ae04
	if (!cr6.gt) goto loc_8261AE04;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_8261ADEC:
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stdx r5,r9,r11
	PPC_STORE_U64(ctx.r9.u32 + r11.u32, ctx.r5.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x8261adec
	if (!cr6.eq) goto loc_8261ADEC;
loc_8261AE04:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x8261addc
	if (!cr6.eq) goto loc_8261ADDC;
loc_8261AE14:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// beq cr6,0x8261ae64
	if (cr6.eq) goto loc_8261AE64;
	// subf r7,r23,r28
	ctx.r7.s64 = r28.s64 - r23.s64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x8261ae64
	if (!cr6.gt) goto loc_8261AE64;
	// subf r9,r7,r28
	ctx.r9.s64 = r28.s64 - ctx.r7.s64;
loc_8261AE2C:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x8261ae54
	if (!cr6.gt) goto loc_8261AE54;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_8261AE3C:
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stdx r5,r9,r11
	PPC_STORE_U64(ctx.r9.u32 + r11.u32, ctx.r5.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x8261ae3c
	if (!cr6.eq) goto loc_8261AE3C;
loc_8261AE54:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// add r9,r9,r23
	ctx.r9.u64 = ctx.r9.u64 + r23.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x8261ae2c
	if (!cr6.eq) goto loc_8261AE2C;
loc_8261AE64:
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_8261AE68"))) PPC_WEAK_FUNC(sub_8261AE68);
PPC_FUNC_IMPL(__imp__sub_8261AE68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lis r20,-32126
	r20.s64 = -2105409536;
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r15,84(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r30,r3,r7
	r30.u64 = ctx.r3.u64 + ctx.r7.u64;
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// add r29,r4,r7
	r29.u64 = ctx.r4.u64 + ctx.r7.u64;
	// subf r11,r15,r24
	r11.s64 = r24.s64 - r15.s64;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r7,3380(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + 3380);
	// cmpwi cr6,r11,16
	cr6.compare<int32_t>(r11.s32, 16, xer);
	// add r11,r30,r10
	r11.u64 = r30.u64 + ctx.r10.u64;
	// subf r16,r7,r30
	r16.s64 = r30.s64 - ctx.r7.s64;
	// subf r17,r7,r29
	r17.s64 = r29.s64 - ctx.r7.s64;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// stw r9,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r9.u32);
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// addi r23,r11,-1
	r23.s64 = r11.s64 + -1;
	// addi r22,r10,-1
	r22.s64 = ctx.r10.s64 + -1;
	// mr r19,r16
	r19.u64 = r16.u64;
	// mr r21,r17
	r21.u64 = r17.u64;
	// beq cr6,0x8261aef4
	if (cr6.eq) goto loc_8261AEF4;
	// srawi r10,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 1;
	// clrlwi r11,r6,29
	r11.u64 = ctx.r6.u32 & 0x7;
	// srawi r8,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	ctx.r8.s64 = r24.s32 >> 1;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// subf r3,r24,r3
	ctx.r3.s64 = ctx.r3.s64 - r24.s64;
	// subf r4,r24,r4
	ctx.r4.s64 = ctx.r4.s64 - r24.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261aef4
	if (cr6.eq) goto loc_8261AEF4;
	// subfic r11,r11,8
	xer.ca = r11.u32 <= 8;
	r11.s64 = 8 - r11.s64;
	// stw r11,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r11.u32);
loc_8261AEF4:
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// bge cr6,0x8261afc0
	if (!cr6.lt) goto loc_8261AFC0;
	// subf r14,r17,r16
	r14.s64 = r16.s64 - r17.s64;
	// subf r18,r5,r6
	r18.s64 = ctx.r6.s64 - ctx.r5.s64;
loc_8261AF04:
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lbz r5,0(r23)
	ctx.r5.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// rotlwi r28,r11,8
	r28.u64 = __builtin_rotateleft32(r11.u32, 8);
	// lbz r31,0(r22)
	r31.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// rotlwi r27,r6,8
	r27.u64 = __builtin_rotateleft32(ctx.r6.u32, 8);
	// rotlwi r26,r5,8
	r26.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// rotlwi r25,r31,8
	r25.u64 = __builtin_rotateleft32(r31.u32, 8);
	// or r11,r28,r11
	r11.u64 = r28.u64 | r11.u64;
	// or r6,r27,r6
	ctx.r6.u64 = r27.u64 | ctx.r6.u64;
	// or r5,r26,r5
	ctx.r5.u64 = r26.u64 | ctx.r5.u64;
	// or r31,r25,r31
	r31.u64 = r25.u64 | r31.u64;
	// rlwinm r28,r11,16,0,15
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r27,r6,16,0,15
	r27.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r26,r5,16,0,15
	r26.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r25,r31,16,0,15
	r25.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF0000;
	// or r28,r28,r11
	r28.u64 = r28.u64 | r11.u64;
	// or r27,r27,r6
	r27.u64 = r27.u64 | ctx.r6.u64;
	// or r26,r26,r5
	r26.u64 = r26.u64 | ctx.r5.u64;
	// or r25,r25,r31
	r25.u64 = r25.u64 | r31.u64;
	// ble cr6,0x8261af9c
	if (!cr6.gt) goto loc_8261AF9C;
	// add r7,r21,r15
	ctx.r7.u64 = r21.u64 + r15.u64;
	// subf r5,r21,r14
	ctx.r5.s64 = r14.s64 - r21.s64;
	// mr r11,r21
	r11.u64 = r21.u64;
	// subf r6,r21,r19
	ctx.r6.s64 = r19.s64 - r21.s64;
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r31,r21,r7
	r31.s64 = ctx.r7.s64 - r21.s64;
loc_8261AF78:
	// stwx r28,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, r28.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// stwx r26,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + r11.u32, r26.u32);
	// stwx r25,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, r25.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r7,3380(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + 3380);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x8261af78
	if (cr6.lt) goto loc_8261AF78;
loc_8261AF9C:
	// addi r18,r18,-1
	r18.s64 = r18.s64 + -1;
	// add r19,r19,r24
	r19.u64 = r19.u64 + r24.u64;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// add r30,r30,r24
	r30.u64 = r30.u64 + r24.u64;
	// add r29,r29,r24
	r29.u64 = r29.u64 + r24.u64;
	// add r23,r23,r24
	r23.u64 = r23.u64 + r24.u64;
	// add r22,r22,r24
	r22.u64 = r22.u64 + r24.u64;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// bne cr6,0x8261af04
	if (!cr6.eq) goto loc_8261AF04;
loc_8261AFC0:
	// lwz r11,60(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// srawi r29,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r29.s64 = ctx.r8.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b034
	if (cr6.eq) goto loc_8261B034;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8261b034
	if (!cr6.gt) goto loc_8261B034;
	// neg r30,r24
	r30.s64 = -r24.s64;
	// subf r8,r3,r17
	ctx.r8.s64 = r17.s64 - ctx.r3.s64;
	// subf r31,r17,r16
	r31.s64 = r16.s64 - r17.s64;
	// subf r5,r3,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r3.s64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
loc_8261AFEC:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8261b020
	if (!cr6.gt) goto loc_8261B020;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// add r7,r8,r31
	ctx.r7.u64 = ctx.r8.u64 + r31.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_8261B000:
	// lwzx r4,r7,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// lwzx r4,r8,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// stwx r4,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + r11.u32, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x8261b000
	if (!cr6.eq) goto loc_8261B000;
loc_8261B020:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r3,r3,r24
	ctx.r3.u64 = ctx.r3.u64 + r24.u64;
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8261afec
	if (!cr6.eq) goto loc_8261AFEC;
loc_8261B034:
	// lwz r11,68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b0b8
	if (cr6.eq) goto loc_8261B0B8;
	// lwz r11,-160(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// subf r8,r24,r19
	ctx.r8.s64 = r19.s64 - r24.s64;
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// subf r11,r24,r21
	r11.s64 = r21.s64 - r24.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8261b0b8
	if (!cr6.gt) goto loc_8261B0B8;
	// neg r4,r24
	ctx.r4.s64 = -r24.s64;
	// subf r9,r19,r11
	ctx.r9.s64 = r11.s64 - r19.s64;
	// subf r5,r11,r8
	ctx.r5.s64 = ctx.r8.s64 - r11.s64;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
loc_8261B068:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8261b0a0
	if (!cr6.gt) goto loc_8261B0A0;
	// mr r11,r19
	r11.u64 = r19.u64;
	// add r8,r9,r5
	ctx.r8.u64 = ctx.r9.u64 + ctx.r5.u64;
	// subf r7,r19,r21
	ctx.r7.s64 = r21.s64 - r19.s64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_8261B080:
	// lwzx r3,r8,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// lwzx r3,r9,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// stwx r3,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x8261b080
	if (!cr6.eq) goto loc_8261B080;
loc_8261B0A0:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r19,r19,r24
	r19.u64 = r19.u64 + r24.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8261b068
	if (!cr6.eq) goto loc_8261B068;
loc_8261B0B8:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8261B0BC"))) PPC_WEAK_FUNC(sub_8261B0BC);
PPC_FUNC_IMPL(__imp__sub_8261B0BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261B0C0"))) PPC_WEAK_FUNC(sub_8261B0C0);
PPC_FUNC_IMPL(__imp__sub_8261B0C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r26,-32126
	r26.s64 = -2105409536;
	// lwz r10,21212(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21212);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r9,216(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r25,r9,r8
	r25.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// beq cr6,0x8261b160
	if (cr6.eq) goto loc_8261B160;
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261b10c
	if (!cr6.eq) goto loc_8261B10C;
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// b 0x8261b110
	goto loc_8261B110;
loc_8261B10C:
	// lwz r8,3776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3776);
loc_8261B110:
	// lwz r9,21220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21220);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,3776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// addi r6,r9,8
	ctx.r6.s64 = ctx.r9.s64 + 8;
	// ble cr6,0x8261b1a8
	if (!cr6.gt) goto loc_8261B1A8;
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - r11.s64;
loc_8261B128:
	// lbzx r9,r7,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// srawi r8,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 3;
	// lwz r9,3384(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 3384);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r9,128(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 128);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8261b128
	if (!cr6.eq) goto loc_8261B128;
	// b 0x8261b1a8
	goto loc_8261B1A8;
loc_8261B160:
	// lwz r11,220(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r27,188(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r9,3720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r10,3776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// lwz r28,204(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r29,r9,r11
	r29.u64 = ctx.r9.u64 + r11.u64;
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// ble cr6,0x8261b1a8
	if (!cr6.gt) goto loc_8261B1A8;
loc_8261B184:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r29,r28,r29
	r29.u64 = r28.u64 + r29.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8261b184
	if (!cr6.eq) goto loc_8261B184;
loc_8261B1A8:
	// lwz r11,21216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b274
	if (cr6.eq) goto loc_8261B274;
	// lwz r11,15564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261b1cc
	if (!cr6.eq) goto loc_8261B1CC;
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r5,3728(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// b 0x8261b1d4
	goto loc_8261B1D4;
loc_8261B1CC:
	// lwz r9,3780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r5,3784(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3784);
loc_8261B1D4:
	// lwz r10,21224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21224);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lwz r11,3780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// addi r7,r10,8
	ctx.r7.s64 = ctx.r10.s64 + 8;
	// ble cr6,0x8261b224
	if (!cr6.gt) goto loc_8261B224;
	// subf r6,r11,r9
	ctx.r6.s64 = ctx.r9.s64 - r11.s64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_8261B1F0:
	// lbzx r9,r6,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// srawi r8,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 3;
	// lwz r9,3384(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 3384);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r9,128(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 128);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8261b1f0
	if (!cr6.eq) goto loc_8261B1F0;
loc_8261B224:
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8261b304
	if (!cr6.gt) goto loc_8261B304;
	// subf r6,r11,r5
	ctx.r6.s64 = ctx.r5.s64 - r11.s64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_8261B238:
	// lbzx r9,r6,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// srawi r8,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 3;
	// lwz r9,3384(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 3384);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbz r9,128(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 128);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8261b238
	if (!cr6.eq) goto loc_8261B238;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_8261B274:
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r10,3780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r27,200(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// add r29,r9,r11
	r29.u64 = ctx.r9.u64 + r11.u64;
	// lwz r28,208(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x8261b2bc
	if (!cr6.gt) goto loc_8261B2BC;
loc_8261B298:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r29,r28,r29
	r29.u64 = r28.u64 + r29.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8261b298
	if (!cr6.eq) goto loc_8261B298;
loc_8261B2BC:
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r28,200(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r10,3784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// lwz r29,208(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r30,r9,r11
	r30.u64 = ctx.r9.u64 + r11.u64;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ble cr6,0x8261b304
	if (!cr6.gt) goto loc_8261B304;
loc_8261B2E0:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// add r31,r29,r31
	r31.u64 = r29.u64 + r31.u64;
	// add r30,r29,r30
	r30.u64 = r29.u64 + r30.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8261b2e0
	if (!cr6.eq) goto loc_8261B2E0;
loc_8261B304:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8261B30C"))) PPC_WEAK_FUNC(sub_8261B30C);
PPC_FUNC_IMPL(__imp__sub_8261B30C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261B310"))) PPC_WEAK_FUNC(sub_8261B310);
PPC_FUNC_IMPL(__imp__sub_8261B310) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261b4a0
	if (cr6.eq) goto loc_8261B4A0;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261b498
	if (!cr6.eq) goto loc_8261B498;
	// lwz r11,15052(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15052);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x8261b54c
	if (!cr6.eq) goto loc_8261B54C;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,3720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r29,15856(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// li r8,1
	ctx.r8.s64 = 1;
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,172(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r30,196(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// li r8,1
	ctx.r8.s64 = 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r10,176(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// lwz r4,3728(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// srawi r6,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r10.s32 >> 1;
	// lwz r29,15852(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// rlwinm r30,r10,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// lwz r29,15856(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// rlwinm r30,r10,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// lwz r4,3728(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// li r5,0
	ctx.r5.s64 = 0;
	// srawi r6,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 1;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// lwz r31,15852(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
loc_8261B498:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8261b4a4
	if (!cr6.eq) goto loc_8261B4A4;
loc_8261B4A0:
	// li r30,0
	r30.s64 = 0;
loc_8261B4A4:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// li r8,1
	ctx.r8.s64 = 1;
	// rlwinm r29,r10,27,31,31
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// lwz r28,15856(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 15856);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// mtctr r28
	ctr.u64 = r28.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r29,196(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// li r5,0
	ctx.r5.s64 = 0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lwz r4,3728(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// rlwinm r28,r10,27,31,31
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// lwz r31,15852(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 15852);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8261B54C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8261B554"))) PPC_WEAK_FUNC(sub_8261B554);
PPC_FUNC_IMPL(__imp__sub_8261B554) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261B558"))) PPC_WEAK_FUNC(sub_8261B558);
PPC_FUNC_IMPL(__imp__sub_8261B558) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// lwz r6,3688(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3688);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r8,3724(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r28,3704(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3704);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r27,608(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + 608);
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + r11.u64;
	// lwz r10,3720(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r25,204(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r29,3780(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// srawi r10,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r10.s64 = r25.s32 >> 1;
	// lwz r30,3784(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r7,3728(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r5,3776(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// stw r27,608(r28)
	PPC_STORE_U32(r28.u32 + 608, r27.u32);
	// srawi r28,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r28.s64 = ctx.r8.s32 >> 1;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r27,200(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// mullw r8,r10,r4
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// add r9,r29,r11
	ctx.r9.u64 = r29.u64 + r11.u64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// mullw r10,r28,r4
	ctx.r10.s64 = int64_t(r28.s32) * int64_t(ctx.r4.s32);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// add r24,r3,r8
	r24.u64 = ctx.r3.u64 + ctx.r8.u64;
	// add r25,r5,r8
	r25.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r29,r6,r10
	r29.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r27,r7,r10
	r27.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x8261b678
	if (!cr6.gt) goto loc_8261B678;
loc_8261B5F0:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// srawi r5,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r5.s64 = r11.s32 >> 1;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// srawi r5,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r5.s64 = r11.s32 >> 1;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// srawi r5,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r5.s64 = r11.s32 >> 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x8261b5f0
	if (cr6.lt) goto loc_8261B5F0;
loc_8261B678:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8261B680"))) PPC_WEAK_FUNC(sub_8261B680);
PPC_FUNC_IMPL(__imp__sub_8261B680) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// lwz r11,21236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b7f4
	if (cr6.eq) goto loc_8261B7F4;
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b6e8
	if (cr6.eq) goto loc_8261B6E8;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b6e8
	if (cr6.eq) goto loc_8261B6E8;
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261b6e8
	if (!cr6.eq) goto loc_8261B6E8;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8261b6ec
	goto loc_8261B6EC;
loc_8261B6E8:
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
loc_8261B6EC:
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r20,284(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// addi r28,r11,-1
	r28.s64 = r11.s64 + -1;
	// cmpw cr6,r28,r20
	cr6.compare<int32_t>(r28.s32, r20.s32, xer);
	// bge cr6,0x8261ba78
	if (!cr6.lt) goto loc_8261BA78;
	// lwz r27,292(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// rlwinm r25,r28,2,0,29
	r25.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r21,r28,1
	r21.s64 = r28.s64 + 1;
loc_8261B710:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8261b72c
	if (cr6.eq) goto loc_8261B72C;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// li r29,0
	r29.s64 = 0;
	// lwzx r11,r25,r11
	r11.u64 = PPC_LOAD_U32(r25.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b730
	if (cr6.eq) goto loc_8261B730;
loc_8261B72C:
	// li r29,1
	r29.s64 = 1;
loc_8261B730:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bne cr6,0x8261b748
	if (!cr6.eq) goto loc_8261B748;
	// cmpw cr6,r21,r20
	cr6.compare<int32_t>(r21.s32, r20.s32, xer);
	// beq cr6,0x8261b760
	if (cr6.eq) goto loc_8261B760;
loc_8261B748:
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// li r30,0
	r30.s64 = 0;
	// add r11,r25,r11
	r11.u64 = r25.u64 + r11.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261b764
	if (cr6.eq) goto loc_8261B764;
loc_8261B760:
	// li r30,1
	r30.s64 = 1;
loc_8261B764:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82617590
	sub_82617590(ctx, base);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// add r26,r26,r10
	r26.u64 = r26.u64 + ctx.r10.u64;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// add r23,r11,r23
	r23.u64 = r11.u64 + r23.u64;
	// cmpw cr6,r28,r20
	cr6.compare<int32_t>(r28.s32, r20.s32, xer);
	// blt cr6,0x8261b710
	if (cr6.lt) goto loc_8261B710;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
loc_8261B7F4:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bne cr6,0x8261b810
	if (!cr6.eq) goto loc_8261B810;
	// lwz r11,284(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// addi r21,r11,-1
	r21.s64 = r11.s64 + -1;
	// b 0x8261b814
	goto loc_8261B814;
loc_8261B810:
	// lwz r21,284(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
loc_8261B814:
	// cntlzw r11,r21
	r11.u64 = r21.u32 == 0 ? 32 : __builtin_clz(r21.u32);
	// lwz r27,292(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// rlwinm r20,r11,27,31,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bne cr6,0x8261b848
	if (!cr6.eq) goto loc_8261B848;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82617590
	sub_82617590(ctx, base);
loc_8261B848:
	// lwz r25,276(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// cmpw cr6,r25,r21
	cr6.compare<int32_t>(r25.s32, r21.s32, xer);
	// mullw r11,r25,r11
	r11.s64 = int64_t(r25.s32) * int64_t(r11.s32);
	// add r29,r11,r26
	r29.u64 = r11.u64 + r26.u64;
	// bge cr6,0x8261b898
	if (!cr6.lt) goto loc_8261B898;
	// subf r30,r25,r21
	r30.s64 = r21.s64 - r25.s64;
loc_8261B864:
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82617590
	sub_82617590(ctx, base);
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r29,r29,r11
	r29.u64 = r29.u64 + r11.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8261b864
	if (!cr6.eq) goto loc_8261B864;
loc_8261B898:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bne cr6,0x8261b8d0
	if (!cr6.eq) goto loc_8261B8D0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x8261b8d0
	if (!cr6.eq) goto loc_8261B8D0;
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82617590
	sub_82617590(ctx, base);
loc_8261B8D0:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x8261b8f8
	if (!cr6.eq) goto loc_8261B8F8;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
loc_8261B8F8:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// cmpw cr6,r25,r21
	cr6.compare<int32_t>(r25.s32, r21.s32, xer);
	// mullw r11,r25,r11
	r11.s64 = int64_t(r25.s32) * int64_t(r11.s32);
	// add r30,r11,r24
	r30.u64 = r11.u64 + r24.u64;
	// bge cr6,0x8261b944
	if (!cr6.lt) goto loc_8261B944;
	// subf r29,r25,r21
	r29.s64 = r21.s64 - r25.s64;
loc_8261B910:
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8261b910
	if (!cr6.eq) goto loc_8261B910;
loc_8261B944:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bne cr6,0x8261b9a4
	if (!cr6.eq) goto loc_8261B9A4;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x8261b9a4
	if (!cr6.eq) goto loc_8261B9A4;
	// lwz r28,208(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r22,-1
	r29.s64 = r22.s64 + -1;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r11,3
	r30.s64 = r11.s64 + 3;
	// ble cr6,0x8261b9a4
	if (!cr6.gt) goto loc_8261B9A4;
loc_8261B978:
	// lwz r11,15868(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8261b978
	if (!cr6.eq) goto loc_8261B978;
loc_8261B9A4:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x8261b9cc
	if (!cr6.eq) goto loc_8261B9CC;
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
loc_8261B9CC:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// cmpw cr6,r25,r21
	cr6.compare<int32_t>(r25.s32, r21.s32, xer);
	// mullw r11,r25,r11
	r11.s64 = int64_t(r25.s32) * int64_t(r11.s32);
	// add r30,r11,r23
	r30.u64 = r11.u64 + r23.u64;
	// bge cr6,0x8261ba18
	if (!cr6.lt) goto loc_8261BA18;
	// subf r29,r25,r21
	r29.s64 = r21.s64 - r25.s64;
loc_8261B9E4:
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826177a8
	sub_826177A8(ctx, base);
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8261b9e4
	if (!cr6.eq) goto loc_8261B9E4;
loc_8261BA18:
	// lwz r11,3356(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// bne cr6,0x8261ba78
	if (!cr6.eq) goto loc_8261BA78;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x8261ba78
	if (!cr6.eq) goto loc_8261BA78;
	// lwz r28,208(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r22,-1
	r29.s64 = r22.s64 + -1;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r11,3
	r30.s64 = r11.s64 + 3;
	// ble cr6,0x8261ba78
	if (!cr6.gt) goto loc_8261BA78;
loc_8261BA4C:
	// lwz r11,15868(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15868);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8261ba4c
	if (!cr6.eq) goto loc_8261BA4C;
loc_8261BA78:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8261BA80"))) PPC_WEAK_FUNC(sub_8261BA80);
PPC_FUNC_IMPL(__imp__sub_8261BA80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8261bb40
	if (!cr6.gt) goto loc_8261BB40;
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
loc_8261BAC4:
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mullw r10,r10,r26
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r26.s32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r10,r25
	r30.u64 = ctx.r10.u64 + r25.u64;
	// ble cr6,0x8261bb34
	if (!cr6.gt) goto loc_8261BB34;
loc_8261BAE0:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// addi r4,r30,16
	ctx.r4.s64 = r30.s64 + 16;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8261bae0
	if (cr6.lt) goto loc_8261BAE0;
loc_8261BB34:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmpw cr6,r26,r24
	cr6.compare<int32_t>(r26.s32, r24.s32, xer);
	// blt cr6,0x8261bac4
	if (cr6.lt) goto loc_8261BAC4;
loc_8261BB40:
	// srawi r25,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r25.s64 = r24.s32 >> 1;
	// mr r27,r23
	r27.u64 = r23.u64;
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8261bbdc
	if (!cr6.gt) goto loc_8261BBDC;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
loc_8261BB60:
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mullw r10,r10,r26
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r26.s32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r10,r22
	r30.u64 = ctx.r10.u64 + r22.u64;
	// ble cr6,0x8261bbd0
	if (!cr6.gt) goto loc_8261BBD0;
loc_8261BB7C:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// addi r4,r30,16
	ctx.r4.s64 = r30.s64 + 16;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8261bb7c
	if (cr6.lt) goto loc_8261BB7C;
loc_8261BBD0:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmpw cr6,r26,r25
	cr6.compare<int32_t>(r26.s32, r25.s32, xer);
	// blt cr6,0x8261bb60
	if (cr6.lt) goto loc_8261BB60;
loc_8261BBDC:
	// mr r27,r21
	r27.u64 = r21.u64;
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8261bc74
	if (!cr6.gt) goto loc_8261BC74;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
loc_8261BBF8:
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mullw r10,r10,r26
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r26.s32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r10,r20
	r30.u64 = ctx.r10.u64 + r20.u64;
	// ble cr6,0x8261bc68
	if (!cr6.gt) goto loc_8261BC68;
loc_8261BC14:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// addi r4,r30,16
	ctx.r4.s64 = r30.s64 + 16;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82617d58
	sub_82617D58(ctx, base);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8261bc14
	if (cr6.lt) goto loc_8261BC14;
loc_8261BC68:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmpw cr6,r26,r25
	cr6.compare<int32_t>(r26.s32, r25.s32, xer);
	// blt cr6,0x8261bbf8
	if (cr6.lt) goto loc_8261BBF8;
loc_8261BC74:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8261BC7C"))) PPC_WEAK_FUNC(sub_8261BC7C);
PPC_FUNC_IMPL(__imp__sub_8261BC7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261BC80"))) PPC_WEAK_FUNC(sub_8261BC80);
PPC_FUNC_IMPL(__imp__sub_8261BC80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r20,r8
	r20.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// addi r11,r4,-5
	r11.s64 = ctx.r4.s64 + -5;
	// addi r21,r5,-5
	r21.s64 = ctx.r5.s64 + -5;
	// addi r19,r6,-5
	r19.s64 = ctx.r6.s64 + -5;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x8261bd38
	if (!cr6.gt) goto loc_8261BD38;
	// addi r10,r20,31
	ctx.r10.s64 = r20.s64 + 31;
	// mr r24,r11
	r24.u64 = r11.u64;
	// srawi r26,r10,5
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	r26.s64 = ctx.r10.s32 >> 5;
	// mr r23,r25
	r23.u64 = r25.u64;
loc_8261BCC4:
	// mr r30,r24
	r30.u64 = r24.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8261bd28
	if (!cr6.gt) goto loc_8261BD28;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r29,r26
	r29.u64 = r26.u64;
loc_8261BCD8:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r11,r6,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// rlwinm r11,r6,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bne cr6,0x8261bcd8
	if (!cr6.eq) goto loc_8261BCD8;
loc_8261BD28:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x8261bcc4
	if (!cr6.eq) goto loc_8261BCC4;
loc_8261BD38:
	// srawi r24,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r24.s64 = r25.s32 >> 1;
	// mr r27,r22
	r27.u64 = r22.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8261bdd0
	if (!cr6.gt) goto loc_8261BDD0;
	// srawi r11,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r11.s64 = r20.s32 >> 1;
	// mr r25,r21
	r25.u64 = r21.u64;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// mr r23,r24
	r23.u64 = r24.u64;
	// srawi r26,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r26.s64 = r11.s32 >> 5;
loc_8261BD5C:
	// mr r30,r25
	r30.u64 = r25.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8261bdc0
	if (!cr6.gt) goto loc_8261BDC0;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r29,r26
	r29.u64 = r26.u64;
loc_8261BD70:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r11,r6,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// rlwinm r11,r6,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bne cr6,0x8261bd70
	if (!cr6.eq) goto loc_8261BD70;
loc_8261BDC0:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x8261bd5c
	if (!cr6.eq) goto loc_8261BD5C;
loc_8261BDD0:
	// lwz r27,276(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x8261be60
	if (!cr6.gt) goto loc_8261BE60;
	// srawi r11,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r11.s64 = r20.s32 >> 1;
	// mr r25,r19
	r25.u64 = r19.u64;
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// srawi r26,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r26.s64 = r11.s32 >> 5;
loc_8261BDEC:
	// mr r30,r25
	r30.u64 = r25.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8261be50
	if (!cr6.gt) goto loc_8261BE50;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r29,r26
	r29.u64 = r26.u64;
loc_8261BE00:
	// lbz r28,0(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r5,r28,28,4,31
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xFFFFFFF;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// clrlwi r5,r28,28
	ctx.r5.u64 = r28.u32 & 0xF;
	// lwz r7,248(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// rlwinm r11,r6,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x82617f10
	sub_82617F10(ctx, base);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// rlwinm r11,r6,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bne cr6,0x8261be00
	if (!cr6.eq) goto loc_8261BE00;
loc_8261BE50:
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x8261bdec
	if (!cr6.eq) goto loc_8261BDEC;
loc_8261BE60:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8261BE68"))) PPC_WEAK_FUNC(sub_8261BE68);
PPC_FUNC_IMPL(__imp__sub_8261BE68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// lwz r11,21236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r15,r7
	r15.u64 = ctx.r7.u64;
	// mr r14,r8
	r14.u64 = ctx.r8.u64;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// stw r6,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r6.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r5,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r5.u32);
	// beq cr6,0x8261c1ac
	if (cr6.eq) goto loc_8261C1AC;
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r26,444(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// mr r25,r26
	r25.u64 = r26.u64;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// beq cr6,0x8261bf0c
	if (cr6.eq) goto loc_8261BF0C;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261bf0c
	if (cr6.eq) goto loc_8261BF0C;
	// lwz r11,21000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261bf0c
	if (!cr6.eq) goto loc_8261BF0C;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x8261bf10
	goto loc_8261BF10;
loc_8261BF0C:
	// lwz r11,21268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21268);
loc_8261BF10:
	// lwz r9,452(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// stw r11,21264(r31)
	PPC_STORE_U32(r31.u32 + 21264, r11.u32);
	// cmplw cr6,r26,r9
	cr6.compare<uint32_t>(r26.u32, ctx.r9.u32, xer);
	// bge cr6,0x8261bf54
	if (!cr6.lt) goto loc_8261BF54;
	// addi r25,r26,1
	r25.s64 = r26.s64 + 1;
	// cmplw cr6,r25,r9
	cr6.compare<uint32_t>(r25.u32, ctx.r9.u32, xer);
	// bge cr6,0x8261bf54
	if (!cr6.lt) goto loc_8261BF54;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8261BF38:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8261bf54
	if (!cr6.eq) goto loc_8261BF54;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r25,r9
	cr6.compare<uint32_t>(r25.u32, ctx.r9.u32, xer);
	// blt cr6,0x8261bf38
	if (cr6.lt) goto loc_8261BF38;
loc_8261BF54:
	// subf r30,r26,r25
	r30.s64 = r25.s64 - r26.s64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8261c314
	if (cr6.eq) goto loc_8261C314;
	// lwz r22,428(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r21,420(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r20,412(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// lwz r19,404(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r18,396(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// lwz r17,388(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// lwz r16,380(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r24,436(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// b 0x8261bf90
	goto loc_8261BF90;
loc_8261BF84:
	// lwz r5,364(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,356(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
loc_8261BF90:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x8261bfb8
	if (cr6.eq) goto loc_8261BFB8;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261bfb8
	if (!cr6.eq) goto loc_8261BFB8;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// b 0x8261bfc4
	goto loc_8261BFC4;
loc_8261BFB8:
	// rlwinm r11,r7,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r23,r8,3,0,28
	r23.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_8261BFC4:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x8261bfe4
	if (cr6.eq) goto loc_8261BFE4;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// beq cr6,0x8261bfe8
	if (cr6.eq) goto loc_8261BFE8;
loc_8261BFE4:
	// li r11,1
	r11.s64 = 1;
loc_8261BFE8:
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// lwz r8,372(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826197d8
	sub_826197D8(ctx, base);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r5,r18
	ctx.r5.u64 = r18.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261a0e8
	sub_8261A0E8(ctx, base);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x8261c05c
	if (cr6.eq) goto loc_8261C05C;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x8261c060
	if (cr6.eq) goto loc_8261C060;
loc_8261C05C:
	// li r11,1
	r11.s64 = 1;
loc_8261C060:
	// lwz r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// rlwinm r26,r30,1,0,30
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// lwz r10,356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// add r4,r7,r29
	ctx.r4.u64 = ctx.r7.u64 + r29.u64;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// add r6,r23,r27
	ctx.r6.u64 = r23.u64 + r27.u64;
	// add r5,r23,r28
	ctx.r5.u64 = r23.u64 + r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// subf r7,r11,r26
	ctx.r7.s64 = r26.s64 - r11.s64;
	// bl 0x8261ba80
	sub_8261BA80(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
	// lwz r9,372(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r6,r11,r27
	ctx.r6.u64 = r11.u64 + r27.u64;
	// lwz r8,364(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// add r5,r11,r28
	ctx.r5.u64 = r11.u64 + r28.u64;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261ba80
	sub_8261BA80(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r26,r30,4,0,27
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r19,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r19.u32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// addi r6,r27,8
	ctx.r6.s64 = r27.s64 + 8;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// addi r5,r28,8
	ctx.r5.s64 = r28.s64 + 8;
	// addi r4,r29,8
	ctx.r4.s64 = r29.s64 + 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// bl 0x8261bc80
	sub_8261BC80(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// mr r9,r20
	ctx.r9.u64 = r20.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r27,4
	ctx.r6.s64 = r27.s64 + 4;
	// addi r5,r28,4
	ctx.r5.s64 = r28.s64 + 4;
	// addi r4,r29,4
	ctx.r4.s64 = r29.s64 + 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261bc80
	sub_8261BC80(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r26,r25
	r26.u64 = r25.u64;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r6,452(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r8,r30
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r5,r7,r30
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r5,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r25,r6
	cr6.compare<uint32_t>(r25.u32, ctx.r6.u32, xer);
	// add r29,r9,r29
	r29.u64 = ctx.r9.u64 + r29.u64;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// add r27,r10,r27
	r27.u64 = ctx.r10.u64 + r27.u64;
	// bge cr6,0x8261c198
	if (!cr6.lt) goto loc_8261C198;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// cmplw cr6,r25,r6
	cr6.compare<uint32_t>(r25.u32, ctx.r6.u32, xer);
	// bge cr6,0x8261c198
	if (!cr6.lt) goto loc_8261C198;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8261C17C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8261c198
	if (!cr6.eq) goto loc_8261C198;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r25,r6
	cr6.compare<uint32_t>(r25.u32, ctx.r6.u32, xer);
	// blt cr6,0x8261c17c
	if (cr6.lt) goto loc_8261C17C;
loc_8261C198:
	// subf r30,r26,r25
	r30.s64 = r25.s64 - r26.s64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8261bf84
	if (!cr6.eq) goto loc_8261BF84;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
loc_8261C1AC:
	// lwz r30,444(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8261c1cc
	if (!cr6.eq) goto loc_8261C1CC;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r24,r10,3,0,28
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_8261C1CC:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// lwz r25,452(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// lwz r23,436(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// rlwinm r26,r11,27,31,31
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r22,380(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r21,372(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826197d8
	sub_826197D8(ctx, base);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// subf r30,r30,r25
	r30.s64 = r25.s64 - r30.s64;
	// lwz r23,428(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r20,420(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r19,412(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// lwz r18,404(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// lwz r17,396(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// lwz r16,388(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// bl 0x8261a0e8
	sub_8261A0E8(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// rlwinm r25,r30,1,0,30
	r25.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// add r6,r24,r27
	ctx.r6.u64 = r24.u64 + r27.u64;
	// add r5,r24,r28
	ctx.r5.u64 = r24.u64 + r28.u64;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// subf r7,r26,r25
	ctx.r7.s64 = r25.s64 - r26.s64;
	// bl 0x8261ba80
	sub_8261BA80(ctx, base);
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,364(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r6,r29
	ctx.r4.u64 = ctx.r6.u64 + r29.u64;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r6,r11,r27
	ctx.r6.u64 = r11.u64 + r27.u64;
	// add r5,r11,r28
	ctx.r5.u64 = r11.u64 + r28.u64;
	// bl 0x8261ba80
	sub_8261BA80(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r30,r30,4,0,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
	// addi r6,r27,8
	ctx.r6.s64 = r27.s64 + 8;
	// addi r5,r28,8
	ctx.r5.s64 = r28.s64 + 8;
	// addi r4,r29,8
	ctx.r4.s64 = r29.s64 + 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// bl 0x8261bc80
	sub_8261BC80(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r23.u32);
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r27,4
	ctx.r6.s64 = r27.s64 + 4;
	// addi r5,r28,4
	ctx.r5.s64 = r28.s64 + 4;
	// addi r4,r29,4
	ctx.r4.s64 = r29.s64 + 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261bc80
	sub_8261BC80(ctx, base);
loc_8261C314:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8261C31C"))) PPC_WEAK_FUNC(sub_8261C31C);
PPC_FUNC_IMPL(__imp__sub_8261C31C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261C320"))) PPC_WEAK_FUNC(sub_8261C320);
PPC_FUNC_IMPL(__imp__sub_8261C320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	r11.s64 = -2107506688;
	// addi r10,r10,-21272
	ctx.r10.s64 = ctx.r10.s64 + -21272;
	// addi r11,r11,-20888
	r11.s64 = r11.s64 + -20888;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r10,15856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15856, ctx.r10.u32);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stw r11,15852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15852, r11.u32);
	// lwz r11,19696(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19696);
	// stw r11,13632(r10)
	PPC_STORE_U32(ctx.r10.u32 + 13632, r11.u32);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// lwz r11,19700(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19700);
	// stw r11,3380(r10)
	PPC_STORE_U32(ctx.r10.u32 + 3380, r11.u32);
	// beqlr cr6
	if (cr6.eq) return;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	r11.s64 = -2107506688;
	// addi r10,r10,-27848
	ctx.r10.s64 = ctx.r10.s64 + -27848;
	// addi r11,r11,-27376
	r11.s64 = r11.s64 + -27376;
	// stw r10,15856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15856, ctx.r10.u32);
	// stw r11,15852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15852, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261C374"))) PPC_WEAK_FUNC(sub_8261C374);
PPC_FUNC_IMPL(__imp__sub_8261C374) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261C378"))) PPC_WEAK_FUNC(sub_8261C378);
PPC_FUNC_IMPL(__imp__sub_8261C378) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// vspltish v15,15
	// li r0,0
	r0.s64 = 0;
	// addi r8,r3,128
	ctx.r8.s64 = ctx.r3.s64 + 128;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r12,r6,r6
	r12.u64 = ctx.r6.u64 + ctx.r6.u64;
	// vslb v8,v15,v15
	ctx.v8.u8[0] = v15.u8[0] << (v15.u8[0] & 0x7);
	ctx.v8.u8[1] = v15.u8[1] << (v15.u8[1] & 0x7);
	ctx.v8.u8[2] = v15.u8[2] << (v15.u8[2] & 0x7);
	ctx.v8.u8[3] = v15.u8[3] << (v15.u8[3] & 0x7);
	ctx.v8.u8[4] = v15.u8[4] << (v15.u8[4] & 0x7);
	ctx.v8.u8[5] = v15.u8[5] << (v15.u8[5] & 0x7);
	ctx.v8.u8[6] = v15.u8[6] << (v15.u8[6] & 0x7);
	ctx.v8.u8[7] = v15.u8[7] << (v15.u8[7] & 0x7);
	ctx.v8.u8[8] = v15.u8[8] << (v15.u8[8] & 0x7);
	ctx.v8.u8[9] = v15.u8[9] << (v15.u8[9] & 0x7);
	ctx.v8.u8[10] = v15.u8[10] << (v15.u8[10] & 0x7);
	ctx.v8.u8[11] = v15.u8[11] << (v15.u8[11] & 0x7);
	ctx.v8.u8[12] = v15.u8[12] << (v15.u8[12] & 0x7);
	ctx.v8.u8[13] = v15.u8[13] << (v15.u8[13] & 0x7);
	ctx.v8.u8[14] = v15.u8[14] << (v15.u8[14] & 0x7);
	ctx.v8.u8[15] = v15.u8[15] << (v15.u8[15] & 0x7);
	// add r11,r9,r9
	r11.u64 = ctx.r9.u64 + ctx.r9.u64;
	// add r10,r12,r9
	ctx.r10.u64 = r12.u64 + ctx.r9.u64;
	// lvx128 v0,r3,r0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v1,r8,r0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lvx128 v4,r3,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v16,v0,v8
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v3,r8,r12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v2,r3,r12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v17,v1,v8
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v7,r8,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r12,r10,r11
	r12.u64 = ctx.r10.u64 + r11.u64;
	// lvx128 v6,r3,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// vaddshs v18,v2,v8
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v0,v16,v16
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v19,v3,v8
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v11,r8,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r3,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v1,v17,v17
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)v17.s16)));
	// lvx128 v5,r8,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r6,4
	ctx.r6.s64 = 4;
	// vaddshs v20,v4,v8
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v2,v18,v18
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v18.s16), _mm_load_si128((__m128i*)v18.s16)));
	// lvx128 v9,r8,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v21,v5,v8
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v13,r8,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v16,v8,v8
	_mm_store_si128((__m128i*)v16.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// lvx128 v15,r8,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r7,r4
	ctx.r8.u64 = ctx.r7.u64 + ctx.r4.u64;
	// vaddshs v22,v6,v8
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v3,v19,v19
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v19.s16), _mm_load_si128((__m128i*)v19.s16)));
	// vaddshs v23,v7,v8
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v12,r3,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r3,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// lvx128 v14,r3,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v4,v20,v20
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)v20.s16)));
	// stvewx v0,r4,r0
	ea = (ctx.r4.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// add r10,r7,r8
	ctx.r10.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stvewx v0,r4,r6
	ea = (ctx.r4.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v5,v21,v21
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)v21.s16)));
	// stvewx v1,r5,r0
	ea = (ctx.r5.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v1.u32[3 - ((ea & 0xF) >> 2)]);
	// add r11,r7,r9
	r11.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stvewx v1,r5,r6
	ea = (ctx.r5.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v1.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v6,v22,v22
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)v22.s16)));
	// stvewx v2,r8,r0
	ea = (ctx.r8.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v2.u32[3 - ((ea & 0xF) >> 2)]);
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stvewx v2,r8,r6
	ea = (ctx.r8.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v2.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v24,v8,v16
	_mm_store_si128((__m128i*)v24.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v3,r9,r0
	ea = (ctx.r9.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v25,v9,v16
	_mm_store_si128((__m128i*)v25.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v3,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v3.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v26,v10,v16
	_mm_store_si128((__m128i*)v26.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v4,r10,r0
	ea = (ctx.r10.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v4.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v27,v11,v16
	_mm_store_si128((__m128i*)v27.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v28,v12,v16
	_mm_store_si128((__m128i*)v28.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v4,r10,r6
	ea = (ctx.r10.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v4.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v29,v13,v16
	_mm_store_si128((__m128i*)v29.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v5,r11,r0
	ea = (r11.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v5.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v30,v14,v16
	_mm_store_si128((__m128i*)v30.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v5,r11,r6
	ea = (r11.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v5.u32[3 - ((ea & 0xF) >> 2)]);
	// vaddshs v31,v15,v16
	_mm_store_si128((__m128i*)v31.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvewx v6,r4,r0
	ea = (ctx.r4.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v7,v23,v23
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)v23.s16)));
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + r11.u64;
	// vpkshus v8,v24,v24
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v24.s16)));
	// stvewx v6,r4,r6
	ea = (ctx.r4.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v9,v25,v25
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v25.s16)));
	// vpkshus v10,v26,v26
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v26.s16)));
	// vpkshus v11,v27,v27
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v27.s16)));
	// vpkshus v12,v28,v28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vpkshus v13,v29,v29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v29.s16)));
	// vpkshus v14,v30,v30
	_mm_store_si128((__m128i*)v14.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vpkshus v15,v31,v31
	_mm_store_si128((__m128i*)v15.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v31.s16)));
	// add r8,r7,r4
	ctx.r8.u64 = ctx.r7.u64 + ctx.r4.u64;
	// stvewx v7,r5,r0
	ea = (ctx.r5.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// stvewx v7,r5,r6
	ea = (ctx.r5.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// add r10,r7,r8
	ctx.r10.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r11,r7,r9
	r11.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stvewx v8,r8,r0
	ea = (ctx.r8.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + r11.u64;
	// stvewx v8,r8,r6
	ea = (ctx.r8.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// add r8,r7,r4
	ctx.r8.u64 = ctx.r7.u64 + ctx.r4.u64;
	// stvewx v9,r9,r0
	ea = (ctx.r9.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v9,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// stvewx v10,r10,r0
	ea = (ctx.r10.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v10,r10,r6
	ea = (ctx.r10.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r11,r0
	ea = (r11.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r11,r6
	ea = (r11.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r4,r0
	ea = (ctx.r4.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r4,r6
	ea = (ctx.r4.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r5,r0
	ea = (ctx.r5.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r5,r6
	ea = (ctx.r5.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v14,r8,r0
	ea = (ctx.r8.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, v14.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v14,r8,r6
	ea = (ctx.r8.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v14.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v15,r9,r0
	ea = (ctx.r9.u32 + r0.u32) & ~0x3;
	PPC_STORE_U32(ea, v15.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v15,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, v15.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261C528"))) PPC_WEAK_FUNC(sub_8261C528);
PPC_FUNC_IMPL(__imp__sub_8261C528) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	// vspltish v15,15
	// addi r9,r3,128
	ctx.r9.s64 = ctx.r3.s64 + 128;
	// li r0,0
	r0.s64 = 0;
	// add r12,r6,r6
	r12.u64 = ctx.r6.u64 + ctx.r6.u64;
	// addi r10,r3,256
	ctx.r10.s64 = ctx.r3.s64 + 256;
	// vslb v8,v15,v15
	ctx.v8.u8[0] = v15.u8[0] << (v15.u8[0] & 0x7);
	ctx.v8.u8[1] = v15.u8[1] << (v15.u8[1] & 0x7);
	ctx.v8.u8[2] = v15.u8[2] << (v15.u8[2] & 0x7);
	ctx.v8.u8[3] = v15.u8[3] << (v15.u8[3] & 0x7);
	ctx.v8.u8[4] = v15.u8[4] << (v15.u8[4] & 0x7);
	ctx.v8.u8[5] = v15.u8[5] << (v15.u8[5] & 0x7);
	ctx.v8.u8[6] = v15.u8[6] << (v15.u8[6] & 0x7);
	ctx.v8.u8[7] = v15.u8[7] << (v15.u8[7] & 0x7);
	ctx.v8.u8[8] = v15.u8[8] << (v15.u8[8] & 0x7);
	ctx.v8.u8[9] = v15.u8[9] << (v15.u8[9] & 0x7);
	ctx.v8.u8[10] = v15.u8[10] << (v15.u8[10] & 0x7);
	ctx.v8.u8[11] = v15.u8[11] << (v15.u8[11] & 0x7);
	ctx.v8.u8[12] = v15.u8[12] << (v15.u8[12] & 0x7);
	ctx.v8.u8[13] = v15.u8[13] << (v15.u8[13] & 0x7);
	ctx.v8.u8[14] = v15.u8[14] << (v15.u8[14] & 0x7);
	ctx.v8.u8[15] = v15.u8[15] << (v15.u8[15] & 0x7);
	// add r8,r12,r12
	ctx.r8.u64 = r12.u64 + r12.u64;
	// addi r11,r3,384
	r11.s64 = ctx.r3.s64 + 384;
	// lvx128 v10,r3,r0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r12,r8
	ctx.r6.u64 = r12.u64 + ctx.r8.u64;
	// lvx128 v11,r9,r0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v14,r3,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v1,v11,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v15,r9,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v10,v8
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v20,r3,r8
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v14,v8
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v21,r9,r8
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v15,v8
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v12,r10,r0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v5,v21,v8
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v13,r11,r0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v4,v20,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v24,r3,r6
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v10,v0,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// lvx128 v25,r9,r6
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v1,v13,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v16,r10,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v12,v8
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v11,v2,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v6,v24,v8
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v22,r10,r8
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v25,v8
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v23,r11,r8
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v16,v8
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v20,v4,v5
	_mm_store_si128((__m128i*)v20.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v3,v17,v8
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v26,r10,r6
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v5,v23,v8
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v27,r11,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v4,v22,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v12,v0,v1
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vpkshus v21,v6,v7
	_mm_store_si128((__m128i*)v21.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v6,v26,v8
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v7,v27,v8
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v13,v2,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// add r8,r7,r7
	ctx.r8.u64 = ctx.r7.u64 + ctx.r7.u64;
	// stvx v10,r4,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v22,v4,v5
	_mm_store_si128((__m128i*)v22.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// add r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 + ctx.r8.u64;
	// vpkshus v23,v6,v7
	_mm_store_si128((__m128i*)v23.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// stvx v12,r5,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r4,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r5,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v20,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r8,r12,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r12.u32 | (r12.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx v21,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v23,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r12,r8
	ctx.r6.u64 = r12.u64 + ctx.r8.u64;
	// lvx128 v10,r3,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r9,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v10,v8
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v12,r10,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v1,v11,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v13,r11,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r12,r6
	ctx.r8.u64 = r12.u64 + ctx.r6.u64;
	// lvx128 v14,r3,r6
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v15,r9,r6
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v16,r10,r6
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v17,r11,r6
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r12,r8
	ctx.r6.u64 = r12.u64 + ctx.r8.u64;
	// lvx128 v20,r3,r8
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v21,r9,r8
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v22,r10,r8
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v23,r11,r8
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v24,r3,r6
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v25,r9,r6
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v26,r10,r6
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v27,r11,r6
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v14,v8
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v14.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v10,v0,v1
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v3,v15,v8
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v15.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vaddshs v1,v13,v8
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v0,v12,v8
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// add r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 + ctx.r8.u64;
	// vaddshs v5,v21,v8
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v21.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v11,v2,v3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v4,v20,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v20.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v2,v16,v8
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v16.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v3,v17,v8
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v17.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v12,v0,v1
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vaddshs v6,v24,v8
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v10,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v25,v8
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v20,v4,v5
	_mm_store_si128((__m128i*)v20.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// vaddshs v5,v23,v8
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v23.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v13,v2,v3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// vaddshs v4,v22,v8
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v22.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v21,v6,v7
	_mm_store_si128((__m128i*)v21.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vaddshs v6,v26,v8
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v7,v27,v8
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v12,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r7,r6
	ctx.r8.u64 = ctx.r7.u64 + ctx.r6.u64;
	// vpkshus v22,v4,v5
	_mm_store_si128((__m128i*)v22.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvx v11,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v23,v6,v7
	_mm_store_si128((__m128i*)v23.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// add r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stvx v20,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v23,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261C6F8"))) PPC_WEAK_FUNC(sub_8261C6F8);
PPC_FUNC_IMPL(__imp__sub_8261C6F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	// vspltish v15,15
	// li r0,0
	r0.s64 = 0;
	// addi r8,r3,128
	ctx.r8.s64 = ctx.r3.s64 + 128;
	// add r12,r6,r6
	r12.u64 = ctx.r6.u64 + ctx.r6.u64;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// vslb v8,v15,v15
	ctx.v8.u8[0] = v15.u8[0] << (v15.u8[0] & 0x7);
	ctx.v8.u8[1] = v15.u8[1] << (v15.u8[1] & 0x7);
	ctx.v8.u8[2] = v15.u8[2] << (v15.u8[2] & 0x7);
	ctx.v8.u8[3] = v15.u8[3] << (v15.u8[3] & 0x7);
	ctx.v8.u8[4] = v15.u8[4] << (v15.u8[4] & 0x7);
	ctx.v8.u8[5] = v15.u8[5] << (v15.u8[5] & 0x7);
	ctx.v8.u8[6] = v15.u8[6] << (v15.u8[6] & 0x7);
	ctx.v8.u8[7] = v15.u8[7] << (v15.u8[7] & 0x7);
	ctx.v8.u8[8] = v15.u8[8] << (v15.u8[8] & 0x7);
	ctx.v8.u8[9] = v15.u8[9] << (v15.u8[9] & 0x7);
	ctx.v8.u8[10] = v15.u8[10] << (v15.u8[10] & 0x7);
	ctx.v8.u8[11] = v15.u8[11] << (v15.u8[11] & 0x7);
	ctx.v8.u8[12] = v15.u8[12] << (v15.u8[12] & 0x7);
	ctx.v8.u8[13] = v15.u8[13] << (v15.u8[13] & 0x7);
	ctx.v8.u8[14] = v15.u8[14] << (v15.u8[14] & 0x7);
	ctx.v8.u8[15] = v15.u8[15] << (v15.u8[15] & 0x7);
	// add r11,r7,r7
	r11.u64 = ctx.r7.u64 + ctx.r7.u64;
	// lvx128 v0,r3,r0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r12,r9
	ctx.r10.u64 = r12.u64 + ctx.r9.u64;
	// lvx128 v1,r8,r0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v2,r3,r12
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v16,v0,v8
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v3,r8,r12
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v17,v1,v8
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v4,r3,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v18,v2,v8
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v5,r8,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v19,v3,v8
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v7,r8,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r3,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v20,v4,v8
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v16,r4,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r11,r6
	ctx.r7.u64 = r11.u64 + ctx.r6.u64;
	// vaddshs v21,v5,v8
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v17,r5,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v22,v6,v8
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v18,r4,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v7,v8
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v19,r5,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v20,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r4,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v23,r5,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r9,r9
	ctx.r7.u64 = ctx.r9.u64 + ctx.r9.u64;
	// add r10,r12,r7
	ctx.r10.u64 = r12.u64 + ctx.r7.u64;
	// lvx128 v1,r8,r7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r3,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r12,r10
	ctx.r7.u64 = r12.u64 + ctx.r10.u64;
	// lvx128 v3,r8,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v16,v0,v8
	_mm_store_si128((__m128i*)v16.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v2,r3,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r12,r7
	ctx.r10.u64 = r12.u64 + ctx.r7.u64;
	// vaddshs v17,v1,v8
	_mm_store_si128((__m128i*)v17.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vaddshs v18,v2,v8
	_mm_store_si128((__m128i*)v18.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v5,r8,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v19,v3,v8
	_mm_store_si128((__m128i*)v19.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// lvx128 v4,r3,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r6,r6
	ctx.r7.u64 = ctx.r6.u64 + ctx.r6.u64;
	// lvx128 v7,r8,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v20,v4,v8
	_mm_store_si128((__m128i*)v20.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// add r8,r11,r7
	ctx.r8.u64 = r11.u64 + ctx.r7.u64;
	// lvx128 v6,r3,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 + ctx.r7.u64;
	// vaddshs v21,v5,v8
	_mm_store_si128((__m128i*)v21.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// add r10,r6,r8
	ctx.r10.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vaddshs v22,v6,v8
	_mm_store_si128((__m128i*)v22.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v16,r4,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v23,v7,v8
	_mm_store_si128((__m128i*)v23.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvx v17,r5,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v18,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v19,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v20,r4,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v21,r5,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v22,r4,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v23,r5,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261C804"))) PPC_WEAK_FUNC(sub_8261C804);
PPC_FUNC_IMPL(__imp__sub_8261C804) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261C808"))) PPC_WEAK_FUNC(sub_8261C808);
PPC_FUNC_IMPL(__imp__sub_8261C808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// add r12,r6,r6
	r12.u64 = ctx.r6.u64 + ctx.r6.u64;
	// vspltish v15,15
	// li r0,0
	r0.s64 = 0;
	// add r6,r12,r12
	ctx.r6.u64 = r12.u64 + r12.u64;
	// addi r10,r3,256
	ctx.r10.s64 = ctx.r3.s64 + 256;
	// add r8,r7,r7
	ctx.r8.u64 = ctx.r7.u64 + ctx.r7.u64;
	// vslb v16,v15,v15
	v16.u8[0] = v15.u8[0] << (v15.u8[0] & 0x7);
	v16.u8[1] = v15.u8[1] << (v15.u8[1] & 0x7);
	v16.u8[2] = v15.u8[2] << (v15.u8[2] & 0x7);
	v16.u8[3] = v15.u8[3] << (v15.u8[3] & 0x7);
	v16.u8[4] = v15.u8[4] << (v15.u8[4] & 0x7);
	v16.u8[5] = v15.u8[5] << (v15.u8[5] & 0x7);
	v16.u8[6] = v15.u8[6] << (v15.u8[6] & 0x7);
	v16.u8[7] = v15.u8[7] << (v15.u8[7] & 0x7);
	v16.u8[8] = v15.u8[8] << (v15.u8[8] & 0x7);
	v16.u8[9] = v15.u8[9] << (v15.u8[9] & 0x7);
	v16.u8[10] = v15.u8[10] << (v15.u8[10] & 0x7);
	v16.u8[11] = v15.u8[11] << (v15.u8[11] & 0x7);
	v16.u8[12] = v15.u8[12] << (v15.u8[12] & 0x7);
	v16.u8[13] = v15.u8[13] << (v15.u8[13] & 0x7);
	v16.u8[14] = v15.u8[14] << (v15.u8[14] & 0x7);
	v16.u8[15] = v15.u8[15] << (v15.u8[15] & 0x7);
	// addi r11,r3,384
	r11.s64 = ctx.r3.s64 + 384;
	// lvx128 v27,r3,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r12,r6
	ctx.r7.u64 = r12.u64 + ctx.r6.u64;
	// lvx128 v31,r3,r0
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,128
	ctx.r9.s64 = ctx.r3.s64 + 128;
	// lvx128 v29,r10,r0
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v31,v16
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v25,r10,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v2,v29,v16
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v28,r11,r0
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v6,v25,v16
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v28,v16
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v29,r10,r6
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v24,v16
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v25,r10,r7
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lvx128 v30,r9,r0
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v4,v27,v16
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v28,r11,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v1,v30,v16
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v24,r11,r7
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v26,r9,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v10,v29,v16
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v5,v26,v16
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v31,r3,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v30,r9,r6
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r8,r8
	ctx.r6.u64 = ctx.r8.u64 + ctx.r8.u64;
	// lvx128 v27,r3,r7
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v8,v31,v16
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v26,r9,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v9,v30,v16
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v0,r4,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r8,r6
	ctx.r7.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stvx v1,r10,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v28,v16
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v2,r5,r0
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v12,v27,v16
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v3,r11,r0
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r0.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v13,v26,v16
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v4,r4,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v14,v25,v16
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v5,r10,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v15,v24,v16
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v6,r5,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r11,r8
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r10,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r11,r6
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r6,r12,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r12.u32 | (r12.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx v12,r4,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r10,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r3,256
	ctx.r10.s64 = ctx.r3.s64 + 256;
	// stvx v14,r5,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v15,r11,r7
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r3,384
	r11.s64 = ctx.r3.s64 + 384;
	// add r7,r12,r6
	ctx.r7.u64 = r12.u64 + ctx.r6.u64;
	// lvx128 v31,r3,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v30,r9,r6
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v0,v31,v16
	_mm_store_si128((__m128i*)ctx.v0.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v29,r10,r6
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v1,v30,v16
	_mm_store_si128((__m128i*)ctx.v1.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v16.s16)));
	// vaddshs v2,v29,v16
	_mm_store_si128((__m128i*)ctx.v2.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v28,r11,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r12,r7
	ctx.r6.u64 = r12.u64 + ctx.r7.u64;
	// lvx128 v27,r3,r7
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v3,v28,v16
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v26,r9,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v4,v27,v16
	_mm_store_si128((__m128i*)ctx.v4.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v25,r10,r7
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v24,r11,r7
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r12,r6
	ctx.r7.u64 = r12.u64 + ctx.r6.u64;
	// lvx128 v31,r3,r6
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v6,v25,v16
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v29,r10,r6
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v7,v24,v16
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v28,r11,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v5,v26,v16
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v30,r9,r6
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v25,r10,r7
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lvx128 v24,r11,r7
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v27,r3,r7
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v8,v31,v16
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v16.s16)));
	// lvx128 v26,r9,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r8,r6
	ctx.r7.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stvx v0,r4,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v9,v30,v16
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v1,r10,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r8,r7
	ctx.r3.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stvx v2,r5,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v10,v29,v16
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v3,r11,r6
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r12,r8,r3
	r12.u64 = ctx.r8.u64 + ctx.r3.u64;
	// stvx v4,r4,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v11,v28,v16
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v28.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v5,r10,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v12,v27,v16
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v27.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v6,r5,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v13,v26,v16
	_mm_store_si128((__m128i*)ctx.v13.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v26.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v7,r11,r7
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v14,v25,v16
	_mm_store_si128((__m128i*)v14.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v25.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v8,r4,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddshs v15,v24,v16
	_mm_store_si128((__m128i*)v15.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v24.s16), _mm_load_si128((__m128i*)v16.s16)));
	// stvx v9,r10,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r5,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r11,r3
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r4,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r10,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v14,r5,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v15,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261C9F4"))) PPC_WEAK_FUNC(sub_8261C9F4);
PPC_FUNC_IMPL(__imp__sub_8261C9F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261C9F8"))) PPC_WEAK_FUNC(sub_8261C9F8);
PPC_FUNC_IMPL(__imp__sub_8261C9F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// addi r9,r4,16
	ctx.r9.s64 = ctx.r4.s64 + 16;
	// addi r8,r4,32
	ctx.r8.s64 = ctx.r4.s64 + 32;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r7,r4,48
	ctx.r7.s64 = ctx.r4.s64 + 48;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// addi r6,r4,64
	ctx.r6.s64 = ctx.r4.s64 + 64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addi r5,r4,80
	ctx.r5.s64 = ctx.r4.s64 + 80;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r3,r4,96
	ctx.r3.s64 = ctx.r4.s64 + 96;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r31,r4,112
	r31.s64 = ctx.r4.s64 + 112;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v0,v0,1
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_set1_epi16(short(0xD0C))));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261CA78"))) PPC_WEAK_FUNC(sub_8261CA78);
PPC_FUNC_IMPL(__imp__sub_8261CA78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r8,20964(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20964);
	// li r10,2
	ctx.r10.s64 = 2;
	// lwz r7,20968(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20968);
	// li r5,71
	ctx.r5.s64 = 71;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// lwz r11,19984(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19984);
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// lwz r9,368(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 368);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// li r6,1
	ctx.r6.s64 = 1;
	// xori r31,r8,1
	r31.u64 = ctx.r8.u64 ^ 1;
	// rlwinm r8,r7,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// lis r29,256
	r29.s64 = 16777216;
	// xori r30,r8,1
	r30.u64 = ctx.r8.u64 ^ 1;
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r4,r11,4
	ctx.r4.s64 = r11.s64 + 4;
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// lis r11,14563
	r11.s64 = 954400768;
	// ori r3,r11,36409
	ctx.r3.u64 = r11.u64 | 36409;
loc_8261CAD8:
	// mulhw r11,r10,r3
	r11.s64 = (int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32)) >> 32;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// rlwinm r9,r11,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261cb14
	if (cr6.eq) goto loc_8261CB14;
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// add r7,r11,r31
	ctx.r7.u64 = r11.u64 + r31.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// slw r11,r6,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r9.u8 & 0x3F));
	// subf r9,r31,r11
	ctx.r9.s64 = r11.s64 - r31.s64;
	// b 0x8261cb1c
	goto loc_8261CB1C;
loc_8261CB14:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8261CB1C:
	// mulhw r11,r10,r3
	r11.s64 = (int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32)) >> 32;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// rlwinm r8,r11,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261cb4c
	if (cr6.eq) goto loc_8261CB4C;
	// add r8,r11,r30
	ctx.r8.u64 = r11.u64 + r30.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// slw r8,r6,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r8.u8 & 0x3F));
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// b 0x8261cb54
	goto loc_8261CB54;
loc_8261CB4C:
	// li r11,0
	r11.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_8261CB54:
	// rlwimi r9,r8,8,16,23
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 8) & 0xFF00) | (ctx.r9.u64 & 0xFFFFFFFFFFFF00FF);
	// mr r28,r11
	r28.u64 = r11.u64;
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r11,r11,24,0,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF000000;
	// rlwimi r28,r9,4,0,27
	r28.u64 = (__builtin_rotateleft32(ctx.r9.u32, 4) & 0xFFFFFFF0) | (r28.u64 & 0xFFFFFFFF0000000F);
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// rlwinm r9,r28,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// clrlwi r8,r7,28
	ctx.r8.u64 = ctx.r7.u32 & 0xF;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bne cr6,0x8261cad8
	if (!cr6.eq) goto loc_8261CAD8;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8261CB9C"))) PPC_WEAK_FUNC(sub_8261CB9C);
PPC_FUNC_IMPL(__imp__sub_8261CB9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261CBA0"))) PPC_WEAK_FUNC(sub_8261CBA0);
PPC_FUNC_IMPL(__imp__sub_8261CBA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcfc
	// lwz r11,20964(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20964);
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r9,19984(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19984);
	// li r4,125
	ctx.r4.s64 = 125;
	// cntlzw r7,r11
	ctx.r7.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r10,372(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 372);
	// mulli r11,r9,504
	r11.s64 = ctx.r9.s64 * 504;
	// lwz r9,20968(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20968);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// xori r30,r7,1
	r30.u64 = ctx.r7.u64 ^ 1;
	// cntlzw r10,r9
	ctx.r10.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// li r29,1
	r29.s64 = 1;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// lis r11,14563
	r11.s64 = 954400768;
	// xori r31,r10,1
	r31.u64 = ctx.r10.u64 ^ 1;
	// ori r5,r11,36409
	ctx.r5.u64 = r11.u64 | 36409;
loc_8261CBF4:
	// mulhw r11,r8,r5
	r11.s64 = (int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32)) >> 32;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261cc30
	if (cr6.eq) goto loc_8261CC30;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// add r7,r11,r30
	ctx.r7.u64 = r11.u64 + r30.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r11,r29,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// subf r6,r30,r11
	ctx.r6.s64 = r11.s64 - r30.s64;
	// b 0x8261cc38
	goto loc_8261CC38;
loc_8261CC30:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
loc_8261CC38:
	// mulhw r11,r8,r5
	r11.s64 = (int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32)) >> 32;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// srawi. r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8261cc70
	if (cr0.eq) goto loc_8261CC70;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// rlwinm r11,r31,1,0,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// slw r10,r29,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// subf r10,r31,r10
	ctx.r10.s64 = ctx.r10.s64 - r31.s64;
	// b 0x8261cc78
	goto loc_8261CC78;
loc_8261CC70:
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8261CC78:
	// rlwimi r10,r9,8,23,23
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 8) & 0x100) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFEFF);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// clrlwi r10,r10,23
	ctx.r10.u64 = ctx.r10.u32 & 0x1FF;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwimi r6,r10,8,0,23
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r10.u32, 8) & 0xFFFFFF00) | (ctx.r6.u64 & 0xFFFFFFFF000000FF);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// rlwimi r11,r6,4,0,27
	r11.u64 = (__builtin_rotateleft32(ctx.r6.u32, 4) & 0xFFFFFFF0) | (r11.u64 & 0xFFFFFFFF0000000F);
	// rlwimi r7,r11,4,0,27
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0xFFFFFFF0) | (ctx.r7.u64 & 0xFFFFFFFF0000000F);
	// stw r7,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r7.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne cr6,0x8261cbf4
	if (!cr6.eq) goto loc_8261CBF4;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8261CCA8"))) PPC_WEAK_FUNC(sub_8261CCA8);
PPC_FUNC_IMPL(__imp__sub_8261CCA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd0
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r30,0(r5)
	r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// li r21,0
	r21.s64 = 0;
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mullw r9,r11,r30
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// stw r21,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r21.u32);
	// stw r21,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r21.u32);
	// rlwinm r31,r9,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// mr r26,r21
	r26.u64 = r21.u64;
	// mr r24,r21
	r24.u64 = r21.u64;
	// mr r25,r21
	r25.u64 = r21.u64;
	// rlwinm r20,r30,5,0,26
	r20.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r19,r10,5,0,26
	r19.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r31,r10
	r11.u64 = r31.u64 + ctx.r10.u64;
	// addi r18,r9,-4
	r18.s64 = ctx.r9.s64 + -4;
	// bne cr6,0x8261d020
	if (!cr6.eq) goto loc_8261D020;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261cd24
	if (cr6.eq) goto loc_8261CD24;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r25,1
	r25.s64 = 1;
	// add r31,r9,r6
	r31.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lhz r31,-2(r31)
	r31.u64 = PPC_LOAD_U16(r31.u32 + -2);
	// lhz r9,-2(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + -2);
	// extsh r28,r31
	r28.s64 = r31.s16;
	// extsh r27,r9
	r27.s64 = ctx.r9.s16;
	// b 0x8261cd70
	goto loc_8261CD70;
loc_8261CD24:
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x8261cd68
	if (!cr6.eq) goto loc_8261CD68;
	// rotlwi r10,r9,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r6.u32);
	// lhzx r11,r11,r7
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r7.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
loc_8261CD50:
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x8261d050
	if (!cr6.eq) goto loc_8261D050;
loc_8261CD58:
	// stw r21,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r21.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r21,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r21.u32);
	// b 0x8239bd20
	return;
loc_8261CD68:
	// mr r28,r21
	r28.u64 = r21.u64;
	// mr r27,r21
	r27.u64 = r21.u64;
loc_8261CD70:
	// addi r9,r28,-16384
	ctx.r9.s64 = r28.s64 + -16384;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r22,r9,27,31,31
	r22.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x8261cd8c
	if (cr6.eq) goto loc_8261CD8C;
	// mr r27,r21
	r27.u64 = r21.u64;
	// mr r28,r21
	r28.u64 = r21.u64;
loc_8261CD8C:
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r31,r9,r6
	r31.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r6.u32);
	// lhzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r7.u32);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r29,r9
	r29.s64 = ctx.r9.s16;
	// addi r9,r31,-16384
	ctx.r9.s64 = r31.s64 + -16384;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r23,r9,27,31,31
	r23.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8261cdc8
	if (cr6.eq) goto loc_8261CDC8;
	// mr r29,r21
	r29.u64 = r21.u64;
	// mr r31,r21
	r31.u64 = r21.u64;
loc_8261CDC8:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8261ce20
	if (cr6.eq) goto loc_8261CE20;
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// beq cr6,0x8261ce04
	if (cr6.eq) goto loc_8261CE04;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r6.u32);
	// lhzx r9,r11,r7
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r7.u32);
	// b 0x8261ce9c
	goto loc_8261CE9C;
loc_8261CE04:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r6.u32);
	// lhzx r9,r11,r7
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r7.u32);
	// b 0x8261ce9c
	goto loc_8261CE9C;
loc_8261CE20:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261ce7c
	if (cr6.eq) goto loc_8261CE7C;
	// xor r9,r30,r10
	ctx.r9.u64 = r30.u64 ^ ctx.r10.u64;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8261ce50
	if (cr6.eq) goto loc_8261CE50;
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// blt cr6,0x8261ce54
	if (cr6.lt) goto loc_8261CE54;
loc_8261CE50:
	// li r10,1
	ctx.r10.s64 = 1;
loc_8261CE54:
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subfic r10,r10,1
	xer.ca = ctx.r10.u32 <= 1;
	ctx.r10.s64 = 1 - ctx.r10.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r6.u32);
	// lhzx r9,r11,r7
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r7.u32);
	// b 0x8261ce9c
	goto loc_8261CE9C;
loc_8261CE7C:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lhz r10,2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
loc_8261CE9C:
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// addi r9,r11,-16384
	ctx.r9.s64 = r11.s64 + -16384;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8261cec0
	if (cr6.eq) goto loc_8261CEC0;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_8261CEC0:
	// addic. r9,r25,2
	xer.ca = r25.u32 > 4294967293;
	ctx.r9.s64 = r25.s64 + 2;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x8261cd58
	if (cr0.eq) goto loc_8261CD58;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8261cf00
	if (cr6.eq) goto loc_8261CF00;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// bne cr6,0x8261cf00
	if (!cr6.eq) goto loc_8261CF00;
	// rlwinm r9,r27,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8261cef4
	if (cr6.eq) goto loc_8261CEF4;
	// stw r28,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r28.u32);
	// li r24,1
	r24.s64 = 1;
	// stw r27,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r27.u32);
	// b 0x8261cf00
	goto loc_8261CF00;
loc_8261CEF4:
	// stw r28,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r28.u32);
	// li r26,1
	r26.s64 = 1;
	// stw r27,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r27.u32);
loc_8261CF00:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8261cf40
	if (!cr6.eq) goto loc_8261CF40;
	// rlwinm r9,r29,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8261cf28
	if (cr6.eq) goto loc_8261CF28;
	// rlwinm r9,r24,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// b 0x8261cf38
	goto loc_8261CF38;
loc_8261CF28:
	// rlwinm r9,r26,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-192
	ctx.r7.s64 = ctx.r1.s64 + -192;
	// addi r6,r1,-176
	ctx.r6.s64 = ctx.r1.s64 + -176;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
loc_8261CF38:
	// stwx r29,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, r29.u32);
	// stwx r31,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, r31.u32);
loc_8261CF40:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x8261cf80
	if (!cr6.eq) goto loc_8261CF80;
	// rlwinm r9,r10,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8261cf68
	if (cr6.eq) goto loc_8261CF68;
	// rlwinm r9,r24,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-160
	ctx.r8.s64 = ctx.r1.s64 + -160;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// b 0x8261cf78
	goto loc_8261CF78;
loc_8261CF68:
	// rlwinm r9,r26,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-192
	ctx.r8.s64 = ctx.r1.s64 + -192;
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
loc_8261CF78:
	// stwx r10,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r10.u32);
	// stwx r11,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, r11.u32);
loc_8261CF80:
	// cmpwi cr6,r26,3
	cr6.compare<int32_t>(r26.s32, 3, xer);
	// beq cr6,0x8261cfb4
	if (cr6.eq) goto loc_8261CFB4;
	// cmpwi cr6,r24,3
	cr6.compare<int32_t>(r24.s32, 3, xer);
	// beq cr6,0x8261cfb4
	if (cr6.eq) goto loc_8261CFB4;
	// cmpw cr6,r26,r24
	cr6.compare<int32_t>(r26.s32, r24.s32, xer);
	// ble cr6,0x8261cfa4
	if (!cr6.gt) goto loc_8261CFA4;
loc_8261CF98:
	// lwz r10,-192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// b 0x8261cd50
	goto loc_8261CD50;
loc_8261CFA4:
	// bge cr6,0x8261cf98
	if (!cr6.lt) goto loc_8261CF98;
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r11,-144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -144);
	// b 0x8261cd50
	goto loc_8261CD50;
loc_8261CFB4:
	// subf r7,r11,r31
	ctx.r7.s64 = r31.s64 - r11.s64;
	// subf r9,r28,r31
	ctx.r9.s64 = r31.s64 - r28.s64;
	// subf r6,r28,r11
	ctx.r6.s64 = r11.s64 - r28.s64;
	// subf r8,r27,r29
	ctx.r8.s64 = r29.s64 - r27.s64;
	// subf r30,r10,r29
	r30.s64 = r29.s64 - ctx.r10.s64;
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// subf r26,r27,r10
	r26.s64 = ctx.r10.s64 - r27.s64;
	// xor r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r9.u64;
	// xor r30,r30,r8
	r30.u64 = r30.u64 ^ ctx.r8.u64;
	// srawi r9,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 31;
	// xor r26,r26,r8
	r26.u64 = r26.u64 ^ ctx.r8.u64;
	// srawi r8,r6,31
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 31;
	// srawi r7,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = r30.s32 >> 31;
	// srawi r6,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = r26.s32 >> 31;
	// or r30,r9,r8
	r30.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r26,r7,r6
	r26.u64 = ctx.r7.u64 | ctx.r6.u64;
	// andc r11,r11,r30
	r11.u64 = r11.u64 & ~r30.u64;
	// and r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 & r31.u64;
	// andc r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 & ~r26.u64;
	// and r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 & r29.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// and r9,r8,r28
	ctx.r9.u64 = ctx.r8.u64 & r28.u64;
	// or r8,r10,r7
	ctx.r8.u64 = ctx.r10.u64 | ctx.r7.u64;
	// and r7,r6,r27
	ctx.r7.u64 = ctx.r6.u64 & r27.u64;
	// or r10,r11,r9
	ctx.r10.u64 = r11.u64 | ctx.r9.u64;
	// or r11,r8,r7
	r11.u64 = ctx.r8.u64 | ctx.r7.u64;
	// b 0x8261cd50
	goto loc_8261CD50;
loc_8261D020:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261d048
	if (cr6.eq) goto loc_8261D048;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r7
	ctx.r10.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// lhz r10,-2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// lhz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// b 0x8261cd50
	goto loc_8261CD50;
loc_8261D048:
	// mr r11,r21
	r11.u64 = r21.u64;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
loc_8261D050:
	// rlwinm r9,r11,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r9,140(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// rlwinm r9,r9,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0xFFFFFFC0;
	// beq cr6,0x8261d070
	if (cr6.eq) goto loc_8261D070;
	// li r7,-124
	ctx.r7.s64 = -124;
	// addi r6,r9,-8
	ctx.r6.s64 = ctx.r9.s64 + -8;
	// b 0x8261d078
	goto loc_8261D078;
loc_8261D070:
	// li r7,-120
	ctx.r7.s64 = -120;
	// addi r6,r9,-4
	ctx.r6.s64 = ctx.r9.s64 + -4;
loc_8261D078:
	// add r9,r10,r19
	ctx.r9.u64 = ctx.r10.u64 + r19.u64;
	// add r8,r11,r20
	ctx.r8.u64 = r11.u64 + r20.u64;
	// cmpwi cr6,r9,-60
	cr6.compare<int32_t>(ctx.r9.s32, -60, xer);
	// bge cr6,0x8261d094
	if (!cr6.lt) goto loc_8261D094;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// addi r10,r10,-60
	ctx.r10.s64 = ctx.r10.s64 + -60;
	// b 0x8261d0a4
	goto loc_8261D0A4;
loc_8261D094:
	// cmpw cr6,r9,r18
	cr6.compare<int32_t>(ctx.r9.s32, r18.s32, xer);
	// ble cr6,0x8261d0a4
	if (!cr6.gt) goto loc_8261D0A4;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
loc_8261D0A4:
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// bge cr6,0x8261d0c4
	if (!cr6.lt) goto loc_8261D0C4;
	// subf r9,r8,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// b 0x8239bd20
	return;
loc_8261D0C4:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// ble cr6,0x8261d0d4
	if (!cr6.gt) goto loc_8261D0D4;
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_8261D0D4:
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_8261D0E4"))) PPC_WEAK_FUNC(sub_8261D0E4);
PPC_FUNC_IMPL(__imp__sub_8261D0E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261D0E8"))) PPC_WEAK_FUNC(sub_8261D0E8);
PPC_FUNC_IMPL(__imp__sub_8261D0E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8261d254
	if (cr6.eq) goto loc_8261D254;
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r10,216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// srawi r30,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r30.s64 = r11.s32 >> 1;
	// srawi r29,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r29.s64 = ctx.r10.s32 >> 1;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r7,r10,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r30,212(r31)
	PPC_STORE_U32(r31.u32 + 212, r30.u32);
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r29,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r29.u32);
	// rlwinm r8,r11,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r7,232(r31)
	PPC_STORE_U32(r31.u32 + 232, ctx.r7.u32);
	// stw r8,228(r31)
	PPC_STORE_U32(r31.u32 + 228, ctx.r8.u32);
	// stw r9,236(r31)
	PPC_STORE_U32(r31.u32 + 236, ctx.r9.u32);
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stw r10,208(r31)
	PPC_STORE_U32(r31.u32 + 208, ctx.r10.u32);
	// beq cr6,0x8261d264
	if (cr6.eq) goto loc_8261D264;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d264
	if (cr6.eq) goto loc_8261D264;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r8,268(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r8
	ctx.r4.u64 = r11.u64 + ctx.r8.u64;
	// beq cr6,0x8261d264
	if (cr6.eq) goto loc_8261D264;
loc_8261D1A4:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x8261d23c
	if (!cr6.gt) goto loc_8261D23C;
	// cntlzw r10,r6
	ctx.r10.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// rlwinm r5,r10,28,30,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
loc_8261D1CC:
	// lwz r7,136(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cntlzw r3,r11
	ctx.r3.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r8,140(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// subf r7,r11,r7
	ctx.r7.s64 = ctx.r7.s64 - r11.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// rlwinm r8,r8,28,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0x2;
	// rlwinm r3,r3,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 27) & 0x1;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// or r7,r3,r5
	ctx.r7.u64 = ctx.r3.u64 | ctx.r5.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r7,r7,28
	ctx.r7.u64 = ctx.r7.u32 & 0xF;
	// rlwinm r3,r30,0,20,15
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFFFFFFFF0FFF;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r8,12,0,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFFFF000;
	// or r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 | ctx.r3.u64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x8261d1cc
	if (cr6.lt) goto loc_8261D1CC;
loc_8261D23C:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// blt cr6,0x8261d1a4
	if (cr6.lt) goto loc_8261D1A4;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_8261D254:
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
loc_8261D264:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8261D26C"))) PPC_WEAK_FUNC(sub_8261D26C);
PPC_FUNC_IMPL(__imp__sub_8261D26C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8261D270"))) PPC_WEAK_FUNC(sub_8261D270);
PPC_FUNC_IMPL(__imp__sub_8261D270) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// li r28,1
	r28.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r26,r27
	r26.u64 = r27.u64;
	// stw r27,17440(r29)
	PPC_STORE_U32(r29.u32 + 17440, r27.u32);
	// stw r28,19328(r29)
	PPC_STORE_U32(r29.u32 + 19328, r28.u32);
	// stw r28,1944(r29)
	PPC_STORE_U32(r29.u32 + 1944, r28.u32);
	// bl 0x8261d0e8
	sub_8261D0E8(ctx, base);
	// ld r11,3576(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 3576);
	// stw r27,21000(r29)
	PPC_STORE_U32(r29.u32 + 21000, r27.u32);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// bne cr6,0x8261d2cc
	if (!cr6.eq) goto loc_8261D2CC;
	// lwz r11,20836(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20836);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d2c4
	if (cr6.eq) goto loc_8261D2C4;
	// stw r28,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, r28.u32);
	// b 0x8261d2d8
	goto loc_8261D2D8;
loc_8261D2C4:
	// stw r27,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, r27.u32);
	// b 0x8261d2d8
	goto loc_8261D2D8;
loc_8261D2CC:
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r11,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, r11.u32);
loc_8261D2D8:
	// lwz r11,20836(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20836);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d2ec
	if (cr6.eq) goto loc_8261D2EC;
	// stw r27,19984(r29)
	PPC_STORE_U32(r29.u32 + 19984, r27.u32);
	// b 0x8261d2f0
	goto loc_8261D2F0;
loc_8261D2EC:
	// stw r28,19984(r29)
	PPC_STORE_U32(r29.u32 + 19984, r28.u32);
loc_8261D2F0:
	// lwz r11,21072(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21072);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,284(r29)
	PPC_STORE_U32(r29.u32 + 284, r11.u32);
	// bl 0x826322d0
	sub_826322D0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8261d760
	if (!cr6.eq) goto loc_8261D760;
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d328
	if (!cr6.eq) goto loc_8261D328;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,21080(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 21080);
	// bl 0x82634b40
	sub_82634B40(ctx, base);
	// b 0x8261d370
	goto loc_8261D370;
loc_8261D328:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261d370
	if (!cr6.eq) goto loc_8261D370;
	// lwz r10,14776(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 14776);
	// lwz r9,3392(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 3392);
	// lwz r11,21080(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21080);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r4,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x8261d35c
	if (!cr6.lt) goto loc_8261D35C;
	// mr r31,r27
	r31.u64 = r27.u64;
loc_8261D35C:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82634b40
	sub_82634B40(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82634c28
	sub_82634C28(ctx, base);
loc_8261D370:
	// lwz r10,284(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8261d380
	if (!cr6.eq) goto loc_8261D380;
	// stw r27,3380(r29)
	PPC_STORE_U32(r29.u32 + 3380, r27.u32);
loc_8261D380:
	// lwz r11,21072(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21072);
	// stw r28,1944(r29)
	PPC_STORE_U32(r29.u32 + 1944, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d398
	if (cr6.eq) goto loc_8261D398;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x8261d49c
	if (!cr6.eq) goto loc_8261D49C;
loc_8261D398:
	// lwz r11,14788(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d49c
	if (cr6.eq) goto loc_8261D49C;
	// lwz r11,14772(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261d48c
	if (!cr6.gt) goto loc_8261D48C;
	// lwz r11,3376(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3376);
	// cmpwi cr6,r11,-3
	cr6.compare<int32_t>(r11.s32, -3, xer);
	// bne cr6,0x8261d3f4
	if (!cr6.eq) goto loc_8261D3F4;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// lwz r11,21368(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21368);
	// lwz r10,3396(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 3396);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,3376(r29)
	PPC_STORE_U32(r29.u32 + 3376, r11.u32);
	// beq cr6,0x8261d3e8
	if (cr6.eq) goto loc_8261D3E8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_8261D3E8:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
	// b 0x8261d49c
	goto loc_8261D49C;
loc_8261D3F4:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d45c
	if (!cr6.eq) goto loc_8261D45C;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bne cr6,0x8261d438
	if (!cr6.eq) goto loc_8261D438;
	// lwz r11,21004(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21004);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r28,3408(r29)
	PPC_STORE_U32(r29.u32 + 3408, r28.u32);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r28,21000(r29)
	PPC_STORE_U32(r29.u32 + 21000, r28.u32);
	// stw r11,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, r11.u32);
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_8261D438:
	// stw r27,3376(r29)
	PPC_STORE_U32(r29.u32 + 3376, r27.u32);
	// bl 0x826277e8
	sub_826277E8(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,19984(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
	// stw r27,3384(r29)
	PPC_STORE_U32(r29.u32 + 3384, r27.u32);
	// b 0x8261d49c
	goto loc_8261D49C;
loc_8261D45C:
	// lwz r11,3396(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d470
	if (cr6.eq) goto loc_8261D470;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_8261D470:
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8261d484
	if (cr6.eq) goto loc_8261D484;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
loc_8261D484:
	// stw r27,3384(r29)
	PPC_STORE_U32(r29.u32 + 3384, r27.u32);
	// b 0x8261d49c
	goto loc_8261D49C;
loc_8261D48C:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r4,r11,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
loc_8261D49C:
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d708
	if (cr6.eq) goto loc_8261D708;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8261d708
	if (cr6.eq) goto loc_8261D708;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825ebb40
	sub_825EBB40(ctx, base);
	// lwz r11,14788(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d53c
	if (cr6.eq) goto loc_8261D53C;
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x8261d53c
	if (cr6.eq) goto loc_8261D53C;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261d58c
	if (!cr6.eq) goto loc_8261D58C;
	// lwz r11,3376(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d524
	if (!cr6.eq) goto loc_8261D524;
	// lwz r11,21368(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21368);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261d524
	if (!cr6.eq) goto loc_8261D524;
	// lwz r11,21004(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21004);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r28,3408(r29)
	PPC_STORE_U32(r29.u32 + 3408, r28.u32);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r28,21000(r29)
	PPC_STORE_U32(r29.u32 + 21000, r28.u32);
	// stw r11,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, r11.u32);
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_8261D524:
	// lwz r11,3396(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d58c
	if (cr6.eq) goto loc_8261D58C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
	// b 0x8261d58c
	goto loc_8261D58C;
loc_8261D53C:
	// lwz r11,14772(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261d558
	if (!cr6.eq) goto loc_8261D558;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825f50e0
	sub_825F50E0(ctx, base);
	// b 0x8261d588
	goto loc_8261D588;
loc_8261D558:
	// lwz r11,3376(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3376);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d56c
	if (!cr6.eq) goto loc_8261D56C;
	// stw r27,3376(r29)
	PPC_STORE_U32(r29.u32 + 3376, r27.u32);
	// b 0x8261d578
	goto loc_8261D578;
loc_8261D56C:
	// lwz r11,3396(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d580
	if (cr6.eq) goto loc_8261D580;
loc_8261D578:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826277e8
	sub_826277E8(ctx, base);
loc_8261D580:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82627880
	sub_82627880(ctx, base);
loc_8261D588:
	// stw r27,3384(r29)
	PPC_STORE_U32(r29.u32 + 3384, r27.u32);
loc_8261D58C:
	// lwz r11,3964(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d680
	if (cr6.eq) goto loc_8261D680;
	// lwz r11,14772(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d5ac
	if (cr6.eq) goto loc_8261D5AC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_8261D5AC:
	// lwz r11,20024(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20024);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d5cc
	if (cr6.eq) goto loc_8261D5CC;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,20036(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 20036);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,20032(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 20032);
	// bl 0x82625910
	sub_82625910(ctx, base);
loc_8261D5CC:
	// lwz r11,20028(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20028);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d5ec
	if (cr6.eq) goto loc_8261D5EC;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r6,20044(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 20044);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,20040(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 20040);
	// bl 0x82625910
	sub_82625910(ctx, base);
loc_8261D5EC:
	// lwz r11,21520(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21520);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261d680
	if (!cr6.eq) goto loc_8261D680;
	// lwz r11,204(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r5,172(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 172);
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,184(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 184);
	// lwz r9,164(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 164);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,220(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 3732);
	// lwz r31,15856(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 15856);
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,208(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 208);
	// lwz r31,196(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 196);
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r6,176(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 176);
	// lwz r10,168(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r7,224(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 224);
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,3740(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 3740);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,3736(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 3736);
	// lwz r30,15852(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 15852);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mtctr r30
	ctr.u64 = r30.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8261D680:
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261d698
	if (!cr6.eq) goto loc_8261D698;
	// bl 0x8271d1a0
	sub_8271D1A0(ctx, base);
	// b 0x8261d69c
	goto loc_8261D69C;
loc_8261D698:
	// bl 0x8271d060
	sub_8271D060(ctx, base);
loc_8261D69C:
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,4
	cr6.compare<int32_t>(r31.s32, 4, xer);
	// bne cr6,0x8261d758
	if (!cr6.eq) goto loc_8261D758;
loc_8261D6A8:
	// li r26,4
	r26.s64 = 4;
loc_8261D6AC:
	// lwz r11,21072(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21072);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d790
	if (cr6.eq) goto loc_8261D790;
	// lwz r11,21076(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21076);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261d790
	if (!cr6.eq) goto loc_8261D790;
	// lwz r11,14788(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d790
	if (cr6.eq) goto loc_8261D790;
	// lwz r11,14772(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261d790
	if (!cr6.gt) goto loc_8261D790;
	// lwz r11,3376(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3376);
	// cmpwi cr6,r11,-3
	cr6.compare<int32_t>(r11.s32, -3, xer);
	// bne cr6,0x8261d780
	if (!cr6.eq) goto loc_8261D780;
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// lwz r11,21368(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21368);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,3376(r29)
	PPC_STORE_U32(r29.u32 + 3376, r11.u32);
	// b 0x8261d790
	goto loc_8261D790;
loc_8261D708:
	// lwz r11,15508(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 15508);
	// stw r28,3668(r29)
	PPC_STORE_U32(r29.u32 + 3668, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261d728
	if (!cr6.eq) goto loc_8261D728;
	// lwz r11,152(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// bne cr6,0x8261d72c
	if (!cr6.eq) goto loc_8261D72C;
loc_8261D728:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_8261D72C:
	// lwz r10,14820(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 14820);
	// stw r11,15476(r29)
	PPC_STORE_U32(r29.u32 + 15476, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261d744
	if (cr6.eq) goto loc_8261D744;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825f7ab8
	sub_825F7AB8(ctx, base);
loc_8261D744:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8271cd80
	sub_8271CD80(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,4
	cr6.compare<int32_t>(r31.s32, 4, xer);
	// beq cr6,0x8261d6a8
	if (cr6.eq) goto loc_8261D6A8;
loc_8261D758:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x8261d6ac
	if (cr6.eq) goto loc_8261D6AC;
loc_8261D760:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_8261D780:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d78c
	if (!cr6.eq) goto loc_8261D78C;
	// stw r27,3376(r29)
	PPC_STORE_U32(r29.u32 + 3376, r27.u32);
loc_8261D78C:
	// stw r27,3384(r29)
	PPC_STORE_U32(r29.u32 + 3384, r27.u32);
loc_8261D790:
	// lwz r11,19976(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// lwz r11,19980(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,19984(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// lwz r10,21004(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 21004);
	// lwz r9,21076(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 21076);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// stw r28,21000(r29)
	PPC_STORE_U32(r29.u32 + 21000, r28.u32);
	// stw r11,19984(r29)
	PPC_STORE_U32(r29.u32 + 19984, r11.u32);
	// stw r10,21004(r29)
	PPC_STORE_U32(r29.u32 + 21004, ctx.r10.u32);
	// stw r9,284(r29)
	PPC_STORE_U32(r29.u32 + 284, ctx.r9.u32);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d880
	if (cr6.eq) goto loc_8261D880;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bge cr6,0x8261d86c
	if (!cr6.lt) goto loc_8261D86C;
loc_8261D7FC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8261d844
	if (cr6.gt) goto loc_8261D844;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,40
	cr6.compare<int32_t>(ctx.r10.s32, 40, xer);
	// bgt cr6,0x8261d97c
	if (cr6.gt) goto loc_8261D97C;
	// subfic r7,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x8261d85c
	goto loc_8261D85C;
loc_8261D844:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d86c
	if (!cr6.eq) goto loc_8261D86C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_8261D85C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// blt cr6,0x8261d7fc
	if (cr6.lt) goto loc_8261D7FC;
loc_8261D86C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
loc_8261D874:
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
loc_8261D880:
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261d90c
	if (cr6.eq) goto loc_8261D90C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r28
	r30.u64 = r28.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261d8e4
	if (!cr6.lt) goto loc_8261D8E4;
loc_8261D8A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261d8e4
	if (cr6.eq) goto loc_8261D8E4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261d8d4
	if (!cr0.lt) goto loc_8261D8D4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261D8D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261d8a4
	if (cr6.gt) goto loc_8261D8A4;
loc_8261D8E4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261d90c
	if (!cr0.lt) goto loc_8261D90C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261D90C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// bge cr6,0x8261d9e0
	if (!cr6.lt) goto loc_8261D9E0;
loc_8261D934:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8261d9b8
	if (cr6.gt) goto loc_8261D9B8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,40
	cr6.compare<int32_t>(ctx.r10.s32, 40, xer);
	// bgt cr6,0x8261dadc
	if (cr6.gt) goto loc_8261DADC;
	// subfic r7,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x8261d9d0
	goto loc_8261D9D0;
loc_8261D97C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bge cr6,0x8261d86c
	if (!cr6.lt) goto loc_8261D86C;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8261d86c
	if (cr6.gt) goto loc_8261D86C;
	// addi r9,r10,248
	ctx.r9.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rldicl r11,r11,33,31
	r11.u64 = __builtin_rotateleft64(r11.u64, 33) & 0x1FFFFFFFF;
	// b 0x8261d874
	goto loc_8261D874;
loc_8261D9B8:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261d9e0
	if (!cr6.eq) goto loc_8261D9E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_8261D9D0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x8261d934
	if (cr6.lt) goto loc_8261D934;
loc_8261D9E0:
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
loc_8261D9E4:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8261de1c
	if (!cr6.eq) goto loc_8261DE1C;
loc_8261D9F0:
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8261dba8
	if (!cr6.eq) goto loc_8261DBA8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,16
	r30.s64 = 16;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x8261da54
	if (!cr6.lt) goto loc_8261DA54;
loc_8261DA14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261da54
	if (cr6.eq) goto loc_8261DA54;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261da44
	if (!cr0.lt) goto loc_8261DA44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DA44:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261da14
	if (cr6.gt) goto loc_8261DA14;
loc_8261DA54:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261da7c
	if (!cr0.lt) goto loc_8261DA7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DA7C:
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// bge cr6,0x8261db44
	if (!cr6.lt) goto loc_8261DB44;
loc_8261DA94:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8261db1c
	if (cr6.gt) goto loc_8261DB1C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,40
	cr6.compare<int32_t>(ctx.r10.s32, 40, xer);
	// bgt cr6,0x8261db68
	if (cr6.gt) goto loc_8261DB68;
	// subfic r7,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x8261db34
	goto loc_8261DB34;
loc_8261DADC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// bge cr6,0x8261d9e0
	if (!cr6.lt) goto loc_8261D9E0;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8261d9e0
	if (cr6.gt) goto loc_8261D9E0;
	// addi r9,r10,248
	ctx.r9.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rldicl r11,r11,48,16
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFFFFFFFFFF;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// b 0x8261d9e4
	goto loc_8261D9E4;
loc_8261DB1C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261db44
	if (!cr6.eq) goto loc_8261DB44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_8261DB34:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x8261da94
	if (cr6.lt) goto loc_8261DA94;
loc_8261DB44:
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
loc_8261DB48:
	// lwz r9,84(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lwz r9,20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8261e140
	if (!cr6.eq) goto loc_8261E140;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261d9f0
	if (cr6.eq) goto loc_8261D9F0;
	// b 0x8261dbb0
	goto loc_8261DBB0;
loc_8261DB68:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// bge cr6,0x8261db44
	if (!cr6.lt) goto loc_8261DB44;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8261db44
	if (cr6.gt) goto loc_8261DB44;
	// addi r9,r10,248
	ctx.r9.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rldicl r11,r11,48,16
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFFFFFFFFFF;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// b 0x8261db48
	goto loc_8261DB48;
loc_8261DBA8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8261de1c
	if (cr6.eq) goto loc_8261DE1C;
loc_8261DBB0:
	// cmplwi cr6,r11,268
	cr6.compare<uint32_t>(r11.u32, 268, xer);
	// bne cr6,0x8261dc3c
	if (!cr6.eq) goto loc_8261DC3C;
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x8261dc10
	if (!cr6.lt) goto loc_8261DC10;
loc_8261DBD0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261dc10
	if (cr6.eq) goto loc_8261DC10;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261dc00
	if (!cr0.lt) goto loc_8261DC00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DC00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261dbd0
	if (cr6.gt) goto loc_8261DBD0;
loc_8261DC10:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261de34
	if (!cr0.lt) goto loc_8261DE34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// b 0x8261de34
	goto loc_8261DE34;
loc_8261DC3C:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x8261de34
	if (!cr6.eq) goto loc_8261DE34;
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8261dc9c
	if (!cr6.lt) goto loc_8261DC9C;
loc_8261DC5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261dc9c
	if (cr6.eq) goto loc_8261DC9C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261dc8c
	if (!cr0.lt) goto loc_8261DC8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DC8C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261dc5c
	if (cr6.gt) goto loc_8261DC5C;
loc_8261DC9C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261dcc4
	if (!cr0.lt) goto loc_8261DCC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DCC4:
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// bge cr6,0x8261dd4c
	if (!cr6.lt) goto loc_8261DD4C;
loc_8261DCDC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8261dd24
	if (cr6.gt) goto loc_8261DD24;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,40
	cr6.compare<int32_t>(ctx.r10.s32, 40, xer);
	// bgt cr6,0x8261dddc
	if (cr6.gt) goto loc_8261DDDC;
	// subfic r7,r10,40
	xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// b 0x8261dd3c
	goto loc_8261DD3C;
loc_8261DD24:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261dd4c
	if (!cr6.eq) goto loc_8261DD4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb590
	sub_825EB590(ctx, base);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_8261DD3C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x8261dcdc
	if (cr6.lt) goto loc_8261DCDC;
loc_8261DD4C:
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
loc_8261DD50:
	// cmplwi cr6,r11,268
	cr6.compare<uint32_t>(r11.u32, 268, xer);
	// bne cr6,0x8261de34
	if (!cr6.eq) goto loc_8261DE34;
	// lwz r31,84(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x8261ddb0
	if (!cr6.lt) goto loc_8261DDB0;
loc_8261DD70:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ddb0
	if (cr6.eq) goto loc_8261DDB0;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261dda0
	if (!cr0.lt) goto loc_8261DDA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261DDA0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261dd70
	if (cr6.gt) goto loc_8261DD70;
loc_8261DDB0:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261de34
	if (!cr0.lt) goto loc_8261DE34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// b 0x8261de34
	goto loc_8261DE34;
loc_8261DDDC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// bge cr6,0x8261dd4c
	if (!cr6.lt) goto loc_8261DD4C;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8261dd4c
	if (cr6.gt) goto loc_8261DD4C;
	// addi r9,r10,248
	ctx.r9.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// srd r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rldicl r11,r11,48,16
	r11.u64 = __builtin_rotateleft64(r11.u64, 48) & 0xFFFFFFFFFFFF;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// b 0x8261dd50
	goto loc_8261DD50;
loc_8261DE1C:
	// lwz r11,21480(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261de34
	if (cr6.eq) goto loc_8261DE34;
	// lwz r11,21344(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21344);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
loc_8261DE34:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826322d0
	sub_826322D0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8261d760
	if (!cr6.eq) goto loc_8261D760;
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261de64
	if (!cr6.eq) goto loc_8261DE64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,21080(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 21080);
	// bl 0x82634b40
	sub_82634B40(ctx, base);
	// b 0x8261deac
	goto loc_8261DEAC;
loc_8261DE64:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261deac
	if (!cr6.eq) goto loc_8261DEAC;
	// lwz r10,14776(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 14776);
	// lwz r9,3392(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 3392);
	// lwz r11,21080(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21080);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r4,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 8;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x8261de98
	if (!cr6.lt) goto loc_8261DE98;
	// mr r31,r27
	r31.u64 = r27.u64;
loc_8261DE98:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82634b40
	sub_82634B40(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82634c28
	sub_82634C28(ctx, base);
loc_8261DEAC:
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261debc
	if (!cr6.eq) goto loc_8261DEBC;
	// stw r27,3380(r29)
	PPC_STORE_U32(r29.u32 + 3380, r27.u32);
loc_8261DEBC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r28,1944(r29)
	PPC_STORE_U32(r29.u32 + 1944, r28.u32);
	// beq cr6,0x8261e0c8
	if (cr6.eq) goto loc_8261E0C8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8261e0c8
	if (cr6.eq) goto loc_8261E0C8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825ebb40
	sub_825EBB40(ctx, base);
	// lwz r11,3964(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e034
	if (cr6.eq) goto loc_8261E034;
	// lwz r11,14772(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14772);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261def8
	if (cr6.eq) goto loc_8261DEF8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825f3a70
	sub_825F3A70(ctx, base);
loc_8261DEF8:
	// lwz r11,20024(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20024);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261df34
	if (cr6.eq) goto loc_8261DF34;
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261df20
	if (!cr6.eq) goto loc_8261DF20;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8261b558
	sub_8261B558(ctx, base);
	// stw r28,15564(r29)
	PPC_STORE_U32(r29.u32 + 15564, r28.u32);
loc_8261DF20:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,20036(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 20036);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,20032(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 20032);
	// bl 0x82625910
	sub_82625910(ctx, base);
loc_8261DF34:
	// lwz r11,20028(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20028);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261df70
	if (cr6.eq) goto loc_8261DF70;
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261df5c
	if (!cr6.eq) goto loc_8261DF5C;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8261b558
	sub_8261B558(ctx, base);
	// stw r28,15564(r29)
	PPC_STORE_U32(r29.u32 + 15564, r28.u32);
loc_8261DF5C:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r6,20044(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 20044);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,20040(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 20040);
	// bl 0x82625910
	sub_82625910(ctx, base);
loc_8261DF70:
	// lwz r11,21520(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 21520);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261e034
	if (!cr6.eq) goto loc_8261E034;
	// lwz r11,20024(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20024);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261df94
	if (cr6.eq) goto loc_8261DF94;
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261dfac
	if (cr6.eq) goto loc_8261DFAC;
loc_8261DF94:
	// lwz r11,20028(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20028);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e034
	if (cr6.eq) goto loc_8261E034;
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8261e034
	if (!cr6.eq) goto loc_8261E034;
loc_8261DFAC:
	// lwz r11,204(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r5,172(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 172);
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r10,184(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 184);
	// lwz r9,164(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 164);
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,220(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 220);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 3732);
	// lwz r31,15856(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 15856);
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,208(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 208);
	// lwz r31,196(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 196);
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lwz r6,176(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 176);
	// lwz r10,168(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r7,224(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 224);
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,3740(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 3740);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,3736(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 3736);
	// lwz r30,15852(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 15852);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mtctr r30
	ctr.u64 = r30.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8261E034:
	// lwz r11,284(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 284);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261e04c
	if (!cr6.eq) goto loc_8261E04C;
	// bl 0x8271d1a0
	sub_8271D1A0(ctx, base);
	// b 0x8261e050
	goto loc_8261E050;
loc_8261E04C:
	// bl 0x8271d060
	sub_8271D060(ctx, base);
loc_8261E050:
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,4
	cr6.compare<int32_t>(r31.s32, 4, xer);
	// bne cr6,0x8261e118
	if (!cr6.eq) goto loc_8261E118;
loc_8261E05C:
	// li r26,4
	r26.s64 = 4;
loc_8261E060:
	// lwz r11,15564(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 15564);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e078
	if (cr6.eq) goto loc_8261E078;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,19984(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// bl 0x8261b558
	sub_8261B558(ctx, base);
loc_8261E078:
	// lwz r11,19976(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// lwz r11,19980(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e140
	if (cr6.eq) goto loc_8261E140;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,19984(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// bl 0x8261b310
	sub_8261B310(ctx, base);
	// lwz r11,19984(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 19984);
	// li r4,0
	ctx.r4.s64 = 0;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,19984(r29)
	PPC_STORE_U32(r29.u32 + 19984, r11.u32);
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_8261E0C8:
	// lwz r11,15508(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 15508);
	// stw r28,3668(r29)
	PPC_STORE_U32(r29.u32 + 3668, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261e0e8
	if (!cr6.eq) goto loc_8261E0E8;
	// lwz r11,152(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// bne cr6,0x8261e0ec
	if (!cr6.eq) goto loc_8261E0EC;
loc_8261E0E8:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_8261E0EC:
	// lwz r10,14820(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 14820);
	// stw r11,15476(r29)
	PPC_STORE_U32(r29.u32 + 15476, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261e104
	if (cr6.eq) goto loc_8261E104;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825f7ab8
	sub_825F7AB8(ctx, base);
loc_8261E104:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8271cd80
	sub_8271CD80(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,4
	cr6.compare<int32_t>(r31.s32, 4, xer);
	// beq cr6,0x8261e05c
	if (cr6.eq) goto loc_8261E05C;
loc_8261E118:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x8261e060
	if (cr6.eq) goto loc_8261E060;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
loc_8261E140:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82608240
	sub_82608240(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82607d08
	sub_82607D08(ctx, base);
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8261E160"))) PPC_WEAK_FUNC(sub_8261E160);
PPC_FUNC_IMPL(__imp__sub_8261E160) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// lwz r10,16(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// blelr cr6
	if (!cr6.gt) return;
	// addi r11,r3,2
	r11.s64 = ctx.r3.s64 + 2;
	// addi r7,r4,-1
	ctx.r7.s64 = ctx.r4.s64 + -1;
loc_8261E18C:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261e1b4
	if (cr6.eq) goto loc_8261E1B4;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// ble cr6,0x8261e1ac
	if (!cr6.gt) goto loc_8261E1AC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// b 0x8261e1b0
	goto loc_8261E1B0;
loc_8261E1AC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
loc_8261E1B0:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_8261E1B4:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x8261e18c
	if (!cr6.eq) goto loc_8261E18C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8261E1C8"))) PPC_WEAK_FUNC(sub_8261E1C8);
PPC_FUNC_IMPL(__imp__sub_8261E1C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r25,0
	r25.s64 = 0;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// bl 0x825f2db0
	sub_825F2DB0(ctx, base);
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x8261e484
	if (!cr6.eq) goto loc_8261E484;
	// lwz r11,20832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20832);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e438
	if (cr6.eq) goto loc_8261E438;
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e384
	if (cr6.eq) goto loc_8261E384;
	// lwz r11,21548(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261e384
	if (!cr6.eq) goto loc_8261E384;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r26,1
	r26.s64 = 1;
	// mr r29,r25
	r29.u64 = r25.u64;
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e28c
	if (!cr6.lt) goto loc_8261E28C;
loc_8261E234:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e28c
	if (cr6.eq) goto loc_8261E28C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e27c
	if (!cr0.lt) goto loc_8261E27C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E27C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e234
	if (cr6.gt) goto loc_8261E234;
loc_8261E28C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e2c8
	if (!cr0.lt) goto loc_8261E2C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E2C8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// stw r28,20836(r27)
	PPC_STORE_U32(r27.u32 + 20836, r28.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e340
	if (!cr6.lt) goto loc_8261E340;
loc_8261E2E8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e340
	if (cr6.eq) goto loc_8261E340;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e330
	if (!cr0.lt) goto loc_8261E330;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E330:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e2e8
	if (cr6.gt) goto loc_8261E2E8;
loc_8261E340:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e37c
	if (!cr0.lt) goto loc_8261E37C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E37C:
	// stw r30,20840(r27)
	PPC_STORE_U32(r27.u32 + 20840, r30.u32);
	// b 0x8261e438
	goto loc_8261E438;
loc_8261E384:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261e3f8
	if (!cr6.lt) goto loc_8261E3F8;
loc_8261E3A0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e3f8
	if (cr6.eq) goto loc_8261E3F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e3e8
	if (!cr0.lt) goto loc_8261E3E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E3E8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e3a0
	if (cr6.gt) goto loc_8261E3A0;
loc_8261E3F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e434
	if (!cr0.lt) goto loc_8261E434;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E434:
	// stw r30,21164(r27)
	PPC_STORE_U32(r27.u32 + 21164, r30.u32);
loc_8261E438:
	// lwz r11,21372(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e454
	if (cr6.eq) goto loc_8261E454;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f2800
	sub_825F2800(ctx, base);
loc_8261E454:
	// lwz r11,14772(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14772);
	// stw r25,21528(r27)
	PPC_STORE_U32(r27.u32 + 21528, r25.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261e46c
	if (!cr6.gt) goto loc_8261E46C;
	// lwz r11,20980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20980);
	// stw r11,20972(r27)
	PPC_STORE_U32(r27.u32 + 20972, r11.u32);
loc_8261E46C:
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825e6560
	sub_825E6560(ctx, base);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd3c
	return;
loc_8261E484:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e4a4
	if (cr6.eq) goto loc_8261E4A4;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x8261e4a4
	if (cr6.eq) goto loc_8261E4A4;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8261e4a4
	if (cr6.eq) goto loc_8261E4A4;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82620688
	if (!cr6.eq) goto loc_82620688;
loc_8261E4A4:
	// lwz r11,20848(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20848);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e530
	if (cr6.eq) goto loc_8261E530;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8261e508
	if (!cr6.lt) goto loc_8261E508;
loc_8261E4C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e508
	if (cr6.eq) goto loc_8261E508;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8261e4f8
	if (!cr0.lt) goto loc_8261E4F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E4F8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e4c8
	if (cr6.gt) goto loc_8261E4C8;
loc_8261E508:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8261e530
	if (!cr0.lt) goto loc_8261E530;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E530:
	// lwz r11,20832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20832);
	// li r26,1
	r26.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e778
	if (cr6.eq) goto loc_8261E778;
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e6c4
	if (cr6.eq) goto loc_8261E6C4;
	// lwz r11,21548(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8261e6c4
	if (!cr6.eq) goto loc_8261E6C4;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e5cc
	if (!cr6.lt) goto loc_8261E5CC;
loc_8261E574:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e5cc
	if (cr6.eq) goto loc_8261E5CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e5bc
	if (!cr0.lt) goto loc_8261E5BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E5BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e574
	if (cr6.gt) goto loc_8261E574;
loc_8261E5CC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e608
	if (!cr0.lt) goto loc_8261E608;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E608:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// stw r28,20836(r27)
	PPC_STORE_U32(r27.u32 + 20836, r28.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e680
	if (!cr6.lt) goto loc_8261E680;
loc_8261E628:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e680
	if (cr6.eq) goto loc_8261E680;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e670
	if (!cr0.lt) goto loc_8261E670;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E670:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e628
	if (cr6.gt) goto loc_8261E628;
loc_8261E680:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e6bc
	if (!cr0.lt) goto loc_8261E6BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E6BC:
	// stw r30,20840(r27)
	PPC_STORE_U32(r27.u32 + 20840, r30.u32);
	// b 0x8261e778
	goto loc_8261E778;
loc_8261E6C4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261e738
	if (!cr6.lt) goto loc_8261E738;
loc_8261E6E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e738
	if (cr6.eq) goto loc_8261E738;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e728
	if (!cr0.lt) goto loc_8261E728;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E728:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e6e0
	if (cr6.gt) goto loc_8261E6E0;
loc_8261E738:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e774
	if (!cr0.lt) goto loc_8261E774;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E774:
	// stw r30,21164(r27)
	PPC_STORE_U32(r27.u32 + 21164, r30.u32);
loc_8261E778:
	// lwz r11,21372(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e794
	if (cr6.eq) goto loc_8261E794;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f2800
	sub_825F2800(ctx, base);
loc_8261E794:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e808
	if (!cr6.lt) goto loc_8261E808;
loc_8261E7B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e808
	if (cr6.eq) goto loc_8261E808;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e7f8
	if (!cr0.lt) goto loc_8261E7F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E7F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e7b0
	if (cr6.gt) goto loc_8261E7B0;
loc_8261E808:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e844
	if (!cr0.lt) goto loc_8261E844;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E844:
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// stw r30,3904(r27)
	PPC_STORE_U32(r27.u32 + 3904, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e908
	if (cr6.eq) goto loc_8261E908;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261e8c8
	if (!cr6.lt) goto loc_8261E8C8;
loc_8261E870:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e8c8
	if (cr6.eq) goto loc_8261E8C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e8b8
	if (!cr0.lt) goto loc_8261E8B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E8B8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e870
	if (cr6.gt) goto loc_8261E870;
loc_8261E8C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e904
	if (!cr0.lt) goto loc_8261E904;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E904:
	// stw r30,20972(r27)
	PPC_STORE_U32(r27.u32 + 20972, r30.u32);
loc_8261E908:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x8261e97c
	if (!cr6.lt) goto loc_8261E97C;
loc_8261E924:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261e97c
	if (cr6.eq) goto loc_8261E97C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261e96c
	if (!cr0.lt) goto loc_8261E96C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E96C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e924
	if (cr6.gt) goto loc_8261E924;
loc_8261E97C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261e9b8
	if (!cr0.lt) goto loc_8261E9B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261E9B8:
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// stw r30,3952(r27)
	PPC_STORE_U32(r27.u32 + 3952, r30.u32);
	// bgt cr6,0x8261ea7c
	if (cr6.gt) goto loc_8261EA7C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261ea38
	if (!cr6.lt) goto loc_8261EA38;
loc_8261E9E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ea38
	if (cr6.eq) goto loc_8261EA38;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261ea28
	if (!cr0.lt) goto loc_8261EA28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EA28:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261e9e0
	if (cr6.gt) goto loc_8261E9E0;
loc_8261EA38:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261ea74
	if (!cr0.lt) goto loc_8261EA74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EA74:
	// stw r30,252(r27)
	PPC_STORE_U32(r27.u32 + 252, r30.u32);
	// b 0x8261ea80
	goto loc_8261EA80;
loc_8261EA7C:
	// stw r25,252(r27)
	PPC_STORE_U32(r27.u32 + 252, r25.u32);
loc_8261EA80:
	// lwz r11,3440(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261eb40
	if (cr6.eq) goto loc_8261EB40;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261eb00
	if (!cr6.lt) goto loc_8261EB00;
loc_8261EAA8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261eb00
	if (cr6.eq) goto loc_8261EB00;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261eaf0
	if (!cr0.lt) goto loc_8261EAF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EAF0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261eaa8
	if (cr6.gt) goto loc_8261EAA8;
loc_8261EB00:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261eb3c
	if (!cr0.lt) goto loc_8261EB3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EB3C:
	// stw r30,3428(r27)
	PPC_STORE_U32(r27.u32 + 3428, r30.u32);
loc_8261EB40:
	// lwz r11,3432(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,3952(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3952);
	// bne cr6,0x8261eb78
	if (!cr6.eq) goto loc_8261EB78;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x8261eb60
	if (cr6.gt) goto loc_8261EB60;
	// stw r26,3428(r27)
	PPC_STORE_U32(r27.u32 + 3428, r26.u32);
	// b 0x8261eb78
	goto loc_8261EB78;
loc_8261EB60:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// stw r25,3428(r27)
	PPC_STORE_U32(r27.u32 + 3428, r25.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,4872
	ctx.r10.s64 = ctx.r10.s64 + 4872;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
loc_8261EB78:
	// lwz r10,2972(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 2972);
	// stw r11,248(r27)
	PPC_STORE_U32(r27.u32 + 248, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r25,2968(r27)
	PPC_STORE_U32(r27.u32 + 2968, r25.u32);
	// beq cr6,0x8261ebc0
	if (cr6.eq) goto loc_8261EBC0;
	// lwz r10,284(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x8261ebc0
	if (cr6.eq) goto loc_8261EBC0;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// blt cr6,0x8261eba8
	if (cr6.lt) goto loc_8261EBA8;
	// stw r26,2968(r27)
	PPC_STORE_U32(r27.u32 + 2968, r26.u32);
	// b 0x8261ebc0
	goto loc_8261EBC0;
loc_8261EBA8:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8261ebb8
	if (cr6.eq) goto loc_8261EBB8;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x8261ebc0
	if (!cr6.eq) goto loc_8261EBC0;
loc_8261EBB8:
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r10,2968(r27)
	PPC_STORE_U32(r27.u32 + 2968, ctx.r10.u32);
loc_8261EBC0:
	// lwz r9,3428(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 3428);
	// addi r10,r27,3988
	ctx.r10.s64 = r27.s64 + 3988;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8261ebd4
	if (!cr6.eq) goto loc_8261EBD4;
	// addi r10,r27,5268
	ctx.r10.s64 = r27.s64 + 5268;
loc_8261EBD4:
	// stw r10,6548(r27)
	PPC_STORE_U32(r27.u32 + 6548, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r10,r27,6560
	ctx.r10.s64 = r27.s64 + 6560;
	// bne cr6,0x8261ebe8
	if (!cr6.eq) goto loc_8261EBE8;
	// addi r10,r27,10656
	ctx.r10.s64 = r27.s64 + 10656;
loc_8261EBE8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// stw r10,14752(r27)
	PPC_STORE_U32(r27.u32 + 14752, ctx.r10.u32);
	// stw r11,248(r27)
	PPC_STORE_U32(r27.u32 + 248, r11.u32);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82620688
	if (!cr6.eq) goto loc_82620688;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82620688
	if (!cr6.gt) goto loc_82620688;
	// cmpwi cr6,r11,31
	cr6.compare<int32_t>(r11.s32, 31, xer);
	// bgt cr6,0x82620688
	if (cr6.gt) goto loc_82620688;
	// lwz r11,20868(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20868);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261eccc
	if (cr6.eq) goto loc_8261ECCC;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261ec8c
	if (!cr6.lt) goto loc_8261EC8C;
loc_8261EC34:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ec8c
	if (cr6.eq) goto loc_8261EC8C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261ec7c
	if (!cr0.lt) goto loc_8261EC7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EC7C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261ec34
	if (cr6.gt) goto loc_8261EC34;
loc_8261EC8C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261ecc8
	if (!cr0.lt) goto loc_8261ECC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261ECC8:
	// stw r30,20872(r27)
	PPC_STORE_U32(r27.u32 + 20872, r30.u32);
loc_8261ECCC:
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261ee64
	if (!cr6.eq) goto loc_8261EE64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x8261ed4c
	if (!cr6.lt) goto loc_8261ED4C;
loc_8261ECF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ed4c
	if (cr6.eq) goto loc_8261ED4C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261ed3c
	if (!cr0.lt) goto loc_8261ED3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261ED3C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261ecf4
	if (cr6.gt) goto loc_8261ECF4;
loc_8261ED4C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261ed88
	if (!cr0.lt) goto loc_8261ED88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261ED88:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x8261ee54
	if (!cr6.eq) goto loc_8261EE54;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x8261ee04
	if (!cr6.lt) goto loc_8261EE04;
loc_8261EDAC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ee04
	if (cr6.eq) goto loc_8261EE04;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261edf4
	if (!cr0.lt) goto loc_8261EDF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EDF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261edac
	if (cr6.gt) goto loc_8261EDAC;
loc_8261EE04:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261ee40
	if (!cr0.lt) goto loc_8261EE40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EE40:
	// cmpwi cr6,r30,14
	cr6.compare<int32_t>(r30.s32, 14, xer);
	// bge cr6,0x82620688
	if (!cr6.lt) goto loc_82620688;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r30,112
	ctx.r4.s64 = r30.s64 + 112;
	// b 0x8261ee5c
	goto loc_8261EE5C;
loc_8261EE54:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
loc_8261EE5C:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f39c0
	sub_825F39C0(ctx, base);
loc_8261EE64:
	// lwz r11,3952(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3952);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x8261ee80
	if (cr6.gt) goto loc_8261EE80;
	// addi r11,r27,2840
	r11.s64 = r27.s64 + 2840;
	// addi r10,r27,2800
	ctx.r10.s64 = r27.s64 + 2800;
	// stw r11,2904(r27)
	PPC_STORE_U32(r27.u32 + 2904, r11.u32);
	// stw r10,2916(r27)
	PPC_STORE_U32(r27.u32 + 2916, ctx.r10.u32);
loc_8261EE80:
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261fef8
	if (cr6.eq) goto loc_8261FEF8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8261fef8
	if (cr6.eq) goto loc_8261FEF8;
	// lwz r11,20864(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20864);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261f0d0
	if (cr6.eq) goto loc_8261F0D0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261ef18
	if (!cr6.lt) goto loc_8261EF18;
loc_8261EEC0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261ef18
	if (cr6.eq) goto loc_8261EF18;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261ef08
	if (!cr0.lt) goto loc_8261EF08;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EF08:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261eec0
	if (cr6.gt) goto loc_8261EEC0;
loc_8261EF18:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261ef54
	if (!cr0.lt) goto loc_8261EF54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EF54:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8261f014
	if (cr6.eq) goto loc_8261F014;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261efd4
	if (!cr6.lt) goto loc_8261EFD4;
loc_8261EF7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261efd4
	if (cr6.eq) goto loc_8261EFD4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261efc4
	if (!cr0.lt) goto loc_8261EFC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261EFC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261ef7c
	if (cr6.gt) goto loc_8261EF7C;
loc_8261EFD4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f010
	if (!cr0.lt) goto loc_8261F010;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F010:
	// add r4,r30,r28
	ctx.r4.u64 = r30.u64 + r28.u64;
loc_8261F014:
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x8261f0d0
	if (!cr6.eq) goto loc_8261F0D0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f090
	if (!cr6.lt) goto loc_8261F090;
loc_8261F038:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f090
	if (cr6.eq) goto loc_8261F090;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f080
	if (!cr0.lt) goto loc_8261F080;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F080:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f038
	if (cr6.gt) goto loc_8261F038;
loc_8261F090:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f0cc
	if (!cr0.lt) goto loc_8261F0CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F0CC:
	// addi r4,r30,2
	ctx.r4.s64 = r30.s64 + 2;
loc_8261F0D0:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lwz r11,404(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 404);
	// bne cr6,0x8261f364
	if (!cr6.eq) goto loc_8261F364;
	// stw r11,21528(r27)
	PPC_STORE_U32(r27.u32 + 21528, r11.u32);
loc_8261F0EC:
	// lwz r11,20956(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261f350
	if (cr6.eq) goto loc_8261F350;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f16c
	if (!cr6.lt) goto loc_8261F16C;
loc_8261F114:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f16c
	if (cr6.eq) goto loc_8261F16C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f15c
	if (!cr0.lt) goto loc_8261F15C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F15C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f114
	if (cr6.gt) goto loc_8261F114;
loc_8261F16C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f1a8
	if (!cr0.lt) goto loc_8261F1A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F1A8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,20960(r27)
	PPC_STORE_U32(r27.u32 + 20960, r30.u32);
	// beq cr6,0x8261f270
	if (cr6.eq) goto loc_8261F270;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f228
	if (!cr6.lt) goto loc_8261F228;
loc_8261F1D0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f228
	if (cr6.eq) goto loc_8261F228;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f218
	if (!cr0.lt) goto loc_8261F218;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F218:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f1d0
	if (cr6.gt) goto loc_8261F1D0;
loc_8261F228:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f264
	if (!cr0.lt) goto loc_8261F264;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F264:
	// lwz r11,20960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20960);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,20960(r27)
	PPC_STORE_U32(r27.u32 + 20960, r11.u32);
loc_8261F270:
	// lwz r11,20960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261f338
	if (!cr6.eq) goto loc_8261F338;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f2f0
	if (!cr6.lt) goto loc_8261F2F0;
loc_8261F298:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f2f0
	if (cr6.eq) goto loc_8261F2F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f2e0
	if (!cr0.lt) goto loc_8261F2E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F2E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f298
	if (cr6.gt) goto loc_8261F298;
loc_8261F2F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f32c
	if (!cr0.lt) goto loc_8261F32C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F32C:
	// lwz r11,20960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20960);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,20960(r27)
	PPC_STORE_U32(r27.u32 + 20960, r11.u32);
loc_8261F338:
	// lwz r11,20960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20960);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// stw r10,20964(r27)
	PPC_STORE_U32(r27.u32 + 20964, ctx.r10.u32);
	// stw r11,20968(r27)
	PPC_STORE_U32(r27.u32 + 20968, r11.u32);
loc_8261F350:
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261f37c
	if (!cr6.eq) goto loc_8261F37C;
	// stw r26,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r26.u32);
	// b 0x8261f438
	goto loc_8261F438;
loc_8261F364:
	// lwz r10,21528(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21528);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8261f0ec
	if (!cr6.lt) goto loc_8261F0EC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd3c
	return;
loc_8261F37C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f3f0
	if (!cr6.lt) goto loc_8261F3F0;
loc_8261F398:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f3f0
	if (cr6.eq) goto loc_8261F3F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f3e0
	if (!cr0.lt) goto loc_8261F3E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F3E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f398
	if (cr6.gt) goto loc_8261F398;
loc_8261F3F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f42c
	if (!cr0.lt) goto loc_8261F42C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F42C:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,3960(r27)
	PPC_STORE_U32(r27.u32 + 3960, r11.u32);
loc_8261F438:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// stw r25,3964(r27)
	PPC_STORE_U32(r27.u32 + 3964, r25.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261f4b0
	if (!cr6.lt) goto loc_8261F4B0;
loc_8261F458:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f4b0
	if (cr6.eq) goto loc_8261F4B0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f4a0
	if (!cr0.lt) goto loc_8261F4A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F4A0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f458
	if (cr6.gt) goto loc_8261F458;
loc_8261F4B0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f4ec
	if (!cr0.lt) goto loc_8261F4EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F4EC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8261f504
	if (cr6.eq) goto loc_8261F504;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x8261f504
	if (cr6.eq) goto loc_8261F504;
	// stw r26,3964(r27)
	PPC_STORE_U32(r27.u32 + 3964, r26.u32);
loc_8261F504:
	// lwz r11,3964(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261f678
	if (cr6.eq) goto loc_8261F678;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8261f584
	if (!cr6.lt) goto loc_8261F584;
loc_8261F52C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f584
	if (cr6.eq) goto loc_8261F584;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f574
	if (!cr0.lt) goto loc_8261F574;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F574:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f52c
	if (cr6.gt) goto loc_8261F52C;
loc_8261F584:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f5c0
	if (!cr0.lt) goto loc_8261F5C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F5C0:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,3968(r27)
	PPC_STORE_U32(r27.u32 + 3968, r28.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8261f638
	if (!cr6.lt) goto loc_8261F638;
loc_8261F5E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f638
	if (cr6.eq) goto loc_8261F638;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f628
	if (!cr0.lt) goto loc_8261F628;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F628:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f5e0
	if (cr6.gt) goto loc_8261F5E0;
loc_8261F638:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f674
	if (!cr0.lt) goto loc_8261F674;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F674:
	// stw r30,3972(r27)
	PPC_STORE_U32(r27.u32 + 3972, r30.u32);
loc_8261F678:
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261f6ec
	if (!cr6.eq) goto loc_8261F6EC;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262068c
	if (!cr6.eq) goto loc_8262068C;
	// lwz r11,14804(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14804);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261f6ec
	if (cr6.eq) goto loc_8261F6EC;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8261f6ec
	if (!cr6.gt) goto loc_8261F6EC;
loc_8261F6B8:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r10,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8261f6d0
	if (cr6.eq) goto loc_8261F6D0;
	// rlwimi r10,r26,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r26.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x8261f6d4
	goto loc_8261F6D4;
loc_8261F6D0:
	// rlwinm r10,r10,0,27,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFF1F;
loc_8261F6D4:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x8261f6b8
	if (cr6.lt) goto loc_8261F6B8;
loc_8261F6EC:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262068c
	if (!cr6.eq) goto loc_8262068C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// stw r26,448(r27)
	PPC_STORE_U32(r27.u32 + 448, r26.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261f778
	if (!cr6.lt) goto loc_8261F778;
loc_8261F720:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f778
	if (cr6.eq) goto loc_8261F778;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f768
	if (!cr0.lt) goto loc_8261F768;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F768:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f720
	if (cr6.gt) goto loc_8261F720;
loc_8261F778:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f7b4
	if (!cr0.lt) goto loc_8261F7B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F7B4:
	// lwz r11,3960(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3960);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r30,5017
	r11.s64 = r30.s64 + 5017;
	// beq cr6,0x8261f7c8
	if (cr6.eq) goto loc_8261F7C8;
	// addi r11,r30,5033
	r11.s64 = r30.s64 + 5033;
loc_8261F7C8:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,20064(r27)
	PPC_STORE_U32(r27.u32 + 20064, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261f848
	if (!cr6.lt) goto loc_8261F848;
loc_8261F7F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f848
	if (cr6.eq) goto loc_8261F848;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f838
	if (!cr0.lt) goto loc_8261F838;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F838:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f7f0
	if (cr6.gt) goto loc_8261F7F0;
loc_8261F848:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f884
	if (!cr0.lt) goto loc_8261F884;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F884:
	// addi r11,r30,599
	r11.s64 = r30.s64 + 599;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,2376(r27)
	PPC_STORE_U32(r27.u32 + 2376, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x8261f908
	if (!cr6.lt) goto loc_8261F908;
loc_8261F8B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f908
	if (cr6.eq) goto loc_8261F908;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f8f8
	if (!cr0.lt) goto loc_8261F8F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F8F8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f8b0
	if (cr6.gt) goto loc_8261F8B0;
loc_8261F908:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261f944
	if (!cr0.lt) goto loc_8261F944;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F944:
	// addi r11,r30,5254
	r11.s64 = r30.s64 + 5254;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,21012(r27)
	PPC_STORE_U32(r27.u32 + 21012, r11.u32);
	// stw r11,21008(r27)
	PPC_STORE_U32(r27.u32 + 21008, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261f9cc
	if (!cr6.lt) goto loc_8261F9CC;
loc_8261F974:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261f9cc
	if (cr6.eq) goto loc_8261F9CC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261f9bc
	if (!cr0.lt) goto loc_8261F9BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261F9BC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261f974
	if (cr6.gt) goto loc_8261F974;
loc_8261F9CC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fa08
	if (!cr0.lt) goto loc_8261FA08;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FA08:
	// addi r11,r30,5072
	r11.s64 = r30.s64 + 5072;
	// lwz r10,3960(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 3960);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,20284(r27)
	PPC_STORE_U32(r27.u32 + 20284, r11.u32);
	// beq cr6,0x8261fa30
	if (cr6.eq) goto loc_8261FA30;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8261faf0
	if (!cr6.eq) goto loc_8261FAF0;
loc_8261FA30:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261faa4
	if (!cr6.lt) goto loc_8261FAA4;
loc_8261FA4C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261faa4
	if (cr6.eq) goto loc_8261FAA4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fa94
	if (!cr0.lt) goto loc_8261FA94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FA94:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fa4c
	if (cr6.gt) goto loc_8261FA4C;
loc_8261FAA4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fae0
	if (!cr0.lt) goto loc_8261FAE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FAE0:
	// addi r11,r30,5067
	r11.s64 = r30.s64 + 5067;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,20264(r27)
	PPC_STORE_U32(r27.u32 + 20264, r11.u32);
loc_8261FAF0:
	// lwz r11,3980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261fb08
	if (cr6.eq) goto loc_8261FB08;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
loc_8261FB08:
	// lwz r11,436(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261fca0
	if (cr6.eq) goto loc_8261FCA0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261fb88
	if (!cr6.lt) goto loc_8261FB88;
loc_8261FB30:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261fb88
	if (cr6.eq) goto loc_8261FB88;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fb78
	if (!cr0.lt) goto loc_8261FB78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FB78:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fb30
	if (cr6.gt) goto loc_8261FB30;
loc_8261FB88:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fbc4
	if (!cr0.lt) goto loc_8261FBC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FBC4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8261fc98
	if (!cr6.eq) goto loc_8261FC98;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// stw r25,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r25.u32);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8261fc44
	if (!cr6.lt) goto loc_8261FC44;
loc_8261FBEC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261fc44
	if (cr6.eq) goto loc_8261FC44;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fc34
	if (!cr0.lt) goto loc_8261FC34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FC34:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fbec
	if (cr6.gt) goto loc_8261FBEC;
loc_8261FC44:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fc80
	if (!cr0.lt) goto loc_8261FC80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FC80:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,14348
	r11.s64 = r11.s64 + 14348;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,336(r27)
	PPC_STORE_U32(r27.u32 + 336, r11.u32);
	// b 0x8261fca4
	goto loc_8261FCA4;
loc_8261FC98:
	// stw r26,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r26.u32);
	// b 0x8261fca4
	goto loc_8261FCA4;
loc_8261FCA0:
	// stw r25,328(r27)
	PPC_STORE_U32(r27.u32 + 328, r25.u32);
loc_8261FCA4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82620688
	if (!cr6.eq) goto loc_82620688;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261fd24
	if (!cr6.lt) goto loc_8261FD24;
loc_8261FCCC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261fd24
	if (cr6.eq) goto loc_8261FD24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fd14
	if (!cr0.lt) goto loc_8261FD14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FD14:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fccc
	if (cr6.gt) goto loc_8261FCCC;
loc_8261FD24:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fd60
	if (!cr0.lt) goto loc_8261FD60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FD60:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r27)
	PPC_STORE_U32(r27.u32 + 2928, r30.u32);
	// beq cr6,0x8261fe28
	if (cr6.eq) goto loc_8261FE28;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261fde0
	if (!cr6.lt) goto loc_8261FDE0;
loc_8261FD88:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261fde0
	if (cr6.eq) goto loc_8261FDE0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fdd0
	if (!cr0.lt) goto loc_8261FDD0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FDD0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fd88
	if (cr6.gt) goto loc_8261FD88;
loc_8261FDE0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fe1c
	if (!cr0.lt) goto loc_8261FE1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FE1C:
	// lwz r11,2928(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2928);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,2928(r27)
	PPC_STORE_U32(r27.u32 + 2928, r11.u32);
loc_8261FE28:
	// lwz r11,2928(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2928);
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r25
	r29.u64 = r25.u64;
	// stw r11,2936(r27)
	PPC_STORE_U32(r27.u32 + 2936, r11.u32);
	// stw r11,2932(r27)
	PPC_STORE_U32(r27.u32 + 2932, r11.u32);
	// stw r11,2948(r27)
	PPC_STORE_U32(r27.u32 + 2948, r11.u32);
	// stw r11,2944(r27)
	PPC_STORE_U32(r27.u32 + 2944, r11.u32);
	// stw r11,2940(r27)
	PPC_STORE_U32(r27.u32 + 2940, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8261feb4
	if (!cr6.lt) goto loc_8261FEB4;
loc_8261FE5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8261feb4
	if (cr6.eq) goto loc_8261FEB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8261fea4
	if (!cr0.lt) goto loc_8261FEA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FEA4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261fe5c
	if (cr6.gt) goto loc_8261FE5C;
loc_8261FEB4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8261fef0
	if (!cr0.lt) goto loc_8261FEF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8261FEF0:
	// stw r30,2088(r27)
	PPC_STORE_U32(r27.u32 + 2088, r30.u32);
	// b 0x82620678
	goto loc_82620678;
loc_8261FEF8:
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262068c
	if (!cr6.eq) goto loc_8262068C;
	// lwz r11,19988(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19988);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261ff58
	if (cr6.eq) goto loc_8261FF58;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261ff58
	if (!cr6.gt) goto loc_8261FF58;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_8261FF2C:
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,17,15,15
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 17) & 0x10000) | (ctx.r7.u64 & 0xFFFFFFFFFFFEFFFF);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x8261ff2c
	if (cr6.lt) goto loc_8261FF2C;
loc_8261FF58:
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262068c
	if (!cr6.eq) goto loc_8262068C;
	// lwz r11,20004(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20004);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261ffbc
	if (cr6.eq) goto loc_8261FFBC;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8261ffbc
	if (!cr6.gt) goto loc_8261FFBC;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_8261FF8C:
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,4,28,28
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 4) & 0x8) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFF7);
	// rlwinm r8,r7,0,28,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFFEF;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x8261ff8c
	if (cr6.lt) goto loc_8261FF8C;
loc_8261FFBC:
	// lwz r11,2968(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2968);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82620228
	if (cr6.eq) goto loc_82620228;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620040
	if (!cr6.lt) goto loc_82620040;
loc_8261FFE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620040
	if (cr6.eq) goto loc_82620040;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620030
	if (!cr0.lt) goto loc_82620030;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620030:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8261ffe8
	if (cr6.gt) goto loc_8261FFE8;
loc_82620040:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8262007c
	if (!cr0.lt) goto loc_8262007C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262007C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826200c8
	if (!cr6.eq) goto loc_826200C8;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stw r25,2968(r27)
	PPC_STORE_U32(r27.u32 + 2968, r25.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826202a4
	if (!cr6.gt) goto loc_826202A4;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_8262009C:
	// lwz r9,268(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r8,0,21,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF7FF;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x8262009c
	if (cr6.lt) goto loc_8262009C;
	// b 0x826202a4
	goto loc_826202A4;
loc_826200C8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8262013c
	if (!cr6.lt) goto loc_8262013C;
loc_826200E4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262013c
	if (cr6.eq) goto loc_8262013C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8262012c
	if (!cr0.lt) goto loc_8262012C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262012C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826200e4
	if (cr6.gt) goto loc_826200E4;
loc_8262013C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620178
	if (!cr0.lt) goto loc_82620178;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620178:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826201c4
	if (!cr6.eq) goto loc_826201C4;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r11,r25
	r11.u64 = r25.u64;
	// stw r26,2968(r27)
	PPC_STORE_U32(r27.u32 + 2968, r26.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826202a4
	if (!cr6.gt) goto loc_826202A4;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_82620198:
	// lwz r9,268(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// ori r8,r8,2048
	ctx.r8.u64 = ctx.r8.u64 | 2048;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82620198
	if (cr6.lt) goto loc_82620198;
	// b 0x826202a4
	goto loc_826202A4;
loc_826201C4:
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262068c
	if (!cr6.eq) goto loc_8262068C;
	// lwz r11,20940(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20940);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826202a4
	if (cr6.eq) goto loc_826202A4;
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826202a4
	if (!cr6.gt) goto loc_826202A4;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_826201F8:
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,12,20,20
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 12) & 0x800) | (ctx.r7.u64 & 0xFFFFFFFFFFFFF7FF);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x826201f8
	if (cr6.lt) goto loc_826201F8;
	// b 0x826202a4
	goto loc_826202A4;
loc_82620228:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,144(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// beq cr6,0x82620270
	if (cr6.eq) goto loc_82620270;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826202a4
	if (!cr6.gt) goto loc_826202A4;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_82620244:
	// lwz r9,268(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// ori r8,r8,2048
	ctx.r8.u64 = ctx.r8.u64 | 2048;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82620244
	if (cr6.lt) goto loc_82620244;
	// b 0x826202a4
	goto loc_826202A4;
loc_82620270:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826202a4
	if (!cr6.gt) goto loc_826202A4;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_8262027C:
	// lwz r9,268(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r8,0,21,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF7FF;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x8262027c
	if (cr6.lt) goto loc_8262027C;
loc_826202A4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620318
	if (!cr6.lt) goto loc_82620318;
loc_826202C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620318
	if (cr6.eq) goto loc_82620318;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620308
	if (!cr0.lt) goto loc_82620308;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620308:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826202c0
	if (cr6.gt) goto loc_826202C0;
loc_82620318:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620354
	if (!cr0.lt) goto loc_82620354;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620354:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r27)
	PPC_STORE_U32(r27.u32 + 2928, r30.u32);
	// beq cr6,0x8262041c
	if (cr6.eq) goto loc_8262041C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826203d4
	if (!cr6.lt) goto loc_826203D4;
loc_8262037C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826203d4
	if (cr6.eq) goto loc_826203D4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826203c4
	if (!cr0.lt) goto loc_826203C4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826203C4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8262037c
	if (cr6.gt) goto loc_8262037C;
loc_826203D4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620410
	if (!cr0.lt) goto loc_82620410;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620410:
	// lwz r11,2928(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2928);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2928(r27)
	PPC_STORE_U32(r27.u32 + 2928, r11.u32);
loc_8262041C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620490
	if (!cr6.lt) goto loc_82620490;
loc_82620438:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620490
	if (cr6.eq) goto loc_82620490;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620480
	if (!cr0.lt) goto loc_82620480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620480:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620438
	if (cr6.gt) goto loc_82620438;
loc_82620490:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826204cc
	if (!cr0.lt) goto loc_826204CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826204CC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2940(r27)
	PPC_STORE_U32(r27.u32 + 2940, r30.u32);
	// beq cr6,0x82620594
	if (cr6.eq) goto loc_82620594;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8262054c
	if (!cr6.lt) goto loc_8262054C;
loc_826204F4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262054c
	if (cr6.eq) goto loc_8262054C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8262053c
	if (!cr0.lt) goto loc_8262053C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262053C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826204f4
	if (cr6.gt) goto loc_826204F4;
loc_8262054C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620588
	if (!cr0.lt) goto loc_82620588;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620588:
	// lwz r11,2940(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2940);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2940(r27)
	PPC_STORE_U32(r27.u32 + 2940, r11.u32);
loc_82620594:
	// lwz r11,2940(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2940);
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r25
	r29.u64 = r25.u64;
	// stw r11,2948(r27)
	PPC_STORE_U32(r27.u32 + 2948, r11.u32);
	// stw r11,2944(r27)
	PPC_STORE_U32(r27.u32 + 2944, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620614
	if (!cr6.lt) goto loc_82620614;
loc_826205BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620614
	if (cr6.eq) goto loc_82620614;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620604
	if (!cr0.lt) goto loc_82620604;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620604:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826205bc
	if (cr6.gt) goto loc_826205BC;
loc_82620614:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620650
	if (!cr0.lt) goto loc_82620650;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620650:
	// lwz r11,3980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 3980);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r30,2088(r27)
	PPC_STORE_U32(r27.u32 + 2088, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620670
	if (cr6.eq) goto loc_82620670;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
	// b 0x82620674
	goto loc_82620674;
loc_82620670:
	// bl 0x826179f8
	sub_826179F8(ctx, base);
loc_82620674:
	// stw r25,21528(r27)
	PPC_STORE_U32(r27.u32 + 21528, r25.u32);
loc_82620678:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8261e46c
	if (cr6.eq) goto loc_8261E46C;
loc_82620688:
	// li r3,4
	ctx.r3.s64 = 4;
loc_8262068C:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82620694"))) PPC_WEAK_FUNC(sub_82620694);
PPC_FUNC_IMPL(__imp__sub_82620694) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82620698"))) PPC_WEAK_FUNC(sub_82620698);
PPC_FUNC_IMPL(__imp__sub_82620698) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x8262071c
	if (!cr6.lt) goto loc_8262071C;
loc_826206C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262071c
	if (cr6.eq) goto loc_8262071C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8262070c
	if (!cr0.lt) goto loc_8262070C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262070C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826206c4
	if (cr6.gt) goto loc_826206C4;
loc_8262071C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620758
	if (!cr0.lt) goto loc_82620758;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620758:
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,14396
	ctx.r10.s64 = ctx.r10.s64 + 14396;
	// addi r9,r10,-32
	ctx.r9.s64 = ctx.r10.s64 + -32;
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r9,21072(r28)
	PPC_STORE_U32(r28.u32 + 21072, ctx.r9.u32);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r11,21076(r28)
	PPC_STORE_U32(r28.u32 + 21076, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82620780"))) PPC_WEAK_FUNC(sub_82620780);
PPC_FUNC_IMPL(__imp__sub_82620780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x82620698
	sub_82620698(ctx, base);
	// lwz r11,20848(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20848);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620828
	if (cr6.eq) goto loc_82620828;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,8
	r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x82620800
	if (!cr6.lt) goto loc_82620800;
loc_826207C0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620800
	if (cr6.eq) goto loc_82620800;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x826207f0
	if (!cr0.lt) goto loc_826207F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826207F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826207c0
	if (cr6.gt) goto loc_826207C0;
loc_82620800:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x82620828
	if (!cr0.lt) goto loc_82620828;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620828:
	// lwz r11,20832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20832);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620a6c
	if (cr6.eq) goto loc_82620A6C;
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826209b8
	if (cr6.eq) goto loc_826209B8;
	// lwz r11,21548(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21548);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826209b8
	if (!cr6.eq) goto loc_826209B8;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826208c0
	if (!cr6.lt) goto loc_826208C0;
loc_82620868:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826208c0
	if (cr6.eq) goto loc_826208C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826208b0
	if (!cr0.lt) goto loc_826208B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826208B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620868
	if (cr6.gt) goto loc_82620868;
loc_826208C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826208fc
	if (!cr0.lt) goto loc_826208FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826208FC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,20836(r27)
	PPC_STORE_U32(r27.u32 + 20836, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620974
	if (!cr6.lt) goto loc_82620974;
loc_8262091C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620974
	if (cr6.eq) goto loc_82620974;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620964
	if (!cr0.lt) goto loc_82620964;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620964:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8262091c
	if (cr6.gt) goto loc_8262091C;
loc_82620974:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826209b0
	if (!cr0.lt) goto loc_826209B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826209B0:
	// stw r30,20840(r27)
	PPC_STORE_U32(r27.u32 + 20840, r30.u32);
	// b 0x82620a6c
	goto loc_82620A6C;
loc_826209B8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82620a2c
	if (!cr6.lt) goto loc_82620A2C;
loc_826209D4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620a2c
	if (cr6.eq) goto loc_82620A2C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620a1c
	if (!cr0.lt) goto loc_82620A1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620A1C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826209d4
	if (cr6.gt) goto loc_826209D4;
loc_82620A2C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620a68
	if (!cr0.lt) goto loc_82620A68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620A68:
	// stw r30,21164(r27)
	PPC_STORE_U32(r27.u32 + 21164, r30.u32);
loc_82620A6C:
	// lwz r11,21372(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620a88
	if (cr6.eq) goto loc_82620A88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f2800
	sub_825F2800(ctx, base);
loc_82620A88:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620afc
	if (!cr6.lt) goto loc_82620AFC;
loc_82620AA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620afc
	if (cr6.eq) goto loc_82620AFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620aec
	if (!cr0.lt) goto loc_82620AEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620AEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620aa4
	if (cr6.gt) goto loc_82620AA4;
loc_82620AFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620b38
	if (!cr0.lt) goto loc_82620B38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620B38:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// stw r28,3904(r27)
	PPC_STORE_U32(r27.u32 + 3904, r28.u32);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620bb0
	if (!cr6.lt) goto loc_82620BB0;
loc_82620B58:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620bb0
	if (cr6.eq) goto loc_82620BB0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620ba0
	if (!cr0.lt) goto loc_82620BA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620BA0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620b58
	if (cr6.gt) goto loc_82620B58;
loc_82620BB0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620bec
	if (!cr0.lt) goto loc_82620BEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620BEC:
	// lwz r11,21376(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21376);
	// stw r30,20972(r27)
	PPC_STORE_U32(r27.u32 + 20972, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620d94
	if (cr6.eq) goto loc_82620D94;
	// lwz r11,21072(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21072);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82620c10
	if (cr6.eq) goto loc_82620C10;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82620d94
	if (!cr6.eq) goto loc_82620D94;
loc_82620C10:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82620c84
	if (!cr6.lt) goto loc_82620C84;
loc_82620C2C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620c84
	if (cr6.eq) goto loc_82620C84;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620c74
	if (!cr0.lt) goto loc_82620C74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620C74:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620c2c
	if (cr6.gt) goto loc_82620C2C;
loc_82620C84:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620cc0
	if (!cr0.lt) goto loc_82620CC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620CC0:
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// stw r30,21080(r27)
	PPC_STORE_U32(r27.u32 + 21080, r30.u32);
	// bne cr6,0x82620d94
	if (!cr6.eq) goto loc_82620D94;
loc_82620CCC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82620d40
	if (!cr6.lt) goto loc_82620D40;
loc_82620CE8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620d40
	if (cr6.eq) goto loc_82620D40;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620d30
	if (!cr0.lt) goto loc_82620D30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620D30:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620ce8
	if (cr6.gt) goto loc_82620CE8;
loc_82620D40:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620d7c
	if (!cr0.lt) goto loc_82620D7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620D7C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82620d94
	if (cr6.eq) goto loc_82620D94;
	// lwz r11,21080(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21080);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21080(r27)
	PPC_STORE_U32(r27.u32 + 21080, r11.u32);
	// b 0x82620ccc
	goto loc_82620CCC;
loc_82620D94:
	// lwz r11,21072(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21072);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82620da8
	if (cr6.eq) goto loc_82620DA8;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x82620f40
	if (!cr6.eq) goto loc_82620F40;
loc_82620DA8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82620e1c
	if (!cr6.lt) goto loc_82620E1C;
loc_82620DC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620e1c
	if (cr6.eq) goto loc_82620E1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620e0c
	if (!cr0.lt) goto loc_82620E0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620E0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620dc4
	if (cr6.gt) goto loc_82620DC4;
loc_82620E1C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620e58
	if (!cr0.lt) goto loc_82620E58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620E58:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x82620f30
	if (!cr6.eq) goto loc_82620F30;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x82620ed4
	if (!cr6.lt) goto loc_82620ED4;
loc_82620E7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82620ed4
	if (cr6.eq) goto loc_82620ED4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82620ec4
	if (!cr0.lt) goto loc_82620EC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620EC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82620e7c
	if (cr6.gt) goto loc_82620E7C;
loc_82620ED4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82620f10
	if (!cr0.lt) goto loc_82620F10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82620F10:
	// cmpwi cr6,r30,14
	cr6.compare<int32_t>(r30.s32, 14, xer);
	// blt cr6,0x82620f24
	if (cr6.lt) goto loc_82620F24;
loc_82620F18:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd44
	return;
loc_82620F24:
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r30,112
	ctx.r4.s64 = r30.s64 + 112;
	// b 0x82620f38
	goto loc_82620F38;
loc_82620F30:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
loc_82620F38:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825f39c0
	sub_825F39C0(ctx, base);
loc_82620F40:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82620f18
	if (!cr6.eq) goto loc_82620F18;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825e6560
	sub_825E6560(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82620F68"))) PPC_WEAK_FUNC(sub_82620F68);
PPC_FUNC_IMPL(__imp__sub_82620F68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce8
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x8262110c
	if (!cr6.eq) goto loc_8262110C;
	// lis r11,0
	r11.s64 = 0;
	// lhz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 16);
	// mulli r8,r5,506
	ctx.r8.s64 = ctx.r5.s64 * 506;
	// lhz r31,4(r3)
	r31.u64 = PPC_LOAD_U16(ctx.r3.u32 + 4);
	// lhz r30,32(r3)
	r30.u64 = PPC_LOAD_U16(ctx.r3.u32 + 32);
	// lhz r29,6(r3)
	r29.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
	// lhz r28,48(r3)
	r28.u64 = PPC_LOAD_U16(ctx.r3.u32 + 48);
	// lhz r26,64(r3)
	r26.u64 = PPC_LOAD_U16(ctx.r3.u32 + 64);
	// lhz r27,8(r3)
	r27.u64 = PPC_LOAD_U16(ctx.r3.u32 + 8);
	// lhz r25,112(r3)
	r25.u64 = PPC_LOAD_U16(ctx.r3.u32 + 112);
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
	// lhz r24,72(r3)
	r24.u64 = PPC_LOAD_U16(ctx.r3.u32 + 72);
	// mulli r10,r5,3811
	ctx.r10.s64 = ctx.r5.s64 * 3811;
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + r11.u64;
	// mulli r8,r5,135
	ctx.r8.s64 = ctx.r5.s64 * 135;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// mulli r9,r5,487
	ctx.r9.s64 = ctx.r5.s64 * 487;
	// add r4,r8,r11
	ctx.r4.u64 = ctx.r8.u64 + r11.u64;
	// srawi r10,r10,16
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 16;
	// lhz r8,2(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// srawi r10,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 16;
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// subf r31,r10,r31
	r31.s64 = r31.s64 - ctx.r10.s64;
	// subf r30,r10,r30
	r30.s64 = r30.s64 - ctx.r10.s64;
	// sth r7,16(r3)
	PPC_STORE_U16(ctx.r3.u32 + 16, ctx.r7.u16);
	// srawi r10,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 16;
	// lhz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 20);
	// sth r8,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r8.u16);
	// subf r6,r10,r29
	ctx.r6.s64 = r29.s64 - ctx.r10.s64;
	// lhz r8,22(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 22);
	// sth r31,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, r31.u16);
	// subf r31,r10,r28
	r31.s64 = r28.s64 - ctx.r10.s64;
	// srawi r10,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 16;
	// sth r30,32(r3)
	PPC_STORE_U16(ctx.r3.u32 + 32, r30.u16);
	// lhz r29,80(r3)
	r29.u64 = PPC_LOAD_U16(ctx.r3.u32 + 80);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lhz r28,12(r3)
	r28.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// sth r6,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r6.u16);
	// subf r4,r10,r26
	ctx.r4.s64 = r26.s64 - ctx.r10.s64;
	// sth r31,48(r3)
	PPC_STORE_U16(ctx.r3.u32 + 48, r31.u16);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lhz r26,14(r3)
	r26.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// subf r6,r10,r27
	ctx.r6.s64 = r27.s64 - ctx.r10.s64;
	// lhz r27,96(r3)
	r27.u64 = PPC_LOAD_U16(ctx.r3.u32 + 96);
	// sth r7,20(r3)
	PPC_STORE_U16(ctx.r3.u32 + 20, ctx.r7.u16);
	// mulli r7,r5,61
	ctx.r7.s64 = ctx.r5.s64 * 61;
	// sth r8,22(r3)
	PPC_STORE_U16(ctx.r3.u32 + 22, ctx.r8.u16);
	// sth r4,64(r3)
	PPC_STORE_U16(ctx.r3.u32 + 64, ctx.r4.u16);
	// sth r9,34(r3)
	PPC_STORE_U16(ctx.r3.u32 + 34, ctx.r9.u16);
	// lhz r9,50(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 50);
	// sth r6,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r6.u16);
	// lhz r6,26(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 26);
	// mulli r8,r5,173
	ctx.r8.s64 = ctx.r5.s64 * 173;
	// add r4,r7,r11
	ctx.r4.u64 = ctx.r7.u64 + r11.u64;
	// mulli r7,r5,42
	ctx.r7.s64 = ctx.r5.s64 * 42;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r31,r7,r11
	r31.u64 = ctx.r7.u64 + r11.u64;
	// lhz r7,10(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 10);
	// srawi r10,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 16;
	// lhz r8,82(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 82);
	// subf r30,r10,r7
	r30.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r29,r10,r29
	r29.s64 = r29.s64 - ctx.r10.s64;
	// sth r9,50(r3)
	PPC_STORE_U16(ctx.r3.u32 + 50, ctx.r9.u16);
	// srawi r10,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 16;
	// lhz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 24);
	// subf r4,r10,r28
	ctx.r4.s64 = r28.s64 - ctx.r10.s64;
	// sth r30,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, r30.u16);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// subf r30,r10,r27
	r30.s64 = r27.s64 - ctx.r10.s64;
	// sth r29,80(r3)
	PPC_STORE_U16(ctx.r3.u32 + 80, r29.u16);
	// add r7,r8,r10
	ctx.r7.u64 = ctx.r8.u64 + ctx.r10.u64;
	// srawi r10,r31,16
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = r31.s32 >> 16;
	// sth r4,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r4.u16);
	// sth r6,26(r3)
	PPC_STORE_U16(ctx.r3.u32 + 26, ctx.r6.u16);
	// sth r30,96(r3)
	PPC_STORE_U16(ctx.r3.u32 + 96, r30.u16);
	// sth r7,82(r3)
	PPC_STORE_U16(ctx.r3.u32 + 82, ctx.r7.u16);
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r9,r24,r10
	ctx.r9.u64 = r24.u64 + ctx.r10.u64;
	// subf r7,r10,r26
	ctx.r7.s64 = r26.s64 - ctx.r10.s64;
	// subf r6,r10,r25
	ctx.r6.s64 = r25.s64 - ctx.r10.s64;
	// sth r8,24(r3)
	PPC_STORE_U16(ctx.r3.u32 + 24, ctx.r8.u16);
	// mulli r8,r5,1084
	ctx.r8.s64 = ctx.r5.s64 * 1084;
	// sth r9,72(r3)
	PPC_STORE_U16(ctx.r3.u32 + 72, ctx.r9.u16);
	// lhz r9,66(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 66);
	// sth r7,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r7.u16);
	// sth r6,112(r3)
	PPC_STORE_U16(ctx.r3.u32 + 112, ctx.r6.u16);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// add r9,r8,r7
	ctx.r9.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r11,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	r11.s64 = ctx.r9.s32 >> 16;
	// sth r10,66(r3)
	PPC_STORE_U16(ctx.r3.u32 + 66, ctx.r10.u16);
	// lhz r10,18(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 18);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r11,18(r3)
	PPC_STORE_U16(ctx.r3.u32 + 18, r11.u16);
	// b 0x8239bd38
	return;
loc_8262110C:
	// addi r11,r4,-2
	r11.s64 = ctx.r4.s64 + -2;
	// mulli r4,r5,6269
	ctx.r4.s64 = ctx.r5.s64 * 6269;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mulli r31,r5,708
	r31.s64 = ctx.r5.s64 * 708;
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// mulli r30,r5,172
	r30.s64 = ctx.r5.s64 * 172;
	// lwz r11,-25076(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -25076);
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// mulli r5,r5,73
	ctx.r5.s64 = ctx.r5.s64 * 73;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// li r9,1
	ctx.r9.s64 = 1;
	// li r8,3
	ctx.r8.s64 = 3;
	// subfe r10,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// lis r11,0
	r11.s64 = 0;
	// rlwinm r10,r10,0,31,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// add r31,r31,r11
	r31.u64 = r31.u64 + r11.u64;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// srawi r4,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 16;
	// srawi r31,r31,16
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFFF) != 0);
	r31.s64 = r31.s32 >> 16;
	// li r7,5
	ctx.r7.s64 = 5;
	// srawi r30,r30,16
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFFF) != 0);
	r30.s64 = r30.s32 >> 16;
	// srawi r5,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 16;
	// li r6,7
	ctx.r6.s64 = 7;
	// slw r11,r9,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// slw r9,r8,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r11,r7,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// slw r7,r6,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r8,r3
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r3.u32);
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// sthx r7,r8,r3
	PPC_STORE_U16(ctx.r8.u32 + ctx.r3.u32, ctx.r7.u16);
	// lhzx r8,r9,r3
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r3.u32);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// sthx r8,r9,r3
	PPC_STORE_U16(ctx.r9.u32 + ctx.r3.u32, ctx.r8.u16);
	// lhzx r9,r10,r3
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r3.u32);
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// sthx r9,r10,r3
	PPC_STORE_U16(ctx.r10.u32 + ctx.r3.u32, ctx.r9.u16);
	// lhzx r10,r11,r3
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r3.u32);
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// sthx r10,r11,r3
	PPC_STORE_U16(r11.u32 + ctx.r3.u32, ctx.r10.u16);
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826211D0"))) PPC_WEAK_FUNC(sub_826211D0);
PPC_FUNC_IMPL(__imp__sub_826211D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce4
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r26,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r26.u32);
	// stw r26,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r26.u32);
	// bne cr6,0x82621324
	if (!cr6.eq) goto loc_82621324;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x82621230
	if (!cr6.eq) goto loc_82621230;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lis r4,-32640
	ctx.r4.s64 = -2139095040;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// ori r4,r4,32896
	ctx.r4.u64 = ctx.r4.u64 | 32896;
	// li r8,13
	ctx.r8.s64 = 13;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r5.u32);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_82621220:
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82621220
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82621220;
	// b 0x82621528
	goto loc_82621528;
loc_82621230:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// li r31,-1
	r31.s64 = -1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// subf r5,r5,r11
	ctx.r5.s64 = r11.s64 - ctx.r5.s64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// stw r8,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r8.u32);
	// lis r8,257
	ctx.r8.s64 = 16842752;
	// ori r4,r8,257
	ctx.r4.u64 = ctx.r8.u64 | 257;
	// ble 0x82621284
	if (!cr0.gt) goto loc_82621284;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r31,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r31.u32);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r31,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r31.u32);
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r31,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r31.u32);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, r11.u32);
	// b 0x826212a8
	goto loc_826212A8;
loc_82621284:
	// lbz r8,7(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r31,r8,r4
	r31.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r31,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, r31.u32);
	// stw r31,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r31.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
loc_826212A8:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// stw r11,16(r8)
	PPC_STORE_U32(ctx.r8.u32 + 16, r11.u32);
	// lwz r11,4(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// stw r11,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, r11.u32);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lbz r5,6(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lbz r8,7(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lbz r4,5(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r30,3(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// lbz r31,2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// srawi r8,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r8,r8,r29
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
	// stw r8,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r8.u32);
	// stw r8,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r8.u32);
	// stw r8,-12(r11)
	PPC_STORE_U32(r11.u32 + -12, ctx.r8.u32);
	// stw r8,-16(r11)
	PPC_STORE_U32(r11.u32 + -16, ctx.r8.u32);
	// stw r8,-20(r11)
	PPC_STORE_U32(r11.u32 + -20, ctx.r8.u32);
	// b 0x82621528
	goto loc_82621528;
loc_82621324:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// bne cr6,0x82621440
	if (!cr6.eq) goto loc_82621440;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// lis r8,257
	ctx.r8.s64 = 16842752;
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r29,r8,257
	r29.u64 = ctx.r8.u64 | 257;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r24,r5,3,0,28
	r24.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r28,0(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rlwinm r31,r5,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r5,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r24,r5,r24
	r24.s64 = r24.s64 - ctx.r5.s64;
	// stb r28,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, r28.u8);
	// add r28,r5,r4
	r28.u64 = ctx.r5.u64 + ctx.r4.u64;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lbzx r27,r11,r5
	r27.u64 = PPC_LOAD_U8(r11.u32 + ctx.r5.u32);
	// add r25,r5,r4
	r25.u64 = ctx.r5.u64 + ctx.r4.u64;
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r5,r4
	ctx.r4.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r5,-1(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + -1);
	// stb r27,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, r27.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r23,r4,1,0,30
	r23.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbz r4,-2(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -2);
	// stb r31,-3(r8)
	PPC_STORE_U8(ctx.r8.u32 + -3, r31.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbzx r31,r28,r11
	r31.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// lbz r4,-3(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -3);
	// stb r31,-4(r8)
	PPC_STORE_U8(ctx.r8.u32 + -4, r31.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbzx r31,r30,r11
	r31.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbz r4,-4(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -4);
	// stb r31,-5(r8)
	PPC_STORE_U8(ctx.r8.u32 + -5, r31.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbzx r31,r25,r11
	r31.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// lbz r4,-5(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -5);
	// stb r31,-6(r8)
	PPC_STORE_U8(ctx.r8.u32 + -6, r31.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbzx r31,r23,r11
	r31.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// lbz r4,-6(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -6);
	// stb r31,-7(r8)
	PPC_STORE_U8(ctx.r8.u32 + -7, r31.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lbzx r4,r24,r11
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + r11.u32);
	// lbz r11,-7(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + -7);
	// stb r4,-8(r8)
	PPC_STORE_U8(ctx.r8.u32 + -8, ctx.r4.u8);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// lbz r5,-8(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + -8);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// stb r5,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r5.u8);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r11,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, r11.u32);
	// stw r11,16(r8)
	PPC_STORE_U32(ctx.r8.u32 + 16, r11.u32);
	// stw r11,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, r11.u32);
	// stw r11,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r11.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// b 0x82621528
	goto loc_82621528;
loc_82621440:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// subf r31,r5,r11
	r31.s64 = r11.s64 - ctx.r5.s64;
	// bge cr6,0x8262147c
	if (!cr6.lt) goto loc_8262147C;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r30,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r30.u32);
	// lwz r30,8(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r30,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r30.u32);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, r11.u32);
	// b 0x826214a8
	goto loc_826214A8;
loc_8262147C:
	// lis r8,257
	ctx.r8.s64 = 16842752;
	// lbz r30,7(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// ori r29,r8,257
	r29.u64 = ctx.r8.u64 | 257;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mullw r30,r30,r29
	r30.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r30,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, r30.u32);
	// stw r30,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r30.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
loc_826214A8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r11,16(r8)
	PPC_STORE_U32(ctx.r8.u32 + 16, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, r11.u32);
	// addi r8,r4,-1
	ctx.r8.s64 = ctx.r4.s64 + -1;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,20(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r31,r11,r8
	r31.u64 = r11.u64 + ctx.r8.u64;
	// subf r11,r5,r8
	r11.s64 = ctx.r8.s64 - ctx.r5.s64;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// lbz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stb r4,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r4.u8);
	// add r4,r31,r5
	ctx.r4.u64 = r31.u64 + ctx.r5.u64;
	// lbz r30,0(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// stb r30,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r30.u8);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stb r31,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r31.u8);
	// lbz r31,0(r4)
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// stb r31,-2(r11)
	PPC_STORE_U8(r11.u32 + -2, r31.u8);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// stb r31,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r31.u8);
	// lbz r31,0(r4)
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stb r31,-3(r11)
	PPC_STORE_U8(r11.u32 + -3, r31.u8);
	// lbzx r8,r8,r5
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// lbzx r8,r4,r5
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r5.u32);
	// stb r8,-4(r11)
	PPC_STORE_U8(r11.u32 + -4, ctx.r8.u8);
loc_82621528:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// li r27,2
	r27.s64 = 2;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r5,r11,-8
	ctx.r5.s64 = r11.s64 + -8;
	// addi r29,r8,2
	r29.s64 = ctx.r8.s64 + 2;
	// addi r30,r5,1
	r30.s64 = ctx.r5.s64 + 1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mr r28,r11
	r28.u64 = r11.u64;
loc_8262154C:
	// lbz r8,-2(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + -2);
	// lbz r5,-1(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// ble cr6,0x8262156c
	if (!cr6.gt) goto loc_8262156C;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x82621578
	goto loc_82621578;
loc_8262156C:
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// bge cr6,0x82621578
	if (!cr6.lt) goto loc_82621578;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
loc_82621578:
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// ble cr6,0x82621588
	if (!cr6.gt) goto loc_82621588;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x82621594
	goto loc_82621594;
loc_82621588:
	// cmpw cr6,r5,r28
	cr6.compare<int32_t>(ctx.r5.s32, r28.s32, xer);
	// bge cr6,0x82621594
	if (!cr6.lt) goto loc_82621594;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_82621594:
	// lbz r8,-1(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + -1);
	// lbz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// ble cr6,0x826215b4
	if (!cr6.gt) goto loc_826215B4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x826215c0
	goto loc_826215C0;
loc_826215B4:
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// bge cr6,0x826215c0
	if (!cr6.lt) goto loc_826215C0;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
loc_826215C0:
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// ble cr6,0x826215d0
	if (!cr6.gt) goto loc_826215D0;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x826215dc
	goto loc_826215DC;
loc_826215D0:
	// cmpw cr6,r5,r28
	cr6.compare<int32_t>(ctx.r5.s32, r28.s32, xer);
	// bge cr6,0x826215dc
	if (!cr6.lt) goto loc_826215DC;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_826215DC:
	// lbz r8,0(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lbz r5,1(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// ble cr6,0x826215fc
	if (!cr6.gt) goto loc_826215FC;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x82621608
	goto loc_82621608;
loc_826215FC:
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// bge cr6,0x82621608
	if (!cr6.lt) goto loc_82621608;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
loc_82621608:
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// ble cr6,0x82621618
	if (!cr6.gt) goto loc_82621618;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x82621624
	goto loc_82621624;
loc_82621618:
	// cmpw cr6,r5,r28
	cr6.compare<int32_t>(ctx.r5.s32, r28.s32, xer);
	// bge cr6,0x82621624
	if (!cr6.lt) goto loc_82621624;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_82621624:
	// lbz r8,1(r29)
	ctx.r8.u64 = PPC_LOAD_U8(r29.u32 + 1);
	// lbz r5,2(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 2);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// add r4,r5,r8
	ctx.r4.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// ble cr6,0x82621644
	if (!cr6.gt) goto loc_82621644;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x82621650
	goto loc_82621650;
loc_82621644:
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// bge cr6,0x82621650
	if (!cr6.lt) goto loc_82621650;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
loc_82621650:
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// ble cr6,0x82621660
	if (!cr6.gt) goto loc_82621660;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x8262166c
	goto loc_8262166C;
loc_82621660:
	// cmpw cr6,r5,r28
	cr6.compare<int32_t>(ctx.r5.s32, r28.s32, xer);
	// bge cr6,0x8262166c
	if (!cr6.lt) goto loc_8262166C;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_8262166C:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8262154c
	if (!cr6.eq) goto loc_8262154C;
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// subf r4,r28,r11
	ctx.r4.s64 = r11.s64 - r28.s64;
	// cmpw cr6,r4,r30
	cr6.compare<int32_t>(ctx.r4.s32, r30.s32, xer);
	// blt cr6,0x82621698
	if (cr6.lt) goto loc_82621698;
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// bge cr6,0x826216fc
	if (!cr6.lt) goto loc_826216FC;
loc_82621698:
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// stw r26,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r26.u32);
	// bge cr6,0x826216fc
	if (!cr6.lt) goto loc_826216FC;
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r8,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r8.u32);
	// lbz r8,9(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 9);
	// lbz r11,8(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r11,r11,9
	r11.s64 = r11.s64 + 9;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r11,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// subf r5,r11,r5
	ctx.r5.s64 = ctx.r5.s64 - r11.s64;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// srawi r8,r8,5
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
loc_826216FC:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// beq cr6,0x8262171c
	if (cr6.eq) goto loc_8262171C;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// b 0x8239bd34
	return;
loc_8262171C:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82621734
	if (!cr6.eq) goto loc_82621734;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r11,r11,14576
	r11.s64 = r11.s64 + 14576;
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// b 0x82621748
	goto loc_82621748;
loc_82621734:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r11,r11,14576
	r11.s64 = r11.s64 + 14576;
	// bne cr6,0x82621748
	if (!cr6.eq) goto loc_82621748;
	// addi r11,r11,96
	r11.s64 = r11.s64 + 96;
loc_82621748:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// rlwinm r11,r30,1,0,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// bge cr6,0x826217b0
	if (!cr6.lt) goto loc_826217B0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x826217a4
	if (cr6.eq) goto loc_826217A4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826217a4
	if (cr6.eq) goto loc_826217A4;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x8262178c
	if (!cr6.eq) goto loc_8262178C;
	// li r11,11
	r11.s64 = 11;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// b 0x8239bd34
	return;
loc_8262178C:
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// bne cr6,0x826217a8
	if (!cr6.eq) goto loc_826217A8;
	// li r11,10
	r11.s64 = 10;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// b 0x8239bd34
	return;
loc_826217A4:
	// stw r26,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r26.u32);
loc_826217A8:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// b 0x8239bd34
	return;
loc_826217B0:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826217B8"))) PPC_WEAK_FUNC(sub_826217B8);
PPC_FUNC_IMPL(__imp__sub_826217B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bccc
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r11,24
	ctx.r10.s64 = r11.s64 + 24;
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r7,r10,4
	ctx.r7.s64 = ctx.r10.s64 + 4;
	// sth r6,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r6.u16);
	// addi r6,r11,4
	ctx.r6.s64 = r11.s64 + 4;
	// lbz r4,-1(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -1);
	// sth r4,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r4.u16);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// sth r4,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r4.u16);
	// lbz r4,-2(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -2);
	// sth r4,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r4.u16);
	// lbz r4,2(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// sth r4,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r4.u16);
	// lbz r4,-3(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -3);
	// sth r4,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r4.u16);
	// lbz r4,3(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// sth r4,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r4.u16);
	// lbz r4,-4(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -4);
	// sth r4,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r4.u16);
	// lbz r4,4(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// sth r4,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r4.u16);
	// lbz r4,-5(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -5);
	// sth r4,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r4.u16);
	// lbz r4,5(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// sth r4,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r4.u16);
	// lbz r4,-6(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -6);
	// sth r4,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r4.u16);
	// lbz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 6);
	// sth r4,12(r10)
	PPC_STORE_U16(ctx.r10.u32 + 12, ctx.r4.u16);
	// lbz r4,-7(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + -7);
	// sth r4,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r4.u16);
	// lbz r4,7(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 7);
	// sth r4,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, ctx.r4.u16);
	// lbz r8,-8(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + -8);
	// sth r8,14(r11)
	PPC_STORE_U16(r11.u32 + 14, ctx.r8.u16);
	// lbz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 8);
	// sth r8,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r8.u16);
	// sth r5,16(r11)
	PPC_STORE_U16(r11.u32 + 16, ctx.r5.u16);
	// lbz r8,9(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 9);
	// sth r8,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r8.u16);
	// sth r5,18(r11)
	PPC_STORE_U16(r11.u32 + 18, ctx.r5.u16);
	// lbz r8,10(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 10);
	// sth r8,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r8.u16);
	// sth r5,20(r11)
	PPC_STORE_U16(r11.u32 + 20, ctx.r5.u16);
	// lbz r9,11(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 11);
	// sth r9,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r9.u16);
	// sth r5,22(r11)
	PPC_STORE_U16(r11.u32 + 22, ctx.r5.u16);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// srawi r31,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r31.s64 = r11.s32 >> 1;
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r30,8(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r4,r5,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r29,8(r6)
	r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// stw r11,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, r11.u32);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r10,-212(r1)
	PPC_STORE_U32(ctx.r1.u32 + -212, ctx.r10.u32);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r11,r7,12
	r11.s64 = ctx.r7.s64 + 12;
	// lwz r7,44(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// addi r10,r6,12
	ctx.r10.s64 = ctx.r6.s64 + 12;
	// lwz r6,40(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// stw r9,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r9.u32);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stw r8,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r8.u32);
	// srawi r5,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 1;
	// rlwinm r8,r31,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r5,r30,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r4,r29,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r3,r3,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r9,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r9.u32);
	// stw r8,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, ctx.r8.u32);
	// addi r7,r7,-2
	ctx.r7.s64 = ctx.r7.s64 + -2;
	// add r31,r3,r31
	r31.u64 = ctx.r3.u64 + r31.u64;
	// srawi r3,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// add r8,r4,r3
	ctx.r8.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lwz r4,-8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r3,-8(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// rlwinm r29,r4,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r28,r5,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r5,-12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// lwz r4,-12(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	// rlwinm r30,r3,4,0,27
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r8.u32);
	// addi r6,r6,-2
	ctx.r6.s64 = ctx.r6.s64 + -2;
	// stw r9,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r9.u32);
	// rlwinm r9,r31,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// rlwinm r3,r4,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r10,-16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -16);
	// rlwinm r31,r5,4,0,27
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r5,r10,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r9,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, ctx.r9.u32);
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r11,-16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -16);
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r8.u32);
	// rlwinm r4,r11,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r11,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r11.s64 = ctx.r9.s32 >> 1;
	// add r9,r28,r9
	ctx.r9.u64 = r28.u64 + ctx.r9.u64;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// srawi r10,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 1;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// stw r9,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r9.u32);
	// stw r11,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, r11.u32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// lhz r3,-214(r1)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r1.u32 + -214);
	// extsh r22,r3
	r22.s64 = ctx.r3.s16;
	// stw r11,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, r11.u32);
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// stw r10,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r10.u32);
	// add r10,r4,r9
	ctx.r10.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// stw r10,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r10.u32);
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// lhz r11,-176(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -176);
	// lhz r10,-174(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -174);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// mulli r9,r9,181
	ctx.r9.s64 = ctx.r9.s64 * 181;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// extsh r5,r10
	ctx.r5.s64 = ctx.r10.s16;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// lhz r10,-172(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -172);
	// srawi r27,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r27.s64 = ctx.r8.s32 >> 1;
	// mulli r26,r5,181
	r26.s64 = ctx.r5.s64 * 181;
	// lhz r8,-216(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -216);
	// lhz r5,-170(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + -170);
	// clrlwi r28,r9,16
	r28.u64 = ctx.r9.u32 & 0xFFFF;
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// extsh r30,r8
	r30.s64 = ctx.r8.s16;
	// extsh r21,r5
	r21.s64 = ctx.r5.s16;
	// mulli r29,r29,181
	r29.s64 = r29.s64 * 181;
	// lhz r9,-240(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -240);
	// lhz r4,-238(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + -238);
	// extsh r31,r9
	r31.s64 = ctx.r9.s16;
	// extsh r23,r4
	r23.s64 = ctx.r4.s16;
	// add r20,r31,r30
	r20.u64 = r31.u64 + r30.u64;
	// lhz r31,-236(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + -236);
	// lhz r30,-212(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + -212);
	// addi r19,r26,128
	r19.s64 = r26.s64 + 128;
	// add r23,r23,r22
	r23.u64 = r23.u64 + r22.u64;
	// mulli r26,r21,181
	r26.s64 = r21.s64 * 181;
	// addi r22,r29,128
	r22.s64 = r29.s64 + 128;
	// mulli r29,r20,181
	r29.s64 = r20.s64 * 181;
	// extsh r24,r30
	r24.s64 = r30.s16;
	// extsh r25,r31
	r25.s64 = r31.s16;
	// extsh r21,r5
	r21.s64 = ctx.r5.s16;
	// addi r20,r26,128
	r20.s64 = r26.s64 + 128;
	// srawi r5,r19,8
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0xFF) != 0);
	ctx.r5.s64 = r19.s32 >> 8;
	// mulli r26,r23,181
	r26.s64 = r23.s64 * 181;
	// add r25,r25,r24
	r25.u64 = r25.u64 + r24.u64;
	// srawi r23,r22,8
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFF) != 0);
	r23.s64 = r22.s32 >> 8;
	// srawi r24,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r24.s64 = r21.s32 >> 1;
	// extsh r21,r4
	r21.s64 = ctx.r4.s16;
	// addi r22,r29,128
	r22.s64 = r29.s64 + 128;
	// srawi r4,r20,8
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0xFF) != 0);
	ctx.r4.s64 = r20.s32 >> 8;
	// mulli r29,r25,181
	r29.s64 = r25.s64 * 181;
	// addi r20,r26,128
	r20.s64 = r26.s64 + 128;
	// mr r25,r11
	r25.u64 = r11.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r11,-234(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -234);
	// srawi r22,r22,8
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFF) != 0);
	r22.s64 = r22.s32 >> 8;
	// srawi r26,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	r26.s64 = r21.s32 >> 1;
	// addi r21,r29,128
	r21.s64 = r29.s64 + 128;
	// srawi r20,r20,8
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0xFF) != 0);
	r20.s64 = r20.s32 >> 8;
	// srawi r29,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r29.s64 = ctx.r8.s32 >> 1;
	// add r19,r5,r25
	r19.u64 = ctx.r5.u64 + r25.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lhz r10,-210(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -210);
	// clrlwi r25,r23,16
	r25.u64 = r23.u32 & 0xFFFF;
	// clrlwi r5,r22,16
	ctx.r5.u64 = r22.u32 & 0xFFFF;
	// extsh r23,r11
	r23.s64 = r11.s16;
	// srawi r21,r21,8
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xFF) != 0);
	r21.s64 = r21.s32 >> 8;
	// add r18,r28,r27
	r18.u64 = r28.u64 + r27.u64;
	// mr r22,r19
	r22.u64 = r19.u64;
	// add r19,r4,r8
	r19.u64 = ctx.r4.u64 + ctx.r8.u64;
	// extsh r28,r11
	r28.s64 = r11.s16;
	// add r5,r5,r26
	ctx.r5.u64 = ctx.r5.u64 + r26.u64;
	// srawi r8,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	ctx.r8.s64 = r23.s32 >> 1;
	// sth r18,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, r18.u16);
	// clrlwi r11,r21,16
	r11.u64 = r21.u32 & 0xFFFF;
	// add r3,r5,r3
	ctx.r3.u64 = ctx.r5.u64 + ctx.r3.u64;
	// add r5,r11,r8
	ctx.r5.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r6,2
	r11.s64 = ctx.r6.s64 + 2;
	// add r6,r5,r10
	ctx.r6.u64 = ctx.r5.u64 + ctx.r10.u64;
	// extsh r27,r10
	r27.s64 = ctx.r10.s16;
	// add r25,r25,r24
	r25.u64 = r25.u64 + r24.u64;
	// addi r10,r7,2
	ctx.r10.s64 = ctx.r7.s64 + 2;
	// sth r22,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r22.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// clrlwi r4,r20,16
	ctx.r4.u64 = r20.u32 & 0xFFFF;
	// sth r25,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r25.u16);
	// mulli r8,r28,181
	ctx.r8.s64 = r28.s64 * 181;
	// sth r19,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r19.u16);
	// sth r3,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r3.u16);
	// lhz r7,-206(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -206);
	// add r4,r4,r29
	ctx.r4.u64 = ctx.r4.u64 + r29.u64;
	// addi r8,r8,128
	ctx.r8.s64 = ctx.r8.s64 + 128;
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// clrlwi r24,r8,16
	r24.u64 = ctx.r8.u32 & 0xFFFF;
	// lhz r8,-208(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -208);
	// extsh r3,r7
	ctx.r3.s64 = ctx.r7.s16;
	// extsh r4,r8
	ctx.r4.s64 = ctx.r8.s16;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r9,-160(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -160);
	// sth r6,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r6.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r5,r9
	ctx.r5.s64 = ctx.r9.s16;
	// lhz r6,-158(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + -158);
	// add r27,r5,r4
	r27.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lhz r5,-156(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + -156);
	// extsh r29,r6
	r29.s64 = ctx.r6.s16;
	// lhz r4,-204(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + -204);
	// extsh r28,r5
	r28.s64 = ctx.r5.s16;
	// add r25,r3,r29
	r25.u64 = ctx.r3.u64 + r29.u64;
	// lhz r29,-202(r1)
	r29.u64 = PPC_LOAD_U16(ctx.r1.u32 + -202);
	// extsh r26,r4
	r26.s64 = ctx.r4.s16;
	// lhz r3,-154(r1)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r1.u32 + -154);
	// extsh r19,r30
	r19.s64 = r30.s16;
	// lhz r30,-152(r1)
	r30.u64 = PPC_LOAD_U16(ctx.r1.u32 + -152);
	// add r20,r28,r26
	r20.u64 = r28.u64 + r26.u64;
	// lhz r28,-200(r1)
	r28.u64 = PPC_LOAD_U16(ctx.r1.u32 + -200);
	// mulli r21,r27,181
	r21.s64 = r27.s64 * 181;
	// extsh r27,r3
	r27.s64 = ctx.r3.s16;
	// mulli r22,r25,181
	r22.s64 = r25.s64 * 181;
	// extsh r23,r29
	r23.s64 = r29.s16;
	// extsh r26,r30
	r26.s64 = r30.s16;
	// extsh r25,r28
	r25.s64 = r28.s16;
	// addi r21,r21,128
	r21.s64 = r21.s64 + 128;
	// add r18,r27,r23
	r18.u64 = r27.u64 + r23.u64;
	// extsh r17,r6
	r17.s64 = ctx.r6.s16;
	// add r25,r26,r25
	r25.u64 = r26.u64 + r25.u64;
	// mulli r27,r20,181
	r27.s64 = r20.s64 * 181;
	// srawi r23,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r23.s64 = r19.s32 >> 1;
	// addi r22,r22,128
	r22.s64 = r22.s64 + 128;
	// extsh r20,r8
	r20.s64 = ctx.r8.s16;
	// srawi r21,r21,8
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xFF) != 0);
	r21.s64 = r21.s32 >> 8;
	// mulli r6,r18,181
	ctx.r6.s64 = r18.s64 * 181;
	// srawi r26,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r26.s64 = r17.s32 >> 1;
	// srawi r22,r22,8
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFF) != 0);
	r22.s64 = r22.s32 >> 8;
	// mulli r8,r25,181
	ctx.r8.s64 = r25.s64 * 181;
	// srawi r25,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r25.s64 = r20.s32 >> 1;
	// addi r27,r27,128
	r27.s64 = r27.s64 + 128;
	// addi r20,r6,128
	r20.s64 = ctx.r6.s64 + 128;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// clrlwi r6,r21,16
	ctx.r6.u64 = r21.u32 & 0xFFFF;
	// srawi r19,r27,8
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xFF) != 0);
	r19.s64 = r27.s32 >> 8;
	// extsh r27,r4
	r27.s64 = ctx.r4.s16;
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// srawi r4,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 1;
	// addi r21,r8,128
	r21.s64 = ctx.r8.s64 + 128;
	// add r26,r6,r26
	r26.u64 = ctx.r6.u64 + r26.u64;
	// lhz r8,-150(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -150);
	// clrlwi r3,r22,16
	ctx.r3.u64 = r22.u32 & 0xFFFF;
	// add r24,r24,r31
	r24.u64 = r24.u64 + r31.u64;
	// srawi r20,r20,8
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0xFF) != 0);
	r20.s64 = r20.s32 >> 8;
	// add r26,r26,r7
	r26.u64 = r26.u64 + ctx.r7.u64;
	// srawi r27,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r27.s64 = r27.s32 >> 1;
	// clrlwi r6,r19,16
	ctx.r6.u64 = r19.u32 & 0xFFFF;
	// extsh r22,r8
	r22.s64 = ctx.r8.s16;
	// srawi r23,r21,8
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xFF) != 0);
	r23.s64 = r21.s32 >> 8;
	// sth r24,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r24.u16);
	// add r31,r3,r25
	r31.u64 = ctx.r3.u64 + r25.u64;
	// clrlwi r3,r20,16
	ctx.r3.u64 = r20.u32 & 0xFFFF;
	// sth r26,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r26.u16);
	// add r4,r6,r4
	ctx.r4.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r31,r31,r9
	r31.u64 = r31.u64 + ctx.r9.u64;
	// srawi r6,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	ctx.r6.s64 = r22.s32 >> 1;
	// clrlwi r7,r23,16
	ctx.r7.u64 = r23.u32 & 0xFFFF;
	// add r9,r3,r27
	ctx.r9.u64 = ctx.r3.u64 + r27.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r6,r9,r5
	ctx.r6.u64 = ctx.r9.u64 + ctx.r5.u64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,-198(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -198);
	// add r4,r4,r29
	ctx.r4.u64 = ctx.r4.u64 + r29.u64;
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// sth r31,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r31.u16);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// sth r4,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r4.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// mulli r5,r9,181
	ctx.r5.s64 = ctx.r9.s64 * 181;
	// lhz r8,-196(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -196);
	// lhz r9,-148(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -148);
	// sth r6,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r6.u16);
	// sth r7,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r7.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// extsh r31,r9
	r31.s64 = ctx.r9.s16;
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// lhz r7,-146(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + -146);
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// lhz r6,-194(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + -194);
	// addi r24,r5,128
	r24.s64 = ctx.r5.s64 + 128;
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// lhz r5,-192(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + -192);
	// extsh r3,r6
	ctx.r3.s64 = ctx.r6.s16;
	// mulli r29,r31,181
	r29.s64 = r31.s64 * 181;
	// lhz r31,-190(r1)
	r31.u64 = PPC_LOAD_U16(ctx.r1.u32 + -190);
	// add r25,r4,r3
	r25.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lhz r4,-140(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + -140);
	// extsh r22,r28
	r22.s64 = r28.s16;
	// lhz r3,-188(r1)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r1.u32 + -188);
	// mulli r28,r25,181
	r28.s64 = r25.s64 * 181;
	// extsh r23,r5
	r23.s64 = ctx.r5.s16;
	// extsh r26,r3
	r26.s64 = ctx.r3.s16;
	// extsh r27,r4
	r27.s64 = ctx.r4.s16;
	// extsh r25,r31
	r25.s64 = r31.s16;
	// addi r21,r29,128
	r21.s64 = r29.s64 + 128;
	// srawi r20,r24,8
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xFF) != 0);
	r20.s64 = r24.s32 >> 8;
	// extsh r24,r7
	r24.s64 = ctx.r7.s16;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// mulli r29,r23,181
	r29.s64 = r23.s64 * 181;
	// srawi r26,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r26.s64 = r22.s32 >> 1;
	// addi r23,r28,128
	r23.s64 = r28.s64 + 128;
	// mulli r7,r25,181
	ctx.r7.s64 = r25.s64 * 181;
	// extsh r22,r8
	r22.s64 = ctx.r8.s16;
	// srawi r25,r21,8
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xFF) != 0);
	r25.s64 = r21.s32 >> 8;
	// srawi r28,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r28.s64 = r24.s32 >> 1;
	// addi r29,r29,128
	r29.s64 = r29.s64 + 128;
	// mulli r8,r27,181
	ctx.r8.s64 = r27.s64 * 181;
	// srawi r21,r23,8
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFF) != 0);
	r21.s64 = r23.s32 >> 8;
	// addi r23,r7,128
	r23.s64 = ctx.r7.s64 + 128;
	// srawi r24,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r24.s64 = r22.s32 >> 1;
	// srawi r7,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	ctx.r7.s64 = r29.s32 >> 8;
	// clrlwi r27,r20,16
	r27.u64 = r20.u32 & 0xFFFF;
	// addi r22,r8,128
	r22.s64 = ctx.r8.s64 + 128;
	// clrlwi r29,r25,16
	r29.u64 = r25.u32 & 0xFFFF;
	// lhz r8,-138(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -138);
	// srawi r20,r23,8
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFF) != 0);
	r20.s64 = r23.s32 >> 8;
	// add r23,r27,r26
	r23.u64 = r27.u64 + r26.u64;
	// add r26,r29,r28
	r26.u64 = r29.u64 + r28.u64;
	// mr r27,r31
	r27.u64 = r31.u64;
	// extsh r31,r8
	r31.s64 = ctx.r8.s16;
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// lhz r8,-186(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -186);
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r26,r26,r6
	r26.u64 = r26.u64 + ctx.r6.u64;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// clrlwi r25,r21,16
	r25.u64 = r21.u32 & 0xFFFF;
	// extsh r28,r8
	r28.s64 = ctx.r8.s16;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// sth r23,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r23.u16);
	// add r30,r25,r24
	r30.u64 = r25.u64 + r24.u64;
	// sth r26,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r26.u16);
	// srawi r22,r22,8
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFF) != 0);
	r22.s64 = r22.s32 >> 8;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// clrlwi r6,r20,16
	ctx.r6.u64 = r20.u32 & 0xFFFF;
	// add r30,r30,r9
	r30.u64 = r30.u64 + ctx.r9.u64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// add r27,r7,r27
	r27.u64 = ctx.r7.u64 + r27.u64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// clrlwi r7,r22,16
	ctx.r7.u64 = r22.u32 & 0xFFFF;
	// mulli r9,r29,181
	ctx.r9.s64 = r29.s64 * 181;
	// sth r30,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r30.u16);
	// sth r27,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r27.u16);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// extsh r7,r3
	ctx.r7.s64 = ctx.r3.s16;
	// sth r6,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r6.u16);
	// addi r11,r10,2
	r11.s64 = ctx.r10.s64 + 2;
	// clrlwi r10,r9,16
	ctx.r10.u64 = ctx.r9.u32 & 0xFFFF;
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// sth r10,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r10.u16);
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_82621E18"))) PPC_WEAK_FUNC(sub_82621E18);
PPC_FUNC_IMPL(__imp__sub_82621E18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r20,r30
	r20.u64 = r30.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r28,r6,-8
	r28.s64 = ctx.r6.s64 + -8;
	// stw r19,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r19.u32);
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82621f40
	if (cr6.eq) goto loc_82621F40;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r10,8
	ctx.r10.s64 = 8;
loc_82621E58:
	// lhz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lhz r9,2(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lhz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lhz r9,6(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lhz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lhz r9,10(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lhz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,7(r29)
	PPC_STORE_U8(r29.u32 + 7, ctx.r9.u8);
	// addi r9,r29,8
	ctx.r9.s64 = r29.s64 + 8;
	// add r29,r28,r9
	r29.u64 = r28.u64 + ctx.r9.u64;
	// bgt cr6,0x82621e58
	if (cr6.gt) goto loc_82621E58;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x826230e4
	if (!cr6.eq) goto loc_826230E4;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_82621F40:
	// cmplwi cr6,r4,11
	cr6.compare<uint32_t>(ctx.r4.u32, 11, xer);
	// bgt cr6,0x826230dc
	if (cr6.gt) goto loc_826230DC;
	// lis r12,-32158
	r12.s64 = -2107506688;
	// addi r12,r12,8032
	r12.s64 = r12.s64 + 8032;
	// rlwinm r0,r4,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r4.u64) {
	case 0:
		goto loc_82621F90;
	case 1:
		goto loc_826227E4;
	case 2:
		goto loc_8262295C;
	case 3:
		goto loc_82622A64;
	case 4:
		goto loc_826222B0;
	case 5:
		goto loc_82622B70;
	case 6:
		goto loc_82622E68;
	case 7:
		goto loc_82622C18;
	case 8:
		goto loc_826221A0;
	case 9:
		goto loc_82622F70;
	case 10:
		goto loc_82622460;
	case 11:
		goto loc_82622618;
	default:
		__builtin_unreachable();
	}
	// lwz r19,8080(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 8080);
	// lwz r19,10212(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 10212);
	// lwz r19,10588(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 10588);
	// lwz r19,10852(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 10852);
	// lwz r19,8880(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 8880);
	// lwz r19,11120(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 11120);
	// lwz r19,11880(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 11880);
	// lwz r19,11288(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 11288);
	// lwz r19,8608(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 8608);
	// lwz r19,12144(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 12144);
	// lwz r19,9312(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 9312);
	// lwz r19,9752(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 9752);
loc_82621F90:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826217b8
	sub_826217B8(ctx, base);
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lis r10,0
	ctx.r10.s64 = 0;
	// lwz r7,40(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// addi r11,r11,-17648
	r11.s64 = r11.s64 + -17648;
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// ori r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 32768;
loc_82621FB4:
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lhz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// lhz r5,-4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// lhz r3,-2(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// lhz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// lhz r26,0(r9)
	r26.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// mullw r3,r3,r8
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r5,r5,r26
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r26.s32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r27
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r27.u32);
	// stb r5,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r5.u8);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r5,2(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// lhz r27,2(r30)
	r27.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r3,r8
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// extsh r4,r27
	ctx.r4.s64 = r27.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r26.u32);
	// stb r5,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r5.u8);
	// lhz r4,4(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r3,6(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// lhz r27,4(r30)
	r27.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r3,r8
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// extsh r4,r27
	ctx.r4.s64 = r27.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r26.u32);
	// stb r5,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r5.u8);
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lhz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// lhz r3,10(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// lhz r27,6(r30)
	r27.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r3,r8
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// extsh r4,r27
	ctx.r4.s64 = r27.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r26.u32);
	// stb r5,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r5.u8);
	// lhz r4,8(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lhz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// lhz r3,14(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// lhz r27,8(r30)
	r27.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r3,r8
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// extsh r4,r27
	ctx.r4.s64 = r27.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r26.u32);
	// stb r5,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r5.u8);
	// lhz r4,10(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lhz r5,16(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// lhz r3,18(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 18);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r3,r8
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r8.s32);
	// lhz r3,10(r30)
	ctx.r3.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// extsh r4,r3
	ctx.r4.s64 = ctx.r3.s16;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbzx r5,r5,r27
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r27.u32);
	// stb r5,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r5.u8);
	// lhz r3,12(r9)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lhz r5,20(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 20);
	// lhz r4,22(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 22);
	// mullw r5,r5,r3
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r3.s32);
	// lhz r27,12(r30)
	r27.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r4,r4,r8
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// extsh r4,r27
	ctx.r4.s64 = r27.s16;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbzx r5,r5,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r26.u32);
	// stb r5,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r5.u8);
	// lhz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 14);
	// lhz r4,26(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 26);
	// lhz r5,24(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 24);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// mullw r8,r4,r8
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r8.s32);
	// lhz r4,14(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r9,r5,r9
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// extsh r8,r4
	ctx.r8.s64 = ctx.r4.s16;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// srawi r9,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 16;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// stb r9,7(r29)
	PPC_STORE_U8(r29.u32 + 7, ctx.r9.u8);
	// addi r9,r29,8
	ctx.r9.s64 = r29.s64 + 8;
	// add r29,r28,r9
	r29.u64 = r28.u64 + ctx.r9.u64;
	// bgt cr6,0x82621fb4
	if (cr6.gt) goto loc_82621FB4;
	// b 0x826230dc
	goto loc_826230DC;
loc_826221A0:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r10,8
	ctx.r10.s64 = 8;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// addi r8,r8,-2
	ctx.r8.s64 = ctx.r8.s64 + -2;
loc_826221B4:
	// lbz r11,0(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r6,r11,r7
	ctx.r6.u64 = r11.u64 + ctx.r7.u64;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r7.u8);
	// lhz r7,2(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r7.u8);
	// lhz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r7.u8);
	// lhz r7,6(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r7.u8);
	// lhz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r7.u8);
	// lhz r7,10(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r7.u8);
	// lhz r7,12(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r7.u8);
	// lhz r7,14(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r11,r7,r11
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// bgt cr6,0x826221b4
	if (cr6.gt) goto loc_826221B4;
	// b 0x826230dc
	goto loc_826230DC;
loc_826222B0:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r10,8
	ctx.r10.s64 = 8;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 16);
	// lbz r8,17(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 17);
	// add r5,r9,r7
	ctx.r5.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r9,18(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 18);
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// addi r3,r5,1
	ctx.r3.s64 = ctx.r5.s64 + 1;
	// lbz r8,19(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 19);
	// lbz r5,3(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addi r27,r6,1
	r27.s64 = ctx.r6.s64 + 1;
	// lbz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 20);
	// add r4,r8,r5
	ctx.r4.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// addi r26,r7,1
	r26.s64 = ctx.r7.s64 + 1;
	// lbz r7,5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r8,21(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 21);
	// add r6,r9,r6
	ctx.r6.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r5,6(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r9,22(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 22);
	// lbz r7,23(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 23);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lbz r11,7(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r5,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r5.s64 = r27.s32 >> 1;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// srawi r7,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r7.s64 = r26.s32 >> 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// srawi r27,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r27.s64 = ctx.r6.s32 >> 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r26,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r26.s64 = ctx.r9.s32 >> 1;
	// srawi r25,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r25.s64 = r11.s32 >> 1;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// clrlwi r7,r5,24
	ctx.r7.u64 = ctx.r5.u32 & 0xFF;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// clrlwi r5,r4,24
	ctx.r5.u64 = ctx.r4.u32 & 0xFF;
	// clrlwi r4,r27,24
	ctx.r4.u64 = r27.u32 & 0xFF;
	// clrlwi r27,r26,24
	r27.u64 = r26.u32 & 0xFF;
	// clrlwi r8,r11,24
	ctx.r8.u64 = r11.u32 & 0xFF;
	// clrlwi r6,r6,24
	ctx.r6.u64 = ctx.r6.u32 & 0xFF;
	// clrlwi r3,r3,24
	ctx.r3.u64 = ctx.r3.u32 & 0xFF;
	// clrlwi r26,r25,24
	r26.u64 = r25.u32 & 0xFF;
loc_82622384:
	// lhz r11,0(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r8
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// stb r11,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r11.u8);
	// lhz r11,2(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// stb r11,1(r29)
	PPC_STORE_U8(r29.u32 + 1, r11.u8);
	// lhz r11,4(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r6
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// stb r11,2(r29)
	PPC_STORE_U8(r29.u32 + 2, r11.u8);
	// lhz r11,6(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r5
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r5.u32);
	// stb r11,3(r29)
	PPC_STORE_U8(r29.u32 + 3, r11.u8);
	// lhz r11,8(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r4
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// stb r11,4(r29)
	PPC_STORE_U8(r29.u32 + 4, r11.u8);
	// lhz r11,10(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r3
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r3.u32);
	// stb r11,5(r29)
	PPC_STORE_U8(r29.u32 + 5, r11.u8);
	// lhz r11,12(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// stb r11,6(r29)
	PPC_STORE_U8(r29.u32 + 6, r11.u8);
	// lhz r11,14(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r26
	r11.u64 = PPC_LOAD_U8(r11.u32 + r26.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// bgt cr6,0x82622384
	if (cr6.gt) goto loc_82622384;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622460:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r10,8
	ctx.r10.s64 = 8;
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lbz r6,5(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lbz r5,7(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// rotlwi r25,r8,1
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rlwinm r26,r4,2,0,29
	r26.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// rotlwi r4,r6,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 2);
	// lbz r11,1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addi r27,r3,2
	r27.s64 = ctx.r3.s64 + 2;
	// add r25,r8,r25
	r25.u64 = ctx.r8.u64 + r25.u64;
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// rotlwi r11,r7,1
	r11.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// rotlwi r24,r5,3
	r24.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r8,r6,r4
	ctx.r8.u64 = ctx.r6.u64 + ctx.r4.u64;
	// rlwinm r7,r25,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r6,r5,r24
	ctx.r6.s64 = r24.s64 - ctx.r5.s64;
	// rlwinm r27,r27,1,0,30
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r25,r11,4
	r25.s64 = r11.s64 + 4;
	// addi r24,r8,4
	r24.s64 = ctx.r8.s64 + 4;
	// addi r23,r7,4
	r23.s64 = ctx.r7.s64 + 4;
	// addi r22,r6,4
	r22.s64 = ctx.r6.s64 + 4;
loc_826224D4:
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r5,r8,r7
	ctx.r5.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rotlwi r8,r11,3
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 3);
	// subf r6,r11,r22
	ctx.r6.s64 = r22.s64 - r11.s64;
	// addi r7,r8,4
	ctx.r7.s64 = ctx.r8.s64 + 4;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// add r4,r3,r8
	ctx.r4.u64 = ctx.r3.u64 + ctx.r8.u64;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbzx r5,r5,r7
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// srawi r7,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 3;
	// stb r5,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r5.u8);
	// lhz r5,2(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r4,r27,r8
	ctx.r4.u64 = r27.u64 + ctx.r8.u64;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// lbzx r5,r5,r7
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// srawi r7,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 3;
	// stb r5,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r5.u8);
	// lhz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r4,r25,r8
	ctx.r4.u64 = r25.u64 + ctx.r8.u64;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// lbzx r5,r5,r7
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// srawi r7,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 3;
	// stb r5,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r5.u8);
	// lhz r5,6(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r4,r26,r8
	ctx.r4.u64 = r26.u64 + ctx.r8.u64;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// lbzx r5,r5,r7
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// srawi r7,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 3;
	// stb r5,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r5.u8);
	// lhz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r4,r24,r8
	ctx.r4.u64 = r24.u64 + ctx.r8.u64;
	// srawi r8,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 3;
	// lbzx r7,r5,r7
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r7.u32);
	// stb r7,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r7.u8);
	// lhz r7,10(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r5,r23,r11
	ctx.r5.u64 = r23.u64 + r11.u64;
	// lbzx r7,r7,r8
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r8.u32);
	// add r8,r6,r11
	ctx.r8.u64 = ctx.r6.u64 + r11.u64;
	// srawi r11,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	r11.s64 = ctx.r5.s32 >> 3;
	// srawi r8,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// stb r7,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r7.u8);
	// lhz r7,12(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r11,r7,r11
	r11.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r11,6(r29)
	PPC_STORE_U8(r29.u32 + 6, r11.u8);
	// lhz r11,14(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// bgt cr6,0x826224d4
	if (cr6.gt) goto loc_826224D4;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622618:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rotlwi r7,r10,3
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r10,3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rotlwi r6,r6,3
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 3);
	// rotlwi r4,r10,3
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// rotlwi r5,r5,3
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// lbz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// rotlwi r3,r3,3
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r3.u32, 3);
	// lbz r11,7(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// rotlwi r27,r27,3
	r27.u64 = __builtin_rotateleft32(r27.u32, 3);
	// rotlwi r26,r10,3
	r26.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// rotlwi r25,r11,3
	r25.u64 = __builtin_rotateleft32(r11.u32, 3);
loc_82622668:
	// lbz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lhz r24,0(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lbz r22,0(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r23,r24,r23
	r23.u64 = r24.u64 + r23.u64;
	// add r24,r7,r10
	r24.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r19,r6,r10
	r19.u64 = ctx.r6.u64 + ctx.r10.u64;
	// srawi r24,r24,3
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7) != 0);
	r24.s64 = r24.s32 >> 3;
	// add r18,r5,r10
	r18.u64 = ctx.r5.u64 + ctx.r10.u64;
	// add r17,r4,r10
	r17.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r16,r3,r10
	r16.u64 = ctx.r3.u64 + ctx.r10.u64;
	// add r15,r27,r10
	r15.u64 = r27.u64 + ctx.r10.u64;
	// add r14,r26,r10
	r14.u64 = r26.u64 + ctx.r10.u64;
	// lbzx r24,r23,r24
	r24.u64 = PPC_LOAD_U8(r23.u32 + r24.u32);
	// add r10,r25,r10
	ctx.r10.u64 = r25.u64 + ctx.r10.u64;
	// subf r7,r22,r7
	ctx.r7.s64 = ctx.r7.s64 - r22.s64;
	// stb r24,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r24.u8);
	// lhz r24,2(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// srawi r10,r19,3
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7) != 0);
	ctx.r10.s64 = r19.s32 >> 3;
	// lbz r22,1(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r6,r22,r6
	ctx.r6.s64 = ctx.r6.s64 - r22.s64;
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r18,3
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7) != 0);
	ctx.r10.s64 = r18.s32 >> 3;
	// stb r24,1(r29)
	PPC_STORE_U8(r29.u32 + 1, r24.u8);
	// lhz r24,4(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// lbz r22,2(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r5,r22,r5
	ctx.r5.s64 = ctx.r5.s64 - r22.s64;
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r17,3
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x7) != 0);
	ctx.r10.s64 = r17.s32 >> 3;
	// stb r24,2(r29)
	PPC_STORE_U8(r29.u32 + 2, r24.u8);
	// lhz r24,6(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// lbz r22,3(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r4,r22,r4
	ctx.r4.s64 = ctx.r4.s64 - r22.s64;
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r16,3
	xer.ca = (r16.s32 < 0) & ((r16.u32 & 0x7) != 0);
	ctx.r10.s64 = r16.s32 >> 3;
	// stb r24,3(r29)
	PPC_STORE_U8(r29.u32 + 3, r24.u8);
	// lhz r24,8(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// lbz r22,4(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r15,3
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x7) != 0);
	ctx.r10.s64 = r15.s32 >> 3;
	// stb r24,4(r29)
	PPC_STORE_U8(r29.u32 + 4, r24.u8);
	// lhz r24,10(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// lbz r22,5(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r27,r22,r27
	r27.s64 = r27.s64 - r22.s64;
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r14,3
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0x7) != 0);
	ctx.r10.s64 = r14.s32 >> 3;
	// stb r24,5(r29)
	PPC_STORE_U8(r29.u32 + 5, r24.u8);
	// lhz r24,12(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// lbz r22,6(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// add r24,r24,r23
	r24.u64 = r24.u64 + r23.u64;
	// subf r26,r22,r26
	r26.s64 = r26.s64 - r22.s64;
	// lwz r22,80(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lbzx r24,r24,r10
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// srawi r10,r22,3
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7) != 0);
	ctx.r10.s64 = r22.s32 >> 3;
	// stb r24,6(r29)
	PPC_STORE_U8(r29.u32 + 6, r24.u8);
	// lbz r11,7(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lhz r24,14(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// subf r25,r11,r25
	r25.s64 = r25.s64 - r11.s64;
	// extsh r11,r24
	r11.s64 = r24.s16;
	// lwz r24,0(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpwi cr6,r9,8
	cr6.compare<int32_t>(ctx.r9.s32, 8, xer);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622668
	if (cr6.lt) goto loc_82622668;
	// lwz r19,300(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// b 0x826230dc
	goto loc_826230DC;
loc_826227E4:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r10,8
	ctx.r10.s64 = 8;
	// addi r11,r11,30376
	r11.s64 = r11.s64 + 30376;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
loc_826227F8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lbz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lbz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,10(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lbz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lbz r9,7(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,14(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,7(r29)
	PPC_STORE_U8(r29.u32 + 7, ctx.r9.u8);
	// addi r9,r29,8
	ctx.r9.s64 = r29.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// add r29,r28,r9
	r29.u64 = r28.u64 + ctx.r9.u64;
	// addi r11,r11,-6
	r11.s64 = r11.s64 + -6;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x826227f8
	if (cr6.gt) goto loc_826227F8;
	// b 0x826230dc
	goto loc_826230DC;
loc_8262295C:
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
loc_82622960:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lhz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lbz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lbz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lhz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lhz r8,10(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lbz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lhz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lbz r9,7(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lbz r11,8(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r8
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622960
	if (cr6.lt) goto loc_82622960;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622A64:
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
loc_82622A68:
	// lhz r11,0(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lbz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lhz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lbz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lhz r8,10(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lhz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lbz r9,6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lbz r11,7(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r8
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622a68
	if (cr6.lt) goto loc_82622A68;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622B70:
	// li r7,1
	ctx.r7.s64 = 1;
loc_82622B74:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bgt cr6,0x82622bc4
	if (cr6.gt) goto loc_82622BC4;
	// neg r10,r7
	ctx.r10.s64 = -ctx.r7.s64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// subfic r10,r10,7
	xer.ca = ctx.r10.u32 <= 7;
	ctx.r10.s64 = 7 - ctx.r10.s64;
loc_82622B90:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// ble cr6,0x82622b90
	if (!cr6.gt) goto loc_82622B90;
loc_82622BC4:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82622c04
	if (!cr6.gt) goto loc_82622C04;
loc_82622BD0:
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// bgt cr6,0x82622bd0
	if (cr6.gt) goto loc_82622BD0;
loc_82622C04:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r29,r28,r29
	r29.u64 = r28.u64 + r29.u64;
	// cmpwi cr6,r7,-7
	cr6.compare<int32_t>(ctx.r7.s32, -7, xer);
	// bgt cr6,0x82622b74
	if (cr6.gt) goto loc_82622B74;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622C18:
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
loc_82622C1C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// ble cr6,0x82622c88
	if (!cr6.gt) goto loc_82622C88;
loc_82622C30:
	// lhz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r8,r30,2
	ctx.r8.s64 = r30.s64 + 2;
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r30,r8,2
	r30.s64 = ctx.r8.s64 + 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r7.u8);
	// addi r7,r29,1
	ctx.r7.s64 = r29.s64 + 1;
	// lhz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r29,r7,1
	r29.s64 = ctx.r7.s64 + 1;
	// extsh r8,r4
	ctx.r8.s64 = ctx.r4.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lbzx r8,r8,r5
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// stb r8,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r8.u8);
	// bgt cr6,0x82622c30
	if (cr6.gt) goto loc_82622C30;
loc_82622C88:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// subfic r11,r11,7
	xer.ca = r11.u32 <= 7;
	r11.s64 = 7 - r11.s64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// addi r9,r29,1
	ctx.r9.s64 = r29.s64 + 1;
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// ble cr6,0x82622d14
	if (!cr6.gt) goto loc_82622D14;
loc_82622CD0:
	// lbz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lhz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r4,r7,r6
	ctx.r4.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// addi r7,r4,1
	ctx.r7.s64 = ctx.r4.s64 + 1;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r3.u32);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bgt cr6,0x82622cd0
	if (cr6.gt) goto loc_82622CD0;
loc_82622D14:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r29,r28,r9
	r29.u64 = r28.u64 + ctx.r9.u64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x82622c1c
	if (cr6.lt) goto loc_82622C1C;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// bge cr6,0x826230dc
	if (!cr6.lt) goto loc_826230DC;
loc_82622D2C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r9,r30,2
	ctx.r9.s64 = r30.s64 + 2;
	// lhz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbzx r8,r8,r6
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r6.u32);
	// stb r8,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r8.u8);
	// addi r8,r29,1
	ctx.r8.s64 = r29.s64 + 1;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r30,r9,2
	r30.s64 = ctx.r9.s64 + 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lbzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r5.u32);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lbz r11,3(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r7
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// addi r11,r8,1
	r11.s64 = ctx.r8.s64 + 1;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622d2c
	if (cr6.lt) goto loc_82622D2C;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622E68:
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
loc_82622E6C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lhz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lbz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lhz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lhz r8,10(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lbz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lhz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r7
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// lbz r11,6(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lbzx r11,r11,r8
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622e6c
	if (cr6.lt) goto loc_82622E6C;
	// b 0x826230dc
	goto loc_826230DC;
loc_82622F70:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// addi r7,r11,30376
	ctx.r7.s64 = r11.s64 + 30376;
loc_82622F7C:
	// lhz r11,0(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 0);
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r7,2
	r11.s64 = ctx.r7.s64 + 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r5,-2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// lbzx r9,r5,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r9.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stb r9,0(r29)
	PPC_STORE_U8(r29.u32 + 0, ctx.r9.u8);
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 2);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,1(r29)
	PPC_STORE_U8(r29.u32 + 1, ctx.r9.u8);
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,2(r29)
	PPC_STORE_U8(r29.u32 + 2, ctx.r9.u8);
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 6);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,3(r29)
	PPC_STORE_U8(r29.u32 + 3, ctx.r9.u8);
	// lbz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 8);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,4(r29)
	PPC_STORE_U8(r29.u32 + 4, ctx.r9.u8);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,10(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 10);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,5(r29)
	PPC_STORE_U8(r29.u32 + 5, ctx.r9.u8);
	// lbz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// lhz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// stb r9,6(r29)
	PPC_STORE_U8(r29.u32 + 6, ctx.r9.u8);
	// lbz r11,5(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lbzx r11,r11,r8
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lbzx r11,r11,r9
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// stb r11,7(r29)
	PPC_STORE_U8(r29.u32 + 7, r11.u8);
	// addi r11,r29,8
	r11.s64 = r29.s64 + 8;
	// add r29,r28,r11
	r29.u64 = r28.u64 + r11.u64;
	// blt cr6,0x82622f7c
	if (cr6.lt) goto loc_82622F7C;
loc_826230DC:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x826231dc
	if (cr6.eq) goto loc_826231DC;
loc_826230E4:
	// addi r11,r20,4
	r11.s64 = r20.s64 + 4;
	// stw r21,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r21.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r21,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r21.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_826231DC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826231E4"))) PPC_WEAK_FUNC(sub_826231E4);
PPC_FUNC_IMPL(__imp__sub_826231E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826231E8"))) PPC_WEAK_FUNC(sub_826231E8);
PPC_FUNC_IMPL(__imp__sub_826231E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r17,348(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// lwz r23,380(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// cntlzw r11,r17
	r11.u64 = r17.u32 == 0 ? 32 : __builtin_clz(r17.u32);
	// mr r25,r10
	r25.u64 = ctx.r10.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// li r30,0
	r30.s64 = 0;
	// stw r27,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r27.u32);
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r26,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r26.u32);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// stw r25,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r25.u32);
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// beq cr6,0x82623264
	if (cr6.eq) goto loc_82623264;
	// srawi r11,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r11.s64 = r26.s32 >> 1;
	// srawi r10,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r10.s64 = r25.s32 >> 1;
	// rotlwi r25,r10,0
	r25.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r11.u32);
	// li r11,7
	r11.s64 = 7;
	// stw r10,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r10.u32);
	// lwz r26,324(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
loc_82623264:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r22,388(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// lwzx r3,r11,r21
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r21.u32);
	// bl 0x8265b568
	sub_8265B568(ctx, base);
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826233d0
	if (!cr6.eq) goto loc_826233D0;
	// not r11,r3
	r11.u64 = ~ctx.r3.u64;
	// srawi r18,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	r18.s64 = ctx.r3.s32 >> 1;
	// clrlwi r14,r11,31
	r14.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// beq cr6,0x826233dc
	if (cr6.eq) goto loc_826233DC;
	// lwz r26,1960(r19)
	r26.u64 = PPC_LOAD_U32(r19.u32 + 1960);
	// li r31,1
	r31.s64 = 1;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// li r15,1
	r15.s64 = 1;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x826232c0
	if (cr6.eq) goto loc_826232C0;
	// li r23,1
	r23.s64 = 1;
	// li r25,1
	r25.s64 = 1;
	// b 0x826232ec
	goto loc_826232EC;
loc_826232C0:
	// lwz r11,356(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// ble cr6,0x826232d8
	if (!cr6.gt) goto loc_826232D8;
	// mr r23,r30
	r23.u64 = r30.u64;
	// mr r17,r30
	r17.u64 = r30.u64;
	// b 0x826232dc
	goto loc_826232DC;
loc_826232D8:
	// li r23,3
	r23.s64 = 3;
loc_826232DC:
	// li r25,2
	r25.s64 = 2;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bge cr6,0x826232ec
	if (!cr6.lt) goto loc_826232EC;
	// mr r26,r30
	r26.u64 = r30.u64;
loc_826232EC:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r9,364(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// lwz r30,316(r19)
	r30.u64 = PPC_LOAD_U32(r19.u32 + 316);
	// addi r11,r11,-17376
	r11.s64 = r11.s64 + -17376;
	// lwz r29,312(r19)
	r29.u64 = PPC_LOAD_U32(r19.u32 + 312);
	// addi r10,r10,-24992
	ctx.r10.s64 = ctx.r10.s64 + -24992;
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// lwzx r28,r11,r10
	r28.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r27,r11,14432
	r27.s64 = r11.s64 + 14432;
loc_8262331C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r17
	cr6.compare<int32_t>(r11.s32, r17.s32, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt cr6,0x82623334
	if (cr6.lt) goto loc_82623334;
	// mr r25,r23
	r25.u64 = r23.u64;
loc_82623334:
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,92
	ctx.r6.s64 = ctx.r1.s64 + 92;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// lwzx r3,r11,r21
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r21.u32);
	// bl 0x8265ae40
	sub_8265AE40(ctx, base);
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826233d0
	if (!cr6.eq) goto loc_826233D0;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mullw r8,r10,r29
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// clrlwi r9,r11,26
	ctx.r9.u64 = r11.u32 & 0x3F;
	// srawi r7,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	ctx.r7.s64 = r11.s32 >> 6;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// srawi r10,r10,15
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 15;
	// and r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 & r26.u64;
	// lbzx r11,r9,r28
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r28.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// or r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 | ctx.r9.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lhzx r9,r7,r27
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r27.u32);
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// xor r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 ^ r30.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// sthx r10,r11,r24
	PPC_STORE_U16(r11.u32 + r24.u32, ctx.r10.u16);
	// beq cr6,0x8262331c
	if (cr6.eq) goto loc_8262331C;
	// cmpwi cr6,r31,64
	cr6.compare<int32_t>(r31.s32, 64, xer);
	// ble cr6,0x82623548
	if (!cr6.gt) goto loc_82623548;
loc_826233D0:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_826233DC:
	// lwz r11,1964(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 1964);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826234c0
	if (cr6.eq) goto loc_826234C0;
	// addi r10,r18,1
	ctx.r10.s64 = r18.s64 + 1;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bge cr6,0x826234c0
	if (!cr6.lt) goto loc_826234C0;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r10,372(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-32768
	r11.s64 = r11.s64 + -32768;
	// srawi r11,r11,16
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFF) != 0);
	r11.s64 = r11.s32 >> 16;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82623434
	if (!cr6.lt) goto loc_82623434;
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x82623440
	goto loc_82623440;
loc_82623434:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82623440
	if (!cr6.gt) goto loc_82623440;
	// li r11,255
	r11.s64 = 255;
loc_82623440:
	// lis r10,257
	ctx.r10.s64 = 16842752;
	// lwz r31,340(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// srawi r9,r27,2
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x3) != 0);
	ctx.r9.s64 = r27.s32 >> 2;
	// lwz r28,364(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// ori r10,r10,257
	ctx.r10.u64 = ctx.r10.u64 | 257;
	// li r15,1
	r15.s64 = 1;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x826234f0
	goto loc_826234F0;
loc_826234C0:
	// mr r15,r18
	r15.u64 = r18.u64;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x82623558
	if (!cr6.eq) goto loc_82623558;
	// lwz r28,364(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
loc_826234D0:
	// lwz r31,340(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r3,1964(r19)
	ctx.r3.u64 = PPC_LOAD_U32(r19.u32 + 1964);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82621e18
	sub_82621E18(ctx, base);
loc_826234F0:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82623510
	if (!cr6.eq) goto loc_82623510;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r3,1968(r19)
	ctx.r3.u64 = PPC_LOAD_U32(r19.u32 + 1968);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8265a700
	sub_8265A700(ctx, base);
loc_82623510:
	// lwz r11,3892(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262353c
	if (cr6.eq) goto loc_8262353C;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// lwz r5,248(r19)
	ctx.r5.u64 = PPC_LOAD_U32(r19.u32 + 248);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8265a3b0
	sub_8265A3B0(ctx, base);
loc_8262353C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_82623548:
	// lwz r26,324(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r25,332(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// lwz r23,380(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r27,308(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
loc_82623558:
	// mullw r11,r18,r16
	r11.s64 = int64_t(r18.s32) * int64_t(r16.s32);
	// lwz r28,364(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// or r9,r26,r25
	ctx.r9.u64 = r26.u64 | r25.u64;
	// sth r11,0(r24)
	PPC_STORE_U16(r24.u32 + 0, r11.u16);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82623580
	if (cr6.eq) goto loc_82623580;
	// addi r11,r18,1
	r11.s64 = r18.s64 + 1;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82623584
	if (!cr6.lt) goto loc_82623584;
loc_82623580:
	// li r10,-1
	ctx.r10.s64 = -1;
loc_82623584:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r3,1768(r19)
	ctx.r3.u64 = PPC_LOAD_U32(r19.u32 + 1768);
	// lwz r29,1964(r19)
	r29.u64 = PPC_LOAD_U32(r19.u32 + 1964);
	// addi r11,r11,14560
	r11.s64 = r11.s64 + 14560;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r11,r10,r11
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// extsb r30,r11
	r30.s64 = r11.s8;
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// extsh r31,r11
	r31.s64 = r11.s16;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826235c4
	if (cr6.eq) goto loc_826235C4;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826235c4
	if (cr6.lt) goto loc_826235C4;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82620f68
	sub_82620F68(ctx, base);
loc_826235C4:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bne cr6,0x82623608
	if (!cr6.eq) goto loc_82623608;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826235dc
	if (cr6.eq) goto loc_826235DC;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x82623608
	if (!cr6.lt) goto loc_82623608;
loc_826235DC:
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// li r10,32
	ctx.r10.s64 = 32;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r9,r11,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826235F8:
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bdnz 0x826235f8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826235F8;
	// b 0x826234d0
	goto loc_826234D0;
loc_82623608:
	// lwz r11,52(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 52);
	// li r6,255
	ctx.r6.s64 = 255;
	// li r5,8
	ctx.r5.s64 = 8;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x826234d0
	goto loc_826234D0;
}

__attribute__((alias("__imp__sub_82623624"))) PPC_WEAK_FUNC(sub_82623624);
PPC_FUNC_IMPL(__imp__sub_82623624) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82623628"))) PPC_WEAK_FUNC(sub_82623628);
PPC_FUNC_IMPL(__imp__sub_82623628) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// li r20,1
	r20.s64 = 1;
	// mr r28,r25
	r28.u64 = r25.u64;
	// mr r29,r20
	r29.u64 = r20.u64;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r10,3728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// add r14,r9,r11
	r14.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r25,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r25.u32);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// add r15,r8,r7
	r15.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826236dc
	if (!cr6.lt) goto loc_826236DC;
loc_82623684:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826236dc
	if (cr6.eq) goto loc_826236DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r9,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x826236cc
	if (!cr0.lt) goto loc_826236CC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826236CC:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x82623684
	if (cr6.gt) goto loc_82623684;
loc_826236DC:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x82623718
	if (!cr0.lt) goto loc_82623718;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82623718:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// rlwinm r11,r29,6,0,25
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 6) & 0xFFFFFFC0;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// stw r11,1960(r31)
	PPC_STORE_U32(r31.u32 + 1960, r11.u32);
	// blt cr6,0x82623740
	if (cr6.lt) goto loc_82623740;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// b 0x82623760
	goto loc_82623760;
loc_82623740:
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r9,312(r31)
	PPC_STORE_U32(r31.u32 + 312, ctx.r9.u32);
	// stw r10,320(r31)
	PPC_STORE_U32(r31.u32 + 320, ctx.r10.u32);
	// stw r11,316(r31)
	PPC_STORE_U32(r31.u32 + 316, r11.u32);
loc_82623760:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b9d0
	sub_8265B9D0(ctx, base);
	// lwz r11,312(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 312);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// stw r11,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r11.u32);
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// ble cr6,0x82623798
	if (!cr6.gt) goto loc_82623798;
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,300(r31)
	PPC_STORE_U32(r31.u32 + 300, ctx.r10.u32);
loc_82623798:
	// srawi r8,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r8.s64 = r11.s32 >> 1;
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lwz r3,1768(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// twllei r11,0
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// rotlwi r9,r8,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// divw r16,r8,r11
	r16.s32 = ctx.r8.s32 / r11.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// li r5,256
	ctx.r5.s64 = 256;
	// andc r11,r11,r9
	r11.u64 = r11.u64 & ~ctx.r9.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// twlgei r11,-1
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// twllei r10,0
	// add r9,r11,r7
	ctx.r9.u64 = r11.u64 + ctx.r7.u64;
	// rotlwi r11,r9,1
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// divw r17,r9,r10
	r17.s32 = ctx.r9.s32 / ctx.r10.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r11,-25076(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -25076);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r11,r11,-24992
	r11.s64 = r11.s64 + -24992;
	// beq cr6,0x82623824
	if (cr6.eq) goto loc_82623824;
	// lwz r10,1824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1824);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,1816(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r10,1820(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// b 0x82623838
	goto loc_82623838;
loc_82623824:
	// lwz r10,1828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1828);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,1804(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1804);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r10,1808(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1808);
loc_82623838:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r10,1788(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1788);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82623860
	if (cr6.eq) goto loc_82623860;
	// lwz r10,1828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1828);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,1804(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1804);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r10,1808(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1808);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_82623860:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r29,r25
	r29.u64 = r25.u64;
	// lwz r10,1964(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r11,1972(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r27,84(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r24,1768(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// addi r26,r11,12
	r26.s64 = r11.s64 + 12;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82623b14
	if (!cr6.gt) goto loc_82623B14;
	// li r18,255
	r18.s64 = 255;
loc_82623894:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// srawi r23,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r23.s64 = r29.s32 >> 1;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r30,r25
	r30.u64 = r25.u64;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r29,r10
	r11.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r21,r11,r15
	r21.u64 = r11.u64 + r15.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// mullw r11,r9,r23
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r23.s32);
	// add r22,r11,r14
	r22.u64 = r11.u64 + r14.u64;
	// add r19,r11,r10
	r19.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x82623b00
	if (!cr6.gt) goto loc_82623B00;
loc_826238D4:
	// addi r8,r1,168
	ctx.r8.s64 = ctx.r1.s64 + 168;
	// lwz r6,248(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// addi r7,r1,152
	ctx.r7.s64 = ctx.r1.s64 + 152;
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1968);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8265a520
	sub_8265A520(ctx, base);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// addi r9,r1,152
	ctx.r9.s64 = ctx.r1.s64 + 152;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// bl 0x826211d0
	sub_826211D0(ctx, base);
	// lwz r6,152(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82623960
	if (cr6.eq) goto loc_82623960;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// lwz r3,16(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8265b3b8
	sub_8265B3B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82623b5c
	if (!cr6.eq) goto loc_82623B5C;
	// lwz r9,148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r10,r9
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
loc_82623960:
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r8,296(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r4,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r4.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// stw r16,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r16.u32);
	// stw r25,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r25.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// bl 0x826231e8
	sub_826231E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82623b38
	if (!cr6.eq) goto loc_82623B38;
	// and r11,r30,r29
	r11.u64 = r30.u64 & r29.u64;
	// addi r21,r21,8
	r21.s64 = r21.s64 + 8;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82623aec
	if (cr6.eq) goto loc_82623AEC;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1968);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8265a730
	sub_8265A730(ctx, base);
	// lwz r6,300(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// srawi r28,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r28.s64 = r30.s32 >> 1;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// stw r11,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r11.u32);
	// bl 0x826211d0
	sub_826211D0(ctx, base);
	// lwz r9,156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r8,300(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r17,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r17.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r20,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r20.u32);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r18,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r18.u32);
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// bl 0x826231e8
	sub_826231E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82623b38
	if (!cr6.eq) goto loc_82623B38;
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1964);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x826211d0
	sub_826211D0(ctx, base);
	// lwz r9,160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r8,300(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r17,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r17.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r20,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r20.u32);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r18,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r18.u32);
	// stw r19,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r19.u32);
	// bl 0x826231e8
	sub_826231E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82623b38
	if (!cr6.eq) goto loc_82623B38;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// addi r19,r19,8
	r19.s64 = r19.s64 + 8;
loc_82623AEC:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x826238d4
	if (cr6.lt) goto loc_826238D4;
loc_82623B00:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x82623894
	if (cr6.lt) goto loc_82623894;
loc_82623B14:
	// lwz r11,15508(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15508);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82623b50
	if (cr6.eq) goto loc_82623B50;
	// stw r25,15528(r31)
	PPC_STORE_U32(r31.u32 + 15528, r25.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r25.u32);
	// stw r25,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r25.u32);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
loc_82623B38:
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82623b5c
	if (!cr6.eq) goto loc_82623B5C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
loc_82623B50:
	// stw r20,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r20.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r25.u32);
loc_82623B5C:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82623B64"))) PPC_WEAK_FUNC(sub_82623B64);
PPC_FUNC_IMPL(__imp__sub_82623B64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82623B68"))) PPC_WEAK_FUNC(sub_82623B68);
PPC_FUNC_IMPL(__imp__sub_82623B68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// vspltisb v9,-1
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vspltish v11,1
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// vspltisb v5,15
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_set1_epi8(char(0xF)));
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// vspltish v6,3
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// vslb v7,v9,v9
	ctx.v7.u8[0] = ctx.v9.u8[0] << (ctx.v9.u8[0] & 0x7);
	ctx.v7.u8[1] = ctx.v9.u8[1] << (ctx.v9.u8[1] & 0x7);
	ctx.v7.u8[2] = ctx.v9.u8[2] << (ctx.v9.u8[2] & 0x7);
	ctx.v7.u8[3] = ctx.v9.u8[3] << (ctx.v9.u8[3] & 0x7);
	ctx.v7.u8[4] = ctx.v9.u8[4] << (ctx.v9.u8[4] & 0x7);
	ctx.v7.u8[5] = ctx.v9.u8[5] << (ctx.v9.u8[5] & 0x7);
	ctx.v7.u8[6] = ctx.v9.u8[6] << (ctx.v9.u8[6] & 0x7);
	ctx.v7.u8[7] = ctx.v9.u8[7] << (ctx.v9.u8[7] & 0x7);
	ctx.v7.u8[8] = ctx.v9.u8[8] << (ctx.v9.u8[8] & 0x7);
	ctx.v7.u8[9] = ctx.v9.u8[9] << (ctx.v9.u8[9] & 0x7);
	ctx.v7.u8[10] = ctx.v9.u8[10] << (ctx.v9.u8[10] & 0x7);
	ctx.v7.u8[11] = ctx.v9.u8[11] << (ctx.v9.u8[11] & 0x7);
	ctx.v7.u8[12] = ctx.v9.u8[12] << (ctx.v9.u8[12] & 0x7);
	ctx.v7.u8[13] = ctx.v9.u8[13] << (ctx.v9.u8[13] & 0x7);
	ctx.v7.u8[14] = ctx.v9.u8[14] << (ctx.v9.u8[14] & 0x7);
	ctx.v7.u8[15] = ctx.v9.u8[15] << (ctx.v9.u8[15] & 0x7);
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r9,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, ctx.r9.u32);
	// vspltish v12,2
	// vspltish v13,4
	// vor v22,v6,v6
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vavgsh v20,v9,v7
	_mm_store_si128((__m128i*)v20.u8, _mm_avg_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// vspltish v10,5
	// vslb v21,v5,v5
	v21.u8[0] = ctx.v5.u8[0] << (ctx.v5.u8[0] & 0x7);
	v21.u8[1] = ctx.v5.u8[1] << (ctx.v5.u8[1] & 0x7);
	v21.u8[2] = ctx.v5.u8[2] << (ctx.v5.u8[2] & 0x7);
	v21.u8[3] = ctx.v5.u8[3] << (ctx.v5.u8[3] & 0x7);
	v21.u8[4] = ctx.v5.u8[4] << (ctx.v5.u8[4] & 0x7);
	v21.u8[5] = ctx.v5.u8[5] << (ctx.v5.u8[5] & 0x7);
	v21.u8[6] = ctx.v5.u8[6] << (ctx.v5.u8[6] & 0x7);
	v21.u8[7] = ctx.v5.u8[7] << (ctx.v5.u8[7] & 0x7);
	v21.u8[8] = ctx.v5.u8[8] << (ctx.v5.u8[8] & 0x7);
	v21.u8[9] = ctx.v5.u8[9] << (ctx.v5.u8[9] & 0x7);
	v21.u8[10] = ctx.v5.u8[10] << (ctx.v5.u8[10] & 0x7);
	v21.u8[11] = ctx.v5.u8[11] << (ctx.v5.u8[11] & 0x7);
	v21.u8[12] = ctx.v5.u8[12] << (ctx.v5.u8[12] & 0x7);
	v21.u8[13] = ctx.v5.u8[13] << (ctx.v5.u8[13] & 0x7);
	v21.u8[14] = ctx.v5.u8[14] << (ctx.v5.u8[14] & 0x7);
	v21.u8[15] = ctx.v5.u8[15] << (ctx.v5.u8[15] & 0x7);
	// vspltish v8,6
	// vsubuhm v19,v20,v11
	_mm_store_si128((__m128i*)v19.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// bne cr6,0x82624044
	if (!cr6.eq) goto loc_82624044;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82623c3c
	if (!cr6.eq) goto loc_82623C3C;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// b 0x82623bec
	goto loc_82623BEC;
loc_82623BE0:
	// lwz r5,36(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r3,20(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
loc_82623BEC:
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// mullw r10,r11,r4
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lvlx v13,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// lvrx v0,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v0,r5,r11
	ea = ctx.r5.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stvrx v0,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// blt cr6,0x82623be0
	if (cr6.lt) goto loc_82623BE0;
	// b 0x8239bd40
	return;
loc_82623C3C:
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// beq cr6,0x82623ee4
	if (cr6.eq) goto loc_82623EE4;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// beq cr6,0x82623db4
	if (cr6.eq) goto loc_82623DB4;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// bne cr6,0x82624ab8
	if (!cr6.eq) goto loc_82624AB8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82623c68
	if (cr6.eq) goto loc_82623C68;
	// vspltish v9,8
	// vslh v30,v9,v12
	// b 0x82623c70
	goto loc_82623C70;
loc_82623C68:
	// vspltish v9,-5
	// vsrh v30,v9,v9
loc_82623C70:
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r1,-384
	ctx.r9.s64 = ctx.r1.s64 + -384;
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r11,0
	r11.s64 = 0;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82623cf8
	goto loc_82623CF8;
loc_82623CF0:
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
loc_82623CF8:
	// li r7,16
	ctx.r7.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,-384
	ctx.r9.s64 = ctx.r1.s64 + -384;
	// addi r8,r1,-400
	ctx.r8.s64 = ctx.r1.s64 + -400;
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvrx v7,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r3,r1,-368
	ctx.r3.s64 = ctx.r1.s64 + -368;
	// vor v5,v9,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r11,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v9,v12
	// lvx128 v7,r11,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v3,v9,v13
	// lvx128 v6,r11,r6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v1,v9,v10
	// vslh v2,v7,v13
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vadduhm v9,v4,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v7,v7,v11
	// vadduhm v31,v6,v5
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v6,v2,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v5,r11,r3
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v5,v3,v30
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vslh v4,v31,v12
	// li r11,4
	r11.s64 = 4;
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v9,v9,v8
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvewx v9,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v9,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82623cf0
	if (cr6.lt) goto loc_82623CF0;
	// b 0x8239bd40
	return;
loc_82623DB4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vspltish v8,8
	// bne cr6,0x82623dc4
	if (!cr6.eq) goto loc_82623DC4;
	// vspltish v8,7
loc_82623DC4:
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v12,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvrx v12,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r1,-384
	ctx.r9.s64 = ctx.r1.s64 + -384;
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r11,0
	r11.s64 = 0;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82623e4c
	goto loc_82623E4C;
loc_82623E44:
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
loc_82623E4C:
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r1,-384
	ctx.r8.s64 = ctx.r1.s64 + -384;
	// addi r7,r1,-400
	ctx.r7.s64 = ctx.r1.s64 + -400;
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvrx v11,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r1,-368
	ctx.r9.s64 = ctx.r1.s64 + -368;
	// vor v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v12,r11,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r11,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v9,r11,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// li r10,4
	ctx.r10.s64 = 4;
	// vslh v10,v12,v22
	// vadduhm v9,v9,v11
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx128 v11,r11,r9
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vsubuhm v11,v8,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsrah v12,v12,v13
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvewx v12,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v12,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82623e44
	if (cr6.lt) goto loc_82623E44;
	// b 0x8239bd40
	return;
loc_82623EE4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82623ef8
	if (cr6.eq) goto loc_82623EF8;
	// vspltish v9,8
	// vslh v31,v9,v12
	// b 0x82623f00
	goto loc_82623F00;
loc_82623EF8:
	// vspltish v9,-5
	// vsrh v31,v9,v9
loc_82623F00:
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v9,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r1,-384
	ctx.r9.s64 = ctx.r1.s64 + -384;
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r11,0
	r11.s64 = 0;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stvx v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82623f88
	goto loc_82623F88;
loc_82623F80:
	// lwz r4,28(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
loc_82623F88:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r9,r1,-400
	ctx.r9.s64 = ctx.r1.s64 + -400;
	// addi r7,r1,-384
	ctx.r7.s64 = ctx.r1.s64 + -384;
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvrx v7,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r3,r1,-368
	ctx.r3.s64 = ctx.r1.s64 + -368;
	// vor v7,v9,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v9,r11,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v5,v9,v12
	// lvx128 v6,r11,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v9,v13
	// lvx128 v1,r11,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v2,v9,v10
	// vslh v3,v6,v13
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v5,v4,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v1,v1,v7
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v6,v6,v11
	// vadduhm v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v7,r11,r3
	_mm_store_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v7,v3,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v4,v1,v12
	// li r11,4
	r11.s64 = 4;
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubuhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v9,v9,v8
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvewx v9,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v9,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82623f80
	if (cr6.lt) goto loc_82623F80;
	// b 0x8239bd40
	return;
loc_82624044:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82624340
	if (!cr6.eq) goto loc_82624340;
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// beq cr6,0x8262423c
	if (cr6.eq) goto loc_8262423C;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// beq cr6,0x82624168
	if (cr6.eq) goto loc_82624168;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// bne cr6,0x82624ab8
	if (!cr6.eq) goto loc_82624AB8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82624078
	if (cr6.eq) goto loc_82624078;
	// vspltish v9,-5
	// vsrh v31,v9,v9
	// b 0x82624080
	goto loc_82624080;
loc_82624078:
	// vspltish v9,8
	// vslh v31,v9,v12
loc_82624080:
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
loc_82624090:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-416
	r11.s64 = ctx.r1.s64 + -416;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v7,v0,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v6,v9,v0,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v9,v0,1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v4,v9,v0,3
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 13));
	// vmrghb v9,v0,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vmrghb v2,v0,v4
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v5,v9,v12
	// vslh v3,v9,v13
	// vslh v1,v9,v10
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-400
	r11.s64 = ctx.r1.s64 + -400;
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v4,v6,v13
	// vadduhm v5,v3,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v31.u16)));
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-368
	r11.s64 = ctx.r1.s64 + -368;
	// vadduhm v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v6,v6,v11
	// stvx v2,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v7,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v7,v4,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// li r11,4
	r11.s64 = 4;
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v6,v2,v12
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubuhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsrah v9,v9,v8
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvewx v9,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v9,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624090
	if (cr6.lt) goto loc_82624090;
	// b 0x8239bd40
	return;
loc_82624168:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vspltish v7,7
	// bne cr6,0x82624178
	if (!cr6.eq) goto loc_82624178;
	// vspltish v7,8
loc_82624178:
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
loc_82624188:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,4
	ctx.r9.s64 = 4;
	// lvrx v11,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-416
	r11.s64 = ctx.r1.s64 + -416;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vmrghb v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v11,v12,v0,1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v10,v12,v0,2
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v9,v12,v0,3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 13));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-400
	r11.s64 = ctx.r1.s64 + -400;
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-368
	r11.s64 = ctx.r1.s64 + -368;
	// vadduhm v11,v8,v10
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v10,v12,v22
	// vsubuhm v11,v7,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vadduhm v12,v10,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsrah v12,v12,v13
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// stvewx v12,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v12,r11,r9
	ea = (r11.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624188
	if (cr6.lt) goto loc_82624188;
	// b 0x8239bd40
	return;
loc_8262423C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82624250
	if (cr6.eq) goto loc_82624250;
	// vspltish v9,-5
	// vsrh v31,v9,v9
	// b 0x82624258
	goto loc_82624258;
loc_82624250:
	// vspltish v9,8
	// vslh v31,v9,v12
loc_82624258:
	// addi r11,r3,-1
	r11.s64 = ctx.r3.s64 + -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
loc_82624268:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-416
	r11.s64 = ctx.r1.s64 + -416;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsldoi v7,v9,v0,1
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// vsldoi v6,v9,v0,2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 14));
	// vsldoi v5,v9,v0,3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 13));
	// vmrghb v9,v0,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-400
	r11.s64 = ctx.r1.s64 + -400;
	// vmrghb v6,v0,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v5,v9,v12
	// vslh v2,v9,v13
	// vslh v1,v9,v10
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v3,v7,v13
	// vadduhm v4,v4,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v5,v2,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v31.u16)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-368
	r11.s64 = ctx.r1.s64 + -368;
	// vadduhm v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v7,v7,v11
	// stvx v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v6,v3,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v9,v5,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// li r11,4
	r11.s64 = 4;
	// vadduhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v6,v4,v12
	// vadduhm v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubuhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsrah v9,v9,v8
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvewx v9,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v9,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r8,36(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,44(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// cmpwi cr6,r10,8
	cr6.compare<int32_t>(ctx.r10.s32, 8, xer);
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624268
	if (cr6.lt) goto loc_82624268;
	// b 0x8239bd40
	return;
loc_82624340:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// beq cr6,0x82624390
	if (cr6.eq) goto loc_82624390;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624374
	if (cr6.eq) goto loc_82624374;
	// clrlwi r10,r8,31
	ctx.r10.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8262436c
	if (cr6.eq) goto loc_8262436C;
	// vspltish v9,8
	// vadduhm v7,v9,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// b 0x826243c8
	goto loc_826243C8;
loc_8262436C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82624388
	if (!cr6.eq) goto loc_82624388;
loc_82624374:
	// clrlwi r11,r8,31
	r11.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82624388
	if (!cr6.eq) goto loc_82624388;
	// vor v7,v11,v11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// b 0x826243c8
	goto loc_826243C8;
loc_82624388:
	// vor v7,v13,v13
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// b 0x826243c8
	goto loc_826243C8;
loc_82624390:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826243b4
	if (cr6.eq) goto loc_826243B4;
	// clrlwi r10,r8,31
	ctx.r10.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826243ac
	if (cr6.eq) goto loc_826243AC;
	// vspltish v7,15
	// b 0x826243c8
	goto loc_826243C8;
loc_826243AC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826243c4
	if (!cr6.eq) goto loc_826243C4;
loc_826243B4:
	// clrlwi r11,r8,31
	r11.u64 = ctx.r8.u32 & 0x1;
	// vspltish v7,0
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826243c8
	if (cr6.eq) goto loc_826243C8;
loc_826243C4:
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
loc_826243C8:
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// beq cr6,0x826246f0
	if (cr6.eq) goto loc_826246F0;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// beq cr6,0x8262458c
	if (cr6.eq) goto loc_8262458C;
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// bne cr6,0x82624898
	if (!cr6.eq) goto loc_82624898;
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-208
	r11.s64 = ctx.r1.s64 + -208;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82624484:
	// li r6,16
	ctx.r6.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r31,r1,-384
	r31.s64 = ctx.r1.s64 + -384;
	// addi r30,r1,-208
	r30.s64 = ctx.r1.s64 + -208;
	// addi r29,r1,-400
	r29.s64 = ctx.r1.s64 + -400;
	// lvrx v8,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r28,r1,-224
	r28.s64 = ctx.r1.s64 + -224;
	// vor v5,v9,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvx128 v9,r11,r31
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,-240
	ctx.r3.s64 = ctx.r1.s64 + -240;
	// lvx128 v8,r11,r30
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v30,v9,v12
	// vslh v29,v8,v12
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// lvx128 v4,r11,r29
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r11,r28
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v28,v9,v13
	// vslh v27,v8,v13
	// vmrghb v2,v0,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v24,v9,v10
	// vmrglb v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v23,v8,v10
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// lvx128 v31,r0,r3
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// addi r27,r1,-368
	r27.s64 = ctx.r1.s64 + -368;
	// vslh v26,v4,v13
	// addi r26,r1,-192
	r26.s64 = ctx.r1.s64 + -192;
	// vslh v25,v3,v13
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vadduhm v30,v28,v7
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vadduhm v29,v27,v7
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// vadduhm v9,v24,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v2,r11,r27
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v8,v23,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stvx128 v5,r11,r26
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v4,v11
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// vslh v3,v3,v11
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vadduhm v28,v26,v1
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v27,v25,v31
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v9,v30,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v29,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v2,v1,v2
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v5,v31,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v3,v3,v27
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v2,v2,v12
	// vslh v5,v5,v12
	// vadduhm v9,v9,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v8,v8,v3
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsubuhm v9,v9,v2
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vsubuhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsrah v9,v9,v6
	// vsrah v8,v8,v6
	// stvx v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624484
	if (cr6.lt) goto loc_82624484;
	// b 0x82624898
	goto loc_82624898;
loc_8262458C:
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-208
	r11.s64 = ctx.r1.s64 + -208;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82624630:
	// li r3,16
	ctx.r3.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r31,r1,-384
	r31.s64 = ctx.r1.s64 + -384;
	// addi r30,r1,-400
	r30.s64 = ctx.r1.s64 + -400;
	// addi r29,r1,-208
	r29.s64 = ctx.r1.s64 + -208;
	// lvrx v8,r10,r3
	temp.u32 = ctx.r10.u32 + ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r28,r1,-224
	r28.s64 = ctx.r1.s64 + -224;
	// vor v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvx128 v8,r11,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,-240
	ctx.r3.s64 = ctx.r1.s64 + -240;
	// lvx128 v6,r11,r30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// vadduhm v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvx128 v6,r11,r29
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r11,r28
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r26,r1,-192
	r26.s64 = ctx.r1.s64 + -192;
	// lvx128 v2,r0,r6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v4,v8,v22
	// addi r27,r1,-368
	r27.s64 = ctx.r1.s64 + -368;
	// vslh v3,v6,v22
	// lvx128 v1,r0,r3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v2,v2,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vadduhm v1,v1,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// stvx128 v9,r11,r26
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v4,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v8,v3,v6
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// stvx128 v5,r11,r27
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubuhm v6,v7,v2
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// vsubuhm v5,v7,v1
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vadduhm v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsrah v9,v9,v11
	// stvx v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v9,v8,v11
	// stvx v9,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624630
	if (cr6.lt) goto loc_82624630;
	// b 0x82624898
	goto loc_82624898;
loc_826246F0:
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r6,16
	ctx.r6.s64 = 16;
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// lvrx v9,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-240
	ctx.r10.s64 = ctx.r1.s64 + -240;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-400
	ctx.r10.s64 = ctx.r1.s64 + -400;
	// lvrx v9,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,0
	ctx.r8.s64 = 0;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-224
	ctx.r10.s64 = ctx.r1.s64 + -224;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v9,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r1,-384
	r11.s64 = ctx.r1.s64 + -384;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vmrghb v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-208
	r11.s64 = ctx.r1.s64 + -208;
	// stvx v9,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82624794:
	// li r31,16
	r31.s64 = 16;
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r6,r1,-400
	ctx.r6.s64 = ctx.r1.s64 + -400;
	// addi r3,r1,-224
	ctx.r3.s64 = ctx.r1.s64 + -224;
	// addi r30,r1,-384
	r30.s64 = ctx.r1.s64 + -384;
	// lvrx v8,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r29,r1,-208
	r29.s64 = ctx.r1.s64 + -208;
	// vor v4,v9,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r31,r1,-368
	r31.s64 = ctx.r1.s64 + -368;
	// lvx128 v9,r11,r6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lvx128 v8,r11,r3
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v1,v9,v12
	// vslh v31,v8,v12
	// lvx128 v3,r11,r30
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v30,v9,v13
	// lvx128 v2,r11,r29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v5,v0,v4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v29,v8,v13
	// vslh v26,v9,v10
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// vslh v25,v8,v10
	// addi r3,r1,-240
	ctx.r3.s64 = ctx.r1.s64 + -240;
	// vslh v28,v3,v13
	// vmrglb v4,v0,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vadduhm v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// addi r30,r1,-192
	r30.s64 = ctx.r1.s64 + -192;
	// vslh v27,v2,v13
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// vadduhm v1,v30,v7
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvx128 v5,r11,r31
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v31,v28,v5
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v28,r0,r6
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v30,v29,v7
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// vadduhm v9,v26,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// stvx128 v4,r11,r30
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v8,v25,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// stw r8,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r8.u32);
	// vadduhm v29,v27,v4
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// vslh v3,v3,v11
	// vslh v2,v2,v11
	// stw r10,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r10.u32);
	// vadduhm v5,v28,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v28,r0,r3
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v9,v1,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v8,v30,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v4,v28,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v3,v3,v31
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v2,v2,v29
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vslh v5,v5,v12
	// vslh v4,v4,v12
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vsubuhm v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vsubuhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v9,v9,v6
	// vsrah v8,v8,v6
	// stvx v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624794
	if (cr6.lt) goto loc_82624794;
loc_82624898:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vor v7,v19,v19
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v19.u8));
	// bne cr6,0x826248a8
	if (!cr6.eq) goto loc_826248A8;
	// vor v7,v20,v20
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v20.u8));
loc_826248A8:
	// vspltish v6,7
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// beq cr6,0x82624a04
	if (cr6.eq) goto loc_82624A04;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// beq cr6,0x82624980
	if (cr6.eq) goto loc_82624980;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// bne cr6,0x82624ab8
	if (!cr6.eq) goto loc_82624AB8;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
loc_826248CC:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// addi r9,r1,-240
	ctx.r9.s64 = ctx.r1.s64 + -240;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r11,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// vsldoi v8,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsldoi v5,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// vsldoi v4,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vsrah v9,v0,v12
	// vsrah v0,v8,v12
	// vsrah v8,v5,v12
	// vsrah v5,v4,v12
	// vor v31,v9,v9
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vslh v4,v0,v12
	// vslh v2,v0,v13
	// vslh v1,v0,v10
	// vslh v3,v8,v13
	// vadduhm v0,v4,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v8,v8,v11
	// vadduhm v5,v3,v31
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v0,v1,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v4,v2,v7
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v9,v9,v12
	// vadduhm v8,v8,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v0,v4,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsubuhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsrah v0,v0,v6
	// vpkshss v0,v0,v0
	// vaddubm v0,v0,v21
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x826248cc
	if (cr6.lt) goto loc_826248CC;
	// b 0x8239bd40
	return;
loc_82624980:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
loc_82624988:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// addi r9,r1,-240
	ctx.r9.s64 = ctx.r1.s64 + -240;
	// li r8,4
	ctx.r8.s64 = 4;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r11,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsldoi v12,v0,v13,2
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 14));
	// vsldoi v11,v0,v13,4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 12));
	// vsldoi v10,v0,v13,6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 10));
	// vadduhm v13,v12,v11
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v0,v0,v10
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v12,v13,v22
	// vsubuhm v0,v7,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vadduhm v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsrah v0,v0,v6
	// vpkshss v0,v0,v0
	// vaddubm v0,v0,v21
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624988
	if (cr6.lt) goto loc_82624988;
	// b 0x8239bd40
	return;
loc_82624A04:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
loc_82624A0C:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-416
	ctx.r10.s64 = ctx.r1.s64 + -416;
	// addi r9,r1,-240
	ctx.r9.s64 = ctx.r1.s64 + -240;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r11,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v3,v0,v12
	// vsldoi v8,v0,v9,2
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 14));
	// li r11,4
	r11.s64 = 4;
	// vsldoi v4,v0,v9,6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 10));
	// vsldoi v5,v0,v9,4
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 12));
	// vsrah v0,v8,v12
	// vsrah v8,v4,v12
	// vsrah v9,v5,v12
	// vslh v5,v0,v12
	// vslh v2,v0,v13
	// vslh v1,v0,v10
	// vslh v4,v9,v13
	// vadduhm v0,v5,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v3,v3,v8
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v9,v9,v11
	// vadduhm v8,v4,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v0,v1,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v5,v2,v7
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v8,v3,v12
	// vadduhm v0,v5,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v0,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsrah v0,v0,v6
	// vpkshss v0,v0,v0
	// vaddubm v0,v0,v21
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_add_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v21.u8)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624a0c
	if (cr6.lt) goto loc_82624A0C;
loc_82624AB8:
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82624ABC"))) PPC_WEAK_FUNC(sub_82624ABC);
PPC_FUNC_IMPL(__imp__sub_82624ABC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82624AC0"))) PPC_WEAK_FUNC(sub_82624AC0);
PPC_FUNC_IMPL(__imp__sub_82624AC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r12{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// vspltish v13,1
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// vspltish v7,2
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// vspltish v8,4
	// vspltish v9,7
	// bne cr6,0x82624af4
	if (!cr6.eq) goto loc_82624AF4;
	// vspltish v9,8
loc_82624AF4:
	// cmplwi cr6,r7,3
	cr6.compare<uint32_t>(ctx.r7.u32, 3, xer);
	// bgt cr6,0x82624c6c
	if (cr6.gt) {
		// ERROR 82624C6C
		return;
	}
	// lis r12,-32158
	r12.s64 = -2107506688;
	// addi r12,r12,19220
	r12.s64 = r12.s64 + 19220;
	// rlwinm r0,r7,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r7.u64) {
	case 0:
		// ERROR: 0x82624B24
		return;
	case 1:
		// ERROR: 0x82624B6C
		return;
	case 2:
		// ERROR: 0x82624BC4
		return;
	case 3:
		// ERROR: 0x82624C18
		return;
	default:
		__builtin_unreachable();
	}
}

__attribute__((alias("__imp__sub_82624B14"))) PPC_WEAK_FUNC(sub_82624B14);
PPC_FUNC_IMPL(__imp__sub_82624B14) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister temp{};
	// lwz r19,19236(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19236);
	// lwz r19,19308(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19308);
	// lwz r19,19396(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19396);
	// lwz r19,19480(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19480);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624B2C:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// lvrx v11,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vslh v12,v12,v7
	// stvx128 v12,r9,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624b2c
	if (cr6.lt) goto loc_82624B2C;
	// b 0x82624c6c
	// ERROR 82624C6C
	return;
}

__attribute__((alias("__imp__sub_82624B6C"))) PPC_WEAK_FUNC(sub_82624B6C);
PPC_FUNC_IMPL(__imp__sub_82624B6C) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624B74:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// lvrx v11,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// vsldoi v11,v12,v0,1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vslh v10,v12,v13
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// stvx128 v12,r9,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624b74
	if (cr6.lt) goto loc_82624B74;
	// b 0x82624c6c
	// ERROR 82624C6C
	return;
}

__attribute__((alias("__imp__sub_82624BC4"))) PPC_WEAK_FUNC(sub_82624BC4);
PPC_FUNC_IMPL(__imp__sub_82624BC4) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624BCC:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// lvrx v11,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// vsldoi v11,v12,v0,1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vslh v12,v12,v13
	// stvx128 v12,r9,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624bcc
	if (cr6.lt) goto loc_82624BCC;
	// b 0x82624c6c
	// ERROR 82624C6C
	return;
}

__attribute__((alias("__imp__sub_82624C18"))) PPC_WEAK_FUNC(sub_82624C18);
PPC_FUNC_IMPL(__imp__sub_82624C18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister temp{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624C20:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// lvrx v11,r3,r10
	temp.u32 = ctx.r3.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// vor v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// vsldoi v11,v12,v0,1
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 15));
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// vmrghb v10,v0,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v11,v10,v12
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vslh v12,v12,v13
	// vadduhm v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// stvx128 v12,r9,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x82624c20
	if (cr6.lt) goto loc_82624C20;
	// cmplwi cr6,r8,3
	cr6.compare<uint32_t>(ctx.r8.u32, 3, xer);
	// bgtlr cr6
	if (cr6.gt) return;
	// lis r12,-32158
	r12.s64 = -2107506688;
	// addi r12,r12,19596
	r12.s64 = r12.s64 + 19596;
	// rlwinm r0,r8,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r8.u64) {
	case 0:
		// ERROR: 0x82624C9C
		return;
	case 1:
		// ERROR: 0x82624CF8
		return;
	case 2:
		// ERROR: 0x82624D64
		return;
	case 3:
		// ERROR: 0x82624DCC
		return;
	default:
		__builtin_unreachable();
	}
}

__attribute__((alias("__imp__sub_82624C8C"))) PPC_WEAK_FUNC(sub_82624C8C);
PPC_FUNC_IMPL(__imp__sub_82624C8C) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r2{};
	PPCRegister r11{};
	PPCRegister r19{};
	uint32_t ea{};
	// lwz r19,19612(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19612);
	// lwz r19,19704(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19704);
	// lwz r19,19812(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19812);
	// lwz r19,19916(r2)
	r19.u64 = PPC_LOAD_U32(r2.u32 + 19916);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624CA4:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// li r9,4
	ctx.r9.s64 = 4;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v0,v0,v7
	// vadduhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v8
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r11,r9
	ea = (r11.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624ca4
	if (cr6.lt) goto loc_82624CA4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82624CF8"))) PPC_WEAK_FUNC(sub_82624CF8);
PPC_FUNC_IMPL(__imp__sub_82624CF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	uint32_t ea{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624D00:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// li r8,4
	ctx.r8.s64 = 4;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r11,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v12,v0,v13
	// vadduhm v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v8
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624d00
	if (cr6.lt) goto loc_82624D00;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82624D64"))) PPC_WEAK_FUNC(sub_82624D64);
PPC_FUNC_IMPL(__imp__sub_82624D64) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	uint32_t ea{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624D6C:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// li r8,4
	ctx.r8.s64 = 4;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r11,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vslh v0,v0,v13
	// vadduhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v8
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624d6c
	if (cr6.lt) goto loc_82624D6C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82624DCC"))) PPC_WEAK_FUNC(sub_82624DCC);
PPC_FUNC_IMPL(__imp__sub_82624DCC) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	uint32_t ea{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
loc_82624DD4:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// li r8,4
	ctx.r8.s64 = 4;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r11,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v12,v0,v13
	// vadduhm v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsrah v0,v0,v8
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stvewx v0,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// lwz r9,36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r11,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r11.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// blt cr6,0x82624dd4
	if (cr6.lt) goto loc_82624DD4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82624E38"))) PPC_WEAK_FUNC(sub_82624E38);
PPC_FUNC_IMPL(__imp__sub_82624E38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lvx128 v13,r0,r4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// add r30,r10,r4
	r30.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// clrlwi r10,r6,28
	ctx.r10.u64 = ctx.r6.u32 & 0xF;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// rlwinm r8,r6,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r9,r4
	r31.u64 = ctx.r9.u64 + ctx.r4.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r6,r8
	r29.u64 = ctx.r6.u64 + ctx.r8.u64;
	// rlwinm r28,r6,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lvx128 v11,r0,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r7,r6,r9
	ctx.r7.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r8,r6,r10
	ctx.r8.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r9,r29,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r6,r28
	ctx.r10.s64 = r28.s64 - ctx.r6.s64;
	// add r11,r4,r6
	r11.u64 = ctx.r4.u64 + ctx.r6.u64;
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82624f10
	if (!cr6.eq) goto loc_82624F10;
	// clrlwi r11,r4,28
	r11.u64 = ctx.r4.u32 & 0xF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82624ef0
	if (!cr6.eq) goto loc_82624EF0;
	// vmrghb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// b 0x82624fb0
	goto loc_82624FB0;
loc_82624EF0:
	// vmrglb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// b 0x82624fac
	goto loc_82624FAC;
loc_82624F10:
	// clrlwi r6,r4,28
	ctx.r6.u64 = ctx.r4.u32 & 0xF;
	// vmrghb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82624f24
	if (cr6.eq) goto loc_82624F24;
	// vmrglb v6,v0,v13
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F24:
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// vmrghb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f38
	if (cr6.eq) goto loc_82624F38;
	// vmrglb v13,v0,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F38:
	// clrlwi r11,r31,28
	r11.u64 = r31.u32 & 0xF;
	// vmrghb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f4c
	if (cr6.eq) goto loc_82624F4C;
	// vmrglb v12,v0,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F4C:
	// clrlwi r11,r7,28
	r11.u64 = ctx.r7.u32 & 0xF;
	// vmrghb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f60
	if (cr6.eq) goto loc_82624F60;
	// vmrglb v11,v0,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F60:
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// vmrghb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f74
	if (cr6.eq) goto loc_82624F74;
	// vmrglb v10,v0,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F74:
	// clrlwi r11,r8,28
	r11.u64 = ctx.r8.u32 & 0xF;
	// vmrghb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f88
	if (cr6.eq) goto loc_82624F88;
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F88:
	// clrlwi r11,r9,28
	r11.u64 = ctx.r9.u32 & 0xF;
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624f9c
	if (cr6.eq) goto loc_82624F9C;
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624F9C:
	// clrlwi r11,r10,28
	r11.u64 = ctx.r10.u32 & 0xF;
	// vmrghb v7,v0,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82624fb0
	if (cr6.eq) goto loc_82624FB0;
loc_82624FAC:
	// vmrglb v7,v0,v5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_82624FB0:
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// vadduhm v0,v6,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// addi r10,r5,32
	ctx.r10.s64 = ctx.r5.s64 + 32;
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// addi r8,r5,64
	ctx.r8.s64 = ctx.r5.s64 + 64;
	// addi r7,r5,80
	ctx.r7.s64 = ctx.r5.s64 + 80;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// addi r11,r5,96
	r11.s64 = ctx.r5.s64 + 96;
	// vadduhm v13,v13,v6
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// addi r6,r5,112
	ctx.r6.s64 = ctx.r5.s64 + 112;
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v11,v5
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvx128 v3,r0,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v2,r0,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v12,v6
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// stvewx v0,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// li r7,4
	ctx.r7.s64 = 4;
	// vpkshus v11,v11,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// li r6,4
	ctx.r6.s64 = 4;
	// vadduhm v10,v10,v4
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// li r5,4
	ctx.r5.s64 = 4;
	// vadduhm v9,v9,v3
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// li r4,4
	ctx.r4.s64 = 4;
	// stvewx v0,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v10,v10,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vadduhm v8,v8,v2
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// li r3,4
	ctx.r3.s64 = 4;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vadduhm v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vpkshus v8,v8,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// stvewx v13,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// vpkshus v7,v7,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvewx v13,r9,r8
	ea = (ctx.r9.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v12,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r9,r7
	ea = (ctx.r9.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v11,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v10,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v10,r9,r5
	ea = (ctx.r9.u32 + ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v9,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v9,r9,r4
	ea = (ctx.r9.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v8,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v8,r9,r8
	ea = (ctx.r9.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stvewx v7,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v7,r11,r3
	ea = (r11.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826250E0"))) PPC_WEAK_FUNC(sub_826250E0);
PPC_FUNC_IMPL(__imp__sub_826250E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// vspltish v0,15
	// lvx128 v13,r0,r5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// addi r10,r5,32
	ctx.r10.s64 = ctx.r5.s64 + 32;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// addi r8,r5,64
	ctx.r8.s64 = ctx.r5.s64 + 64;
	// addi r7,r5,80
	ctx.r7.s64 = ctx.r5.s64 + 80;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r5,96
	ctx.r6.s64 = ctx.r5.s64 + 96;
	// addi r11,r5,112
	r11.s64 = ctx.r5.s64 + 112;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,4
	ctx.r10.s64 = 4;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// lvx128 v7,r0,r6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// li r7,4
	ctx.r7.s64 = 4;
	// vpkshus v12,v12,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vadduhm v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// li r6,4
	ctx.r6.s64 = 4;
	// vadduhm v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// li r5,4
	ctx.r5.s64 = 4;
	// vadduhm v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vpkshus v11,v11,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// li r4,4
	ctx.r4.s64 = 4;
	// vpkshus v10,v10,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vadduhm v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// stvewx v13,r0,r3
	ea = (ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// li r3,4
	ctx.r3.s64 = 4;
	// vpkshus v8,v8,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vadduhm v0,v0,v6
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vpkshus v7,v7,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// li r31,4
	r31.s64 = 4;
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// stvewx v12,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r9,r8
	ea = (ctx.r9.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v11,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r9,r7
	ea = (ctx.r9.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v10,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v10,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v9,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v9,r9,r5
	ea = (ctx.r9.u32 + ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v8,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v8,r9,r4
	ea = (ctx.r9.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v7,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v7,r9,r3
	ea = (ctx.r9.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r31
	ea = (r11.u32 + r31.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8262522C"))) PPC_WEAK_FUNC(sub_8262522C);
PPC_FUNC_IMPL(__imp__sub_8262522C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82625230"))) PPC_WEAK_FUNC(sub_82625230);
PPC_FUNC_IMPL(__imp__sub_82625230) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf4
	// lwz r11,284(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262590c
	if (cr6.eq) goto loc_8262590C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x8262590c
	if (cr6.eq) goto loc_8262590C;
	// lwz r11,3964(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262590c
	if (cr6.eq) goto loc_8262590C;
	// lwz r10,3972(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3972);
	// lwz r11,3732(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// lwz r30,3736(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3736);
	// cmpwi cr6,r10,31
	cr6.compare<int32_t>(ctx.r10.s32, 31, xer);
	// lwz r29,3740(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3740);
	// lwz r8,204(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// lwz r7,212(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// ble cr6,0x82625280
	if (!cr6.gt) goto loc_82625280;
	// addi r10,r10,-64
	ctx.r10.s64 = ctx.r10.s64 + -64;
	// stw r10,3972(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3972, ctx.r10.u32);
loc_82625280:
	// lwz r10,3968(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3968);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826252a0
	if (!cr6.eq) goto loc_826252A0;
	// lwz r10,3972(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3972);
	// li r9,-64
	ctx.r9.s64 = -64;
	// rlwinm r10,r10,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// subfic r10,r10,16320
	xer.ca = ctx.r10.u32 <= 16320;
	ctx.r10.s64 = 16320 - ctx.r10.s64;
	// b 0x826252ac
	goto loc_826252AC;
loc_826252A0:
	// lwz r6,3972(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3972);
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// rlwinm r10,r6,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
loc_826252AC:
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// vspltish v11,6
	// addi r28,r10,30416
	r28.s64 = ctx.r10.s64 + 30416;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// std r6,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.r6.u64);
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// std r10,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r10.u64);
	// lfd f0,-80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,-64(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,-64(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f0,-60(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f0,-56(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f0,-52(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// stfs f13,-80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -80, temp.u32);
	// stfs f13,-76(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -76, temp.u32);
	// stfs f13,-72(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -72, temp.u32);
	// stfs f13,-68(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -68, temp.u32);
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v12,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// dcbt r0,r11
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r11
	// li r10,256
	ctx.r10.s64 = 256;
	// dcbt r10,r11
	// li r10,384
	ctx.r10.s64 = 384;
	// dcbt r10,r11
	// li r10,512
	ctx.r10.s64 = 512;
	// dcbt r10,r11
	// li r10,640
	ctx.r10.s64 = 640;
	// dcbt r10,r11
	// li r10,768
	ctx.r10.s64 = 768;
	// dcbt r10,r11
	// li r10,896
	ctx.r10.s64 = 896;
	// dcbt r10,r11
	// addi r9,r1,-80
	ctx.r9.s64 = ctx.r1.s64 + -80;
	// mullw r10,r7,r8
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826256bc
	if (cr6.eq) goto loc_826256BC;
loc_82625368:
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v6,v0,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r11,32
	ctx.r8.s64 = r11.s64 + 32;
	// vmrglb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r7,r11,48
	ctx.r7.s64 = r11.s64 + 48;
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v4,v0,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v3,v0,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v4,v4,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v4.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vmrglh v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v1,v0,v9
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v3,v3,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vcfsx v31,v10,0
	_mm_store_ps(v31.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vmrghb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrghh v2,v0,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v1,v1,0
	_mm_store_ps(ctx.v1.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v1.u32)));
	// vcfsx v30,v9,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrglb v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghh v29,v0,v10
	_mm_store_si128((__m128i*)v29.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrghh v28,v0,v9
	_mm_store_si128((__m128i*)v28.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v27,v0,v8
	_mm_store_si128((__m128i*)v27.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmaddfp v4,v13,v4,v12
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmrghh v26,v0,v7
	_mm_store_si128((__m128i*)v26.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v29,v29,0
	_mm_store_ps(v29.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v28,v28,0
	_mm_store_ps(v28.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)));
	// vmrglh v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v27,v27,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v27.u32)));
	// vmrglh v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmaddfp v6,v13,v6,v12
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v12.f32)));
	// vcfsx v10,v10,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v26,v26,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmaddfp v3,v13,v3,v12
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v31,v13,v31,v12
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v31.f32)), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v4,v4,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vmaddfp v2,v13,v2,v12
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v5,v13,v5,v12
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v1,v13,v1,v12
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vmaddfp v30,v13,v30,v12
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v30.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v29,v13,v29,v12
	_mm_store_ps(v29.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v29.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v10,v13,v10,v12
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v28,v13,v28,v12
	_mm_store_ps(v28.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v28.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v9,v13,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v27,v13,v27,v12
	_mm_store_ps(v27.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v27.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v8,v13,v8,v12
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v26,v13,v26,v12
	_mm_store_ps(v26.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v26.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v7,v13,v7,v12
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v3,v3,0
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vctsxs v31,v31,0
	_mm_store_si128((__m128i*)v31.s32, _mm_vctsxs(_mm_load_ps(v31.f32)));
	// vpkswss v6,v4,v6
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v1,v1,0
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_vctsxs(_mm_load_ps(ctx.v1.f32)));
	// vctsxs v30,v30,0
	_mm_store_si128((__m128i*)v30.s32, _mm_vctsxs(_mm_load_ps(v30.f32)));
	// vctsxs v29,v29,0
	_mm_store_si128((__m128i*)v29.s32, _mm_vctsxs(_mm_load_ps(v29.f32)));
	// vctsxs v10,v10,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v28,v28,0
	_mm_store_si128((__m128i*)v28.s32, _mm_vctsxs(_mm_load_ps(v28.f32)));
	// vctsxs v9,v9,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vctsxs v27,v27,0
	_mm_store_si128((__m128i*)v27.s32, _mm_vctsxs(_mm_load_ps(v27.f32)));
	// vctsxs v8,v8,0
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vctsxs v26,v26,0
	_mm_store_si128((__m128i*)v26.s32, _mm_vctsxs(_mm_load_ps(v26.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vpkswss v4,v3,v31
	// vpkswss v10,v29,v10
	// addi r6,r11,64
	ctx.r6.s64 = r11.s64 + 64;
	// vpkswss v9,v28,v9
	// vsrah v6,v6,v11
	// vsrah v4,v4,v11
	// vpkswss v8,v27,v8
	// vpkswss v7,v26,v7
	// addi r5,r11,80
	ctx.r5.s64 = r11.s64 + 80;
	// vpkswss v5,v2,v5
	// vsrah v2,v10,v11
	// vpkswss v3,v1,v30
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v1,v9,v11
	// vpkshus v4,v6,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// addi r4,r11,96
	ctx.r4.s64 = r11.s64 + 96;
	// vsrah v31,v8,v11
	// vmrghb v6,v0,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsrah v30,v7,v11
	// vmrglb v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsrah v5,v5,v11
	// vsrah v3,v3,v11
	// vpkshus v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v1.s16), _mm_load_si128((__m128i*)ctx.v2.s16)));
	// lvx128 v9,r0,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r11,112
	r31.s64 = r11.s64 + 112;
	// lvx128 v8,r0,r4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v30.s16), _mm_load_si128((__m128i*)v31.s16)));
	// vmrghh v30,v0,v10
	_mm_store_si128((__m128i*)v30.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v29,v0,v10
	_mm_store_si128((__m128i*)v29.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vpkshus v3,v5,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vmrghb v5,v0,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v7,r0,r31
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghb v10,v0,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v30,v30,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v29,v29,0
	_mm_store_ps(v29.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v31,v0,v6
	_mm_store_si128((__m128i*)v31.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v28,v0,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v27,v0,v9
	_mm_store_si128((__m128i*)v27.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v26,v0,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v24,v0,v8
	_mm_store_si128((__m128i*)v24.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v31,v31,0
	_mm_store_ps(v31.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v31.u32)));
	// vmrghb v9,v0,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v28,v28,0
	_mm_store_ps(v28.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)));
	// vmrglb v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v27,v27,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v27.u32)));
	// vmrghh v25,v0,v10
	_mm_store_si128((__m128i*)v25.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v26,v26,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)));
	// vcfsx v22,v24,0
	_mm_store_ps(v22.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)));
	// vmrglh v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v24,v0,v9
	_mm_store_si128((__m128i*)v24.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmaddfp v30,v13,v30,v12
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v30.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmrghh v23,v0,v7
	_mm_store_si128((__m128i*)v23.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmaddfp v29,v13,v29,v12
	_mm_store_ps(v29.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v29.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmrglh v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v25,v25,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)));
	// vmrglh v10,v0,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrglh v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v24,v24,0
	_mm_store_ps(v24.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v23,v23,0
	_mm_store_ps(v23.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v23.u32)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrglh v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v10,v10,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmaddfp v31,v13,v31,v12
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v31.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v28,v13,v28,v12
	_mm_store_ps(v28.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v28.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v27,v13,v27,v12
	_mm_store_ps(v27.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v27.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v26,v13,v26,v12
	_mm_store_ps(v26.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v26.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v25,v13,v25,v12
	_mm_store_ps(v25.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v25.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v6,v13,v6,v12
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v12.f32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmaddfp v22,v13,v22,v12
	_mm_store_ps(v22.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v22.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v5,v13,v5,v12
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v10,v13,v10,v12
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v8,v13,v8,v12
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v24,v13,v24,v12
	_mm_store_ps(v24.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v24.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v9,v13,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v23,v13,v23,v12
	_mm_store_ps(v23.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v23.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v7,v13,v7,v12
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v12.f32)));
	// stvx v4,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// li r27,1024
	r27.s64 = 1024;
	// vctsxs v31,v31,0
	_mm_store_si128((__m128i*)v31.s32, _mm_vctsxs(_mm_load_ps(v31.f32)));
	// stvx v3,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctsxs v30,v30,0
	_mm_store_si128((__m128i*)v30.s32, _mm_vctsxs(_mm_load_ps(v30.f32)));
	// stvx v2,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctsxs v29,v29,0
	_mm_store_si128((__m128i*)v29.s32, _mm_vctsxs(_mm_load_ps(v29.f32)));
	// stvx v1,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v10,v10,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v9,v9,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vctsxs v8,v8,0
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vctsxs v28,v28,0
	_mm_store_si128((__m128i*)v28.s32, _mm_vctsxs(_mm_load_ps(v28.f32)));
	// vctsxs v27,v27,0
	_mm_store_si128((__m128i*)v27.s32, _mm_vctsxs(_mm_load_ps(v27.f32)));
	// vctsxs v26,v26,0
	_mm_store_si128((__m128i*)v26.s32, _mm_vctsxs(_mm_load_ps(v26.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v25,v25,0
	_mm_store_si128((__m128i*)v25.s32, _mm_vctsxs(_mm_load_ps(v25.f32)));
	// vctsxs v22,v22,0
	_mm_store_si128((__m128i*)v22.s32, _mm_vctsxs(_mm_load_ps(v22.f32)));
	// vpkswss v6,v31,v6
	// vctsxs v24,v24,0
	_mm_store_si128((__m128i*)v24.s32, _mm_vctsxs(_mm_load_ps(v24.f32)));
	// vctsxs v23,v23,0
	_mm_store_si128((__m128i*)v23.s32, _mm_vctsxs(_mm_load_ps(v23.f32)));
	// vpkswss v31,v30,v29
	// vsrah v6,v6,v11
	// vsrah v4,v31,v11
	// vpkswss v5,v28,v5
	// vpkswss v30,v27,v26
	// vpkshus v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// vpkswss v10,v25,v10
	// vsrah v5,v5,v11
	// vpkswss v8,v22,v8
	// vpkswss v9,v24,v9
	// vsrah v31,v30,v11
	// vpkswss v7,v23,v7
	// vsrah v10,v10,v11
	// vsrah v8,v8,v11
	// vsrah v9,v9,v11
	// vpkshus v5,v5,v31
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsrah v7,v7,v11
	// stvx v6,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkshus v10,v10,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// vpkshus v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v5,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// dcbt r27,r11
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82625368
	if (!cr6.eq) goto loc_82625368;
loc_826256BC:
	// dcbt r0,r30
	// dcbt r0,r29
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r30
	// dcbt r11,r29
	// li r11,256
	r11.s64 = 256;
	// dcbt r11,r30
	// dcbt r11,r29
	// li r11,384
	r11.s64 = 384;
	// dcbt r11,r30
	// dcbt r11,r29
	// lwz r11,216(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// lwz r10,208(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r11,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r11.s64 = r11.s32 >> 5;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262590c
	if (cr6.eq) goto loc_8262590C;
	// li r10,16
	ctx.r10.s64 = 16;
	// lvx128 v12,r10,r28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,32
	ctx.r10.s64 = 32;
	// lvx128 v10,r10,r28
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82625710:
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r30,16
	ctx.r10.s64 = r30.s64 + 16;
	// vmrghb v6,v0,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v8,r0,r29
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r29,16
	ctx.r9.s64 = r29.s64 + 16;
	// vmrghb v5,v0,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v3,v0,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v2,v0,v9
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglh v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v3,v3,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrghh v1,v0,v5
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghh v31,v0,v8
	_mm_store_si128((__m128i*)v31.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v30,v9,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrglh v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrghb v9,v0,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v1,v1,0
	_mm_store_ps(ctx.v1.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v1.u32)));
	// vmrglh v5,v0,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v31,v31,0
	_mm_store_ps(v31.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v31.u32)));
	// vcfsx v29,v8,0
	_mm_store_ps(v29.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmrglb v8,v0,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v7,v0,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrghh v27,v0,v8
	_mm_store_si128((__m128i*)v27.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubfp v3,v3,v12
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v12.f32)));
	// vmrghh v26,v0,v7
	_mm_store_si128((__m128i*)v26.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubfp v2,v2,v12
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v12.f32)));
	// vmrglh v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubfp v30,v30,v12
	_mm_store_ps(v30.f32, _mm_sub_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v28,v6,v12
	_mm_store_ps(v28.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)));
	// vmrglb v6,v0,v4
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubfp v1,v1,v12
	_mm_store_ps(ctx.v1.f32, _mm_sub_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v12.f32)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v27,v27,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v27.u32)));
	// vmrghh v25,v0,v6
	_mm_store_si128((__m128i*)v25.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v26,v26,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)));
	// vcfsx v4,v4,0
	_mm_store_ps(ctx.v4.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vmrglh v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vcfsx v25,v25,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)));
	// vmaddfp v3,v13,v3,v10
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v2,v13,v2,v10
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v30,v13,v30,v10
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v30.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v28,v13,v28,v10
	_mm_store_ps(v28.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v28.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v1,v13,v1,v10
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v10.f32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vsubfp v5,v5,v12
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v31,v31,v12
	_mm_store_ps(v31.f32, _mm_sub_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v29,v29,v12
	_mm_store_ps(v29.f32, _mm_sub_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v9,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v4,v4,v12
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v27,v27,v12
	_mm_store_ps(v27.f32, _mm_sub_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v24,v3,0
	_mm_store_si128((__m128i*)v24.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vctsxs v30,v30,0
	_mm_store_si128((__m128i*)v30.s32, _mm_vctsxs(_mm_load_ps(v30.f32)));
	// vsubfp v8,v8,v12
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v28,v28,0
	_mm_store_si128((__m128i*)v28.s32, _mm_vctsxs(_mm_load_ps(v28.f32)));
	// vctsxs v23,v1,0
	_mm_store_si128((__m128i*)v23.s32, _mm_vctsxs(_mm_load_ps(ctx.v1.f32)));
	// vsubfp v26,v26,v12
	_mm_store_ps(v26.f32, _mm_sub_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v7,v7,v12
	_mm_store_ps(ctx.v7.f32, _mm_sub_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v25,v25,v12
	_mm_store_ps(v25.f32, _mm_sub_ps(_mm_load_ps(v25.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v3,v13,v9,v10
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vsubfp v6,v6,v12
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v5,v13,v5,v10
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v31,v13,v31,v10
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v31.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v29,v13,v29,v10
	_mm_store_ps(v29.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v29.f32)), _mm_load_ps(ctx.v10.f32)));
	// vpkswss v1,v2,v30
	// vmaddfp v4,v13,v4,v10
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v2,v13,v27,v10
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v27.f32)), _mm_load_ps(ctx.v10.f32)));
	// vpkswss v9,v24,v28
	// vmaddfp v8,v13,v8,v10
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v10.f32)));
	// vctsxs v29,v29,0
	_mm_store_si128((__m128i*)v29.s32, _mm_vctsxs(_mm_load_ps(v29.f32)));
	// li r8,512
	ctx.r8.s64 = 512;
	// vctsxs v31,v31,0
	_mm_store_si128((__m128i*)v31.s32, _mm_vctsxs(_mm_load_ps(v31.f32)));
	// vmaddfp v7,v13,v7,v10
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v6,v13,v6,v10
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v30,v13,v26,v10
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v26.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v28,v13,v25,v10
	_mm_store_ps(v28.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v25.f32)), _mm_load_ps(ctx.v10.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v4,v4,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v3,v3,0
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vpkswss v31,v31,v29
	// vctsxs v29,v8,0
	_mm_store_si128((__m128i*)v29.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vctsxs v27,v7,0
	_mm_store_si128((__m128i*)v27.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vsrah v7,v9,v11
	// vctsxs v26,v6,0
	_mm_store_si128((__m128i*)v26.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vctsxs v30,v30,0
	_mm_store_si128((__m128i*)v30.s32, _mm_vctsxs(_mm_load_ps(v30.f32)));
	// vctsxs v28,v28,0
	_mm_store_si128((__m128i*)v28.s32, _mm_vctsxs(_mm_load_ps(v28.f32)));
	// vpkswss v5,v23,v5
	// vpkswss v8,v4,v3
	// vsrah v4,v1,v11
	// vsrah v3,v5,v11
	// vsrah v8,v8,v11
	// vpkshus v7,v7,v4
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vpkswss v9,v2,v29
	// vsrah v2,v31,v11
	// vpkswss v6,v30,v27
	// vpkswss v5,v28,v26
	// vsrah v9,v9,v11
	// vpkshus v4,v3,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// stvx v7,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v6,v6,v11
	// vsrah v5,v5,v11
	// vpkshus v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v8,v6,v5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// stvx v4,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// dcbt r8,r30
	// li r10,512
	ctx.r10.s64 = 512;
	// dcbt r10,r29
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r30,r30,32
	r30.s64 = r30.s64 + 32;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82625710
	if (!cr6.eq) goto loc_82625710;
loc_8262590C:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82625910"))) PPC_WEAK_FUNC(sub_82625910);
PPC_FUNC_IMPL(__imp__sub_82625910) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// lwz r11,284(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82625d6c
	if (cr6.eq) goto loc_82625D6C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82625d6c
	if (cr6.eq) goto loc_82625D6C;
	// cmpwi cr6,r6,31
	cr6.compare<int32_t>(ctx.r6.s32, 31, xer);
	// ble cr6,0x82625938
	if (!cr6.gt) goto loc_82625938;
	// addi r6,r6,-64
	ctx.r6.s64 = ctx.r6.s64 + -64;
loc_82625938:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82625950
	if (!cr6.eq) goto loc_82625950;
	// rlwinm r11,r6,7,0,24
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// li r30,-64
	r30.s64 = -64;
	// subfic r31,r11,16320
	xer.ca = r11.u32 <= 16320;
	r31.s64 = 16320 - r11.s64;
	// b 0x82625958
	goto loc_82625958;
loc_82625950:
	// addi r30,r5,32
	r30.s64 = ctx.r5.s64 + 32;
	// rlwinm r31,r6,6,0,25
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
loc_82625958:
	// lwz r11,21000(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21000);
	// addi r8,r3,208
	ctx.r8.s64 = ctx.r3.s64 + 208;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826259a4
	if (!cr6.eq) goto loc_826259A4;
	// lwz r11,204(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// addi r9,r4,-4
	ctx.r9.s64 = ctx.r4.s64 + -4;
	// addi r10,r4,-8
	ctx.r10.s64 = ctx.r4.s64 + -8;
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r29,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r29.s64 = r11.s32 >> 1;
	// lwz r5,3732(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// lwz r6,3736(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3736);
	// mullw r10,r29,r10
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// lwz r7,3740(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3740);
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r5,r6,r9
	ctx.r5.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// b 0x82625ab8
	goto loc_82625AB8;
loc_826259A4:
	// lwz r11,20836(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20836);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,204(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// beq cr6,0x82625a38
	if (cr6.eq) goto loc_82625A38;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x82625a00
	if (!cr6.eq) goto loc_82625A00;
	// lwz r10,224(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// lwz r9,3804(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3804);
	// lwz r6,3808(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3808);
	// subf r29,r10,r9
	r29.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lwz r7,3800(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3800);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r5,220(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// srawi r9,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r7,r9,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r9.s64;
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r5,r9,r29
	ctx.r5.s64 = r29.s64 - ctx.r9.s64;
	// b 0x82625ab8
	goto loc_82625AB8;
loc_82625A00:
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r6,3732(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// lwz r5,3736(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3736);
	// rlwinm r29,r10,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r4,3740(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3740);
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r10,r29
	ctx.r10.s64 = r29.s64 - ctx.r10.s64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subf r5,r9,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r9.s64;
	// subf r7,r9,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r9.s64;
	// b 0x82625ab8
	goto loc_82625AB8;
loc_82625A38:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x82625a70
	if (!cr6.eq) goto loc_82625A70;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r5,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r5.s64 = r11.s32 >> 1;
	// lwz r10,3732(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// lwz r7,3736(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3736);
	// rlwinm r5,r5,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r6,3740(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3740);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r5,r9,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r7,r9,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r9.s64;
	// b 0x82625ab8
	goto loc_82625AB8;
loc_82625A70:
	// lwz r10,224(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// lwz r9,3804(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3804);
	// lwz r7,3808(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3808);
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r29,r10,r7
	r29.s64 = ctx.r7.s64 - ctx.r10.s64;
	// lwz r6,3800(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3800);
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// lwz r5,220(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// rlwinm r28,r10,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r10,r28
	ctx.r10.s64 = r28.s64 - ctx.r10.s64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// subf r7,r9,r29
	ctx.r7.s64 = r29.s64 - ctx.r9.s64;
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r5,r9,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r9.s64;
loc_82625AB8:
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// extsw r4,r31
	ctx.r4.s64 = r31.s32;
	// vspltish v10,6
	// addi r31,r9,30496
	r31.s64 = ctx.r9.s64 + 30496;
	// li r9,-32
	ctx.r9.s64 = -32;
	// li r6,0
	ctx.r6.s64 = 0;
	// std r4,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.r4.u64);
	// srawi r4,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	ctx.r4.s64 = r11.s32 >> 5;
	// lwz r11,188(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lvx128 v13,r9,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// extsw r9,r30
	ctx.r9.s64 = r30.s32;
	// addic. r11,r11,40
	xer.ca = r11.u32 > 4294967255;
	r11.s64 = r11.s64 + 40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// std r9,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r9.u64);
	// lfd f0,-80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,-64(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,-64(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f0,-60(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f0,-56(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f0,-52(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// stfs f13,-80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -80, temp.u32);
	// stfs f13,-76(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -76, temp.u32);
	// stfs f13,-72(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -72, temp.u32);
	// stfs f13,-68(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -68, temp.u32);
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v12,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// addi r11,r1,-80
	r11.s64 = ctx.r1.s64 + -80;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble 0x82625be0
	if (!cr0.gt) goto loc_82625BE0;
loc_82625B40:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82625bc4
	if (!cr6.gt) goto loc_82625BC4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
loc_82625B50:
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// vmrghb v9,v0,v11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vmrghh v8,v0,v9
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v11,v0,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v8,v8,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vcfsx v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmaddfp v8,v13,v8,v12
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v7,v13,v7,v12
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v11,v13,v11,v12
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v9,v13,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v8,v8,0
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v11,v11,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v11.f32)));
	// vctsxs v9,v9,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vpkswss v11,v7,v11
	// vpkswss v9,v8,v9
	// vsrah v11,v11,v10
	// vsrah v9,v9,v10
	// vpkshus v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82625b50
	if (!cr6.eq) goto loc_82625B50;
loc_82625BC4:
	// lwz r9,188(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// lwz r11,204(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// addi r9,r9,40
	ctx.r9.s64 = ctx.r9.s64 + 40;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// blt cr6,0x82625b40
	if (cr6.lt) goto loc_82625B40;
loc_82625BE0:
	// addi r3,r3,200
	ctx.r3.s64 = ctx.r3.s64 + 200;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// srawi r30,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r30.s64 = r11.s32 >> 1;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addic. r11,r11,20
	xer.ca = r11.u32 > 4294967275;
	r11.s64 = r11.s64 + 20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x82625d6c
	if (!cr0.gt) goto loc_82625D6C;
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v11,r0,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r11,r31
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82625C08:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82625d4c
	if (!cr6.gt) goto loc_82625D4C;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// rlwinm r10,r10,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// subf r6,r7,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r7.s64;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_82625C24:
	// li r29,16
	r29.s64 = 16;
	// lvlx v8,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// add r10,r6,r11
	ctx.r10.u64 = ctx.r6.u64 + r11.u64;
	// li r31,16
	r31.s64 = 16;
	// mr r28,r11
	r28.u64 = r11.u64;
	// li r27,16
	r27.s64 = 16;
	// lvrx v9,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r29,r11
	r29.u64 = r11.u64;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvlx v7,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vmrghb v7,v0,v9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghb v6,v0,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghh v5,v0,v7
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v3,v0,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrghh v2,v0,v8
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vmrglh v8,v0,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v5,v5,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrglh v6,v0,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v4,v4,0
	_mm_store_ps(ctx.v4.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vmrglh v7,v0,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v3,v3,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrglh v9,v0,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vsubfp v5,v5,v12
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v4,v4,v12
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v3,v3,v12
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v2,v2,v12
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v8,v8,v12
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v6,v6,v12
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v7,v7,v12
	_mm_store_ps(ctx.v7.f32, _mm_sub_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v9,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v5,v13,v5,v11
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v4,v13,v4,v11
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v3,v13,v3,v11
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v2,v13,v2,v11
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v8,v13,v8,v11
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v6,v13,v6,v11
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v7,v13,v7,v11
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v9,v13,v9,v11
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v11.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v4,v4,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v3,v3,0
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vctsxs v8,v8,0
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v1,v9,0
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vpkswss v8,v2,v8
	// vpkswss v9,v3,v6
	// vpkswss v7,v5,v7
	// vpkswss v6,v4,v1
	// vsrah v8,v8,v10
	// vsrah v9,v9,v10
	// vsrah v7,v7,v10
	// vsrah v6,v6,v10
	// vpkshus v9,v9,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vpkshus v8,v7,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// stvlx v9,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v9.u8[15 - i]);
	// stvrx v9,r10,r31
	ea = ctx.r10.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// stvlx v8,0,r29
	ea = r29.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v8.u8[15 - i]);
	// stvrx v8,r28,r27
	ea = r28.u32 + r27.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v8.u8[i]);
	// bne cr6,0x82625c24
	if (!cr6.eq) goto loc_82625C24;
loc_82625D4C:
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// blt cr6,0x82625c08
	if (cr6.lt) goto loc_82625C08;
loc_82625D6C:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82625D70"))) PPC_WEAK_FUNC(sub_82625D70);
PPC_FUNC_IMPL(__imp__sub_82625D70) {
	PPC_FUNC_PROLOGUE();
	// li r3,4
	ctx.r3.s64 = 4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82625D78"))) PPC_WEAK_FUNC(sub_82625D78);
PPC_FUNC_IMPL(__imp__sub_82625D78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,15560(r11)
	PPC_STORE_U32(r11.u32 + 15560, ctx.r10.u32);
	// stw r9,15564(r11)
	PPC_STORE_U32(r11.u32 + 15564, ctx.r9.u32);
	// stw r10,15536(r11)
	PPC_STORE_U32(r11.u32 + 15536, ctx.r10.u32);
	// stw r10,456(r11)
	PPC_STORE_U32(r11.u32 + 456, ctx.r10.u32);
	// stw r10,3452(r11)
	PPC_STORE_U32(r11.u32 + 3452, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82625DA0"))) PPC_WEAK_FUNC(sub_82625DA0);
PPC_FUNC_IMPL(__imp__sub_82625DA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-1440(r1)
	ea = -1440 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r16,r6
	r16.u64 = ctx.r6.u64;
	// lwz r6,1524(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	// mr r17,r9
	r17.u64 = ctx.r9.u64;
	// srawi r25,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r25.s64 = ctx.r6.s32 >> 1;
	// srawi r24,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r24.s64 = ctx.r10.s32 >> 1;
	// lwz r20,336(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// mr r18,r8
	r18.u64 = ctx.r8.u64;
	// lwz r11,19696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// lwz r9,19700(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r14,208(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r25,r25,r9
	r25.u64 = r25.u64 + ctx.r9.u64;
	// lbz r8,4(r16)
	ctx.r8.u64 = PPC_LOAD_U8(r16.u32 + 4);
	// stw r20,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r20.u32);
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// lwz r20,204(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mullw r25,r25,r14
	r25.s64 = int64_t(r25.s32) * int64_t(r14.s32);
	// lwz r7,328(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// lwz r26,6548(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// lwz r27,3732(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// lwz r28,3760(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3760);
	// lwz r29,3736(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r30,3740(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r5,0(r16)
	ctx.r5.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// lwz r3,3764(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3764);
	// mullw r6,r6,r20
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r20.s32);
	// lwz r4,3768(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3768);
	// lwz r19,1768(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// stw r21,1492(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1492, r21.u32);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r11,r25,r24
	r11.u64 = r25.u64 + r24.u64;
	// rotlwi r25,r8,2
	r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cntlzw r9,r7
	ctx.r9.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// rlwinm r25,r9,27,31,31
	r25.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r27,r6
	ctx.r8.u64 = r27.u64 + ctx.r6.u64;
	// add r24,r9,r26
	r24.u64 = ctx.r9.u64 + r26.u64;
	// add r9,r28,r6
	ctx.r9.u64 = r28.u64 + ctx.r6.u64;
	// add r26,r29,r11
	r26.u64 = r29.u64 + r11.u64;
	// xori r20,r25,1
	r20.u64 = r25.u64 ^ 1;
	// add r29,r30,r11
	r29.u64 = r30.u64 + r11.u64;
	// addi r15,r16,12
	r15.s64 = r16.s64 + 12;
	// stw r24,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r24.u32);
	// rlwinm r14,r5,12,30,31
	r14.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 12) & 0x3;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// add r28,r8,r10
	r28.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r27,r9,r10
	r27.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r30,r3,r11
	r30.u64 = ctx.r3.u64 + r11.u64;
	// add r25,r4,r11
	r25.u64 = ctx.r4.u64 + r11.u64;
	// beq cr6,0x82625e94
	if (cr6.eq) goto loc_82625E94;
	// rlwinm r4,r5,8,29,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0x7;
	// stw r4,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r4.u32);
	// b 0x82625e98
	goto loc_82625E98;
loc_82625E94:
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_82625E98:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82625eb8
	if (cr6.eq) goto loc_82625EB8;
	// rlwinm r11,r5,10,30,31
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 10) & 0x3;
	// addi r11,r11,726
	r11.s64 = r11.s64 + 726;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// b 0x82625ebc
	goto loc_82625EBC;
loc_82625EB8:
	// addi r11,r31,2880
	r11.s64 = r31.s64 + 2880;
loc_82625EBC:
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// rlwinm r11,r5,27,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x82626004
	if (cr6.eq) goto loc_82626004;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// beq cr6,0x8262600c
	if (cr6.eq) goto loc_8262600C;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// beq cr6,0x82625ee4
	if (cr6.eq) goto loc_82625EE4;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x826262ac
	if (!cr6.eq) goto loc_826262AC;
loc_82625EE4:
	// lwz r24,1532(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// lwz r21,1540(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r21,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r21.u32);
	// bne cr6,0x82625f1c
	if (!cr6.eq) goto loc_82625F1C;
	// bl 0x82651100
	sub_82651100(ctx, base);
	// b 0x82625f20
	goto loc_82625F20;
loc_82625F1C:
	// bl 0x82651038
	sub_82651038(ctx, base);
loc_82625F20:
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,0,24,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r11,128
	cr6.compare<uint32_t>(r11.u32, 128, xer);
	// bne cr6,0x82625f3c
	if (!cr6.eq) goto loc_82625F3C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r30,r26
	r30.u64 = r26.u64;
	// b 0x82625f44
	goto loc_82625F44;
loc_82625F3C:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r29,r25
	r29.u64 = r25.u64;
loc_82625F44:
	// lwz r11,3904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,512
	ctx.r6.s64 = ctx.r1.s64 + 512;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82653280
	sub_82653280(ctx, base);
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82625f84
	if (cr6.eq) goto loc_82625F84;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r21,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r21.u32);
loc_82625F84:
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653920
	sub_82653920(ctx, base);
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82625fb8
	if (cr6.eq) goto loc_82625FB8;
	// addi r7,r1,104
	ctx.r7.s64 = ctx.r1.s64 + 104;
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82651f38
	sub_82651F38(ctx, base);
loc_82625FB8:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r6,r1,320
	ctx.r6.s64 = ctx.r1.s64 + 320;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// b 0x826262a0
	goto loc_826262A0;
loc_82626004:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82626030
	if (!cr6.eq) goto loc_82626030;
loc_8262600C:
	// lwz r11,1548(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	// lwz r24,1532(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	// lwz r21,1540(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lwz r11,1556(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	// stw r24,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r24.u32);
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// b 0x82626078
	goto loc_82626078;
loc_82626030:
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r5,1540(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
	// lwz r4,1532(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	// addi r10,r1,100
	ctx.r10.s64 = ctx.r1.s64 + 100;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// rlwinm r6,r11,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826537a0
	sub_826537A0(ctx, base);
	// lwz r24,104(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r21,100(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_82626078:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne cr6,0x826260b8
	if (!cr6.eq) goto loc_826260B8;
	// bl 0x82651100
	sub_82651100(ctx, base);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82651100
	sub_82651100(ctx, base);
	// b 0x826260d4
	goto loc_826260D4;
loc_826260B8:
	// bl 0x82651038
	sub_82651038(ctx, base);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82651038
	sub_82651038(ctx, base);
loc_826260D4:
	// lwz r11,3904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,512
	ctx.r6.s64 = ctx.r1.s64 + 512;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82653280
	sub_82653280(ctx, base);
	// lwz r11,3904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,768
	ctx.r6.s64 = ctx.r1.s64 + 768;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// lwz r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82653280
	sub_82653280(ctx, base);
	// lwz r11,3208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3208);
	// li r10,16
	ctx.r10.s64 = 16;
	// li r9,16
	ctx.r9.s64 = 16;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r7,r1,512
	ctx.r7.s64 = ctx.r1.s64 + 512;
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r1,768
	ctx.r5.s64 = ctx.r1.s64 + 768;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,512
	ctx.r3.s64 = ctx.r1.s64 + 512;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82626174
	if (cr6.eq) goto loc_82626174;
	// stw r24,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r24.u32);
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
loc_82626174:
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653920
	sub_82653920(ctx, base);
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653920
	sub_82653920(ctx, base);
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826261b8
	if (cr6.eq) goto loc_826261B8;
	// addi r7,r1,100
	ctx.r7.s64 = ctx.r1.s64 + 100;
	// addi r6,r1,104
	ctx.r6.s64 = ctx.r1.s64 + 104;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82651f38
	sub_82651F38(ctx, base);
loc_826261B8:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r6,r1,384
	ctx.r6.s64 = ctx.r1.s64 + 384;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// lwz r11,3208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3208);
	// li r10,8
	ctx.r10.s64 = 8;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,8
	ctx.r8.s64 = 8;
	// addi r7,r1,256
	ctx.r7.s64 = ctx.r1.s64 + 256;
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r6,r1,320
	ctx.r6.s64 = ctx.r1.s64 + 320;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,8
	ctx.r7.s64 = 8;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r6,r1,448
	ctx.r6.s64 = ctx.r1.s64 + 448;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// lwz r11,3208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3208);
	// li r10,8
	ctx.r10.s64 = 8;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,8
	ctx.r8.s64 = 8;
	// addi r7,r1,320
	ctx.r7.s64 = ctx.r1.s64 + 320;
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r5,r1,448
	ctx.r5.s64 = ctx.r1.s64 + 448;
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r1,320
	ctx.r3.s64 = ctx.r1.s64 + 320;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826262A0:
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r24,124(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r21,1492(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
loc_826262AC:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// li r22,0
	r22.s64 = 0;
	// addi r11,r11,14720
	r11.s64 = r11.s64 + 14720;
	// addi r23,r1,512
	r23.s64 = ctx.r1.s64 + 512;
	// addi r25,r1,1024
	r25.s64 = ctx.r1.s64 + 1024;
	// mr r27,r22
	r27.u64 = r22.u64;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_826262D0:
	// lbzx r11,r27,r15
	r11.u64 = PPC_LOAD_U8(r27.u32 + r15.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82626cd0
	if (cr6.eq) goto loc_82626CD0;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82626450
	if (cr6.eq) goto loc_82626450;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x82626450
	if (!cr6.eq) goto loc_82626450;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826263e8
	if (cr6.lt) goto loc_826263E8;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826263e0
	if (!cr6.lt) goto loc_826263E0;
loc_82626348:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82626374
	if (cr6.lt) goto loc_82626374;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82626348
	if (cr6.eq) goto loc_82626348;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x82626424
	goto loc_82626424;
loc_82626374:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_826263E0:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x82626424
	goto loc_82626424;
loc_826263E8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826263F0:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826263f0
	if (cr6.lt) goto loc_826263F0;
loc_82626424:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82626fb4
	if (!cr6.eq) goto loc_82626FB4;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r14,r11,r9
	r14.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// rotlwi r4,r10,0
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
loc_82626450:
	// add r10,r27,r16
	ctx.r10.u64 = r27.u64 + r16.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stb r4,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, ctx.r4.u8);
	// bne cr6,0x826264a0
	if (!cr6.eq) goto loc_826264A0;
	// lwz r19,1768(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// b 0x82626b24
	goto loc_82626B24;
loc_826264A0:
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// bne cr6,0x82626694
	if (!cr6.eq) goto loc_82626694;
	// lwz r19,1764(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262656c
	if (cr6.eq) goto loc_8262656C;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x82626558
	if (!cr6.eq) goto loc_82626558;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82626558
	if (!cr6.eq) goto loc_82626558;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82626514
	if (!cr0.lt) goto loc_82626514;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82626514:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x82626600
	if (!cr6.eq) goto loc_82626600;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82626548
	if (!cr0.lt) goto loc_82626548;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82626548:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826265fc
	if (!cr6.eq) goto loc_826265FC;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x82626600
	goto loc_82626600;
loc_82626558:
	// clrlwi r11,r14,24
	r11.u64 = r14.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x82626608
	goto loc_82626608;
loc_8262656C:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8262658c
	if (cr6.eq) goto loc_8262658C;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x82626608
	goto loc_82626608;
loc_8262658C:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826265b8
	if (!cr0.lt) goto loc_826265B8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826265B8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x82626600
	if (!cr6.eq) goto loc_82626600;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826265ec
	if (!cr0.lt) goto loc_826265EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826265EC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826265fc
	if (!cr6.eq) goto loc_826265FC;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x82626600
	goto loc_82626600;
loc_826265FC:
	// mr r29,r22
	r29.u64 = r22.u64;
loc_82626600:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_82626608:
	// stbx r11,r27,r15
	PPC_STORE_U8(r27.u32 + r15.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82626658
	if (cr6.eq) goto loc_82626658;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82626658:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x82626b30
	if (cr6.eq) goto loc_82626B30;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x82626b1c
	goto loc_82626B1C;
loc_82626694:
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x82626888
	if (!cr6.eq) goto loc_82626888;
	// lwz r19,1764(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82626760
	if (cr6.eq) goto loc_82626760;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// bne cr6,0x8262674c
	if (!cr6.eq) goto loc_8262674C;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262674c
	if (!cr6.eq) goto loc_8262674C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82626708
	if (!cr0.lt) goto loc_82626708;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82626708:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826267f4
	if (!cr6.eq) goto loc_826267F4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262673c
	if (!cr0.lt) goto loc_8262673C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262673C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826267f0
	if (!cr6.eq) goto loc_826267F0;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x826267f4
	goto loc_826267F4;
loc_8262674C:
	// clrlwi r11,r14,24
	r11.u64 = r14.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x826267fc
	goto loc_826267FC;
loc_82626760:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x82626780
	if (cr6.eq) goto loc_82626780;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x826267fc
	goto loc_826267FC;
loc_82626780:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826267ac
	if (!cr0.lt) goto loc_826267AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826267AC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826267f4
	if (!cr6.eq) goto loc_826267F4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826267e0
	if (!cr0.lt) goto loc_826267E0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826267E0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x826267f0
	if (!cr6.eq) goto loc_826267F0;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x826267f4
	goto loc_826267F4;
loc_826267F0:
	// mr r29,r22
	r29.u64 = r22.u64;
loc_826267F4:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_826267FC:
	// stbx r11,r27,r15
	PPC_STORE_U8(r27.u32 + r15.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262684c
	if (cr6.eq) goto loc_8262684C;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262684C:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x82626b30
	if (cr6.eq) goto loc_82626B30;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x82626b1c
	goto loc_82626B1C;
loc_82626888:
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// bne cr6,0x82626b30
	if (!cr6.eq) goto loc_82626B30;
	// lwz r19,1764(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x82626998
	if (cr6.lt) goto loc_82626998;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x82626990
	if (!cr6.lt) goto loc_82626990;
loc_826268F8:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82626924
	if (cr6.lt) goto loc_82626924;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826268f8
	if (cr6.eq) goto loc_826268F8;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826269d4
	goto loc_826269D4;
loc_82626924:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_82626990:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826269d4
	goto loc_826269D4;
loc_82626998:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826269A0:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826269a0
	if (cr6.lt) goto loc_826269A0;
loc_826269D4:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82626fb4
	if (!cr6.eq) goto loc_82626FB4;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r29,108(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// stbx r30,r27,r15
	PPC_STORE_U8(r27.u32 + r15.u32, r30.u8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82626a40
	if (cr6.eq) goto loc_82626A40;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82626A40:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82626a90
	if (cr6.eq) goto loc_82626A90;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82626A90:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82626ae0
	if (cr6.eq) goto loc_82626AE0;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82626AE0:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82626b30
	if (cr6.eq) goto loc_82626B30;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82626fac
	if (!cr6.eq) goto loc_82626FAC;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_82626B1C:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_82626B24:
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82626B30:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82626b6c
	if (!cr6.eq) goto loc_82626B6C;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// li r11,64
	r11.s64 = 64;
loc_82626B50:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x82626b50
	if (!cr6.eq) goto loc_82626B50;
loc_82626B6C:
	// mr r20,r22
	r20.u64 = r22.u64;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bge cr6,0x82626b88
	if (!cr6.lt) goto loc_82626B88;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// li r29,16
	r29.s64 = 16;
	// b 0x82626ba4
	goto loc_82626BA4;
loc_82626B88:
	// bne cr6,0x82626b98
	if (!cr6.eq) goto loc_82626B98;
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// b 0x82626ba0
	goto loc_82626BA0;
loc_82626B98:
	// addi r10,r1,320
	ctx.r10.s64 = ctx.r1.s64 + 320;
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
loc_82626BA0:
	// li r29,8
	r29.s64 = 8;
loc_82626BA4:
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// addi r5,r10,2
	ctx.r5.s64 = ctx.r10.s64 + 2;
	// li r3,8
	ctx.r3.s64 = 8;
loc_82626BB8:
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// li r6,2
	ctx.r6.s64 = 2;
loc_82626BC4:
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lbzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82626be4
	if (!cr6.lt) goto loc_82626BE4;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// b 0x82626bf0
	goto loc_82626BF0;
loc_82626BE4:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82626bf0
	if (!cr6.gt) goto loc_82626BF0;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82626BF0:
	// stb r10,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r10.u8);
	// lhz r28,2(r9)
	r28.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lbz r8,-1(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + -1);
	// extsh r10,r28
	ctx.r10.s64 = r28.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r28,96(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// stb r28,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r28.u8);
	// bge cr6,0x82626c1c
	if (!cr6.lt) goto loc_82626C1C;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// b 0x82626c28
	goto loc_82626C28;
loc_82626C1C:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82626c28
	if (!cr6.gt) goto loc_82626C28;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82626C28:
	// stb r10,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r10.u8);
	// lhz r28,4(r9)
	r28.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// extsh r10,r28
	ctx.r10.s64 = r28.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r28,96(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// stb r28,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r28.u8);
	// bge cr6,0x82626c54
	if (!cr6.lt) goto loc_82626C54;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// b 0x82626c60
	goto loc_82626C60;
loc_82626C54:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82626c60
	if (!cr6.gt) goto loc_82626C60;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82626C60:
	// stb r10,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r10.u8);
	// lhz r28,6(r9)
	r28.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lbz r8,1(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// extsh r10,r28
	ctx.r10.s64 = r28.s16;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r28,96(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + 96);
	// stb r28,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r28.u8);
	// bge cr6,0x82626c8c
	if (!cr6.lt) goto loc_82626C8C;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// b 0x82626c98
	goto loc_82626C98;
loc_82626C8C:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82626c98
	if (!cr6.gt) goto loc_82626C98;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82626C98:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// stb r10,3(r11)
	PPC_STORE_U8(r11.u32 + 3, ctx.r10.u8);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x82626bc4
	if (!cr6.eq) goto loc_82626BC4;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// add r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 + r29.u64;
	// add r4,r4,r29
	ctx.r4.u64 = ctx.r4.u64 + r29.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82626bb8
	if (!cr6.eq) goto loc_82626BB8;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// b 0x82626d64
	goto loc_82626D64;
loc_82626CD0:
	// add r11,r27,r16
	r11.u64 = r27.u64 + r16.u64;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// stb r22,6(r11)
	PPC_STORE_U8(r11.u32 + 6, r22.u8);
	// bge cr6,0x82626cf0
	if (!cr6.lt) goto loc_82626CF0;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// b 0x82626d0c
	goto loc_82626D0C;
loc_82626CF0:
	// bne cr6,0x82626d00
	if (!cr6.eq) goto loc_82626D00;
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// b 0x82626d08
	goto loc_82626D08;
loc_82626D00:
	// addi r10,r1,320
	ctx.r10.s64 = ctx.r1.s64 + 320;
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
loc_82626D08:
	// li r5,8
	ctx.r5.s64 = 8;
loc_82626D0C:
	// subf r6,r11,r10
	ctx.r6.s64 = ctx.r10.s64 - r11.s64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// li r7,8
	ctx.r7.s64 = 8;
loc_82626D18:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// li r9,8
	ctx.r9.s64 = 8;
loc_82626D20:
	// lbzx r11,r6,r10
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82626d34
	if (!cr6.lt) goto loc_82626D34;
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x82626d40
	goto loc_82626D40;
loc_82626D34:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82626d40
	if (!cr6.gt) goto loc_82626D40;
	// li r11,255
	r11.s64 = 255;
loc_82626D40:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82626d20
	if (!cr6.eq) goto loc_82626D20;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82626d18
	if (!cr6.eq) goto loc_82626D18;
loc_82626D64:
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bge cr6,0x82626d88
	if (!cr6.lt) goto loc_82626D88;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// bne cr6,0x82626d80
	if (!cr6.eq) goto loc_82626D80;
	// addi r25,r25,120
	r25.s64 = r25.s64 + 120;
	// addi r23,r23,120
	r23.s64 = r23.s64 + 120;
	// b 0x82626d88
	goto loc_82626D88;
loc_82626D80:
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
loc_82626D88:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpwi cr6,r27,6
	cr6.compare<int32_t>(r27.s32, 6, xer);
	// blt cr6,0x826262d0
	if (cr6.lt) goto loc_826262D0;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// addi r10,r1,1025
	ctx.r10.s64 = ctx.r1.s64 + 1025;
loc_82626D9C:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_82626DA0:
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// lbz r7,-1(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r4,2(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stbx r7,r8,r21
	PPC_STORE_U8(ctx.r8.u32 + r21.u32, ctx.r7.u8);
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 + r21.u64;
	// stb r6,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r6.u8);
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 + r21.u64;
	// stb r5,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, ctx.r5.u8);
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// add r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 + r21.u64;
	// cmpwi cr6,r11,16
	cr6.compare<int32_t>(r11.s32, 16, xer);
	// stb r4,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, ctx.r4.u8);
	// blt cr6,0x82626da0
	if (cr6.lt) goto loc_82626DA0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r9,16
	cr6.compare<int32_t>(ctx.r9.s32, 16, xer);
	// blt cr6,0x82626d9c
	if (cr6.lt) goto loc_82626D9C;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// mr r11,r22
	r11.u64 = r22.u64;
loc_82626E20:
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r7,r1,192
	ctx.r7.s64 = ctx.r1.s64 + 192;
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lbzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// lbzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// stbx r9,r8,r18
	PPC_STORE_U8(ctx.r8.u32 + r18.u32, ctx.r9.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r8,r1,129
	ctx.r8.s64 = ctx.r1.s64 + 129;
	// addi r6,r1,193
	ctx.r6.s64 = ctx.r1.s64 + 193;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lbzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// stbx r7,r9,r17
	PPC_STORE_U8(ctx.r9.u32 + r17.u32, ctx.r7.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lbzx r7,r11,r6
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// addi r6,r1,130
	ctx.r6.s64 = ctx.r1.s64 + 130;
	// addi r5,r1,194
	ctx.r5.s64 = ctx.r1.s64 + 194;
	// addi r4,r1,131
	ctx.r4.s64 = ctx.r1.s64 + 131;
	// addi r3,r1,195
	ctx.r3.s64 = ctx.r1.s64 + 195;
	// addi r30,r1,132
	r30.s64 = ctx.r1.s64 + 132;
	// stb r8,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r8.u8);
	// addi r8,r1,196
	ctx.r8.s64 = ctx.r1.s64 + 196;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r29,r1,133
	r29.s64 = ctx.r1.s64 + 133;
	// lbzx r6,r11,r6
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + ctx.r6.u32);
	// addi r28,r1,197
	r28.s64 = ctx.r1.s64 + 197;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lbzx r5,r11,r5
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + ctx.r5.u32);
	// lbzx r4,r11,r4
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + ctx.r4.u32);
	// lbzx r3,r11,r3
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + ctx.r3.u32);
	// lbzx r30,r11,r30
	r30.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// lbzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// lbzx r29,r11,r29
	r29.u64 = PPC_LOAD_U8(r11.u32 + r29.u32);
	// lbzx r28,r11,r28
	r28.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// addi r27,r1,134
	r27.s64 = ctx.r1.s64 + 134;
	// addi r26,r1,198
	r26.s64 = ctx.r1.s64 + 198;
	// stb r7,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r7.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lbzx r7,r11,r27
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lbzx r27,r11,r26
	r27.u64 = PPC_LOAD_U8(r11.u32 + r26.u32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// stb r6,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r6.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// stb r5,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r5.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// stb r4,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r4.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// stb r3,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r3.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// stb r30,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r30.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// stb r8,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, ctx.r8.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// stb r29,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, r29.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// stb r28,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, r28.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// stb r7,6(r9)
	PPC_STORE_U8(ctx.r9.u32 + 6, ctx.r7.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// stb r27,6(r9)
	PPC_STORE_U8(ctx.r9.u32 + 6, r27.u8);
	// addi r8,r1,135
	ctx.r8.s64 = ctx.r1.s64 + 135;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r7,r1,199
	ctx.r7.s64 = ctx.r1.s64 + 199;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lbzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// lbzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + ctx.r7.u32);
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// stb r8,7(r9)
	PPC_STORE_U8(ctx.r9.u32 + 7, ctx.r8.u8);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r7,7(r9)
	PPC_STORE_U8(ctx.r9.u32 + 7, ctx.r7.u8);
	// blt cr6,0x82626e20
	if (cr6.lt) goto loc_82626E20;
	// li r3,0
	ctx.r3.s64 = 0;
loc_82626FAC:
	// addi r1,r1,1440
	ctx.r1.s64 = ctx.r1.s64 + 1440;
	// b 0x8239bd10
	return;
loc_82626FB4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,1440
	ctx.r1.s64 = ctx.r1.s64 + 1440;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82626FC0"))) PPC_WEAK_FUNC(sub_82626FC0);
PPC_FUNC_IMPL(__imp__sub_82626FC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// lis r12,-4289
	r12.s64 = -281083904;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// ori r12,r12,63743
	r12.u64 = r12.u64 | 63743;
	// li r23,0
	r23.s64 = 0;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r30,r23
	r30.u64 = r23.u64;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r11,248(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 248);
	// lwz r10,252(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stb r11,4(r26)
	PPC_STORE_U8(r26.u32 + 4, r11.u8);
	// blt cr6,0x826277dc
	if (cr6.lt) goto loc_826277DC;
	// cmplwi cr6,r10,62
	cr6.compare<uint32_t>(ctx.r10.u32, 62, xer);
	// bgt cr6,0x826277dc
	if (cr6.gt) goto loc_826277DC;
	// stb r23,6(r26)
	PPC_STORE_U8(r26.u32 + 6, r23.u8);
	// stb r23,7(r26)
	PPC_STORE_U8(r26.u32 + 7, r23.u8);
	// stb r23,8(r26)
	PPC_STORE_U8(r26.u32 + 8, r23.u8);
	// stb r23,9(r26)
	PPC_STORE_U8(r26.u32 + 9, r23.u8);
	// stb r23,10(r26)
	PPC_STORE_U8(r26.u32 + 10, r23.u8);
	// stb r23,11(r26)
	PPC_STORE_U8(r26.u32 + 11, r23.u8);
	// lwz r11,14804(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14804);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82627080
	if (!cr6.eq) goto loc_82627080;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82627074
	if (!cr0.lt) goto loc_82627074;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82627074:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r11,r31,5,24,26
	r11.u64 = (__builtin_rotateleft32(r31.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_82627080:
	// lwz r11,344(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 344);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826270c4
	if (!cr6.eq) goto loc_826270C4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826270b8
	if (!cr0.lt) goto loc_826270B8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826270B8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_826270C4:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r10,r11,0,24,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r10,32
	cr6.compare<uint32_t>(ctx.r10.u32, 32, xer);
	// bne cr6,0x826270e8
	if (!cr6.eq) goto loc_826270E8;
	// lwz r11,352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// ori r10,r10,8
	ctx.r10.u64 = ctx.r10.u64 | 8;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x826271d8
	goto loc_826271D8;
loc_826270E8:
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82627108
	if (!cr6.eq) goto loc_82627108;
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r6,352(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r4,2376(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 2376);
	// bl 0x82654158
	sub_82654158(ctx, base);
loc_82627108:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r10,r11,0,24,26
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826271d8
	if (!cr6.eq) goto loc_826271D8;
	// lwz r10,352(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82627138
	if (cr6.eq) goto loc_82627138;
	// li r10,3
	ctx.r10.s64 = 3;
	// rlwimi r11,r10,5,24,26
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x826271d4
	goto loc_826271D4;
loc_82627138:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82627164
	if (!cr0.lt) goto loc_82627164;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82627164:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82627180
	if (!cr6.eq) goto loc_82627180;
	// lwz r11,14780(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14780);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// b 0x826271d8
	goto loc_826271D8;
loc_82627180:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826271ac
	if (!cr0.lt) goto loc_826271AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826271AC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x826271c8
	if (!cr6.eq) goto loc_826271C8;
	// lwz r11,14784(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 14784);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r10,r11,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// b 0x826271d8
	goto loc_826271D8;
loc_826271C8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwimi r11,r10,6,24,26
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 6) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
loc_826271D4:
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_826271D8:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262725c
	if (cr6.eq) goto loc_8262725C;
	// stw r23,12(r26)
	PPC_STORE_U32(r26.u32 + 12, r23.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// sth r23,16(r26)
	PPC_STORE_U16(r26.u32 + 16, r23.u16);
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_8262725C:
	// lwz r11,352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r8,r9,0,28,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82627300
	if (!cr6.eq) goto loc_82627300;
	// oris r11,r10,16384
	r11.u64 = ctx.r10.u64 | 1073741824;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r11,280(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826272ac
	if (cr6.eq) goto loc_826272AC;
	// lwz r11,352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826272ac
	if (cr6.eq) goto loc_826272AC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826277e0
	if (!cr6.eq) goto loc_826277E0;
loc_826272AC:
	// lwz r11,352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262775c
	if (cr6.eq) goto loc_8262775C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826272ec
	if (!cr0.lt) goto loc_826272EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826272EC:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// b 0x8262775c
	goto loc_8262775C;
loc_82627300:
	// rlwinm r10,r10,0,24,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bne cr6,0x82627368
	if (!cr6.eq) goto loc_82627368;
	// addi r6,r11,4
	ctx.r6.s64 = r11.s64 + 4;
	// lwz r4,2376(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 2376);
	// li r5,8
	ctx.r5.s64 = 8;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82654158
	sub_82654158(ctx, base);
	// lwz r11,352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 352);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r9,r10,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8262735c
	if (!cr6.eq) goto loc_8262735C;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r9,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262735c
	if (!cr6.eq) goto loc_8262735C;
	// rlwinm r11,r10,0,28,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82627368
	if (!cr6.eq) goto loc_82627368;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// b 0x82627758
	goto loc_82627758;
loc_8262735C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82627368:
	// lwz r11,328(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 328);
	// lwz r28,392(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82627388
	if (cr6.eq) goto loc_82627388;
	// rlwinm r11,r9,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
	// li r24,1
	r24.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262738c
	if (cr6.eq) goto loc_8262738C;
loc_82627388:
	// mr r24,r23
	r24.u64 = r23.u64;
loc_8262738C:
	// rlwinm r11,r9,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826273d4
	if (cr6.eq) goto loc_826273D4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826273c4
	if (!cr0.lt) goto loc_826273C4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826273C4:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
loc_826273D4:
	// lwz r11,2140(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2140);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// lis r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ori r25,r10,32768
	r25.u64 = ctx.r10.u64 | 32768;
	// blt cr6,0x826274d0
	if (cr6.lt) goto loc_826274D0;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x826274c8
	if (!cr6.lt) goto loc_826274C8;
loc_82627430:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262745c
	if (cr6.lt) goto loc_8262745C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82627430
	if (cr6.eq) goto loc_82627430;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8262750c
	goto loc_8262750C;
loc_8262745C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826274C8:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8262750c
	goto loc_8262750C;
loc_826274D0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826274D8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x826274d8
	if (cr6.lt) goto loc_826274D8;
loc_8262750C:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826277dc
	if (!cr6.eq) goto loc_826277DC;
	// lwz r11,280(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262753c
	if (cr6.eq) goto loc_8262753C;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262735c
	if (!cr6.eq) goto loc_8262735C;
loc_8262753C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// rlwinm r11,r11,0,2,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// beq cr6,0x826275c4
	if (cr6.eq) goto loc_826275C4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262757c
	if (!cr0.lt) goto loc_8262757C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262757C:
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826275b8
	if (cr6.eq) goto loc_826275B8;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826275b4
	if (!cr0.lt) goto loc_826275B4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826275B4:
	// add r11,r31,r29
	r11.u64 = r31.u64 + r29.u64;
loc_826275B8:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r10,r11,22,8,9
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r10.u64 & 0xFFFFFFFFFF3FFFFF);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
loc_826275C4:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x8262775c
	if (cr6.eq) goto loc_8262775C;
	// lwz r11,2516(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2516);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826276c0
	if (cr6.lt) goto loc_826276C0;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826276b8
	if (!cr6.lt) goto loc_826276B8;
loc_82627620:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262764c
	if (cr6.lt) goto loc_8262764C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82627620
	if (cr6.eq) goto loc_82627620;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826276fc
	goto loc_826276FC;
loc_8262764C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826276B8:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826276fc
	goto loc_826276FC;
loc_826276C0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_826276C8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r25
	r11.u64 = r29.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826276c8
	if (cr6.lt) goto loc_826276C8;
loc_826276FC:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826277dc
	if (!cr6.eq) goto loc_826277DC;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x8262771c
	if (cr6.lt) goto loc_8262771C;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
loc_8262771C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// addi r9,r9,14848
	ctx.r9.s64 = ctx.r9.s64 + 14848;
	// addi r8,r9,-64
	ctx.r8.s64 = ctx.r9.s64 + -64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// rlwimi r7,r11,24,5,7
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r7.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r7,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r7.u32);
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
loc_82627758:
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_8262775C:
	// clrlwi r5,r30,31
	ctx.r5.u64 = r30.u32 & 0x1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// srawi r31,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r31.s64 = r30.s32 >> 1;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// clrlwi r5,r31,31
	ctx.r5.u64 = r31.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// clrlwi r5,r31,31
	ctx.r5.u64 = r31.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// clrlwi r5,r31,31
	ctx.r5.u64 = r31.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// clrlwi r5,r31,31
	ctx.r5.u64 = r31.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// srawi r11,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r11.s64 = r31.s32 >> 1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = r11.u32 & 0x1;
	// bl 0x82639288
	sub_82639288(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_826277DC:
	// li r3,4
	ctx.r3.s64 = 4;
loc_826277E0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826277E8"))) PPC_WEAK_FUNC(sub_826277E8);
PPC_FUNC_IMPL(__imp__sub_826277E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,3692(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3692);
	// lwz r11,3688(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// lwz r8,224(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// lwz r9,220(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r10,3688(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3688, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3692(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3692, r11.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,3720(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3720, ctx.r7.u32);
	// rotlwi r7,r7,0
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r6,3724(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3724, ctx.r6.u32);
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,3724(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3724);
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,3728(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3728, ctx.r6.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,3728(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3728);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rotlwi r10,r6,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// stw r6,3760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3760, ctx.r6.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r6,3764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3764, ctx.r6.u32);
	// rotlwi r6,r6,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r7,3800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3800, ctx.r7.u32);
	// stw r5,3804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3804, ctx.r5.u32);
	// stw r8,3808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3808, ctx.r8.u32);
	// stw r6,14764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14764, ctx.r6.u32);
	// stw r11,3768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3768, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r10,14760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14760, ctx.r10.u32);
	// stw r11,14768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14768, r11.u32);
	// add r11,r9,r10
	r11.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r11,3772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3772, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8262787C"))) PPC_WEAK_FUNC(sub_8262787C);
PPC_FUNC_IMPL(__imp__sub_8262787C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82627880"))) PPC_WEAK_FUNC(sub_82627880);
PPC_FUNC_IMPL(__imp__sub_82627880) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,3692(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3692);
	// lwz r11,3696(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3696);
	// lwz r9,220(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r10,3696(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3696, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3692(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3692, r11.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r8,3732(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3732, ctx.r8.u32);
	// rotlwi r8,r8,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r7,3736(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3736, ctx.r7.u32);
	// rotlwi r7,r7,0
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,3740(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3740, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,3740(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3740);
	// stw r10,3760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3760, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r5,3764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3764, ctx.r5.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r7,14764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14764, ctx.r7.u32);
	// stw r10,3772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3772, ctx.r10.u32);
	// stw r6,14768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14768, ctx.r6.u32);
	// stw r8,14760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14760, ctx.r8.u32);
	// stw r11,3768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3768, r11.u32);
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r11,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826278F8"))) PPC_WEAK_FUNC(sub_826278F8);
PPC_FUNC_IMPL(__imp__sub_826278F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-752(r1)
	ea = -752 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// addi r11,r1,252
	r11.s64 = ctx.r1.s64 + 252;
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// addi r7,r1,308
	ctx.r7.s64 = ctx.r1.s64 + 308;
	// lwz r27,3720(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// addi r6,r1,328
	ctx.r6.s64 = ctx.r1.s64 + 328;
	// lwz r28,220(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// addi r5,r1,348
	ctx.r5.s64 = ctx.r1.s64 + 348;
	// lwz r9,228(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r4,r1,368
	ctx.r4.s64 = ctx.r1.s64 + 368;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// stw r11,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r11.u32);
	// stw r30,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, r30.u32);
	// addi r3,r1,388
	ctx.r3.s64 = ctx.r1.s64 + 388;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// li r16,1
	r16.s64 = 1;
	// lwz r25,3724(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// mr r17,r30
	r17.u64 = r30.u64;
	// stw r9,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, ctx.r9.u32);
	// mr r19,r16
	r19.u64 = r16.u64;
	// stw r28,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r28.u32);
	// addi r29,r1,408
	r29.s64 = ctx.r1.s64 + 408;
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// addi r8,r1,236
	ctx.r8.s64 = ctx.r1.s64 + 236;
	// lwz r10,232(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// stw r30,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r30.u32);
	// lwz r26,3728(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r27,3764(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3764);
	// stw r8,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r8.u32);
	// add r8,r11,r25
	ctx.r8.u64 = r11.u64 + r25.u64;
	// stw r10,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r10.u32);
	// lwz r28,3768(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3768);
	// stw r30,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r30.u32);
	// stw r30,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r30.u32);
	// stw r8,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, ctx.r8.u32);
	// addi r8,r1,248
	ctx.r8.s64 = ctx.r1.s64 + 248;
	// stw r30,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r30.u32);
	// stw r30,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r30.u32);
	// stw r10,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r10.u32);
	// stw r30,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r30.u32);
	// stw r8,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, ctx.r8.u32);
	// add r8,r11,r26
	ctx.r8.u64 = r11.u64 + r26.u64;
	// stw r30,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r30.u32);
	// stw r30,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r30.u32);
	// stw r30,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r30.u32);
	// stw r8,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r8.u32);
	// lwz r8,3772(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3772);
	// stw r30,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r30.u32);
	// stw r30,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, r30.u32);
	// stw r9,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, ctx.r9.u32);
	// stw r8,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, ctx.r8.u32);
	// addi r8,r1,240
	ctx.r8.s64 = ctx.r1.s64 + 240;
	// stw r8,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r8.u32);
	// addi r8,r1,228
	ctx.r8.s64 = ctx.r1.s64 + 228;
	// stw r30,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r30.u32);
	// stw r30,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, r30.u32);
	// stw r10,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r10.u32);
	// stw r8,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, ctx.r8.u32);
	// add r8,r27,r11
	ctx.r8.u64 = r27.u64 + r11.u64;
	// stw r8,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r8.u32);
	// addi r8,r1,232
	ctx.r8.s64 = ctx.r1.s64 + 232;
	// stw r30,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r30.u32);
	// stw r30,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r30.u32);
	// stw r10,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, ctx.r10.u32);
	// stw r8,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r8.u32);
	// add r8,r28,r11
	ctx.r8.u64 = r28.u64 + r11.u64;
	// stw r8,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, ctx.r8.u32);
	// lwz r8,3756(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// stw r30,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, r30.u32);
	// stw r9,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r9.u32);
	// stw r8,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r8.u32);
	// addi r8,r1,212
	ctx.r8.s64 = ctx.r1.s64 + 212;
	// stw r8,392(r1)
	PPC_STORE_U32(ctx.r1.u32 + 392, ctx.r8.u32);
	// lwz r27,3736(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// addi r9,r1,216
	ctx.r9.s64 = ctx.r1.s64 + 216;
	// stw r30,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r30.u32);
	// addi r7,r1,428
	ctx.r7.s64 = ctx.r1.s64 + 428;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// lwz r29,3740(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// stw r10,424(r1)
	PPC_STORE_U32(ctx.r1.u32 + 424, ctx.r10.u32);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stw r30,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r30.u32);
	// stw r9,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r9.u32);
	// addi r6,r1,448
	ctx.r6.s64 = ctx.r1.s64 + 448;
	// addi r5,r1,468
	ctx.r5.s64 = ctx.r1.s64 + 468;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r27,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, r27.u32);
	// addi r4,r1,488
	ctx.r4.s64 = ctx.r1.s64 + 488;
	// stw r30,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r30.u32);
	// addi r7,r1,220
	ctx.r7.s64 = ctx.r1.s64 + 220;
	// stw r11,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, r11.u32);
	// lwz r11,268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, ctx.r10.u32);
	// addi r3,r1,528
	ctx.r3.s64 = ctx.r1.s64 + 528;
	// stw r30,440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 440, r30.u32);
	// addi r28,r1,584
	r28.s64 = ctx.r1.s64 + 584;
	// lwz r10,276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// stw r7,432(r1)
	PPC_STORE_U32(ctx.r1.u32 + 432, ctx.r7.u32);
	// li r7,4
	ctx.r7.s64 = 4;
	// stw r30,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r30.u32);
	// stw r11,456(r1)
	PPC_STORE_U32(ctx.r1.u32 + 456, r11.u32);
	// addi r11,r1,184
	r11.s64 = ctx.r1.s64 + 184;
	// stw r30,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, r30.u32);
	// lwz r8,2928(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// addi r26,r8,726
	r26.s64 = ctx.r8.s64 + 726;
	// stw r11,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, r11.u32);
	// li r11,20
	r11.s64 = 20;
	// stw r11,460(r1)
	PPC_STORE_U32(ctx.r1.u32 + 460, r11.u32);
	// stw r30,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r30.u32);
	// stw r10,476(r1)
	PPC_STORE_U32(ctx.r1.u32 + 476, ctx.r10.u32);
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stw r11,480(r1)
	PPC_STORE_U32(ctx.r1.u32 + 480, r11.u32);
	// lwz r11,14808(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// stw r30,484(r1)
	PPC_STORE_U32(ctx.r1.u32 + 484, r30.u32);
	// stw r10,472(r1)
	PPC_STORE_U32(ctx.r1.u32 + 472, ctx.r10.u32);
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r30,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r30.u32);
	// stw r11,496(r1)
	PPC_STORE_U32(ctx.r1.u32 + 496, r11.u32);
	// lwz r11,3048(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// stw r7,500(r1)
	PPC_STORE_U32(ctx.r1.u32 + 500, ctx.r7.u32);
	// stw r30,504(r1)
	PPC_STORE_U32(ctx.r1.u32 + 504, r30.u32);
	// stw r30,524(r1)
	PPC_STORE_U32(ctx.r1.u32 + 524, r30.u32);
	// stw r9,508(r1)
	PPC_STORE_U32(ctx.r1.u32 + 508, ctx.r9.u32);
	// stw r11,516(r1)
	PPC_STORE_U32(ctx.r1.u32 + 516, r11.u32);
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stw r11,492(r1)
	PPC_STORE_U32(ctx.r1.u32 + 492, r11.u32);
	// addi r11,r1,204
	r11.s64 = ctx.r1.s64 + 204;
	// stw r11,512(r1)
	PPC_STORE_U32(ctx.r1.u32 + 512, r11.u32);
	// li r11,8
	r11.s64 = 8;
	// stw r11,520(r1)
	PPC_STORE_U32(ctx.r1.u32 + 520, r11.u32);
	// lwz r11,1892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// stw r30,544(r1)
	PPC_STORE_U32(ctx.r1.u32 + 544, r30.u32);
	// stw r9,548(r1)
	PPC_STORE_U32(ctx.r1.u32 + 548, ctx.r9.u32);
	// stw r30,564(r1)
	PPC_STORE_U32(ctx.r1.u32 + 564, r30.u32);
	// stw r11,536(r1)
	PPC_STORE_U32(ctx.r1.u32 + 536, r11.u32);
	// lwz r11,1896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// stw r9,568(r1)
	PPC_STORE_U32(ctx.r1.u32 + 568, ctx.r9.u32);
	// stw r11,556(r1)
	PPC_STORE_U32(ctx.r1.u32 + 556, r11.u32);
	// addi r11,r1,200
	r11.s64 = ctx.r1.s64 + 200;
	// stw r11,532(r1)
	PPC_STORE_U32(ctx.r1.u32 + 532, r11.u32);
	// li r11,192
	r11.s64 = 192;
	// stw r11,540(r1)
	PPC_STORE_U32(ctx.r1.u32 + 540, r11.u32);
	// addi r11,r1,244
	r11.s64 = ctx.r1.s64 + 244;
	// stw r11,552(r1)
	PPC_STORE_U32(ctx.r1.u32 + 552, r11.u32);
	// li r11,144
	r11.s64 = 144;
	// stw r11,560(r1)
	PPC_STORE_U32(ctx.r1.u32 + 560, r11.u32);
	// stw r30,572(r1)
	PPC_STORE_U32(ctx.r1.u32 + 572, r30.u32);
	// addi r9,r8,729
	ctx.r9.s64 = ctx.r8.s64 + 729;
	// stw r30,576(r1)
	PPC_STORE_U32(ctx.r1.u32 + 576, r30.u32);
	// stw r30,580(r1)
	PPC_STORE_U32(ctx.r1.u32 + 580, r30.u32);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// std r30,0(r28)
	PPC_STORE_U64(r28.u32 + 0, r30.u64);
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// lwz r11,2088(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// stw r10,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r10.u32);
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// lwz r9,3960(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r9,3
	cr6.compare<int32_t>(ctx.r9.s32, 3, xer);
	// stw r10,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r10.u32);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,263
	r11.s64 = r11.s64 + 263;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,2100(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 2100);
	// stw r10,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, ctx.r10.u32);
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// bne cr6,0x82627bbc
	if (!cr6.eq) goto loc_82627BBC;
	// stw r30,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r30.u32);
	// b 0x82627bc0
	goto loc_82627BC0;
loc_82627BBC:
	// stw r16,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r16.u32);
loc_82627BC0:
	// lwz r11,14776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14776);
	// lwz r10,3392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3392);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,3
	r11.s64 = 3;
	// beq cr6,0x82627be8
	if (cr6.eq) goto loc_82627BE8;
	// stw r11,14780(r31)
	PPC_STORE_U32(r31.u32 + 14780, r11.u32);
	// stw r7,14784(r31)
	PPC_STORE_U32(r31.u32 + 14784, ctx.r7.u32);
	// b 0x82627bf0
	goto loc_82627BF0;
loc_82627BE8:
	// stw r7,14780(r31)
	PPC_STORE_U32(r31.u32 + 14780, ctx.r7.u32);
	// stw r11,14784(r31)
	PPC_STORE_U32(r31.u32 + 14784, r11.u32);
loc_82627BF0:
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// beq cr6,0x82627c04
	if (cr6.eq) goto loc_82627C04;
	// cmpwi cr6,r9,3
	cr6.compare<int32_t>(ctx.r9.s32, 3, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// bne cr6,0x82627c08
	if (!cr6.eq) goto loc_82627C08;
loc_82627C04:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_82627C08:
	// lwz r10,1972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,76(r10)
	PPC_STORE_U32(ctx.r10.u32 + 76, r11.u32);
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b9d0
	sub_8265B9D0(ctx, base);
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bge cr6,0x82627c3c
	if (!cr6.lt) goto loc_82627C3C;
	// addi r11,r31,2464
	r11.s64 = r31.s64 + 2464;
	// addi r10,r31,2480
	ctx.r10.s64 = r31.s64 + 2480;
	// addi r9,r31,2520
	ctx.r9.s64 = r31.s64 + 2520;
	// b 0x82627c60
	goto loc_82627C60;
loc_82627C3C:
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bge cr6,0x82627c54
	if (!cr6.lt) goto loc_82627C54;
	// addi r11,r31,2452
	r11.s64 = r31.s64 + 2452;
	// addi r10,r31,2492
	ctx.r10.s64 = r31.s64 + 2492;
	// addi r9,r31,2532
	ctx.r9.s64 = r31.s64 + 2532;
	// b 0x82627c60
	goto loc_82627C60;
loc_82627C54:
	// addi r11,r31,2440
	r11.s64 = r31.s64 + 2440;
	// addi r10,r31,2504
	ctx.r10.s64 = r31.s64 + 2504;
	// addi r9,r31,2544
	ctx.r9.s64 = r31.s64 + 2544;
loc_82627C60:
	// lwz r28,3772(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3772);
	// stw r10,2516(r31)
	PPC_STORE_U32(r31.u32 + 2516, ctx.r10.u32);
	// stw r11,2476(r31)
	PPC_STORE_U32(r31.u32 + 2476, r11.u32);
	// stw r9,2556(r31)
	PPC_STORE_U32(r31.u32 + 2556, ctx.r9.u32);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r5,3728(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// add r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lwz r6,3764(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3764);
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// lwz r7,3768(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3768);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// lwz r8,3736(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lwz r10,3740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// stw r28,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r28.u32);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lwz r28,3756(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r29,15900(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15900);
	// stw r3,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r3.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r4,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r4.u32);
	// stw r5,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r5.u32);
	// stw r28,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r28.u32);
	// stw r6,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r6.u32);
	// stw r7,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r7.u32);
	// stw r8,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r8.u32);
	// stw r10,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r10.u32);
	// beq cr6,0x82627d24
	if (cr6.eq) goto loc_82627D24;
	// lwz r10,15904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15904);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82627d24
	if (cr6.eq) goto loc_82627D24;
	// lwz r10,15908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15908);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x82627d24
	if (!cr6.eq) goto loc_82627D24;
	// lwz r10,15912(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15912);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r9.u32);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// stw r9,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r9.u32);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
loc_82627D24:
	// lwz r9,268(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r9,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r9.u32);
	// lwz r9,276(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// stw r10,340(r31)
	PPC_STORE_U32(r31.u32 + 340, ctx.r10.u32);
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// lwz r9,14808(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// lwz r9,3048(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3048);
	// stw r9,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r9.u32);
	// lwz r9,1892(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// stw r9,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r9.u32);
	// lwz r9,1896(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// stw r9,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r9.u32);
	// addi r9,r11,-3
	ctx.r9.s64 = r11.s64 + -3;
	// cntlzw r10,r9
	ctx.r10.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// stw r10,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r10.u32);
	// beq cr6,0x82627d88
	if (cr6.eq) goto loc_82627D88;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// bne cr6,0x82627d8c
	if (!cr6.eq) goto loc_82627D8C;
loc_82627D88:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_82627D8C:
	// lwz r10,1972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,76(r10)
	PPC_STORE_U32(ctx.r10.u32 + 76, r11.u32);
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b9d0
	sub_8265B9D0(ctx, base);
	// lwz r11,144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r5,r11,6,0,25
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r11,r30
	r11.u64 = r30.u64;
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r30,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r30.u32);
	// stw r30,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r30.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// ble cr6,0x82628900
	if (!cr6.gt) goto loc_82628900;
	// li r14,2
	r14.s64 = 2;
	// li r15,128
	r15.s64 = 128;
	// li r18,16384
	r18.s64 = 16384;
loc_82627DE8:
	// lwz r23,252(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r22,236(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r21,248(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// beq cr6,0x82627e14
	if (cr6.eq) goto loc_82627E14;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r20,r30
	r20.u64 = r30.u64;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82627e18
	if (cr6.eq) goto loc_82627E18;
loc_82627E14:
	// mr r20,r16
	r20.u64 = r16.u64;
loc_82627E18:
	// lwz r10,340(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 340);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r10,340(r31)
	PPC_STORE_U32(r31.u32 + 340, ctx.r10.u32);
	// bne cr6,0x82627e48
	if (!cr6.eq) goto loc_82627E48;
	// lwz r10,1892(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1892);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// lwz r10,1896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1896);
	// stw r10,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r10.u32);
	// lwz r10,14808(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// stw r10,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r10.u32);
loc_82627E48:
	// lwz r10,21236(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82627fbc
	if (cr6.eq) goto loc_82627FBC;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82627fbc
	if (cr6.eq) goto loc_82627FBC;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// lwz r29,84(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// lwz r11,28(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82627f00
	if (cr6.eq) goto loc_82627F00;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r28,r16
	r28.u64 = r16.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82627ed8
	if (!cr6.lt) goto loc_82627ED8;
loc_82627E98:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82627ed8
	if (cr6.eq) goto loc_82627ED8;
	// ld r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
	// bge 0x82627ec8
	if (!cr0.lt) goto loc_82627EC8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82627EC8:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x82627e98
	if (cr6.gt) goto loc_82627E98;
loc_82627ED8:
	// ld r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// clrldi r10,r28,32
	ctx.r10.u64 = r28.u64 & 0xFFFFFFFF;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// subf. r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r10.u64);
	// bge 0x82627f00
	if (!cr0.lt) goto loc_82627F00;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82627F00:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,188(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r16,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r16.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82627f94
	if (cr6.eq) goto loc_82627F94;
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stw r30,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r30.u32);
	// addi r9,r1,180
	ctx.r9.s64 = ctx.r1.s64 + 180;
	// stw r30,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r30.u32);
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// stw r14,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r14.u32);
	// addi r7,r1,188
	ctx.r7.s64 = ctx.r1.s64 + 188;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82628924
	if (!cr6.eq) goto loc_82628924;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x82627f74
	if (cr6.eq) goto loc_82627F74;
	// cmpwi cr6,r17,4
	cr6.compare<int32_t>(r17.s32, 4, xer);
	// bne cr6,0x82627f78
	if (!cr6.eq) goto loc_82627F78;
loc_82627F74:
	// mr r17,r29
	r17.u64 = r29.u64;
loc_82627F78:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x82628900
	if (cr6.eq) goto loc_82628900;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r19,r30
	r19.u64 = r30.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x826288dc
	goto loc_826288DC;
loc_82627F94:
	// lwz r11,19976(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262819c
	if (!cr6.eq) goto loc_8262819C;
	// lwz r11,19980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262819c
	if (!cr6.eq) goto loc_8262819C;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8262819c
	if (!cr6.eq) goto loc_8262819C;
	// mr r19,r16
	r19.u64 = r16.u64;
loc_82627FBC:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r24,r30
	r24.u64 = r30.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// stw r15,2964(r31)
	PPC_STORE_U32(r31.u32 + 2964, r15.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r15,2960(r31)
	PPC_STORE_U32(r31.u32 + 2960, r15.u32);
	// stw r15,2956(r31)
	PPC_STORE_U32(r31.u32 + 2956, r15.u32);
	// ble cr6,0x82628868
	if (!cr6.gt) goto loc_82628868;
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
loc_82627FE0:
	// sth r30,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r30.u16);
	// mr r26,r30
	r26.u64 = r30.u64;
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r25,r30
	r25.u64 = r30.u64;
	// sth r30,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r30.u16);
	// lwz r11,3380(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3380);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// beq cr6,0x82628024
	if (cr6.eq) goto loc_82628024;
	// lwz r10,224(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82628024
	if (cr6.eq) goto loc_82628024;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// bne cr6,0x82628030
	if (!cr6.eq) goto loc_82628030;
loc_82628024:
	// sth r30,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r30.u16);
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// sth r30,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r30.u16);
loc_82628030:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,2,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// lwz r6,188(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r4,184(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// bl 0x82626fc0
	sub_82626FC0(ctx, base);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82628078
	if (cr6.eq) goto loc_82628078;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x826280d8
	if (!cr6.eq) goto loc_826280D8;
loc_82628078:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srawi r10,r10,15
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 15;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r10,16,1,11
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0x7FF00000;
	// rlwinm r10,r10,0,28,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFF000F;
	// srawi r9,r9,15
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 15;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// srawi r10,r10,15
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 15;
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// sth r10,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r10.u16);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r9,r10,16,1,11
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0x7FF00000;
	// rlwinm r10,r10,0,28,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFF000F;
	// srawi r9,r9,15
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 15;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_826280D8:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82628820
	if (!cr6.eq) goto loc_82628820;
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r10,r10,0,24,20
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826286f4
	if (!cr6.eq) goto loc_826286F4;
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lis r9,16384
	ctx.r9.s64 = 1073741824;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x8262814c
	if (!cr6.eq) goto loc_8262814C;
	// stb r30,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r30.u8);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stb r30,7(r11)
	PPC_STORE_U8(r11.u32 + 7, r30.u8);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stb r30,8(r11)
	PPC_STORE_U8(r11.u32 + 8, r30.u8);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stb r30,9(r11)
	PPC_STORE_U8(r11.u32 + 9, r30.u8);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stb r30,10(r11)
	PPC_STORE_U8(r11.u32 + 10, r30.u8);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stb r30,11(r11)
	PPC_STORE_U8(r11.u32 + 11, r30.u8);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
loc_8262814C:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r11,27,29,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x82628460
	if (cr6.eq) goto loc_82628460;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r29,r9,27,31,31
	r29.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// bne cr6,0x82628180
	if (!cr6.eq) goto loc_82628180;
	// li r9,3
	ctx.r9.s64 = 3;
	// rlwimi r11,r9,5,24,26
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 5) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
loc_82628180:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r11,r11,0,24,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r11,128
	cr6.compare<uint32_t>(r11.u32, 128, xer);
	// bne cr6,0x826281fc
	if (!cr6.eq) goto loc_826281FC;
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// lwz r9,1776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// b 0x82628204
	goto loc_82628204;
loc_8262819C:
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stw r30,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r30.u32);
	// addi r9,r1,180
	ctx.r9.s64 = ctx.r1.s64 + 180;
	// stw r30,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r30.u32);
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// stw r14,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r14.u32);
	// addi r7,r1,188
	ctx.r7.s64 = ctx.r1.s64 + 188;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82628930
	if (!cr6.eq) goto loc_82628930;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// bne cr6,0x826281e0
	if (!cr6.eq) goto loc_826281E0;
	// li r17,4
	r17.s64 = 4;
loc_826281E0:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// beq cr6,0x82628900
	if (cr6.eq) goto loc_82628900;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r19,r30
	r19.u64 = r30.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x826288dc
	goto loc_826288DC;
loc_826281FC:
	// lwz r11,1780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// lwz r9,1784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1784);
loc_82628204:
	// lwz r10,15472(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// blt cr6,0x82628240
	if (cr6.lt) goto loc_82628240;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lwz r7,140(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r10,r1,196
	ctx.r10.s64 = ctx.r1.s64 + 196;
	// lwz r6,136(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826514c8
	sub_826514C8(ctx, base);
	// b 0x82628274
	goto loc_82628274;
loc_82628240:
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lwz r8,140(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r3,r1,196
	ctx.r3.s64 = ctx.r1.s64 + 196;
	// lwz r7,136(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r20,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r20.u32);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// li r6,2
	ctx.r6.s64 = 2;
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826511e0
	sub_826511E0(ctx, base);
loc_82628274:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826282e8
	if (!cr6.eq) goto loc_826282E8;
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r11,416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 416);
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r8,424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// lhz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r11,420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 420);
	// lwz r9,208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// lwz r8,428(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 428);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// b 0x826282f8
	goto loc_826282F8;
loc_826282E8:
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r11,208(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
loc_826282F8:
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpwi cr6,r29,1
	cr6.compare<int32_t>(r29.s32, 1, xer);
	// sth r11,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r11.u16);
	// bne cr6,0x82628460
	if (!cr6.eq) goto loc_82628460;
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lhz r29,0(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwimi r9,r16,7,24,26
	ctx.r9.u64 = (__builtin_rotateleft32(r16.u32, 7) & 0xE0) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFF1F);
	// lhz r27,2(r10)
	r27.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r30,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r30.u16);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r30,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r30.u16);
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// blt cr6,0x82628374
	if (cr6.lt) goto loc_82628374;
	// addi r10,r1,196
	ctx.r10.s64 = ctx.r1.s64 + 196;
	// lwz r9,1776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// lwz r8,1772(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// lwz r7,140(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r6,136(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x826514c8
	sub_826514C8(ctx, base);
	// b 0x8262839c
	goto loc_8262839C;
loc_82628374:
	// addi r26,r1,196
	r26.s64 = ctx.r1.s64 + 196;
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// lwz r8,140(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r7,136(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// stw r20,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r20.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// bl 0x826511e0
	sub_826511E0(ctx, base);
loc_8262839C:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82628420
	if (!cr6.eq) goto loc_82628420;
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82628420
	if (cr6.eq) goto loc_82628420;
	// lhz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// lwz r11,416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 416);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r8,424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r11,420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 420);
	// lwz r9,208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// lwz r8,428(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 428);
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// b 0x82628430
	goto loc_82628430;
loc_82628420:
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r11,208(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
loc_82628430:
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r11,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r11.u16);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lhz r26,0(r11)
	r26.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r25,2(r11)
	r25.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r29,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r29.u16);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// sth r27,2(r11)
	PPC_STORE_U16(r11.u32 + 2, r27.u16);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwimi r10,r16,6,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(r16.u32, 6) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_82628460:
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// addi r7,r1,256
	ctx.r7.s64 = ctx.r1.s64 + 256;
	// lwz r10,3960(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// addi r8,r1,268
	ctx.r8.s64 = ctx.r1.s64 + 268;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r9,r10,-3
	ctx.r9.s64 = ctx.r10.s64 + -3;
	// addi r10,r1,264
	ctx.r10.s64 = ctx.r1.s64 + 264;
	// lhz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// cntlzw r6,r9
	ctx.r6.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r9,r1,260
	ctx.r9.s64 = ctx.r1.s64 + 260;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// rlwinm r6,r6,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lwz r8,188(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x826537a0
	sub_826537A0(ctx, base);
	// lwz r6,184(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r29,3064(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3064);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r11,r11,27,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x8262856c
	if (!cr6.eq) goto loc_8262856C;
	// lwz r11,204(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r30,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r30.u32);
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r10,1784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// b 0x826286e4
	goto loc_826286E4;
loc_8262856C:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// bne cr6,0x826285ec
	if (!cr6.eq) goto loc_826285EC;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// extsh r9,r25
	ctx.r9.s64 = r25.s16;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// extsh r5,r26
	ctx.r5.s64 = r26.s16;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r26,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, r26.u16);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r25,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, r25.u16);
	// b 0x826286b4
	goto loc_826286B4;
loc_826285EC:
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// stw r30,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r30.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r30.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,24,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0;
	// cmplwi cr6,r11,128
	cr6.compare<uint32_t>(r11.u32, 128, xer);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// bne cr6,0x82628698
	if (!cr6.eq) goto loc_82628698;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lhz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r9,2(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// sthx r9,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r9.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,256(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,268(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r10,1784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// b 0x826286e4
	goto loc_826286E4;
loc_82628698:
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,264(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
loc_826286B4:
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// sthx r9,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r9.u16);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r10,1784(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r9,2(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// sthx r9,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r9.u16);
loc_826286E4:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82628778
	if (cr6.eq) goto loc_82628778;
	// li r5,-2
	ctx.r5.s64 = -2;
	// b 0x8262882c
	goto loc_8262882C;
loc_826286F4:
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// lwz r11,1784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// sthx r18,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, r18.u16);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r9,1780(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r18,r11,r9
	PPC_STORE_U16(r11.u32 + ctx.r9.u32, r18.u16);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r9,1776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r18,r11,r9
	PPC_STORE_U16(r11.u32 + ctx.r9.u32, r18.u16);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r18,r11,r9
	PPC_STORE_U16(r11.u32 + ctx.r9.u32, r18.u16);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r9,188(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// rlwinm r29,r11,1,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,184(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// bl 0x8262de90
	sub_8262DE90(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82628828
	if (!cr6.eq) goto loc_82628828;
loc_82628778:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x826287a8
	if (!cr6.eq) goto loc_826287A8;
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// sth r30,160(r11)
	PPC_STORE_U16(r11.u32 + 160, r30.u16);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// sth r30,128(r11)
	PPC_STORE_U16(r11.u32 + 128, r30.u16);
	// lwz r11,200(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// sth r30,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r30.u16);
loc_826287A8:
	// lwz r9,224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// addi r23,r23,16
	r23.s64 = r23.s64 + 16;
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// addi r21,r21,8
	r21.s64 = r21.s64 + 8;
	// addi r24,r24,16
	r24.s64 = r24.s64 + 16;
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// lwz r9,204(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r9.u32);
	// lwz r9,200(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r11.u32);
	// addi r9,r9,192
	ctx.r9.s64 = ctx.r9.s64 + 192;
	// stw r9,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r9.u32);
	// lwz r9,244(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// addi r9,r9,144
	ctx.r9.s64 = ctx.r9.s64 + 144;
	// stw r9,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r9.u32);
	// lwz r9,180(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r9.u32);
	// blt cr6,0x82627fe0
	if (cr6.lt) goto loc_82627FE0;
	// b 0x82628868
	goto loc_82628868;
loc_82628820:
	// li r5,-1
	ctx.r5.s64 = -1;
	// b 0x8262882c
	goto loc_8262882C;
loc_82628828:
	// li r5,-3
	ctx.r5.s64 = -3;
loc_8262882C:
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// addi r9,r1,180
	ctx.r9.s64 = ctx.r1.s64 + 180;
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// addi r7,r1,188
	ctx.r7.s64 = ctx.r1.s64 + 188;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82628924
	if (!cr6.eq) goto loc_82628924;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x82628864
	if (cr6.eq) goto loc_82628864;
	// cmpwi cr6,r17,4
	cr6.compare<int32_t>(r17.s32, 4, xer);
	// bne cr6,0x82628868
	if (!cr6.eq) goto loc_82628868;
loc_82628864:
	// mr r17,r29
	r17.u64 = r29.u64;
loc_82628868:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// lwz r9,236(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stw r9,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r9.u32);
	// lwz r9,252(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r9,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r9.u32);
	// lwz r9,248(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stw r9,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r9.u32);
	// lwz r9,212(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r9,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r9.u32);
	// lwz r9,216(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stw r9,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r9.u32);
	// lwz r9,220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stw r9,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r9.u32);
	// lwz r9,240(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r10.u32);
	// lwz r10,228(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stw r10,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r10.u32);
	// lwz r10,232(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r11.u32);
loc_826288DC:
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r9,192(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// stw r9,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r9.u32);
	// blt cr6,0x82627de8
	if (cr6.lt) goto loc_82627DE8;
loc_82628900:
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826289cc
	if (cr6.eq) goto loc_826289CC;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x8262893c
	if (!cr6.eq) goto loc_8262893C;
	// bl 0x8262ada0
	sub_8262ADA0(ctx, base);
	// b 0x82628940
	goto loc_82628940;
loc_82628924:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
loc_82628930:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
loc_8262893C:
	// bl 0x8262acd8
	sub_8262ACD8(ctx, base);
loc_82628940:
	// lwz r29,3812(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3812);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r28,3916(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3916);
	// lwz r27,15760(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 15760);
	// lwz r26,15744(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 15744);
	// lwz r25,15728(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 15728);
	// lwz r24,15752(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 15752);
	// lwz r23,15736(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 15736);
	// lwz r22,15720(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 15720);
	// lwz r21,15712(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 15712);
	// lwz r20,15696(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 15696);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r10,15680(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15680);
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// lwz r9,15704(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15704);
	// lwz r8,15688(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15688);
	// lwz r7,15672(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15672);
	// stw r29,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r29.u32);
	// stw r30,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r30.u32);
	// stw r28,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r28.u32);
	// stw r27,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r27.u32);
	// stw r26,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r26.u32);
	// stw r25,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r25.u32);
	// stw r24,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r24.u32);
	// stw r23,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r23.u32);
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// bl 0x8261be68
	sub_8261BE68(ctx, base);
loc_826289CC:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r16,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r16.u32);
	// stw r30,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r30.u32);
	// stw r16,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r16.u32);
	// stw r16,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r16.u32);
	// addi r1,r1,752
	ctx.r1.s64 = ctx.r1.s64 + 752;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826289E8"))) PPC_WEAK_FUNC(sub_826289E8);
PPC_FUNC_IMPL(__imp__sub_826289E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r5,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r5.u32);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// stw r8,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r8.u32);
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// stw r9,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, ctx.r9.u32);
	// stw r10,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r10.u32);
	// li r27,1
	r27.s64 = 1;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82628a2c
	if (cr6.eq) goto loc_82628A2C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// li r14,0
	r14.s64 = 0;
	// bne cr6,0x82628a30
	if (!cr6.eq) goto loc_82628A30;
loc_82628A2C:
	// li r14,1
	r14.s64 = 1;
loc_82628A30:
	// lwz r11,21436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21436);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82628a44
	if (!cr6.eq) goto loc_82628A44;
	// lwz r10,19984(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// b 0x82628a48
	goto loc_82628A48;
loc_82628A44:
	// li r10,0
	ctx.r10.s64 = 0;
loc_82628A48:
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r7,3724(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r4,r8,r6
	ctx.r4.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r30,204(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + r11.u64;
	// lwz r29,208(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// lwz r8,3736(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// srawi r9,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r9.s64 = r30.s32 >> 1;
	// lwz r3,3740(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// srawi r30,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r30.s64 = r29.s32 >> 1;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r5,3756(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// mullw r10,r30,r10
	ctx.r10.s64 = int64_t(r30.s32) * int64_t(ctx.r10.s32);
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// add r20,r4,r9
	r20.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r19,r5,r9
	r19.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r18,r6,r10
	r18.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r17,r7,r10
	r17.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r16,r8,r10
	r16.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r15,r11,r10
	r15.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x82628ac0
	if (cr6.eq) goto loc_82628AC0;
loc_82628AB4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_82628AC0:
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82628ab4
	if (cr6.eq) goto loc_82628AB4;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82628f74
	if (!cr6.lt) goto loc_82628F74;
loc_82628AD8:
	// lwz r9,21236(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// mullw r23,r27,r22
	r23.s64 = int64_t(r27.s32) * int64_t(r22.s32);
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// lwz r8,232(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// mullw r11,r10,r8
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// rlwinm r7,r23,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r23,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r26,r9,r20
	r26.u64 = ctx.r9.u64 + r20.u64;
	// add r30,r9,r19
	r30.u64 = ctx.r9.u64 + r19.u64;
	// add r25,r11,r18
	r25.u64 = r11.u64 + r18.u64;
	// add r24,r11,r17
	r24.u64 = r11.u64 + r17.u64;
	// add r29,r11,r16
	r29.u64 = r11.u64 + r16.u64;
	// add r28,r11,r15
	r28.u64 = r11.u64 + r15.u64;
	// beq cr6,0x82628b4c
	if (cr6.eq) goto loc_82628B4C;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82628b4c
	if (cr6.eq) goto loc_82628B4C;
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82628da0
	if (cr6.eq) goto loc_82628DA0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bgt cr6,0x82628da0
	if (cr6.gt) goto loc_82628DA0;
loc_82628B4C:
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82628b64
	if (!cr6.eq) goto loc_82628B64;
	// lwz r11,3396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82628d54
	if (cr6.eq) goto loc_82628D54;
loc_82628B64:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// beq cr6,0x82628cf0
	if (cr6.eq) goto loc_82628CF0;
	// ld r11,3576(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 3576);
	// cmpdi cr6,r11,1
	cr6.compare<int64_t>(r11.s64, 1, xer);
	// bne cr6,0x82628cf0
	if (!cr6.eq) goto loc_82628CF0;
	// lwz r11,21436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21436);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r28,r10,1
	r28.s64 = ctx.r10.s64 + 1;
	// bne cr6,0x82628ba4
	if (!cr6.eq) goto loc_82628BA4;
	// lwz r10,21000(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21000);
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82628ba8
	if (!cr6.eq) goto loc_82628BA8;
loc_82628BA4:
	// mr r29,r28
	r29.u64 = r28.u64;
loc_82628BA8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82628ce8
	if (!cr6.eq) goto loc_82628CE8;
	// lwz r30,19984(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 19984);
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// bge cr6,0x82628d54
	if (!cr6.lt) goto loc_82628D54;
loc_82628BBC:
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r7,r30,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// mullw r10,r7,r10
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// srawi r7,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r7.s64 = r11.s32 >> 2;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mullw r9,r11,r30
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r11,r10,r26
	r11.u64 = ctx.r10.u64 + r26.u64;
	// mullw r10,r7,r29
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(r29.s32);
	// mullw r7,r8,r28
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(r28.s32);
	// mullw r6,r8,r29
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(r29.s32);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r25
	ctx.r10.u64 = ctx.r9.u64 + r25.u64;
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// add r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 + r11.u64;
	// subf r8,r6,r11
	ctx.r8.s64 = r11.s64 - ctx.r6.s64;
	// subf r6,r5,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// beq cr6,0x82628cd8
	if (cr6.eq) goto loc_82628CD8;
	// lwz r3,136(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// cmpw cr6,r4,r3
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r3.s32, xer);
	// bge cr6,0x82628cd8
	if (!cr6.lt) goto loc_82628CD8;
loc_82628C28:
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// stw r3,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r3.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r3,0(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// stw r3,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r3.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r3,136(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpw cr6,r4,r3
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r3.s32, xer);
	// blt cr6,0x82628c28
	if (cr6.lt) goto loc_82628C28;
loc_82628CD8:
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// blt cr6,0x82628bbc
	if (cr6.lt) goto loc_82628BBC;
	// b 0x82628d54
	goto loc_82628D54;
loc_82628CE8:
	// li r30,0
	r30.s64 = 0;
	// b 0x82628bbc
	goto loc_82628BBC;
loc_82628CF0:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r22,r23
	r22.u64 = r23.u64;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// bge cr6,0x82628d54
	if (!cr6.lt) goto loc_82628D54;
loc_82628D00:
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r11,3120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3120);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// addi r26,r26,16
	r26.s64 = r26.s64 + 16;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// blt cr6,0x82628d00
	if (cr6.lt) goto loc_82628D00;
loc_82628D54:
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82628d70
	if (cr6.eq) goto loc_82628D70;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_82628D70:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82628ad8
	if (cr6.lt) goto loc_82628AD8;
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,2968(r31)
	PPC_STORE_U32(r31.u32 + 2968, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_82628DA0:
	// lwz r9,21316(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21316);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r8,21256(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21256);
	// li r11,1
	r11.s64 = 1;
	// lwz r6,21272(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// li r10,4
	ctx.r10.s64 = 4;
loc_82628DB8:
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// li r9,-1
	ctx.r9.s64 = -1;
	// bgt cr6,0x82628dcc
	if (cr6.gt) goto loc_82628DCC;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82628DCC:
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x82628dec
	if (cr6.gt) goto loc_82628DEC;
	// lwz r9,21252(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// b 0x82628db8
	goto loc_82628DB8;
loc_82628DEC:
	// lwz r10,21244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82628e24
	if (!cr6.lt) goto loc_82628E24;
	// lwz r10,21252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21252);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,21244(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 21244);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
loc_82628E0C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82628e0c
	if (!cr6.eq) goto loc_82628E0C;
loc_82628E24:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r5,r7,1
	ctx.r5.s64 = ctx.r7.s64 + 1;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// addi r4,r8,-1
	ctx.r4.s64 = ctx.r8.s64 + -1;
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// bl 0x825eb668
	sub_825EB668(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
loc_82628E4C:
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// rldicl r9,r9,1,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 1) & 0x1;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82628e94
	if (!cr6.eq) goto loc_82628E94;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rldicr r9,r9,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// blt cr6,0x82628e4c
	if (cr6.lt) goto loc_82628E4C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_82628E94:
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bge cr6,0x82628ab4
	if (!cr6.lt) goto loc_82628AB4;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// beq cr6,0x82628ec0
	if (cr6.eq) goto loc_82628EC0;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_82628EC0:
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r6,r11,1
	ctx.r6.s64 = r11.s64 + 1;
	// mullw r9,r10,r6
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// bge cr6,0x82628ee0
	if (!cr6.lt) goto loc_82628EE0;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82628EE0:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82628ef0
	if (cr6.eq) goto loc_82628EF0;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_82628EF0:
	// lwz r11,316(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82628f74
	if (cr6.eq) goto loc_82628F74;
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
loc_82628F04:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// beq cr6,0x82628f48
	if (cr6.eq) goto loc_82628F48;
	// divwu r8,r9,r10
	ctx.r8.u32 = ctx.r9.u32 / ctx.r10.u32;
	// lwz r5,-4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// twllei r10,0
	// lwz r4,-12(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// b 0x82628f64
	goto loc_82628F64;
loc_82628F48:
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r5,-12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
loc_82628F64:
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r10,-12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82628f04
	if (!cr6.eq) goto loc_82628F04;
loc_82628F74:
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,2968(r31)
	PPC_STORE_U32(r31.u32 + 2968, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82628F88"))) PPC_WEAK_FUNC(sub_82628F88);
PPC_FUNC_IMPL(__imp__sub_82628F88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// bge cr6,0x82628fac
	if (!cr6.lt) goto loc_82628FAC;
	// addi r11,r3,2464
	r11.s64 = ctx.r3.s64 + 2464;
	// addi r10,r3,2480
	ctx.r10.s64 = ctx.r3.s64 + 2480;
	// addi r9,r3,2520
	ctx.r9.s64 = ctx.r3.s64 + 2520;
	// stw r11,2476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2476, r11.u32);
	// stw r10,2516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2516, ctx.r10.u32);
	// stw r9,2556(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2556, ctx.r9.u32);
	// blr 
	return;
loc_82628FAC:
	// cmpwi cr6,r4,13
	cr6.compare<int32_t>(ctx.r4.s32, 13, xer);
	// bge cr6,0x82628fd0
	if (!cr6.lt) goto loc_82628FD0;
	// addi r11,r3,2452
	r11.s64 = ctx.r3.s64 + 2452;
	// addi r10,r3,2492
	ctx.r10.s64 = ctx.r3.s64 + 2492;
	// addi r9,r3,2532
	ctx.r9.s64 = ctx.r3.s64 + 2532;
	// stw r11,2476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2476, r11.u32);
	// stw r10,2516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2516, ctx.r10.u32);
	// stw r9,2556(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2556, ctx.r9.u32);
	// blr 
	return;
loc_82628FD0:
	// addi r11,r3,2440
	r11.s64 = ctx.r3.s64 + 2440;
	// addi r10,r3,2504
	ctx.r10.s64 = ctx.r3.s64 + 2504;
	// addi r9,r3,2544
	ctx.r9.s64 = ctx.r3.s64 + 2544;
	// stw r11,2476(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2476, r11.u32);
	// stw r10,2516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2516, ctx.r10.u32);
	// stw r9,2556(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2556, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82628FEC"))) PPC_WEAK_FUNC(sub_82628FEC);
PPC_FUNC_IMPL(__imp__sub_82628FEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82628FF0"))) PPC_WEAK_FUNC(sub_82628FF0);
PPC_FUNC_IMPL(__imp__sub_82628FF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r29,1772(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mullw r9,r7,r11
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// rlwinm r28,r9,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r28,r10
	r27.u64 = r28.u64 + ctx.r10.u64;
	// lhz r8,0(r27)
	ctx.r8.u64 = PPC_LOAD_U16(r27.u32 + 0);
	// sth r8,0(r27)
	PPC_STORE_U16(r27.u32 + 0, ctx.r8.u16);
	// beq cr6,0x82629098
	if (cr6.eq) goto loc_82629098;
	// or r8,r30,r11
	ctx.r8.u64 = r30.u64 | r11.u64;
	// clrlwi r8,r8,31
	ctx.r8.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82629098
	if (cr6.eq) goto loc_82629098;
	// rlwinm r11,r11,0,16,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFE;
	// rlwinm r9,r30,0,16,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFE;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r11,r29
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// sthx r9,r28,r29
	PPC_STORE_U16(r28.u32 + r29.u32, ctx.r9.u16);
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// sth r11,0(r27)
	PPC_STORE_U16(r27.u32 + 0, r11.u16);
	// lhzx r11,r28,r29
	r11.u64 = PPC_LOAD_U16(r28.u32 + r29.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r11,r11,-16384
	r11.s64 = r11.s64 + -16384;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd3c
	return;
loc_82629098:
	// lwz r8,0(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r4,r8,0,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826290c0
	if (cr6.eq) goto loc_826290C0;
	// li r11,16384
	r11.s64 = 16384;
	// li r25,1
	r25.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// sthx r11,r28,r29
	PPC_STORE_U16(r28.u32 + r29.u32, r11.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd3c
	return;
loc_826290C0:
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82629174
	if (!cr6.eq) goto loc_82629174;
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826290fc
	if (!cr6.eq) goto loc_826290FC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826290f8
	if (cr6.eq) goto loc_826290F8;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826290fc
	if (cr6.eq) goto loc_826290FC;
loc_826290F8:
	// li r5,1
	ctx.r5.s64 = 1;
loc_826290FC:
	// lwz r10,19980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82629138
	if (cr6.eq) goto loc_82629138;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// lwz r7,1776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// lwz r6,1772(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8261cca8
	sub_8261CCA8(ctx, base);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// b 0x826291c4
	goto loc_826291C4;
loc_82629138:
	// addi r4,r1,116
	ctx.r4.s64 = ctx.r1.s64 + 116;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stw r5,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826511e0
	sub_826511E0(ctx, base);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// b 0x826291c4
	goto loc_826291C4;
loc_82629174:
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bne cr6,0x82629194
	if (!cr6.eq) goto loc_82629194;
	// add r11,r28,r29
	r11.u64 = r28.u64 + r29.u64;
	// lhz r11,-2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// lhz r11,-2(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + -2);
	// b 0x826291ac
	goto loc_826291AC;
loc_82629194:
	// subf r11,r7,r9
	r11.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r11,r29
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
loc_826291AC:
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// bne cr6,0x826291c4
	if (!cr6.eq) goto loc_826291C4;
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
loc_826291C4:
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826291d8
	if (cr6.eq) goto loc_826291D8;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x82629200
	if (!cr6.eq) goto loc_82629200;
loc_826291D8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// srawi r11,r11,15
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFF) != 0);
	r11.s64 = r11.s32 >> 15;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// sth r11,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r11.u16);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r10,r11,16,1,11
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x7FF00000;
	// rlwinm r11,r11,0,28,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFF000F;
	// srawi r10,r10,15
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 15;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_82629200:
	// lhz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r11,416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 416);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r8,424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// sthx r11,r28,r29
	PPC_STORE_U16(r28.u32 + r29.u32, r11.u16);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,420(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 420);
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r8,428(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 428);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// sth r11,0(r27)
	PPC_STORE_U16(r27.u32 + 0, r11.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8262925C"))) PPC_WEAK_FUNC(sub_8262925C);
PPC_FUNC_IMPL(__imp__sub_8262925C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82629260"))) PPC_WEAK_FUNC(sub_82629260);
PPC_FUNC_IMPL(__imp__sub_82629260) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// lwz r27,456(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,16384
	cr6.compare<uint32_t>(ctx.r9.u32, 16384, xer);
	// bne cr6,0x826292e0
	if (!cr6.eq) goto loc_826292E0;
	// li r8,0
	ctx.r8.s64 = 0;
loc_826292AC:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r9,128
	ctx.r9.s64 = 128;
	// li r10,8
	ctx.r10.s64 = 8;
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826292C4:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826292c4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826292C4;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// blt cr6,0x826292ac
	if (cr6.lt) goto loc_826292AC;
	// b 0x82629378
	goto loc_82629378;
loc_826292E0:
	// lhzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// srawi r7,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r7.s64 = r29.s32 >> 1;
	// lwz r9,1776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// srawi r6,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r6.s64 = r30.s32 >> 1;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x82629324
	if (!cr6.eq) goto loc_82629324;
	// bl 0x82651100
	sub_82651100(ctx, base);
	// b 0x82629328
	goto loc_82629328;
loc_82629324:
	// bl 0x82651038
	sub_82651038(ctx, base);
loc_82629328:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r29,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r4,r30,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// lwz r5,3756(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// add r25,r10,r9
	r25.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r11,30
	ctx.r9.u64 = r11.u32 & 0x3;
	// mullw r11,r25,r7
	r11.s64 = int64_t(r25.s32) * int64_t(ctx.r7.s32);
	// srawi r29,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r29.s64 = ctx.r8.s32 >> 2;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
loc_82629378:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x826293a0
	if (cr6.eq) goto loc_826293A0;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826293A0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_826293A8"))) PPC_WEAK_FUNC(sub_826293A8);
PPC_FUNC_IMPL(__imp__sub_826293A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// clrlwi r27,r8,30
	r27.u64 = ctx.r8.u32 & 0x3;
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// clrlwi r25,r9,30
	r25.u64 = ctx.r9.u32 & 0x3;
	// add r28,r11,r4
	r28.u64 = r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82629420
	if (!cr6.eq) goto loc_82629420;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x82629420
	if (!cr6.eq) goto loc_82629420;
	// li r31,16
	r31.s64 = 16;
loc_826293F4:
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r26,r26,r29
	r26.u64 = r26.u64 + r29.u64;
	// add r28,r28,r30
	r28.u64 = r28.u64 + r30.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x826293f4
	if (!cr6.eq) goto loc_826293F4;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_82629420:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8262946c
	if (!cr6.eq) goto loc_8262946C;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r11,3904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// lwz r24,3132(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 3132);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r24
	ctr.u64 = r24.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_8262946C:
	// lwz r11,3144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3144);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// lwz r9,3904(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,3904(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// lwz r11,3144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3144);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r26,8
	ctx.r5.s64 = r26.s64 + 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r28,8
	ctx.r3.s64 = r28.s64 + 8;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r10,r29,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r11,r30,3,0,28
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,3904(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// add r26,r10,r26
	r26.u64 = ctx.r10.u64 + r26.u64;
	// lwz r10,3144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3144);
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,3904(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3904);
	// lwz r11,3144(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3144);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r26,8
	ctx.r5.s64 = r26.s64 + 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r28,8
	ctx.r3.s64 = r28.s64 + 8;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82629524"))) PPC_WEAK_FUNC(sub_82629524);
PPC_FUNC_IMPL(__imp__sub_82629524) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82629528"))) PPC_WEAK_FUNC(sub_82629528);
PPC_FUNC_IMPL(__imp__sub_82629528) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r11,284(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r29,r11
	r11.s64 = int64_t(r29.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x82629578
	if (!cr6.eq) goto loc_82629578;
	// lwz r10,19980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8262958c
	if (cr6.eq) goto loc_8262958C;
loc_82629578:
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x82629618
	if (!cr6.eq) goto loc_82629618;
loc_8262958C:
	// li r30,8
	r30.s64 = 8;
loc_82629590:
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,128
	ctx.r4.s64 = 128;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,128
	ctx.r4.s64 = 128;
	// add r3,r28,r11
	ctx.r3.u64 = r28.u64 + r11.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r11,r26
	r11.u64 = r26.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,128
	ctx.r9.s64 = 128;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826295D0:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826295d0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826295D0;
	// mr r11,r25
	r11.u64 = r25.u64;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826295EC:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826295ec
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826295EC;
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82629590
	if (!cr6.eq) goto loc_82629590;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_82629618:
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// lhzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x82629664
	if (!cr6.eq) goto loc_82629664;
	// bl 0x82651100
	sub_82651100(ctx, base);
	// b 0x82629668
	goto loc_82629668;
loc_82629664:
	// bl 0x82651038
	sub_82651038(ctx, base);
loc_82629668:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r6,r29,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// rlwinm r4,r30,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r9,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r9.s64 = r11.s32 >> 2;
	// lwz r5,3756(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r24,r9,r6
	r24.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// clrlwi r9,r11,30
	ctx.r9.u64 = r11.u32 & 0x3;
	// mullw r11,r24,r7
	r11.s64 = int64_t(r24.s32) * int64_t(ctx.r7.s32);
	// srawi r27,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r27.s64 = ctx.r8.s32 >> 2;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// bl 0x826293a8
	sub_826293A8(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// mullw r11,r29,r11
	r11.s64 = int64_t(r29.s32) * int64_t(r11.s32);
	// lwz r9,1784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// cmpwi cr6,r8,7
	cr6.compare<int32_t>(ctx.r8.s32, 7, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x82629720
	if (!cr6.eq) goto loc_82629720;
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x8262971c
	if (cr6.eq) goto loc_8262971C;
	// bl 0x82651f38
	sub_82651F38(ctx, base);
	// b 0x82629720
	goto loc_82629720;
loc_8262971C:
	// bl 0x82652008
	sub_82652008(ctx, base);
loc_82629720:
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r9,r29,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,19700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// rlwinm r29,r30,3,0,28
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r10,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 2;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r7,30
	ctx.r9.u64 = ctx.r7.u32 & 0x3;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// srawi r28,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r28.s64 = ctx.r8.s32 >> 2;
	// mullw r5,r5,r7
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// add r30,r5,r28
	r30.u64 = ctx.r5.u64 + r28.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r11,3740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// clrlwi r9,r9,30
	ctx.r9.u64 = ctx.r9.u32 & 0x3;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826297B4"))) PPC_WEAK_FUNC(sub_826297B4);
PPC_FUNC_IMPL(__imp__sub_826297B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826297B8"))) PPC_WEAK_FUNC(sub_826297B8);
PPC_FUNC_IMPL(__imp__sub_826297B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x82629838
	if (!cr6.eq) goto loc_82629838;
	// li r8,0
	ctx.r8.s64 = 0;
loc_82629804:
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r9,128
	ctx.r9.s64 = 128;
	// li r10,8
	ctx.r10.s64 = 8;
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_8262981C:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8262981c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262981C;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// blt cr6,0x82629804
	if (cr6.lt) goto loc_82629804;
	// b 0x826298a0
	goto loc_826298A0;
loc_82629838:
	// lwz r9,1784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1784);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,15472(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x8262987c
	if (!cr6.eq) goto loc_8262987C;
	// lwz r11,21480(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21480);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82629878
	if (cr6.eq) goto loc_82629878;
	// bl 0x82651f38
	sub_82651F38(ctx, base);
	// b 0x8262987c
	goto loc_8262987C;
loc_82629878:
	// bl 0x82652008
	sub_82652008(ctx, base);
loc_8262987C:
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
loc_826298A0:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x826298c8
	if (cr6.eq) goto loc_826298C8;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826298C8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826298D0"))) PPC_WEAK_FUNC(sub_826298D0);
PPC_FUNC_IMPL(__imp__sub_826298D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82629920
	if (cr6.eq) goto loc_82629920;
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// beq cr6,0x82629920
	if (cr6.eq) goto loc_82629920;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x82629920
	if (cr6.eq) goto loc_82629920;
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// beq cr6,0x82629920
	if (cr6.eq) goto loc_82629920;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// subf r9,r6,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r6.s64;
	// li r10,16
	ctx.r10.s64 = 16;
loc_82629904:
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x82629904
	if (!cr6.eq) goto loc_82629904;
	// b 0x8239bd44
	return;
loc_82629920:
	// lwz r31,136(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lbz r30,4(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lis r8,2
	ctx.r8.s64 = 131072;
	// rlwinm r28,r31,2,0,29
	r28.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,6548(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// addi r9,r11,29840
	ctx.r9.s64 = r11.s64 + 29840;
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// add r28,r31,r28
	r28.u64 = r31.u64 + r28.u64;
	// rotlwi r31,r30,2
	r31.u64 = __builtin_rotateleft32(r30.u32, 2);
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// subf r30,r28,r7
	r30.s64 = ctx.r7.s64 - r28.s64;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r27,r10
	r27.s64 = ctx.r10.s16;
	// add r28,r31,r4
	r28.u64 = r31.u64 + ctx.r4.u64;
	// addi r10,r5,8
	ctx.r10.s64 = ctx.r5.s64 + 8;
	// lbz r31,4(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + 4);
	// subf r29,r6,r5
	r29.s64 = ctx.r5.s64 - ctx.r6.s64;
	// addi r11,r6,4
	r11.s64 = ctx.r6.s64 + 4;
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r30,16(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r28,r30,r9
	r28.u64 = PPC_LOAD_U32(r30.u32 + ctx.r9.u32);
	// rotlwi r30,r31,2
	r30.u64 = __builtin_rotateleft32(r31.u32, 2);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r31,r4
	ctx.r4.u64 = r31.u64 + ctx.r4.u64;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r27
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r27.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r4.u16);
loc_826299A8:
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rotlwi r31,r31,2
	r31.u64 = __builtin_rotateleft32(r31.u32, 2);
	// lhz r30,-6(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + -6);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lwzx r28,r31,r9
	r28.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r4,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r4.s64;
	// lbz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r30
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r30.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,-2(r11)
	PPC_STORE_U16(r11.u32 + -2, ctx.r4.u16);
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rotlwi r31,r31,2
	r31.u64 = __builtin_rotateleft32(r31.u32, 2);
	// lhzx r30,r11,r29
	r30.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lwzx r28,r31,r9
	r28.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r4,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r4.s64;
	// lbz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r30
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r30.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r4.u16);
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rotlwi r31,r31,2
	r31.u64 = __builtin_rotateleft32(r31.u32, 2);
	// lhz r30,-2(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lwzx r28,r31,r9
	r28.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r4,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r4.s64;
	// lbz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r30
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r30.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r4.u16);
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rotlwi r31,r31,2
	r31.u64 = __builtin_rotateleft32(r31.u32, 2);
	// lhz r30,0(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lwzx r28,r31,r9
	r28.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r4,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r4.s64;
	// lbz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r30
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r30.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r4.u16);
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rotlwi r31,r31,2
	r31.u64 = __builtin_rotateleft32(r31.u32, 2);
	// lhz r30,2(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r10,r10,10
	ctx.r10.s64 = ctx.r10.s64 + 10;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lwzx r28,r31,r9
	r28.u64 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	// rlwinm r31,r4,2,0,29
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r4,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r4.s64;
	// lbz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// mullw r4,r4,r30
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r30.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r4.u16);
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// bne cr6,0x826299a8
	if (!cr6.eq) goto loc_826299A8;
	// lhz r11,0(r6)
	r11.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// sth r11,16(r6)
	PPC_STORE_U16(ctx.r6.u32 + 16, r11.u16);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82629B08"))) PPC_WEAK_FUNC(sub_82629B08);
PPC_FUNC_IMPL(__imp__sub_82629B08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82629b58
	if (cr6.eq) goto loc_82629B58;
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// beq cr6,0x82629b58
	if (cr6.eq) goto loc_82629B58;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x82629b58
	if (cr6.eq) goto loc_82629B58;
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// beq cr6,0x82629b58
	if (cr6.eq) goto loc_82629B58;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// subf r9,r6,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r6.s64;
	// li r10,16
	ctx.r10.s64 = 16;
loc_82629B3C:
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x82629b3c
	if (!cr6.eq) goto loc_82629B3C;
	// b 0x8239bd48
	return;
loc_82629B58:
	// lwz r4,6548(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lbz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// addi r10,r11,29840
	ctx.r10.s64 = r11.s64 + 29840;
	// rotlwi r30,r3,2
	r30.u64 = __builtin_rotateleft32(ctx.r3.u32, 2);
	// lhz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// subf r29,r6,r5
	r29.s64 = ctx.r5.s64 - ctx.r6.s64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// rotlwi r30,r31,2
	r30.u64 = __builtin_rotateleft32(r31.u32, 2);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r30,r3,r4
	r30.u64 = ctx.r3.u64 + ctx.r4.u64;
	// rlwinm r3,r31,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r28,r9
	r28.s64 = ctx.r9.s16;
	// add r4,r3,r4
	ctx.r4.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r9,r5,8
	ctx.r9.s64 = ctx.r5.s64 + 8;
	// lwz r3,16(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// addi r11,r6,4
	r11.s64 = ctx.r6.s64 + 4;
	// li r5,3
	ctx.r5.s64 = 3;
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// lwzx r3,r3,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	// mullw r4,r3,r4
	ctx.r4.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// mullw r4,r4,r28
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r28.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r4.u16);
loc_82629BCC:
	// lbz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lhz r3,-6(r9)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r9.u32 + -6);
	// rotlwi r4,r4,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// mullw r4,r4,r3
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r4,r4,r31
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,-2(r11)
	PPC_STORE_U16(r11.u32 + -2, ctx.r4.u16);
	// lbz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lhzx r3,r11,r29
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// rotlwi r4,r4,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// mullw r4,r4,r3
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r4,r4,r31
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r4.u16);
	// lbz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lhz r3,-2(r9)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r9.u32 + -2);
	// rotlwi r4,r4,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// mullw r4,r4,r3
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r4,r4,r31
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r4.u16);
	// lbz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lhz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// rotlwi r4,r4,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// mullw r4,r4,r31
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// mullw r4,r4,r3
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r4.u16);
	// lbz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lhz r3,2(r9)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// addi r9,r9,10
	ctx.r9.s64 = ctx.r9.s64 + 10;
	// rotlwi r4,r4,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// lbz r31,-16(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + -16);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// mullw r4,r4,r3
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
	// mullw r4,r4,r31
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// srawi r4,r4,18
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 18;
	// sth r4,6(r11)
	PPC_STORE_U16(r11.u32 + 6, ctx.r4.u16);
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// bne cr6,0x82629bcc
	if (!cr6.eq) goto loc_82629BCC;
	// lhz r11,0(r6)
	r11.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// sth r11,16(r6)
	PPC_STORE_U16(ctx.r6.u32 + 16, r11.u16);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82629CC8"))) PPC_WEAK_FUNC(sub_82629CC8);
PPC_FUNC_IMPL(__imp__sub_82629CC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce8
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// lwz r29,0(r7)
	r29.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// beq cr6,0x82629e10
	if (cr6.eq) goto loc_82629E10;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x82629e10
	if (cr6.eq) goto loc_82629E10;
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// beq cr6,0x82629e10
	if (cr6.eq) goto loc_82629E10;
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// bne cr6,0x82629d8c
	if (!cr6.eq) goto loc_82629D8C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lbz r4,4(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// addi r30,r11,29840
	r30.s64 = r11.s64 + 29840;
	// lwz r3,6548(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// lis r11,2
	r11.s64 = 131072;
	// mr r28,r11
	r28.u64 = r11.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// rotlwi r11,r4,2
	r11.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r11,r3
	ctx.r5.u64 = r11.u64 + ctx.r3.u64;
	// lbz r11,4(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// lwz r9,16(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lwzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r30.u32);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// srawi r31,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r31.s64 = r11.s32 >> 18;
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// b 0x8239bd38
	return;
loc_82629D8C:
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x82629ed8
	if (!cr6.eq) goto loc_82629ED8;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r4,6548(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// lbz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// addi r3,r11,29840
	ctx.r3.s64 = r11.s64 + 29840;
	// lbz r5,-16(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + -16);
	// lis r11,2
	r11.s64 = 131072;
	// mr r30,r11
	r30.u64 = r11.u64;
	// mr r28,r11
	r28.u64 = r11.u64;
	// rotlwi r11,r9,2
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// rotlwi r9,r5,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r5,r11,r4
	ctx.r5.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lwz r11,16(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,16(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lwzx r11,r11,r3
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// srawi r29,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r29.s64 = r11.s32 >> 18;
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// b 0x8239bd38
	return;
loc_82629E10:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r4,136(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lbz r30,4(r5)
	r30.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// addi r28,r11,29840
	r28.s64 = r11.s64 + 29840;
	// lwz r9,6548(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// lis r11,2
	r11.s64 = 131072;
	// lbz r3,-16(r5)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r5.u32 + -16);
	// mr r27,r11
	r27.u64 = r11.u64;
	// mr r25,r11
	r25.u64 = r11.u64;
	// mr r26,r11
	r26.u64 = r11.u64;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + r11.u64;
	// rotlwi r11,r30,2
	r11.u64 = __builtin_rotateleft32(r30.u32, 2);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// subf r11,r4,r5
	r11.s64 = ctx.r5.s64 - ctx.r4.s64;
	// rlwinm r5,r30,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r5,r9
	ctx.r4.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rotlwi r5,r3,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r3.u32, 2);
	// add r5,r3,r5
	ctx.r5.u64 = ctx.r3.u64 + ctx.r5.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,16(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// lbz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r24,16(r5)
	r24.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// lbz r5,-16(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + -16);
	// rlwinm r11,r3,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r3,r4,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// rotlwi r30,r5,2
	r30.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// add r3,r4,r3
	ctx.r3.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// lwzx r11,r11,r28
	r11.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r5,16(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// mullw r4,r24,r11
	ctx.r4.s64 = int64_t(r24.s32) * int64_t(r11.s32);
	// lwz r9,16(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r10,r5,r10
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// mullw r9,r4,r29
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(r29.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// add r9,r9,r25
	ctx.r9.u64 = ctx.r9.u64 + r25.u64;
	// srawi r10,r10,18
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 18;
	// srawi r31,r11,18
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFF) != 0);
	r31.s64 = r11.s32 >> 18;
	// srawi r29,r9,18
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFFF) != 0);
	r29.s64 = ctx.r9.s32 >> 18;
loc_82629ED8:
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82629EE8"))) PPC_WEAK_FUNC(sub_82629EE8);
PPC_FUNC_IMPL(__imp__sub_82629EE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// lwz r4,284(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// li r24,1
	r24.s64 = 1;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r25
	r29.u64 = r25.u64;
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// add r6,r11,r7
	ctx.r6.u64 = r11.u64 + ctx.r7.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// bne cr6,0x82629f58
	if (!cr6.eq) goto loc_82629F58;
	// lwz r9,19980(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82629f58
	if (!cr6.eq) goto loc_82629F58;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// srawi r6,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// add r6,r9,r6
	ctx.r6.u64 = ctx.r9.u64 + ctx.r6.u64;
loc_82629F58:
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r28,r25
	r28.u64 = r25.u64;
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x82629f80
	if (!cr6.eq) goto loc_82629F80;
	// lwz r9,19980(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82629f80
	if (!cr6.eq) goto loc_82629F80;
	// clrlwi r9,r8,31
	ctx.r9.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82629fb0
	if (!cr6.eq) goto loc_82629FB0;
loc_82629F80:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82629fe4
	if (cr6.eq) goto loc_82629FE4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82629fb0
	if (cr6.eq) goto loc_82629FB0;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x82629fb0
	if (cr6.eq) goto loc_82629FB0;
	// subf r9,r11,r6
	ctx.r9.s64 = ctx.r6.s64 - r11.s64;
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r3.u32);
	// cmplwi cr6,r9,16384
	cr6.compare<uint32_t>(ctx.r9.u32, 16384, xer);
	// bne cr6,0x82629fe4
	if (!cr6.eq) goto loc_82629FE4;
loc_82629FB0:
	// clrlwi r9,r8,31
	ctx.r9.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// beq cr6,0x82629fd4
	if (cr6.eq) goto loc_82629FD4;
	// srawi r3,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 1;
	// lwz r9,21264(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r3,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r9.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82629fe4
	if (!cr6.eq) goto loc_82629FE4;
loc_82629FD4:
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r29,1928(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 1928);
	// subf r30,r10,r5
	r30.s64 = ctx.r5.s64 - ctx.r10.s64;
	// mr r28,r30
	r28.u64 = r30.u64;
loc_82629FE4:
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x8262a004
	if (!cr6.eq) goto loc_8262A004;
	// lwz r10,19980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8262a004
	if (!cr6.eq) goto loc_8262A004;
	// clrlwi r10,r7,31
	ctx.r10.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8262a034
	if (!cr6.eq) goto loc_8262A034;
loc_8262A004:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8262a13c
	if (cr6.eq) goto loc_8262A13C;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8262a034
	if (cr6.eq) goto loc_8262A034;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x8262a034
	if (cr6.eq) goto loc_8262A034;
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r9,r6,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lhz r10,-2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// bne cr6,0x8262a13c
	if (!cr6.eq) goto loc_8262A13C;
loc_8262A034:
	// addi r10,r5,-32
	ctx.r10.s64 = ctx.r5.s64 + -32;
	// lwz r29,1924(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 1924);
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8262a194
	if (cr6.eq) goto loc_8262A194;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8262a13c
	if (cr6.eq) goto loc_8262A13C;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// cmpwi cr6,r4,2
	cr6.compare<int32_t>(ctx.r4.s32, 2, xer);
	// bne cr6,0x8262a078
	if (!cr6.eq) goto loc_8262A078;
	// lwz r9,19980(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8262a078
	if (!cr6.eq) goto loc_8262A078;
	// or r9,r7,r8
	ctx.r9.u64 = ctx.r7.u64 | ctx.r8.u64;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8262a0a4
	if (!cr6.eq) goto loc_8262A0A4;
loc_8262A078:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8262a0a4
	if (cr6.eq) goto loc_8262A0A4;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// beq cr6,0x8262a0a4
	if (cr6.eq) goto loc_8262A0A4;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x8262a0bc
	if (!cr6.eq) goto loc_8262A0BC;
loc_8262A0A4:
	// lwz r11,1920(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1920);
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_8262A0BC:
	// lwz r11,1920(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1920);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r9,1916(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1916);
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lhzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82629cc8
	sub_82629CC8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8262a13c
	if (!cr6.lt) goto loc_8262A13C;
	// lwz r29,1928(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 1928);
	// mr r28,r30
	r28.u64 = r30.u64;
loc_8262A13C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8262a194
	if (cr6.eq) goto loc_8262A194;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,27,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262a158
	if (!cr6.eq) goto loc_8262A158;
	// mr r24,r25
	r24.u64 = r25.u64;
loc_8262A158:
	// lwz r11,1924(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1924);
	// li r25,1
	r25.s64 = 1;
	// lwz r6,276(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne cr6,0x8262a184
	if (!cr6.eq) goto loc_8262A184;
	// bl 0x82629b08
	sub_82629B08(ctx, base);
	// b 0x8262a188
	goto loc_8262A188;
loc_8262A184:
	// bl 0x826298d0
	sub_826298D0(ctx, base);
loc_8262A188:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x8262a194
	if (!cr6.eq) goto loc_8262A194;
	// li r29,-1
	r29.s64 = -1;
loc_8262A194:
	// lwz r11,1928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1928);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
	// stw r29,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r29.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_8262A1B8"))) PPC_WEAK_FUNC(sub_8262A1B8);
PPC_FUNC_IMPL(__imp__sub_8262A1B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r29,1
	r29.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r31,0
	r31.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8262a1f4
	if (cr6.eq) goto loc_8262A1F4;
	// lwz r8,136(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// lwz r31,1928(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// rlwinm r8,r8,6,0,25
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 6) & 0xFFFFFFC0;
	// subf r6,r8,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r8.s64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
loc_8262A1F4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8262a274
	if (cr6.eq) goto loc_8262A274;
	// addi r30,r5,-32
	r30.s64 = ctx.r5.s64 + -32;
	// lwz r31,1924(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 1924);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8262a294
	if (cr6.eq) goto loc_8262A294;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x8262a274
	if (cr6.eq) goto loc_8262A274;
	// lwz r8,1920(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1920);
	// lwz r7,1916(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1916);
	// addi r5,r8,-16
	ctx.r5.s64 = ctx.r8.s64 + -16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r8,r6
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r6.u32);
	// lhzx r7,r7,r30
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + r30.u32);
	// lhzx r5,r5,r6
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r6.u32);
	// extsh r28,r8
	r28.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subf r8,r28,r8
	ctx.r8.s64 = ctx.r8.s64 - r28.s64;
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// srawi r28,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r28.s64 = ctx.r8.s32 >> 31;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// xor r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 ^ r28.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// subf r8,r28,r8
	ctx.r8.s64 = ctx.r8.s64 - r28.s64;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// bge cr6,0x8262a274
	if (!cr6.lt) goto loc_8262A274;
	// lwz r31,1928(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
loc_8262A274:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8262a294
	if (cr6.eq) goto loc_8262A294;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r8,r8,0,27,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x8262a290
	if (!cr6.eq) goto loc_8262A290;
	// li r29,0
	r29.s64 = 0;
loc_8262A290:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8262A294:
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// bne cr6,0x8262a2a8
	if (!cr6.eq) goto loc_8262A2A8;
	// li r31,-1
	r31.s64 = -1;
loc_8262A2A8:
	// stw r31,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r31.u32);
	// lwz r11,1928(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8262A2C4"))) PPC_WEAK_FUNC(sub_8262A2C4);
PPC_FUNC_IMPL(__imp__sub_8262A2C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8262A2C8"))) PPC_WEAK_FUNC(sub_8262A2C8);
PPC_FUNC_IMPL(__imp__sub_8262A2C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcdc
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r22,1
	r22.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r25,0
	r25.s64 = 0;
	// li r28,0
	r28.s64 = 0;
	// li r23,0
	r23.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8262a34c
	if (cr6.eq) goto loc_8262A34C;
	// lwz r31,21264(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 21264);
	// rlwinm r30,r7,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r31,r31,r30
	r31.u64 = PPC_LOAD_U32(r31.u32 + r30.u32);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8262a34c
	if (!cr6.eq) goto loc_8262A34C;
	// lwz r31,136(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// addi r30,r7,-1
	r30.s64 = ctx.r7.s64 + -1;
	// lwz r29,1780(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 1780);
	// mullw r30,r30,r31
	r30.s64 = int64_t(r30.s32) * int64_t(r31.s32);
	// add r30,r30,r6
	r30.u64 = r30.u64 + ctx.r6.u64;
	// rlwinm r30,r30,1,0,30
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r30,r29
	r30.u64 = PPC_LOAD_U16(r30.u32 + r29.u32);
	// cmplwi cr6,r30,16384
	cr6.compare<uint32_t>(r30.u32, 16384, xer);
	// beq cr6,0x8262a33c
	if (cr6.eq) goto loc_8262A33C;
	// lwz r30,284(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 284);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x8262a33c
	if (cr6.eq) goto loc_8262A33C;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// bne cr6,0x8262a34c
	if (!cr6.eq) goto loc_8262A34C;
loc_8262A33C:
	// rlwinm r31,r31,5,0,26
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r23,1928(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// subf r25,r31,r5
	r25.s64 = ctx.r5.s64 - r31.s64;
	// mr r28,r25
	r28.u64 = r25.u64;
loc_8262A34C:
	// lis r30,-32244
	r30.s64 = -2113142784;
	// lis r31,2
	r31.s64 = 131072;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r30,r30,29840
	r30.s64 = r30.s64 + 29840;
	// beq cr6,0x8262a4fc
	if (cr6.eq) goto loc_8262A4FC;
	// lwz r24,136(r11)
	r24.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// lwz r27,1780(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 1780);
	// mullw r29,r24,r7
	r29.s64 = int64_t(r24.s32) * int64_t(ctx.r7.s32);
	// add r29,r29,r6
	r29.u64 = r29.u64 + ctx.r6.u64;
	// rlwinm r29,r29,1,0,30
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// lhz r29,-2(r29)
	r29.u64 = PPC_LOAD_U16(r29.u32 + -2);
	// cmplwi cr6,r29,16384
	cr6.compare<uint32_t>(r29.u32, 16384, xer);
	// beq cr6,0x8262a398
	if (cr6.eq) goto loc_8262A398;
	// lwz r29,284(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 284);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262a398
	if (cr6.eq) goto loc_8262A398;
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// bne cr6,0x8262a4fc
	if (!cr6.eq) goto loc_8262A4FC;
loc_8262A398:
	// addi r28,r5,-32
	r28.s64 = ctx.r5.s64 + -32;
	// lwz r23,1924(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 1924);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8262a850
	if (cr6.eq) goto loc_8262A850;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x8262a4fc
	if (cr6.eq) goto loc_8262A4FC;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// li r26,0
	r26.s64 = 0;
	// mullw r7,r7,r24
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r24.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + r27.u64;
	// lhz r7,-2(r7)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + -2);
	// cmplwi cr6,r7,16384
	cr6.compare<uint32_t>(ctx.r7.u32, 16384, xer);
	// beq cr6,0x8262a3e8
	if (cr6.eq) goto loc_8262A3E8;
	// lwz r7,284(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 284);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8262a3e8
	if (cr6.eq) goto loc_8262A3E8;
	// cmpwi cr6,r7,4
	cr6.compare<int32_t>(ctx.r7.s32, 4, xer);
	// bne cr6,0x8262a3fc
	if (!cr6.eq) goto loc_8262A3FC;
loc_8262A3E8:
	// lwz r7,1920(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1920);
	// addi r7,r7,-16
	ctx.r7.s64 = ctx.r7.s64 + -16;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r7,r25
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + r25.u32);
	// extsh r26,r7
	r26.s64 = ctx.r7.s16;
loc_8262A3FC:
	// lwz r27,1916(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 1916);
	// lwz r6,136(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rlwinm r27,r27,1,0,30
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lwz r7,6548(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 6548);
	// lbz r29,-16(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// lwz r24,1920(r11)
	r24.u64 = PPC_LOAD_U32(r11.u32 + 1920);
	// lhzx r27,r27,r28
	r27.u64 = PPC_LOAD_U16(r27.u32 + r28.u32);
	// rlwinm r24,r24,1,0,30
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r21,r27
	r21.s64 = r27.s16;
	// rlwinm r27,r6,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r6,r27
	r27.u64 = ctx.r6.u64 + r27.u64;
	// lhzx r24,r24,r25
	r24.u64 = PPC_LOAD_U16(r24.u32 + r25.u32);
	// rotlwi r6,r5,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// rlwinm r27,r27,2,0,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// subf r6,r27,r4
	ctx.r6.s64 = ctx.r4.s64 - r27.s64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// extsh r24,r24
	r24.s64 = r24.s16;
	// add r27,r5,r7
	r27.u64 = ctx.r5.u64 + ctx.r7.u64;
	// rotlwi r5,r29,2
	ctx.r5.u64 = __builtin_rotateleft32(r29.u32, 2);
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// lbz r29,4(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r27,16(r27)
	r27.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lwz r5,16(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// mullw r21,r5,r21
	r21.s64 = int64_t(ctx.r5.s32) * int64_t(r21.s32);
	// lbz r5,-16(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + -16);
	// rlwinm r6,r27,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r27,r5,2
	r27.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// add r5,r5,r27
	ctx.r5.u64 = ctx.r5.u64 + r27.u64;
	// rotlwi r27,r29,2
	r27.u64 = __builtin_rotateleft32(r29.u32, 2);
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r6,r30
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + r30.u32);
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// add r27,r5,r7
	r27.u64 = ctx.r5.u64 + ctx.r7.u64;
	// rlwinm r5,r29,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r29,r21,r6
	r29.s64 = int64_t(r21.s32) * int64_t(ctx.r6.s32);
	// add r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lwz r5,16(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// lwz r27,16(r7)
	r27.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// mullw r7,r5,r6
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// mullw r7,r7,r26
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r26.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r5,r29,r31
	ctx.r5.u64 = r29.u64 + r31.u64;
	// mullw r29,r27,r24
	r29.s64 = int64_t(r27.s32) * int64_t(r24.s32);
	// srawi r7,r7,18
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 18;
	// srawi r5,r5,18
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3FFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 18;
	// mullw r6,r29,r6
	ctx.r6.s64 = int64_t(r29.s32) * int64_t(ctx.r6.s32);
	// subf r5,r5,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r29,r5,31
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = ctx.r5.s32 >> 31;
	// srawi r6,r6,18
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 18;
	// xor r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 ^ r29.u64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r6,r29,r5
	ctx.r6.s64 = ctx.r5.s64 - r29.s64;
	// srawi r5,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// bge cr6,0x8262a4fc
	if (!cr6.lt) goto loc_8262A4FC;
	// lwz r23,1928(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// mr r28,r25
	r28.u64 = r25.u64;
loc_8262A4FC:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8262a850
	if (cr6.eq) goto loc_8262A850;
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r7,r7,0,27,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x8262a518
	if (!cr6.eq) goto loc_8262A518;
	// li r22,0
	r22.s64 = 0;
loc_8262A518:
	// lwz r7,1924(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1924);
	// li r3,1
	ctx.r3.s64 = 1;
	// lwz r29,6548(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 6548);
	// addi r6,r28,8
	ctx.r6.s64 = r28.s64 + 8;
	// cmpw cr6,r23,r7
	cr6.compare<int32_t>(r23.s32, ctx.r7.s32, xer);
	// lhz r7,0(r28)
	ctx.r7.u64 = PPC_LOAD_U16(r28.u32 + 0);
	// li r5,3
	ctx.r5.s64 = 3;
	// extsh r24,r7
	r24.s64 = ctx.r7.s16;
	// addi r7,r10,4
	ctx.r7.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x8262a684
	if (!cr6.eq) goto loc_8262A684;
	// subf r25,r10,r28
	r25.s64 = r28.s64 - ctx.r10.s64;
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// rotlwi r26,r28,2
	r26.u64 = __builtin_rotateleft32(r28.u32, 2);
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// rotlwi r26,r27,2
	r26.u64 = __builtin_rotateleft32(r27.u32, 2);
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r26,r28,r29
	r26.u64 = r28.u64 + r29.u64;
	// rlwinm r28,r27,2,0,29
	r28.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r28,r29
	r29.u64 = r28.u64 + r29.u64;
	// lwz r28,16(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r29,16(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// lwzx r28,r28,r30
	r28.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// mullw r29,r28,r29
	r29.s64 = int64_t(r28.s32) * int64_t(r29.s32);
	// mullw r29,r29,r24
	r29.s64 = int64_t(r29.s32) * int64_t(r24.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r29.u16);
loc_8262A590:
	// lbz r29,4(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lhz r28,-6(r6)
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + -6);
	// rotlwi r29,r29,2
	r29.u64 = __builtin_rotateleft32(r29.u32, 2);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lwzx r29,r29,r30
	r29.u64 = PPC_LOAD_U32(r29.u32 + r30.u32);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,-2(r7)
	PPC_STORE_U16(ctx.r7.u32 + -2, r29.u16);
	// lbz r29,4(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lhzx r28,r25,r7
	r28.u64 = PPC_LOAD_U16(r25.u32 + ctx.r7.u32);
	// rotlwi r29,r29,2
	r29.u64 = __builtin_rotateleft32(r29.u32, 2);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lwzx r29,r29,r30
	r29.u64 = PPC_LOAD_U32(r29.u32 + r30.u32);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r29.u16);
	// lbz r29,4(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lhz r28,-2(r6)
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// rotlwi r29,r29,2
	r29.u64 = __builtin_rotateleft32(r29.u32, 2);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lwzx r29,r29,r30
	r29.u64 = PPC_LOAD_U32(r29.u32 + r30.u32);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,2(r7)
	PPC_STORE_U16(ctx.r7.u32 + 2, r29.u16);
	// lbz r29,4(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lhz r28,0(r6)
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// rotlwi r29,r29,2
	r29.u64 = __builtin_rotateleft32(r29.u32, 2);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lwzx r29,r29,r30
	r29.u64 = PPC_LOAD_U32(r29.u32 + r30.u32);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,4(r7)
	PPC_STORE_U16(ctx.r7.u32 + 4, r29.u16);
	// lbz r29,4(r4)
	r29.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lhz r28,2(r6)
	r28.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// addi r6,r6,10
	ctx.r6.s64 = ctx.r6.s64 + 10;
	// rotlwi r29,r29,2
	r29.u64 = __builtin_rotateleft32(r29.u32, 2);
	// lbz r27,-16(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + -16);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lwzx r29,r29,r30
	r29.u64 = PPC_LOAD_U32(r29.u32 + r30.u32);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,6(r7)
	PPC_STORE_U16(ctx.r7.u32 + 6, r29.u16);
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// bne cr6,0x8262a590
	if (!cr6.eq) goto loc_8262A590;
	// b 0x8262a83c
	goto loc_8262A83C;
loc_8262A684:
	// subf r26,r10,r28
	r26.s64 = r28.s64 - ctx.r10.s64;
	// lwz r28,136(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// lbz r27,4(r4)
	r27.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rlwinm r25,r28,2,0,29
	r25.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r28,r25
	r25.u64 = r28.u64 + r25.u64;
	// rotlwi r28,r27,2
	r28.u64 = __builtin_rotateleft32(r27.u32, 2);
	// rlwinm r25,r25,2,0,29
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r27,r28
	r28.u64 = r27.u64 + r28.u64;
	// subf r27,r25,r4
	r27.s64 = ctx.r4.s64 - r25.s64;
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r28,r29
	r25.u64 = r28.u64 + r29.u64;
	// lbz r28,4(r27)
	r28.u64 = PPC_LOAD_U8(r27.u32 + 4);
	// lwz r27,16(r25)
	r27.u64 = PPC_LOAD_U32(r25.u32 + 16);
	// rlwinm r27,r27,2,0,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r25,r27,r30
	r25.u64 = PPC_LOAD_U32(r27.u32 + r30.u32);
	// rotlwi r27,r28,2
	r27.u64 = __builtin_rotateleft32(r28.u32, 2);
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r28,r29
	r29.u64 = r28.u64 + r29.u64;
	// lwz r29,16(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + 16);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r24
	r29.s64 = int64_t(r29.s32) * int64_t(r24.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r29.u16);
loc_8262A6E8:
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lwz r29,136(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lhz r27,-6(r6)
	r27.u64 = PPC_LOAD_U16(ctx.r6.u32 + -6);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lwzx r25,r28,r30
	r25.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r29,r4
	r29.s64 = ctx.r4.s64 - r29.s64;
	// lbz r29,4(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 4);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,-2(r7)
	PPC_STORE_U16(ctx.r7.u32 + -2, r29.u16);
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lwz r29,136(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lhzx r27,r7,r26
	r27.u64 = PPC_LOAD_U16(ctx.r7.u32 + r26.u32);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lwzx r25,r28,r30
	r25.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r29,r4
	r29.s64 = ctx.r4.s64 - r29.s64;
	// lbz r29,4(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 4);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r29.u16);
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lwz r29,136(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lhz r27,-2(r6)
	r27.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lwzx r25,r28,r30
	r25.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r29,r4
	r29.s64 = ctx.r4.s64 - r29.s64;
	// lbz r29,4(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 4);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,2(r7)
	PPC_STORE_U16(ctx.r7.u32 + 2, r29.u16);
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lwz r29,136(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lhz r27,0(r6)
	r27.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lwzx r25,r28,r30
	r25.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r29,r4
	r29.s64 = ctx.r4.s64 - r29.s64;
	// lbz r29,4(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 4);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,4(r7)
	PPC_STORE_U16(ctx.r7.u32 + 4, r29.u16);
	// lbz r28,4(r4)
	r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lwz r29,136(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// rotlwi r28,r28,2
	r28.u64 = __builtin_rotateleft32(r28.u32, 2);
	// lhz r27,2(r6)
	r27.u64 = PPC_LOAD_U16(ctx.r6.u32 + 2);
	// addi r6,r6,10
	ctx.r6.s64 = ctx.r6.s64 + 10;
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lwzx r25,r28,r30
	r25.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// rlwinm r28,r29,2,0,29
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r29,r4
	r29.s64 = ctx.r4.s64 - r29.s64;
	// lbz r29,4(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 4);
	// mullw r29,r29,r25
	r29.s64 = int64_t(r29.s32) * int64_t(r25.s32);
	// mullw r29,r29,r27
	r29.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// srawi r29,r29,18
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3FFFF) != 0);
	r29.s64 = r29.s32 >> 18;
	// sth r29,6(r7)
	PPC_STORE_U16(ctx.r7.u32 + 6, r29.u16);
	// addi r7,r7,10
	ctx.r7.s64 = ctx.r7.s64 + 10;
	// bne cr6,0x8262a6e8
	if (!cr6.eq) goto loc_8262A6E8;
loc_8262A83C:
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// sth r7,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r7.u16);
	// bne cr6,0x8262a850
	if (!cr6.eq) goto loc_8262A850;
	// li r23,-1
	r23.s64 = -1;
loc_8262A850:
	// lwz r11,1928(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// subf r11,r23,r11
	r11.s64 = r11.s64 - r23.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r23,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r23.u32);
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_8262A86C"))) PPC_WEAK_FUNC(sub_8262A86C);
PPC_FUNC_IMPL(__imp__sub_8262A86C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8262A870"))) PPC_WEAK_FUNC(sub_8262A870);
PPC_FUNC_IMPL(__imp__sub_8262A870) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r28,1
	r28.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r31,0
	r31.s64 = 0;
	// li r29,0
	r29.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8262a8ac
	if (cr6.eq) goto loc_8262A8AC;
	// lwz r7,136(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// lwz r30,1928(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// rlwinm r7,r7,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r31,r7,r5
	r31.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mr r29,r31
	r29.u64 = r31.u64;
loc_8262A8AC:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x8262a92c
	if (cr6.eq) goto loc_8262A92C;
	// addi r29,r5,-32
	r29.s64 = ctx.r5.s64 + -32;
	// lwz r30,1924(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 1924);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8262a94c
	if (cr6.eq) goto loc_8262A94C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8262a92c
	if (cr6.eq) goto loc_8262A92C;
	// lwz r7,1920(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 1920);
	// lwz r6,1916(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1916);
	// addi r5,r7,-16
	ctx.r5.s64 = ctx.r7.s64 + -16;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r7,r31
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + r31.u32);
	// lhzx r6,r6,r29
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r6.u32 + r29.u32);
	// lhzx r5,r5,r31
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + r31.u32);
	// extsh r27,r7
	r27.s64 = ctx.r7.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// subf r6,r6,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r7,r27,r7
	ctx.r7.s64 = ctx.r7.s64 - r27.s64;
	// srawi r5,r6,31
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 31;
	// srawi r27,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = ctx.r7.s32 >> 31;
	// xor r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 ^ ctx.r5.u64;
	// xor r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 ^ r27.u64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r7,r27,r7
	ctx.r7.s64 = ctx.r7.s64 - r27.s64;
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// bge cr6,0x8262a92c
	if (!cr6.lt) goto loc_8262A92C;
	// lwz r30,1928(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// mr r29,r31
	r29.u64 = r31.u64;
loc_8262A92C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8262a94c
	if (cr6.eq) goto loc_8262A94C;
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r7,r7,0,27,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x8262a948
	if (!cr6.eq) goto loc_8262A948;
	// li r28,0
	r28.s64 = 0;
loc_8262A948:
	// li r3,1
	ctx.r3.s64 = 1;
loc_8262A94C:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r29,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r29.u32);
	// bne cr6,0x8262a95c
	if (!cr6.eq) goto loc_8262A95C;
	// li r30,-1
	r30.s64 = -1;
loc_8262A95C:
	// lwz r11,1928(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1928);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r30.u32);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8262A978"))) PPC_WEAK_FUNC(sub_8262A978);
PPC_FUNC_IMPL(__imp__sub_8262A978) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// clrlwi r9,r5,31
	ctx.r9.u64 = ctx.r5.u32 & 0x1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r11,136(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 136);
	// mullw r9,r11,r5
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// bne cr6,0x8262a9c0
	if (!cr6.eq) goto loc_8262A9C0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x8262a9dc
	if (cr6.eq) goto loc_8262A9DC;
	// srawi r7,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 1;
	// lwz r9,21264(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 21264);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r7,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8262a9dc
	if (!cr6.eq) goto loc_8262A9DC;
loc_8262A9C0:
	// subf r9,r8,r11
	ctx.r9.s64 = r11.s64 - ctx.r8.s64;
	// lwz r8,1772(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1772);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r8
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r8.u32);
	// cmplwi cr6,r9,16384
	cr6.compare<uint32_t>(ctx.r9.u32, 16384, xer);
	// bne cr6,0x8262a9dc
	if (!cr6.eq) goto loc_8262A9DC;
	// li r3,1
	ctx.r3.s64 = 1;
loc_8262A9DC:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r10,1772(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1772);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r11,-2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8262AA04"))) PPC_WEAK_FUNC(sub_8262AA04);
PPC_FUNC_IMPL(__imp__sub_8262AA04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8262AA08"))) PPC_WEAK_FUNC(sub_8262AA08);
PPC_FUNC_IMPL(__imp__sub_8262AA08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x8262aa54
	if (cr6.eq) goto loc_8262AA54;
	// lwz r10,21264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21264);
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8262aa54
	if (!cr6.eq) goto loc_8262AA54;
	// lwz r10,136(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// addi r9,r5,-1
	ctx.r9.s64 = ctx.r5.s64 + -1;
	// lwz r8,1780(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 1780);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r8
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// bne cr6,0x8262aa54
	if (!cr6.eq) goto loc_8262AA54;
	// li r3,1
	ctx.r3.s64 = 1;
loc_8262AA54:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r9,136(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// lwz r10,1780(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 1780);
	// mullw r11,r9,r5
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,-2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8262AA88"))) PPC_WEAK_FUNC(sub_8262AA88);
PPC_FUNC_IMPL(__imp__sub_8262AA88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r8,3
	ctx.r8.s64 = 3;
	// addi r11,r28,4024
	r11.s64 = r28.s64 + 4024;
	// li r7,-3
	ctx.r7.s64 = -3;
	// li r5,31
	ctx.r5.s64 = 31;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r27,8
	r27.s64 = 8;
loc_8262AAB0:
	// addi r10,r8,-1
	ctx.r10.s64 = ctx.r8.s64 + -1;
	// stw r6,-12(r11)
	PPC_STORE_U32(r11.u32 + -12, ctx.r6.u32);
	// addi r4,r7,1
	ctx.r4.s64 = ctx.r7.s64 + 1;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// stw r10,-16(r11)
	PPC_STORE_U32(r11.u32 + -16, ctx.r10.u32);
	// stw r4,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r4.u32);
	// stw r10,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r10.u32);
	// bgt cr6,0x8262aaf4
	if (cr6.gt) goto loc_8262AAF4;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,14756(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 14756);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8262ab00
	if (cr6.eq) goto loc_8262AB00;
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bgt cr6,0x8262ab00
	if (cr6.gt) goto loc_8262AB00;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x8262aafc
	goto loc_8262AAFC;
loc_8262AAF4:
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_8262AAFC:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_8262AB00:
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r10,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 1;
	// stw r6,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r6.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// stw r7,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r7.u32);
	// bgt cr6,0x8262ab3c
	if (cr6.gt) goto loc_8262AB3C;
	// stw r27,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r27.u32);
	// lwz r9,14756(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 14756);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8262ab48
	if (cr6.eq) goto loc_8262AB48;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x8262ab48
	if (cr6.gt) goto loc_8262AB48;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x8262ab44
	goto loc_8262AB44;
loc_8262AB3C:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_8262AB44:
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
loc_8262AB48:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r11,r11,40
	r11.s64 = r11.s64 + 40;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// addi r7,r7,-2
	ctx.r7.s64 = ctx.r7.s64 + -2;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x8262aab0
	if (!cr6.eq) goto loc_8262AAB0;
	// addi r29,r28,4008
	r29.s64 = r28.s64 + 4008;
	// addi r30,r28,6624
	r30.s64 = r28.s64 + 6624;
	// li r31,62
	r31.s64 = 62;
loc_8262AB6C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826571e0
	sub_826571E0(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r30,r30,64
	r30.s64 = r30.s64 + 64;
	// addi r29,r29,20
	r29.s64 = r29.s64 + 20;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8262ab6c
	if (!cr6.eq) goto loc_8262AB6C;
	// li r7,3
	ctx.r7.s64 = 3;
	// addi r11,r28,5296
	r11.s64 = r28.s64 + 5296;
	// li r6,31
	ctx.r6.s64 = 31;
loc_8262AB98:
	// addi r9,r7,-1
	ctx.r9.s64 = ctx.r7.s64 + -1;
	// stw r9,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r9.u32);
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// blt cr6,0x8262abbc
	if (cr6.lt) goto loc_8262ABBC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r10.u32);
	// b 0x8262abd0
	goto loc_8262ABD0;
loc_8262ABBC:
	// not r8,r10
	ctx.r8.u64 = ~ctx.r10.u64;
	// clrlwi r8,r8,31
	ctx.r8.u64 = ctx.r8.u32 & 0x1;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r8,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r8.u32);
loc_8262ABD0:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// bgt cr6,0x8262ac08
	if (cr6.gt) goto loc_8262AC08;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
	// lwz r9,14756(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 14756);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8262ac14
	if (cr6.eq) goto loc_8262AC14;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x8262ac14
	if (cr6.gt) goto loc_8262AC14;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x8262ac10
	goto loc_8262AC10;
loc_8262AC08:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_8262AC10:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_8262AC14:
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// srawi r10,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 1;
	// blt cr6,0x8262ac38
	if (cr6.lt) goto loc_8262AC38;
	// add r9,r10,r7
	ctx.r9.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// b 0x8262ac50
	goto loc_8262AC50;
loc_8262AC38:
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// stw r8,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r8.u32);
loc_8262AC50:
	// lwz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// bgt cr6,0x8262ac84
	if (cr6.gt) goto loc_8262AC84;
	// stw r27,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r27.u32);
	// lwz r9,14756(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 14756);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8262ac90
	if (cr6.eq) goto loc_8262AC90;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x8262ac90
	if (cr6.gt) goto loc_8262AC90;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x8262ac8c
	goto loc_8262AC8C;
loc_8262AC84:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_8262AC8C:
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
loc_8262AC90:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r11,40
	r11.s64 = r11.s64 + 40;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8262ab98
	if (!cr6.eq) goto loc_8262AB98;
	// addi r29,r28,5288
	r29.s64 = r28.s64 + 5288;
	// addi r30,r28,10720
	r30.s64 = r28.s64 + 10720;
	// li r31,62
	r31.s64 = 62;
loc_8262ACB0:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826571e0
	sub_826571E0(ctx, base);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r30,r30,64
	r30.s64 = r30.s64 + 64;
	// addi r29,r29,20
	r29.s64 = r29.s64 + 20;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8262acb0
	if (!cr6.eq) goto loc_8262ACB0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8262ACD8"))) PPC_WEAK_FUNC(sub_8262ACD8);
PPC_FUNC_IMPL(__imp__sub_8262ACD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r11,140(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 140);
	// lwz r31,268(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 268);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262ad94
	if (!cr6.gt) goto loc_8262AD94;
	// lwz r11,136(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 136);
loc_8262AD00:
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262ad84
	if (!cr6.gt) goto loc_8262AD84;
	// cntlzw r11,r28
	r11.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// rlwinm r27,r11,27,31,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_8262AD14:
	// addi r8,r31,-8
	ctx.r8.s64 = r31.s64 + -8;
	// lwz r11,136(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 136);
	// cntlzw r10,r30
	ctx.r10.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r9,r31,-14
	ctx.r9.s64 = r31.s64 + -14;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// rlwinm r4,r10,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r31,6
	ctx.r7.s64 = r31.s64 + 6;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// rlwinm r6,r6,24,29,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0x7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r26,r11,12
	r26.s64 = r11.s64 + 12;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r8,r11,6
	ctx.r8.s64 = r11.s64 + 6;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// bl 0x826181e8
	sub_826181E8(ctx, base);
	// lwz r11,136(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 136);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x8262ad14
	if (cr6.lt) goto loc_8262AD14;
loc_8262AD84:
	// lwz r10,140(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 140);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// blt cr6,0x8262ad00
	if (cr6.lt) goto loc_8262AD00;
loc_8262AD94:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8262AD9C"))) PPC_WEAK_FUNC(sub_8262AD9C);
PPC_FUNC_IMPL(__imp__sub_8262AD9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8262ADA0"))) PPC_WEAK_FUNC(sub_8262ADA0);
PPC_FUNC_IMPL(__imp__sub_8262ADA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r9,144(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 144);
	// lwz r11,19984(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 19984);
	// lwz r10,268(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 268);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,21236(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 21236);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// beq cr6,0x8262aeac
	if (cr6.eq) goto loc_8262AEAC;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262af50
	if (!cr6.gt) goto loc_8262AF50;
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// li r28,0
	r28.s64 = 0;
loc_8262ADF4:
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262ae90
	if (!cr6.gt) goto loc_8262AE90;
loc_8262AE00:
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// addi r8,r11,6
	ctx.r8.s64 = r11.s64 + 6;
	// beq cr6,0x8262ae38
	if (cr6.eq) goto loc_8262AE38;
	// lwz r11,21264(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21264);
	// lwzx r11,r11,r28
	r11.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// beq cr6,0x8262ae3c
	if (cr6.eq) goto loc_8262AE3C;
loc_8262AE38:
	// li r11,1
	r11.s64 = 1;
loc_8262AE3C:
	// cntlzw r9,r29
	ctx.r9.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addi r5,r31,-8
	ctx.r5.s64 = r31.s64 + -8;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r9,27,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// addi r9,r31,-14
	ctx.r9.s64 = r31.s64 + -14;
	// addi r7,r31,6
	ctx.r7.s64 = r31.s64 + 6;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// rlwinm r6,r6,24,29,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0x7;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82618a90
	sub_82618A90(ctx, base);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8262ae00
	if (cr6.lt) goto loc_8262AE00;
loc_8262AE90:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// blt cr6,0x8262adf4
	if (cr6.lt) goto loc_8262ADF4;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd40
	return;
loc_8262AEAC:
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262af50
	if (!cr6.gt) goto loc_8262AF50;
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
loc_8262AEBC:
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8262af40
	if (!cr6.gt) goto loc_8262AF40;
	// cntlzw r11,r28
	r11.u64 = r28.u32 == 0 ? 32 : __builtin_clz(r28.u32);
	// rlwinm r27,r11,27,31,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_8262AED0:
	// addi r8,r31,-8
	ctx.r8.s64 = r31.s64 + -8;
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// cntlzw r10,r29
	ctx.r10.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r9,r31,-14
	ctx.r9.s64 = r31.s64 + -14;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// rlwinm r4,r10,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r31,6
	ctx.r7.s64 = r31.s64 + 6;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// rlwinm r6,r6,24,29,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0x7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r26,r11,12
	r26.s64 = r11.s64 + 12;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r8,r11,6
	ctx.r8.s64 = r11.s64 + 6;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// bl 0x82618a90
	sub_82618A90(ctx, base);
	// lwz r11,136(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8262aed0
	if (cr6.lt) goto loc_8262AED0;
loc_8262AF40:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// blt cr6,0x8262aebc
	if (cr6.lt) goto loc_8262AEBC;
loc_8262AF50:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8262AF58"))) PPC_WEAK_FUNC(sub_8262AF58);
PPC_FUNC_IMPL(__imp__sub_8262AF58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,4(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r6,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r6.u32);
	// addi r9,r4,12
	ctx.r9.s64 = ctx.r4.s64 + 12;
	// stw r7,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r7.u32);
	// mr r21,r5
	r21.u64 = ctx.r5.u64;
	// stw r4,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r4.u32);
	// rotlwi r6,r11,2
	ctx.r6.u64 = __builtin_rotateleft32(r11.u32, 2);
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
	// lwz r7,6548(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r14,336(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// lwz r9,328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r3,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r3.u32);
	// stw r5,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r5.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r8,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r8.u32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cntlzw r4,r9
	ctx.r4.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r6,r4,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// rlwinm r15,r8,12,30,31
	r15.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0x3;
	// xori r23,r6,1
	r23.u64 = ctx.r6.u64 ^ 1;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// beq cr6,0x8262afdc
	if (cr6.eq) goto loc_8262AFDC;
	// rlwinm r14,r8,8,29,31
	r14.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0x7;
loc_8262AFDC:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262b00c
	if (cr6.eq) goto loc_8262B00C;
	// rlwinm r11,r8,10,30,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x3;
	// addi r9,r11,726
	ctx.r9.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// add r17,r11,r31
	r17.u64 = r11.u64 + r31.u64;
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// b 0x8262b018
	goto loc_8262B018;
loc_8262B00C:
	// addi r11,r31,2880
	r11.s64 = r31.s64 + 2880;
	// addi r17,r31,2892
	r17.s64 = r31.s64 + 2892;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_8262B018:
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// li r22,0
	r22.s64 = 0;
	// lwz r11,19696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// rlwinm r19,r5,1,0,30
	r19.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r7,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 1;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// lwz r8,3732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// add r10,r7,r11
	ctx.r10.u64 = ctx.r7.u64 + r11.u64;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// rlwinm r18,r3,1,0,30
	r18.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// li r16,8
	r16.s64 = 8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r24,r10,r11
	r24.u64 = ctx.r10.u64 + r11.u64;
	// lis r11,-32138
	r11.s64 = -2106195968;
	// addi r11,r11,14912
	r11.s64 = r11.s64 + 14912;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
loc_8262B064:
	// lwz r10,352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// rlwinm r9,r22,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r11,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r11.s64 = r22.s32 >> 1;
	// lwz r30,120(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// clrlwi r20,r22,31
	r20.u64 = r22.u32 & 0x1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// add r27,r19,r20
	r27.u64 = r19.u64 + r20.u64;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// add r26,r11,r18
	r26.u64 = r11.u64 + r18.u64;
	// lbzx r28,r22,r30
	r28.u64 = PPC_LOAD_U8(r22.u32 + r30.u32);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// rlwinm r11,r10,30,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x1;
	// bne cr6,0x8262b0b0
	if (!cr6.eq) goto loc_8262B0B0;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x8262b0b0
	if (!cr6.eq) goto loc_8262B0B0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262b0b0
	if (!cr6.eq) goto loc_8262B0B0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262b36c
	if (cr6.eq) goto loc_8262B36C;
loc_8262B0B0:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// beq cr6,0x8262b36c
	if (cr6.eq) goto loc_8262B36C;
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r5,460(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// lwz r4,364(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r29,0
	r29.s64 = 0;
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r30,r11,r5
	r30.u64 = r11.u64 + ctx.r5.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82629ee8
	sub_82629EE8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262b11c
	if (cr6.eq) goto loc_8262B11C;
	// addi r29,r1,144
	r29.s64 = ctx.r1.s64 + 144;
loc_8262B11C:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// stw r16,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r16.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82629260
	sub_82629260(ctx, base);
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r22,r11
	r11.u64 = r22.u64 + r11.u64;
	// stb r10,6(r11)
	PPC_STORE_U8(r11.u32 + 6, ctx.r10.u8);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262bc98
	if (cr6.eq) goto loc_8262BC98;
	// rlwinm r11,r22,0,30,30
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 0) & 0x2;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r10,r27,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,745
	ctx.r8.s64 = r11.s64 + 745;
	// rlwinm r11,r9,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262B1CC:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b1cc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B1CC;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r25,16
	ctx.r5.s64 = r25.s64 + 16;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262B200:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b200
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B200;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r25,32
	ctx.r7.s64 = r25.s64 + 32;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262B238:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b238
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B238;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r25,48
	ctx.r5.s64 = r25.s64 + 48;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262B270:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b270
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B270;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r25,64
	ctx.r7.s64 = r25.s64 + 64;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262B2A8:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b2a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B2A8;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r25,80
	ctx.r5.s64 = r25.s64 + 80;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262B2E0:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b2e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B2E0;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r25,96
	ctx.r5.s64 = r25.s64 + 96;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262B31C:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262b31c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B31C;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r7,r9,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r9,r25,112
	ctx.r9.s64 = r25.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262B354:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262b354
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262B354;
	// b 0x8262bc98
	goto loc_8262BC98;
loc_8262B36C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8262bc6c
	if (cr6.eq) goto loc_8262BC6C;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262b4f0
	if (cr6.eq) goto loc_8262B4F0;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262b4f0
	if (!cr6.eq) goto loc_8262B4F0;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262b484
	if (cr6.lt) goto loc_8262B484;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262b47c
	if (!cr6.lt) goto loc_8262B47C;
loc_8262B3E4:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262b410
	if (cr6.lt) goto loc_8262B410;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262b3e4
	if (cr6.eq) goto loc_8262B3E4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262b4c8
	goto loc_8262B4C8;
loc_8262B410:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262B47C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262b4c8
	goto loc_8262B4C8;
loc_8262B484:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r15,r11,32768
	r15.u64 = r11.u64 | 32768;
loc_8262B494:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262b494
	if (cr6.lt) goto loc_8262B494;
loc_8262B4C8:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r30,120(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r14,r11,r10
	r14.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r15,r11,r9
	r15.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8262B4F0:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// stb r14,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r14.u8);
	// bne cr6,0x8262b544
	if (!cr6.eq) goto loc_8262B544;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x8262bc00
	goto loc_8262BC00;
loc_8262B544:
	// cmpwi cr6,r14,1
	cr6.compare<int32_t>(r14.s32, 1, xer);
	// bne cr6,0x8262b74c
	if (!cr6.eq) goto loc_8262B74C;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262b618
	if (cr6.eq) goto loc_8262B618;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262b600
	if (!cr6.eq) goto loc_8262B600;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262b600
	if (!cr6.eq) goto loc_8262B600;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b5bc
	if (!cr0.lt) goto loc_8262B5BC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B5BC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b6b4
	if (!cr6.eq) goto loc_8262B6B4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b5f0
	if (!cr0.lt) goto loc_8262B5F0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B5F0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b6b0
	if (!cr6.eq) goto loc_8262B6B0;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262b6b4
	goto loc_8262B6B4;
loc_8262B600:
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r30
	PPC_STORE_U8(r22.u32 + r30.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262b6c4
	goto loc_8262B6C4;
loc_8262B618:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262b640
	if (cr6.eq) goto loc_8262B640;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r30
	PPC_STORE_U8(r22.u32 + r30.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262b6c4
	goto loc_8262B6C4;
loc_8262B640:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b66c
	if (!cr0.lt) goto loc_8262B66C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B66C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b6b4
	if (!cr6.eq) goto loc_8262B6B4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b6a0
	if (!cr0.lt) goto loc_8262B6A0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B6A0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b6b0
	if (!cr6.eq) goto loc_8262B6B0;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262b6b4
	goto loc_8262B6B4;
loc_8262B6B0:
	// li r29,0
	r29.s64 = 0;
loc_8262B6B4:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
	// stbx r11,r22,r10
	PPC_STORE_U8(r22.u32 + ctx.r10.u32, r11.u8);
loc_8262B6C4:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262b710
	if (cr6.eq) goto loc_8262B710;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262B710:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262bc0c
	if (cr6.eq) goto loc_8262BC0C;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262bbf8
	goto loc_8262BBF8;
loc_8262B74C:
	// cmpwi cr6,r14,2
	cr6.compare<int32_t>(r14.s32, 2, xer);
	// bne cr6,0x8262b954
	if (!cr6.eq) goto loc_8262B954;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262b820
	if (cr6.eq) goto loc_8262B820;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262b808
	if (!cr6.eq) goto loc_8262B808;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262b808
	if (!cr6.eq) goto loc_8262B808;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b7c4
	if (!cr0.lt) goto loc_8262B7C4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B7C4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b8bc
	if (!cr6.eq) goto loc_8262B8BC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b7f8
	if (!cr0.lt) goto loc_8262B7F8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B7F8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b8b8
	if (!cr6.eq) goto loc_8262B8B8;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262b8bc
	goto loc_8262B8BC;
loc_8262B808:
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r30
	PPC_STORE_U8(r22.u32 + r30.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262b8cc
	goto loc_8262B8CC;
loc_8262B820:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262b848
	if (cr6.eq) goto loc_8262B848;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r30
	PPC_STORE_U8(r22.u32 + r30.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262b8cc
	goto loc_8262B8CC;
loc_8262B848:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b874
	if (!cr0.lt) goto loc_8262B874;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B874:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b8bc
	if (!cr6.eq) goto loc_8262B8BC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262b8a8
	if (!cr0.lt) goto loc_8262B8A8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262B8A8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262b8b8
	if (!cr6.eq) goto loc_8262B8B8;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262b8bc
	goto loc_8262B8BC;
loc_8262B8B8:
	// li r29,0
	r29.s64 = 0;
loc_8262B8BC:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
	// stbx r11,r22,r10
	PPC_STORE_U8(r22.u32 + ctx.r10.u32, r11.u8);
loc_8262B8CC:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262b918
	if (cr6.eq) goto loc_8262B918;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262B918:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262bc0c
	if (cr6.eq) goto loc_8262BC0C;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262bbf8
	goto loc_8262BBF8;
loc_8262B954:
	// cmpwi cr6,r14,4
	cr6.compare<int32_t>(r14.s32, 4, xer);
	// bne cr6,0x8262bc0c
	if (!cr6.eq) goto loc_8262BC0C;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262ba64
	if (cr6.lt) goto loc_8262BA64;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x8262ba5c
	if (!cr6.lt) goto loc_8262BA5C;
loc_8262B9C4:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262b9f0
	if (cr6.lt) goto loc_8262B9F0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262b9c4
	if (cr6.eq) goto loc_8262B9C4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262baa8
	goto loc_8262BAA8;
loc_8262B9F0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262BA5C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262baa8
	goto loc_8262BAA8;
loc_8262BA64:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r23,r11,32768
	r23.u64 = r11.u64 | 32768;
loc_8262BA74:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r23
	r11.u64 = r29.u64 + r23.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262ba74
	if (cr6.lt) goto loc_8262BA74;
loc_8262BAA8:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stbx r30,r22,r10
	PPC_STORE_U8(r22.u32 + ctx.r10.u32, r30.u8);
	// beq cr6,0x8262bb1c
	if (cr6.eq) goto loc_8262BB1C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262BB1C:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262bb6c
	if (cr6.eq) goto loc_8262BB6C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262BB6C:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262bbbc
	if (cr6.eq) goto loc_8262BBBC;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262BBBC:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262bc0c
	if (cr6.eq) goto loc_8262BC0C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_8262BBF8:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_8262BC00:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262BC0C:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8262bc48
	if (!cr6.eq) goto loc_8262BC48;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// li r11,64
	r11.s64 = 64;
loc_8262BC2C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8262bc2c
	if (!cr6.eq) goto loc_8262BC2C;
loc_8262BC48:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82629260
	sub_82629260(ctx, base);
	// li r23,0
	r23.s64 = 0;
	// b 0x8262bc98
	goto loc_8262BC98;
loc_8262BC6C:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82629260
	sub_82629260(ctx, base);
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r22,r11
	r11.u64 = r22.u64 + r11.u64;
	// stb r10,6(r11)
	PPC_STORE_U8(r11.u32 + 6, ctx.r10.u8);
loc_8262BC98:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8262bca8
	if (cr6.eq) goto loc_8262BCA8;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// b 0x8262bcac
	goto loc_8262BCAC;
loc_8262BCA8:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_8262BCAC:
	// add r21,r11,r21
	r21.u64 = r11.u64 + r21.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8262bcc0
	if (cr6.eq) goto loc_8262BCC0;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// b 0x8262bcc4
	goto loc_8262BCC4;
loc_8262BCC0:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_8262BCC4:
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// add r24,r11,r24
	r24.u64 = r11.u64 + r24.u64;
	// cmpwi cr6,r22,4
	cr6.compare<int32_t>(r22.s32, 4, xer);
	// blt cr6,0x8262b064
	if (cr6.lt) goto loc_8262B064;
	// lwz r10,436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lwz r11,19700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// lwz r9,428(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// lwz r4,136(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r21,404(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// srawi r6,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// mullw r7,r3,r7
	ctx.r7.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r7.s32);
	// lwz r28,396(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// lwz r5,1780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// lwz r8,3736(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r9,3740(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mullw r10,r4,r21
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(r21.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r8,r11
	r27.u64 = ctx.r8.u64 + r11.u64;
	// add r24,r9,r11
	r24.u64 = ctx.r9.u64 + r11.u64;
	// lhzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r5.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// addi r7,r7,-16384
	ctx.r7.s64 = ctx.r7.s64 + -16384;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r26,r7,27,31,31
	r26.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x8262bfdc
	if (cr6.eq) goto loc_8262BFDC;
	// lwz r9,464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// rlwinm r11,r10,5,0,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r20,364(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r19,0
	r19.s64 = 0;
	// add r30,r11,r9
	r30.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r29,r19
	r29.u64 = r19.u64;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262bd8c
	if (cr6.eq) goto loc_8262BD8C;
	// addi r29,r1,144
	r29.s64 = ctx.r1.s64 + 144;
loc_8262BD8C:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// stw r16,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r16.u32);
	// stw r19,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r19.u32);
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r25.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// lwz r4,380(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// stb r19,10(r20)
	PPC_STORE_U8(r20.u32 + 10, r19.u8);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c91c
	if (cr6.eq) goto loc_8262C91C;
	// lwz r11,396(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BE30:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262be30
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BE30;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r6,r25,16
	ctx.r6.s64 = r25.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BE64:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262be64
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BE64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r25,32
	ctx.r8.s64 = r25.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BE9C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262be9c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BE9C;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r25,48
	ctx.r9.s64 = r25.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BED4:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262bed4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BED4;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r25,64
	ctx.r8.s64 = r25.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BF0C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262bf0c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BF0C;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r25,80
	ctx.r9.s64 = r25.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BF44:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262bf44
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BF44;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r25,96
	ctx.r9.s64 = r25.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262BF80:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262bf80
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BF80;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r25,112
	ctx.r9.s64 = r25.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262BFB8:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262bfb8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262BFB8;
	// b 0x8262c91c
	goto loc_8262C91C;
loc_8262BFD0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
loc_8262BFDC:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lbz r11,4(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262c8f4
	if (cr6.eq) goto loc_8262C8F4;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262c16c
	if (cr6.eq) goto loc_8262C16C;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262c16c
	if (!cr6.eq) goto loc_8262C16C;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262c104
	if (cr6.lt) goto loc_8262C104;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262c0fc
	if (!cr6.lt) goto loc_8262C0FC;
loc_8262C064:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262c090
	if (cr6.lt) goto loc_8262C090;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262c064
	if (cr6.eq) goto loc_8262C064;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262c148
	goto loc_8262C148;
loc_8262C090:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262C0FC:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262c148
	goto loc_8262C148;
loc_8262C104:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r21,r11,32768
	r21.u64 = r11.u64 | 32768;
loc_8262C114:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r21
	r11.u64 = r29.u64 + r21.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262c114
	if (cr6.lt) goto loc_8262C114;
loc_8262C148:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r14,r11,r10
	r14.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r15,r11,r9
	r15.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8262C16C:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// stb r14,10(r10)
	PPC_STORE_U8(ctx.r10.u32 + 10, r14.u8);
	// bne cr6,0x8262c1bc
	if (!cr6.eq) goto loc_8262C1BC;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x8262c888
	goto loc_8262C888;
loc_8262C1BC:
	// cmpwi cr6,r14,1
	cr6.compare<int32_t>(r14.s32, 1, xer);
	// bne cr6,0x8262c3cc
	if (!cr6.eq) goto loc_8262C3CC;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c294
	if (cr6.eq) goto loc_8262C294;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262c278
	if (!cr6.eq) goto loc_8262C278;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262c278
	if (!cr6.eq) goto loc_8262C278;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c234
	if (!cr0.lt) goto loc_8262C234;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C234:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c334
	if (!cr6.eq) goto loc_8262C334;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c268
	if (!cr0.lt) goto loc_8262C268;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C268:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c330
	if (!cr6.eq) goto loc_8262C330;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262c334
	goto loc_8262C334;
loc_8262C278:
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stb r11,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262c344
	goto loc_8262C344;
loc_8262C294:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262c2c0
	if (cr6.eq) goto loc_8262C2C0;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r9
	PPC_STORE_U8(r22.u32 + ctx.r9.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262c344
	goto loc_8262C344;
loc_8262C2C0:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c2ec
	if (!cr0.lt) goto loc_8262C2EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C2EC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c334
	if (!cr6.eq) goto loc_8262C334;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c320
	if (!cr0.lt) goto loc_8262C320;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C320:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c330
	if (!cr6.eq) goto loc_8262C330;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262c334
	goto loc_8262C334;
loc_8262C330:
	// li r29,0
	r29.s64 = 0;
loc_8262C334:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
	// stb r11,4(r10)
	PPC_STORE_U8(ctx.r10.u32 + 4, r11.u8);
loc_8262C344:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262c390
	if (cr6.eq) goto loc_8262C390;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C390:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262c894
	if (cr6.eq) goto loc_8262C894;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262c880
	goto loc_8262C880;
loc_8262C3CC:
	// cmpwi cr6,r14,2
	cr6.compare<int32_t>(r14.s32, 2, xer);
	// bne cr6,0x8262c5dc
	if (!cr6.eq) goto loc_8262C5DC;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c4a4
	if (cr6.eq) goto loc_8262C4A4;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262c488
	if (!cr6.eq) goto loc_8262C488;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262c488
	if (!cr6.eq) goto loc_8262C488;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c444
	if (!cr0.lt) goto loc_8262C444;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C444:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c544
	if (!cr6.eq) goto loc_8262C544;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c478
	if (!cr0.lt) goto loc_8262C478;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C478:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c540
	if (!cr6.eq) goto loc_8262C540;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262c544
	goto loc_8262C544;
loc_8262C488:
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stb r11,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262c554
	goto loc_8262C554;
loc_8262C4A4:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262c4d0
	if (cr6.eq) goto loc_8262C4D0;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r22,r9
	PPC_STORE_U8(r22.u32 + ctx.r9.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262c554
	goto loc_8262C554;
loc_8262C4D0:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c4fc
	if (!cr0.lt) goto loc_8262C4FC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C4FC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c544
	if (!cr6.eq) goto loc_8262C544;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262c530
	if (!cr0.lt) goto loc_8262C530;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262C530:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262c540
	if (!cr6.eq) goto loc_8262C540;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262c544
	goto loc_8262C544;
loc_8262C540:
	// li r29,0
	r29.s64 = 0;
loc_8262C544:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
	// stb r11,4(r10)
	PPC_STORE_U8(ctx.r10.u32 + 4, r11.u8);
loc_8262C554:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262c5a0
	if (cr6.eq) goto loc_8262C5A0;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C5A0:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262c894
	if (cr6.eq) goto loc_8262C894;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262c880
	goto loc_8262C880;
loc_8262C5DC:
	// cmpwi cr6,r14,4
	cr6.compare<int32_t>(r14.s32, 4, xer);
	// bne cr6,0x8262c894
	if (!cr6.eq) goto loc_8262C894;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262c6ec
	if (cr6.lt) goto loc_8262C6EC;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x8262c6e4
	if (!cr6.lt) goto loc_8262C6E4;
loc_8262C64C:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262c678
	if (cr6.lt) goto loc_8262C678;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262c64c
	if (cr6.eq) goto loc_8262C64C;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262c730
	goto loc_8262C730;
loc_8262C678:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262C6E4:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262c730
	goto loc_8262C730;
loc_8262C6EC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r23,r11,32768
	r23.u64 = r11.u64 | 32768;
loc_8262C6FC:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r23
	r11.u64 = r29.u64 + r23.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262c6fc
	if (cr6.lt) goto loc_8262C6FC;
loc_8262C730:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stb r30,4(r10)
	PPC_STORE_U8(ctx.r10.u32 + 4, r30.u8);
	// beq cr6,0x8262c7a4
	if (cr6.eq) goto loc_8262C7A4;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C7A4:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c7f4
	if (cr6.eq) goto loc_8262C7F4;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C7F4:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c844
	if (cr6.eq) goto loc_8262C844;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C844:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262c894
	if (cr6.eq) goto loc_8262C894;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_8262C880:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_8262C888:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262C894:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8262c8d0
	if (!cr6.eq) goto loc_8262C8D0;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// li r11,64
	r11.s64 = 64;
loc_8262C8B4:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8262c8b4
	if (!cr6.eq) goto loc_8262C8B4;
loc_8262C8D0:
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// lwz r7,404(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r6,396(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,380(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// li r23,0
	r23.s64 = 0;
	// b 0x8262c91c
	goto loc_8262C91C;
loc_8262C8F4:
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// lwz r4,380(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r11,0
	r11.s64 = 0;
	// stb r11,10(r10)
	PPC_STORE_U8(ctx.r10.u32 + 10, r11.u8);
loc_8262C91C:
	// addi r27,r22,1
	r27.s64 = r22.s64 + 1;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x8262cbc0
	if (cr6.eq) goto loc_8262CBC0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r20,0
	r20.s64 = 0;
	// lwz r25,404(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lwz r26,396(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// mullw r11,r11,r25
	r11.s64 = int64_t(r11.s32) * int64_t(r25.s32);
	// lwz r6,468(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// lwz r21,364(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// add r29,r11,r6
	r29.u64 = r11.u64 + ctx.r6.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r28,r20
	r28.u64 = r20.u64;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262c984
	if (cr6.eq) goto loc_8262C984;
	// addi r28,r1,144
	r28.s64 = ctx.r1.s64 + 144;
loc_8262C984:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lbz r6,5(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// stw r16,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r16.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// lwz r4,388(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// stb r20,11(r21)
	PPC_STORE_U8(r21.u32 + 11, r20.u8);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d4d8
	if (cr6.eq) goto loc_8262D4D8;
	// rlwinm r10,r26,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CA20:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ca20
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CA20;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r6,r30,16
	ctx.r6.s64 = r30.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CA54:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ca54
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CA54;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,32
	ctx.r8.s64 = r30.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CA8C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ca8c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CA8C;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CAC4:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262cac4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CAC4;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CAFC:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262cafc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CAFC;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,80
	ctx.r9.s64 = r30.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CB34:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262cb34
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CB34;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,96
	ctx.r9.s64 = r30.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262CB70:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262cb70
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CB70;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,112
	ctx.r9.s64 = r30.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262CBA8:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262cba8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262CBA8;
	// b 0x8262d4d8
	goto loc_8262D4D8;
loc_8262CBC0:
	// lwz r22,120(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lbz r11,5(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 5);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262d4ac
	if (cr6.eq) goto loc_8262D4AC;
	// lwz r21,364(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r20,0
	r20.s64 = 0;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262cd60
	if (cr6.eq) goto loc_8262CD60;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262cd60
	if (!cr6.eq) goto loc_8262CD60;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262ccf4
	if (cr6.lt) goto loc_8262CCF4;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262cce4
	if (!cr6.lt) goto loc_8262CCE4;
loc_8262CC44:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262cc78
	if (cr6.lt) goto loc_8262CC78;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262cc44
	if (cr6.eq) goto loc_8262CC44;
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
	// b 0x8262cd38
	goto loc_8262CD38;
loc_8262CC78:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262CCE4:
	// lis r11,0
	r11.s64 = 0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
	// b 0x8262cd38
	goto loc_8262CD38;
loc_8262CCF4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_8262CD04:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262cd04
	if (cr6.lt) goto loc_8262CD04;
loc_8262CD38:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r14,r11,r10
	r14.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r15,r11,r9
	r15.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// b 0x8262cd68
	goto loc_8262CD68;
loc_8262CD60:
	// lis r11,0
	r11.s64 = 0;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_8262CD68:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// stb r14,11(r21)
	PPC_STORE_U8(r21.u32 + 11, r14.u8);
	// bne cr6,0x8262cdb4
	if (!cr6.eq) goto loc_8262CDB4;
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x8262d444
	goto loc_8262D444;
loc_8262CDB4:
	// cmpwi cr6,r14,1
	cr6.compare<int32_t>(r14.s32, 1, xer);
	// bne cr6,0x8262cfac
	if (!cr6.eq) goto loc_8262CFAC;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262ce80
	if (cr6.eq) goto loc_8262CE80;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262ce6c
	if (!cr6.eq) goto loc_8262CE6C;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262ce6c
	if (!cr6.eq) goto loc_8262CE6C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262ce28
	if (!cr0.lt) goto loc_8262CE28;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262CE28:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262cf18
	if (!cr6.eq) goto loc_8262CF18;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262ce5c
	if (!cr0.lt) goto loc_8262CE5C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262CE5C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262cf14
	if (!cr6.eq) goto loc_8262CF14;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262cf18
	goto loc_8262CF18;
loc_8262CE6C:
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262cf20
	goto loc_8262CF20;
loc_8262CE80:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262cea4
	if (cr6.eq) goto loc_8262CEA4;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r27,r22
	PPC_STORE_U8(r27.u32 + r22.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262cf24
	goto loc_8262CF24;
loc_8262CEA4:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262ced0
	if (!cr0.lt) goto loc_8262CED0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262CED0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262cf18
	if (!cr6.eq) goto loc_8262CF18;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262cf04
	if (!cr0.lt) goto loc_8262CF04;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262CF04:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262cf14
	if (!cr6.eq) goto loc_8262CF14;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262cf18
	goto loc_8262CF18;
loc_8262CF14:
	// li r29,0
	r29.s64 = 0;
loc_8262CF18:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262CF20:
	// stb r11,5(r22)
	PPC_STORE_U8(r22.u32 + 5, r11.u8);
loc_8262CF24:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262cf70
	if (cr6.eq) goto loc_8262CF70;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262CF70:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262d450
	if (cr6.eq) goto loc_8262D450;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262d43c
	goto loc_8262D43C;
loc_8262CFAC:
	// cmpwi cr6,r14,2
	cr6.compare<int32_t>(r14.s32, 2, xer);
	// bne cr6,0x8262d1a4
	if (!cr6.eq) goto loc_8262D1A4;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d078
	if (cr6.eq) goto loc_8262D078;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8262d064
	if (!cr6.eq) goto loc_8262D064;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262d064
	if (!cr6.eq) goto loc_8262D064;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262d020
	if (!cr0.lt) goto loc_8262D020;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262D020:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262d110
	if (!cr6.eq) goto loc_8262D110;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262d054
	if (!cr0.lt) goto loc_8262D054;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262D054:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262d10c
	if (!cr6.eq) goto loc_8262D10C;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262d110
	goto loc_8262D110;
loc_8262D064:
	// clrlwi r11,r15,24
	r11.u64 = r15.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262d118
	goto loc_8262D118;
loc_8262D078:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x8262d09c
	if (cr6.eq) goto loc_8262D09C;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// stbx r11,r27,r22
	PPC_STORE_U8(r27.u32 + r22.u32, r11.u8);
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262d11c
	goto loc_8262D11C;
loc_8262D09C:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262d0c8
	if (!cr0.lt) goto loc_8262D0C8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262D0C8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262d110
	if (!cr6.eq) goto loc_8262D110;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262d0fc
	if (!cr0.lt) goto loc_8262D0FC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262D0FC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262d10c
	if (!cr6.eq) goto loc_8262D10C;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262d110
	goto loc_8262D110;
loc_8262D10C:
	// li r29,0
	r29.s64 = 0;
loc_8262D110:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262D118:
	// stb r11,5(r22)
	PPC_STORE_U8(r22.u32 + 5, r11.u8);
loc_8262D11C:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262d168
	if (cr6.eq) goto loc_8262D168;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262D168:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262d450
	if (cr6.eq) goto loc_8262D450;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262d43c
	goto loc_8262D43C;
loc_8262D1A4:
	// cmpwi cr6,r14,4
	cr6.compare<int32_t>(r14.s32, 4, xer);
	// bne cr6,0x8262d450
	if (!cr6.eq) goto loc_8262D450;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262d2b4
	if (cr6.lt) goto loc_8262D2B4;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x8262d2ac
	if (!cr6.lt) goto loc_8262D2AC;
loc_8262D214:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262d240
	if (cr6.lt) goto loc_8262D240;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262d214
	if (cr6.eq) goto loc_8262D214;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262d2f0
	goto loc_8262D2F0;
loc_8262D240:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262D2AC:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262d2f0
	goto loc_8262D2F0;
loc_8262D2B4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262D2BC:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r26
	r11.u64 = r29.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262d2bc
	if (cr6.lt) goto loc_8262D2BC;
loc_8262D2F0:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262bfd0
	if (!cr6.eq) goto loc_8262BFD0;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stb r30,5(r22)
	PPC_STORE_U8(r22.u32 + 5, r30.u8);
	// beq cr6,0x8262d360
	if (cr6.eq) goto loc_8262D360;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262D360:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d3b0
	if (cr6.eq) goto loc_8262D3B0;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262D3B0:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d400
	if (cr6.eq) goto loc_8262D400;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262D400:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d450
	if (cr6.eq) goto loc_8262D450;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262d4e8
	if (!cr6.eq) goto loc_8262D4E8;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_8262D43C:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_8262D444:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262D450:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8262d48c
	if (!cr6.eq) goto loc_8262D48C;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// li r11,64
	r11.s64 = 64;
loc_8262D470:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8262d470
	if (!cr6.eq) goto loc_8262D470;
loc_8262D48C:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// lwz r7,404(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r6,396(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,388(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// b 0x8262d4d8
	goto loc_8262D4D8;
loc_8262D4AC:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// lwz r7,404(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,396(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,388(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// bl 0x826297b8
	sub_826297B8(ctx, base);
	// lwz r21,364(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r11,0
	r11.s64 = 0;
	// lwz r20,132(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stb r11,11(r21)
	PPC_STORE_U8(r21.u32 + 11, r11.u8);
loc_8262D4D8:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r20,31,0,0
	r11.u64 = (__builtin_rotateleft32(r20.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
loc_8262D4E8:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8262D4F0"))) PPC_WEAK_FUNC(sub_8262D4F0);
PPC_FUNC_IMPL(__imp__sub_8262D4F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r6,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r6.u32);
	// mr r15,r8
	r15.u64 = ctx.r8.u64;
	// stw r7,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r7.u32);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// mr r18,r4
	r18.u64 = ctx.r4.u64;
	// mr r19,r5
	r19.u64 = ctx.r5.u64;
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// addi r16,r18,12
	r16.s64 = r18.s64 + 12;
	// stw r8,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r8.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262d53c
	if (!cr6.eq) goto loc_8262D53C;
	// lwz r11,21236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// li r14,1
	r14.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d540
	if (cr6.eq) goto loc_8262D540;
loc_8262D53C:
	// li r14,0
	r14.s64 = 0;
loc_8262D540:
	// lwz r9,392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// lbz r11,4(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 4);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r10,6548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// rotlwi r9,r11,2
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 2);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r17,r11,r10
	r17.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x8262d58c
	if (cr6.eq) goto loc_8262D58C;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,10,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r22,r11,r31
	r22.u64 = r11.u64 + r31.u64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// b 0x8262d598
	goto loc_8262D598;
loc_8262D58C:
	// addi r11,r31,2880
	r11.s64 = r31.s64 + 2880;
	// addi r22,r31,2892
	r22.s64 = r31.s64 + 2892;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
loc_8262D598:
	// li r25,0
	r25.s64 = 0;
	// rlwinm r21,r8,1,0,30
	r21.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r15,1,0,30
	r20.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 1) & 0xFFFFFFFE;
	// li r24,8
	r24.s64 = 8;
loc_8262D5A8:
	// srawi r11,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r11.s64 = r25.s32 >> 1;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r23,r25,31
	r23.u64 = r25.u32 & 0x1;
	// lwz r10,460(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// lbzx r27,r25,r16
	r27.u64 = PPC_LOAD_U8(r25.u32 + r16.u32);
	// add r28,r20,r23
	r28.u64 = r20.u64 + r23.u64;
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// add r8,r11,r21
	ctx.r8.u64 = r11.u64 + r21.u64;
	// li r26,0
	r26.s64 = 0;
	// mullw r11,r9,r8
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r1,116
	ctx.r10.s64 = ctx.r1.s64 + 116;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x8262d64c
	if (cr6.eq) goto loc_8262D64C;
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8262a1b8
	sub_8262A1B8(ctx, base);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// b 0x8262d698
	goto loc_8262D698;
loc_8262D64C:
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82629ee8
	sub_82629EE8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262d664
	if (cr6.eq) goto loc_8262D664;
	// addi r26,r1,128
	r26.s64 = ctx.r1.s64 + 128;
loc_8262D664:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
loc_8262D698:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262de84
	if (!cr6.eq) goto loc_8262DE84;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262d87c
	if (cr6.eq) goto loc_8262D87C;
	// rlwinm r11,r25,0,30,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x2;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r10,r28,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,745
	ctx.r8.s64 = r11.s64 + 745;
	// rlwinm r11,r9,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262D6E0:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d6e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D6E0;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r30,16
	ctx.r5.s64 = r30.s64 + 16;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262D714:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d714
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D714;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r30,32
	ctx.r7.s64 = r30.s64 + 32;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262D74C:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d74c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D74C;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r30,48
	ctx.r5.s64 = r30.s64 + 48;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262D784:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d784
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D784;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r30,64
	ctx.r7.s64 = r30.s64 + 64;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262D7BC:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d7bc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D7BC;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r30,80
	ctx.r5.s64 = r30.s64 + 80;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262D7F4:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d7f4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D7F4;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r30,96
	ctx.r5.s64 = r30.s64 + 96;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262D830:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262d830
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D830;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r7,r9,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r9,r30,112
	ctx.r9.s64 = r30.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262D868:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262d868
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D868;
loc_8262D87C:
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// lwz r11,3152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3152);
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// add r11,r25,r18
	r11.u64 = r25.u64 + r18.u64;
	// li r23,0
	r23.s64 = 0;
	// stb r23,6(r11)
	PPC_STORE_U8(r11.u32 + 6, r23.u8);
	// beq cr6,0x8262d8b8
	if (cr6.eq) goto loc_8262D8B8;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// b 0x8262d8bc
	goto loc_8262D8BC;
loc_8262D8B8:
	// mr r11,r24
	r11.u64 = r24.u64;
loc_8262D8BC:
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// add r19,r11,r19
	r19.u64 = r11.u64 + r19.u64;
	// cmpwi cr6,r25,4
	cr6.compare<int32_t>(r25.s32, 4, xer);
	// blt cr6,0x8262d5a8
	if (cr6.lt) goto loc_8262D5A8;
	// lwz r26,388(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r28,r23
	r28.u64 = r23.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// lwz r10,464(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x8262d958
	if (cr6.eq) goto loc_8262D958;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// bl 0x8262a870
	sub_8262A870(ctx, base);
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// lwz r22,124(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lbz r6,4(r16)
	ctx.r6.u64 = PPC_LOAD_U8(r16.u32 + 4);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// b 0x8262d9a8
	goto loc_8262D9A8;
loc_8262D958:
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262d96c
	if (cr6.eq) goto loc_8262D96C;
	// addi r28,r1,128
	r28.s64 = ctx.r1.s64 + 128;
loc_8262D96C:
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r22,124(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lbz r6,4(r16)
	ctx.r6.u64 = PPC_LOAD_U8(r16.u32 + 4);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
loc_8262D9A8:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262de84
	if (!cr6.eq) goto loc_8262DE84;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262db80
	if (cr6.eq) goto loc_8262DB80;
	// rlwinm r10,r15,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262D9E4:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262d9e4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262D9E4;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r6,r30,16
	ctx.r6.s64 = r30.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DA18:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262da18
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DA18;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r30,32
	ctx.r8.s64 = r30.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DA50:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262da50
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DA50;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DA88:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262da88
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DA88;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DAC0:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dac0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DAC0;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r30,80
	ctx.r9.s64 = r30.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DAF8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262daf8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DAF8;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r30,96
	ctx.r9.s64 = r30.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DB34:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262db34
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DB34;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r30,112
	ctx.r9.s64 = r30.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262DB6C:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262db6c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DB6C;
loc_8262DB80:
	// lwz r4,364(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3152);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stb r23,10(r18)
	PPC_STORE_U8(r18.u32 + 10, r23.u8);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r28,r25,1
	r28.s64 = r25.s64 + 1;
	// lwz r10,468(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// mr r27,r23
	r27.u64 = r23.u64;
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// add r11,r11,r15
	r11.u64 = r11.u64 + r15.u64;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x8262dc2c
	if (cr6.eq) goto loc_8262DC2C;
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// bl 0x8262a870
	sub_8262A870(ctx, base);
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lbz r6,5(r16)
	ctx.r6.u64 = PPC_LOAD_U8(r16.u32 + 5);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// b 0x8262dc78
	goto loc_8262DC78;
loc_8262DC2C:
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262dc40
	if (cr6.eq) goto loc_8262DC40;
	// addi r27,r1,128
	r27.s64 = ctx.r1.s64 + 128;
loc_8262DC40:
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// lbz r6,5(r16)
	ctx.r6.u64 = PPC_LOAD_U8(r16.u32 + 5);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// stw r24,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r24.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
loc_8262DC78:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8262de84
	if (!cr6.eq) goto loc_8262DE84;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262de50
	if (cr6.eq) goto loc_8262DE50;
	// rlwinm r10,r15,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r15.u32 | (r15.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DCB4:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dcb4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DCB4;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r6,r30,16
	ctx.r6.s64 = r30.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DCE8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dce8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DCE8;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,32
	ctx.r8.s64 = r30.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DD20:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dd20
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DD20;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DD58:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dd58
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DD58;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DD90:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262dd90
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DD90;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,80
	ctx.r9.s64 = r30.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DDC8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ddc8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DDC8;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,96
	ctx.r9.s64 = r30.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262DE04:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262de04
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DE04;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,112
	ctx.r9.s64 = r30.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262DE3C:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262de3c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262DE3C;
loc_8262DE50:
	// lwz r4,372(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3152);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r23,11(r18)
	PPC_STORE_U8(r18.u32 + 11, r23.u8);
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
loc_8262DE84:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8262DE8C"))) PPC_WEAK_FUNC(sub_8262DE8C);
PPC_FUNC_IMPL(__imp__sub_8262DE8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8262DE90"))) PPC_WEAK_FUNC(sub_8262DE90);
PPC_FUNC_IMPL(__imp__sub_8262DE90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,4(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// stw r4,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r4.u32);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rotlwi r7,r11,2
	ctx.r7.u64 = __builtin_rotateleft32(r11.u32, 2);
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// stw r5,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, ctx.r5.u32);
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,6548(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// addi r14,r4,12
	r14.s64 = ctx.r4.s64 + 12;
	// cntlzw r4,r10
	ctx.r4.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lwz r16,336(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// rlwinm r7,r4,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// stw r6,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r6.u32);
	// stw r30,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r30.u32);
	// rlwinm r17,r9,12,30,31
	r17.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0x3;
	// stw r29,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, r29.u32);
	// xori r25,r7,1
	r25.u64 = ctx.r7.u64 ^ 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// beq cr6,0x8262df14
	if (cr6.eq) goto loc_8262DF14;
	// rlwinm r16,r9,8,29,31
	r16.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0x7;
loc_8262DF14:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262df44
	if (cr6.eq) goto loc_8262DF44;
	// rlwinm r11,r9,10,30,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// add r19,r11,r31
	r19.u64 = r11.u64 + r31.u64;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// b 0x8262df50
	goto loc_8262DF50;
loc_8262DF44:
	// addi r11,r31,2880
	r11.s64 = r31.s64 + 2880;
	// addi r19,r31,2892
	r19.s64 = r31.s64 + 2892;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_8262DF50:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r24,0
	r24.s64 = 0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,30,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x1;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// bne cr6,0x8262df88
	if (!cr6.eq) goto loc_8262DF88;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x8262df88
	if (!cr6.eq) goto loc_8262DF88;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262df88
	if (!cr6.eq) goto loc_8262DF88;
	// li r11,1
	r11.s64 = 1;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// b 0x8262df8c
	goto loc_8262DF8C;
loc_8262DF88:
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r24.u32);
loc_8262DF8C:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82629528
	sub_82629528(ctx, base);
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r21,r30,1,0,30
	r21.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,14912
	r11.s64 = r11.s64 + 14912;
	// rlwinm r20,r29,1,0,30
	r20.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// li r18,8
	r18.s64 = 8;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// lis r11,0
	r11.s64 = 0;
	// ori r15,r11,32768
	r15.u64 = r11.u64 | 32768;
loc_8262DFC0:
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// clrlwi r22,r24,31
	r22.u64 = r24.u32 & 0x1;
	// lbzx r27,r24,r14
	r27.u64 = PPC_LOAD_U8(r24.u32 + r14.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// srawi r11,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r11.s64 = r24.s32 >> 1;
	// add r30,r21,r22
	r30.u64 = r21.u64 + r22.u64;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// add r8,r11,r20
	ctx.r8.u64 = r11.u64 + r20.u64;
	// cntlzw r11,r27
	r11.u64 = r27.u32 == 0 ? 32 : __builtin_clz(r27.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// beq cr6,0x8262e2b0
	if (cr6.eq) goto loc_8262E2B0;
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lwz r5,460(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// lwz r4,364(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r28,0
	r28.s64 = 0;
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r29,r11,r5
	r29.u64 = r11.u64 + ctx.r5.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82629ee8
	sub_82629EE8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262e050
	if (cr6.eq) goto loc_8262E050;
	// addi r28,r1,144
	r28.s64 = ctx.r1.s64 + 144;
loc_8262E050:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// stw r18,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r18.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262e270
	if (cr6.eq) goto loc_8262E270;
	// rlwinm r11,r24,0,30,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x2;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r10,r30,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r8,r11,745
	ctx.r8.s64 = r11.s64 + 745;
	// rlwinm r11,r9,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262E0D4:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e0d4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E0D4;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r26,16
	ctx.r5.s64 = r26.s64 + 16;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262E108:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e108
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E108;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r26,32
	ctx.r7.s64 = r26.s64 + 32;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262E140:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e140
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E140;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r26,48
	ctx.r5.s64 = r26.s64 + 48;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262E178:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e178
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E178;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r7,r26,64
	ctx.r7.s64 = r26.s64 + 64;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8262E1B0:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e1b0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E1B0;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r26,80
	ctx.r5.s64 = r26.s64 + 80;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262E1E8:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e1e8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E1E8;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwzx r6,r9,r31
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r5,r26,96
	ctx.r5.s64 = r26.s64 + 96;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262E224:
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8262e224
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E224;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r7,r9,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// addi r9,r26,112
	ctx.r9.s64 = r26.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262E25C:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262e25c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262E25C;
loc_8262E270:
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262E290:
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// stb r10,6(r11)
	PPC_STORE_U8(r11.u32 + 6, ctx.r10.u8);
loc_8262E2A0:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x8262eb84
	if (cr6.eq) goto loc_8262EB84;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// b 0x8262eb88
	goto loc_8262EB88;
loc_8262E2B0:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8262e290
	if (cr6.eq) goto loc_8262E290;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262e428
	if (cr6.eq) goto loc_8262E428;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262e428
	if (!cr6.eq) goto loc_8262E428;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262e3c8
	if (cr6.lt) goto loc_8262E3C8;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262e3c0
	if (!cr6.lt) goto loc_8262E3C0;
loc_8262E328:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262e354
	if (cr6.lt) goto loc_8262E354;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262e328
	if (cr6.eq) goto loc_8262E328;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262e404
	goto loc_8262E404;
loc_8262E354:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262E3C0:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262e404
	goto loc_8262E404;
loc_8262E3C8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262E3D0:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262e3d0
	if (cr6.lt) goto loc_8262E3D0;
loc_8262E404:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r16,r11,r10
	r16.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r17,r11,r9
	r17.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8262E428:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// add r10,r24,r10
	ctx.r10.u64 = r24.u64 + ctx.r10.u64;
	// stb r16,6(r10)
	PPC_STORE_U8(ctx.r10.u32 + 6, r16.u8);
	// bne cr6,0x8262e47c
	if (!cr6.eq) goto loc_8262E47C;
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// b 0x8262eb14
	goto loc_8262EB14;
loc_8262E47C:
	// cmpwi cr6,r16,1
	cr6.compare<int32_t>(r16.s32, 1, xer);
	// bne cr6,0x8262e678
	if (!cr6.eq) goto loc_8262E678;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262e54c
	if (cr6.eq) goto loc_8262E54C;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262e538
	if (!cr6.eq) goto loc_8262E538;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262e538
	if (!cr6.eq) goto loc_8262E538;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e4f4
	if (!cr0.lt) goto loc_8262E4F4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E4F4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e5e4
	if (!cr6.eq) goto loc_8262E5E4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e528
	if (!cr0.lt) goto loc_8262E528;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E528:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e5e0
	if (!cr6.eq) goto loc_8262E5E0;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262e5e4
	goto loc_8262E5E4;
loc_8262E538:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262e5ec
	goto loc_8262E5EC;
loc_8262E54C:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262e570
	if (cr6.eq) goto loc_8262E570;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262e5ec
	goto loc_8262E5EC;
loc_8262E570:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e59c
	if (!cr0.lt) goto loc_8262E59C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E59C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e5e4
	if (!cr6.eq) goto loc_8262E5E4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e5d0
	if (!cr0.lt) goto loc_8262E5D0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E5D0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e5e0
	if (!cr6.eq) goto loc_8262E5E0;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262e5e4
	goto loc_8262E5E4;
loc_8262E5E0:
	// li r29,0
	r29.s64 = 0;
loc_8262E5E4:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262E5EC:
	// stbx r11,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262e63c
	if (cr6.eq) goto loc_8262E63C;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262E63C:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262eb20
	if (cr6.eq) goto loc_8262EB20;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262eb0c
	goto loc_8262EB0C;
loc_8262E678:
	// cmpwi cr6,r16,2
	cr6.compare<int32_t>(r16.s32, 2, xer);
	// bne cr6,0x8262e874
	if (!cr6.eq) goto loc_8262E874;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262e748
	if (cr6.eq) goto loc_8262E748;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262e734
	if (!cr6.eq) goto loc_8262E734;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262e734
	if (!cr6.eq) goto loc_8262E734;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e6f0
	if (!cr0.lt) goto loc_8262E6F0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E6F0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e7e0
	if (!cr6.eq) goto loc_8262E7E0;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e724
	if (!cr0.lt) goto loc_8262E724;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E724:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e7dc
	if (!cr6.eq) goto loc_8262E7DC;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262e7e0
	goto loc_8262E7E0;
loc_8262E734:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262e7e8
	goto loc_8262E7E8;
loc_8262E748:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262e76c
	if (cr6.eq) goto loc_8262E76C;
	// lwz r11,364(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262e7e8
	goto loc_8262E7E8;
loc_8262E76C:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e798
	if (!cr0.lt) goto loc_8262E798;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E798:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e7e0
	if (!cr6.eq) goto loc_8262E7E0;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262e7cc
	if (!cr0.lt) goto loc_8262E7CC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262E7CC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262e7dc
	if (!cr6.eq) goto loc_8262E7DC;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262e7e0
	goto loc_8262E7E0;
loc_8262E7DC:
	// li r29,0
	r29.s64 = 0;
loc_8262E7E0:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262E7E8:
	// stbx r11,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262e838
	if (cr6.eq) goto loc_8262E838;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262E838:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262eb20
	if (cr6.eq) goto loc_8262EB20;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262eb0c
	goto loc_8262EB0C;
loc_8262E874:
	// cmpwi cr6,r16,4
	cr6.compare<int32_t>(r16.s32, 4, xer);
	// bne cr6,0x8262eb20
	if (!cr6.eq) goto loc_8262EB20;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262e984
	if (cr6.lt) goto loc_8262E984;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x8262e97c
	if (!cr6.lt) goto loc_8262E97C;
loc_8262E8E4:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262e910
	if (cr6.lt) goto loc_8262E910;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262e8e4
	if (cr6.eq) goto loc_8262E8E4;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262e9c0
	goto loc_8262E9C0;
loc_8262E910:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262E97C:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262e9c0
	goto loc_8262E9C0;
loc_8262E984:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262E98C:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262e98c
	if (cr6.lt) goto loc_8262E98C;
loc_8262E9C0:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stbx r30,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r30.u8);
	// beq cr6,0x8262ea30
	if (cr6.eq) goto loc_8262EA30;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262EA30:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262ea80
	if (cr6.eq) goto loc_8262EA80;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262EA80:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262ead0
	if (cr6.eq) goto loc_8262EAD0;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262EAD0:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262eb20
	if (cr6.eq) goto loc_8262EB20;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_8262EB0C:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_8262EB14:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262EB20:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8262eb5c
	if (!cr6.eq) goto loc_8262EB5C;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// li r11,64
	r11.s64 = 64;
loc_8262EB40:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8262eb40
	if (!cr6.eq) goto loc_8262EB40;
loc_8262EB5C:
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r25,0
	r25.s64 = 0;
	// b 0x8262e2a0
	goto loc_8262E2A0;
loc_8262EB84:
	// mr r11,r18
	r11.u64 = r18.u64;
loc_8262EB88:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// add r23,r11,r23
	r23.u64 = r11.u64 + r23.u64;
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// blt cr6,0x8262dfc0
	if (cr6.lt) goto loc_8262DFC0;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262ee48
	if (cr6.eq) goto loc_8262EE48;
	// lwz r7,404(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// li r23,0
	r23.s64 = 0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lwz r28,396(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// lwz r5,464(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// lwz r27,364(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r30,r11,r5
	r30.u64 = r11.u64 + ctx.r5.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r29,r23
	r29.u64 = r23.u64;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262ebfc
	if (cr6.eq) goto loc_8262EBFC;
	// addi r29,r1,144
	r29.s64 = ctx.r1.s64 + 144;
loc_8262EBFC:
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lbz r6,4(r14)
	ctx.r6.u64 = PPC_LOAD_U8(r14.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
	// stw r18,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r18.u32);
	// stw r23,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r23.u32);
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262ee14
	if (cr6.eq) goto loc_8262EE14;
	// rlwinm r10,r28,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262EC78:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ec78
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262EC78;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r6,r26,16
	ctx.r6.s64 = r26.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262ECAC:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ecac
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262ECAC;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r26,32
	ctx.r8.s64 = r26.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262ECE4:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ece4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262ECE4;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r26,48
	ctx.r9.s64 = r26.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262ED1C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ed1c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262ED1C;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r8,r26,64
	ctx.r8.s64 = r26.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262ED54:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ed54
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262ED54;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r26,80
	ctx.r9.s64 = r26.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262ED8C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262ed8c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262ED8C;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,2992(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r26,96
	ctx.r9.s64 = r26.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262EDC8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262edc8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262EDC8;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// addi r9,r26,112
	ctx.r9.s64 = r26.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262EE00:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262ee00
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262EE00;
loc_8262EE14:
	// lwz r4,380(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stb r23,10(r27)
	PPC_STORE_U8(r27.u32 + 10, r23.u8);
	// b 0x8262f71c
	goto loc_8262F71C;
loc_8262EE3C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
loc_8262EE48:
	// lbz r11,4(r14)
	r11.u64 = PPC_LOAD_U8(r14.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262f710
	if (cr6.eq) goto loc_8262F710;
	// lwz r27,364(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262efcc
	if (cr6.eq) goto loc_8262EFCC;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262efcc
	if (!cr6.eq) goto loc_8262EFCC;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262ef6c
	if (cr6.lt) goto loc_8262EF6C;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262ef64
	if (!cr6.lt) goto loc_8262EF64;
loc_8262EECC:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262eef8
	if (cr6.lt) goto loc_8262EEF8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262eecc
	if (cr6.eq) goto loc_8262EECC;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262efa8
	goto loc_8262EFA8;
loc_8262EEF8:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262EF64:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262efa8
	goto loc_8262EFA8;
loc_8262EF6C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262EF74:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262ef74
	if (cr6.lt) goto loc_8262EF74;
loc_8262EFA8:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r16,r11,r10
	r16.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r17,r11,r9
	r17.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8262EFCC:
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// stb r16,10(r27)
	PPC_STORE_U8(r27.u32 + 10, r16.u8);
	// bne cr6,0x8262f018
	if (!cr6.eq) goto loc_8262F018;
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// b 0x8262f6a0
	goto loc_8262F6A0;
loc_8262F018:
	// cmpwi cr6,r16,1
	cr6.compare<int32_t>(r16.s32, 1, xer);
	// bne cr6,0x8262f20c
	if (!cr6.eq) goto loc_8262F20C;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f0e4
	if (cr6.eq) goto loc_8262F0E4;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262f0d0
	if (!cr6.eq) goto loc_8262F0D0;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262f0d0
	if (!cr6.eq) goto loc_8262F0D0;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f08c
	if (!cr0.lt) goto loc_8262F08C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F08C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f178
	if (!cr6.eq) goto loc_8262F178;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f0c0
	if (!cr0.lt) goto loc_8262F0C0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F0C0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f174
	if (!cr6.eq) goto loc_8262F174;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262f178
	goto loc_8262F178;
loc_8262F0D0:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262f180
	goto loc_8262F180;
loc_8262F0E4:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262f104
	if (cr6.eq) goto loc_8262F104;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262f180
	goto loc_8262F180;
loc_8262F104:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f130
	if (!cr0.lt) goto loc_8262F130;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F130:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f178
	if (!cr6.eq) goto loc_8262F178;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f164
	if (!cr0.lt) goto loc_8262F164;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F164:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f174
	if (!cr6.eq) goto loc_8262F174;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262f178
	goto loc_8262F178;
loc_8262F174:
	// li r29,0
	r29.s64 = 0;
loc_8262F178:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262F180:
	// stbx r11,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262f1d0
	if (cr6.eq) goto loc_8262F1D0;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F1D0:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262f6ac
	if (cr6.eq) goto loc_8262F6AC;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262f698
	goto loc_8262F698;
loc_8262F20C:
	// cmpwi cr6,r16,2
	cr6.compare<int32_t>(r16.s32, 2, xer);
	// bne cr6,0x8262f400
	if (!cr6.eq) goto loc_8262F400;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f2d8
	if (cr6.eq) goto loc_8262F2D8;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262f2c4
	if (!cr6.eq) goto loc_8262F2C4;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262f2c4
	if (!cr6.eq) goto loc_8262F2C4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f280
	if (!cr0.lt) goto loc_8262F280;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F280:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f36c
	if (!cr6.eq) goto loc_8262F36C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f2b4
	if (!cr0.lt) goto loc_8262F2B4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F2B4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f368
	if (!cr6.eq) goto loc_8262F368;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262f36c
	goto loc_8262F36C;
loc_8262F2C4:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262f374
	goto loc_8262F374;
loc_8262F2D8:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262f2f8
	if (cr6.eq) goto loc_8262F2F8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262f374
	goto loc_8262F374;
loc_8262F2F8:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f324
	if (!cr0.lt) goto loc_8262F324;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F324:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f36c
	if (!cr6.eq) goto loc_8262F36C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262f358
	if (!cr0.lt) goto loc_8262F358;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262F358:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262f368
	if (!cr6.eq) goto loc_8262F368;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262f36c
	goto loc_8262F36C;
loc_8262F368:
	// li r29,0
	r29.s64 = 0;
loc_8262F36C:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262F374:
	// stbx r11,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262f3c4
	if (cr6.eq) goto loc_8262F3C4;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F3C4:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8262f6ac
	if (cr6.eq) goto loc_8262F6AC;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x8262f698
	goto loc_8262F698;
loc_8262F400:
	// cmpwi cr6,r16,4
	cr6.compare<int32_t>(r16.s32, 4, xer);
	// bne cr6,0x8262f6ac
	if (!cr6.eq) goto loc_8262F6AC;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262f510
	if (cr6.lt) goto loc_8262F510;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x8262f508
	if (!cr6.lt) goto loc_8262F508;
loc_8262F470:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262f49c
	if (cr6.lt) goto loc_8262F49C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262f470
	if (cr6.eq) goto loc_8262F470;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262f54c
	goto loc_8262F54C;
loc_8262F49C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262F508:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262f54c
	goto loc_8262F54C;
loc_8262F510:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262F518:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262f518
	if (cr6.lt) goto loc_8262F518;
loc_8262F54C:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stbx r30,r24,r14
	PPC_STORE_U8(r24.u32 + r14.u32, r30.u8);
	// beq cr6,0x8262f5bc
	if (cr6.eq) goto loc_8262F5BC;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F5BC:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f60c
	if (cr6.eq) goto loc_8262F60C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F60C:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f65c
	if (cr6.eq) goto loc_8262F65C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F65C:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f6ac
	if (cr6.eq) goto loc_8262F6AC;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_8262F698:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_8262F6A0:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262F6AC:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8262f6e8
	if (!cr6.eq) goto loc_8262F6E8;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// li r11,64
	r11.s64 = 64;
loc_8262F6CC:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8262f6cc
	if (!cr6.eq) goto loc_8262F6CC;
loc_8262F6E8:
	// lwz r4,380(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r25,0
	r25.s64 = 0;
	// b 0x8262f71c
	goto loc_8262F71C;
loc_8262F710:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// li r11,0
	r11.s64 = 0;
	// stb r11,10(r10)
	PPC_STORE_U8(ctx.r10.u32 + 10, r11.u8);
loc_8262F71C:
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r27,r24,1
	r27.s64 = r24.s64 + 1;
	// lwz r24,364(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f9c0
	if (cr6.eq) goto loc_8262F9C0;
	// lwz r7,404(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// li r23,0
	r23.s64 = 0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lwz r26,396(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// lwz r5,468(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// add r29,r11,r5
	r29.u64 = r11.u64 + ctx.r5.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r28,r23
	r28.u64 = r23.u64;
	// bl 0x8262a2c8
	sub_8262A2C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8262f784
	if (cr6.eq) goto loc_8262F784;
	// addi r28,r1,144
	r28.s64 = ctx.r1.s64 + 144;
loc_8262F784:
	// lwz r30,1768(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lbz r6,5(r14)
	ctx.r6.u64 = PPC_LOAD_U8(r14.u32 + 5);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// stw r18,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r18.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// bl 0x8264be50
	sub_8264BE50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262f998
	if (cr6.eq) goto loc_8262F998;
	// rlwinm r10,r26,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F7FC:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f7fc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F7FC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r6,r30,16
	ctx.r6.s64 = r30.s64 + 16;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F830:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f830
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F830;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,32
	ctx.r8.s64 = r30.s64 + 32;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F868:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f868
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F868;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F8A0:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f8a0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F8A0;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F8D8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f8d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F8D8;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,80
	ctx.r9.s64 = r30.s64 + 80;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F910:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f910
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F910;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,3000(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,96
	ctx.r9.s64 = r30.s64 + 96;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8262F94C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8262f94c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F94C;
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,3000(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// addi r9,r30,112
	ctx.r9.s64 = r30.s64 + 112;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8262F984:
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x8262f984
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8262F984;
loc_8262F998:
	// lwz r4,388(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stb r23,11(r24)
	PPC_STORE_U8(r24.u32 + 11, r23.u8);
	// b 0x82630288
	goto loc_82630288;
loc_8262F9C0:
	// lbz r11,5(r14)
	r11.u64 = PPC_LOAD_U8(r14.u32 + 5);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263027c
	if (cr6.eq) goto loc_8263027C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r23,0
	r23.s64 = 0;
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8262fb3c
	if (cr6.eq) goto loc_8262FB3C;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262fb3c
	if (!cr6.eq) goto loc_8262FB3C;
	// lwz r11,2556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2556);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262fadc
	if (cr6.lt) goto loc_8262FADC;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8262fad4
	if (!cr6.lt) goto loc_8262FAD4;
loc_8262FA3C:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8262fa68
	if (cr6.lt) goto loc_8262FA68;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262fa3c
	if (cr6.eq) goto loc_8262FA3C;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262fb18
	goto loc_8262FB18;
loc_8262FA68:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_8262FAD4:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8262fb18
	goto loc_8262FB18;
loc_8262FADC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8262FAE4:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8262fae4
	if (cr6.lt) goto loc_8262FAE4;
loc_8262FB18:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// lwzx r16,r11,r10
	r16.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r17,r11,r9
	r17.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
loc_8262FB3C:
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// stb r16,11(r24)
	PPC_STORE_U8(r24.u32 + 11, r16.u8);
	// bne cr6,0x8262fb88
	if (!cr6.eq) goto loc_8262FB88;
	// lwz r26,1768(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// b 0x82630210
	goto loc_82630210;
loc_8262FB88:
	// cmpwi cr6,r16,1
	cr6.compare<int32_t>(r16.s32, 1, xer);
	// bne cr6,0x8262fd7c
	if (!cr6.eq) goto loc_8262FD7C;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262fc54
	if (cr6.eq) goto loc_8262FC54;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262fc40
	if (!cr6.eq) goto loc_8262FC40;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262fc40
	if (!cr6.eq) goto loc_8262FC40;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fbfc
	if (!cr0.lt) goto loc_8262FBFC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FBFC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fce8
	if (!cr6.eq) goto loc_8262FCE8;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fc30
	if (!cr0.lt) goto loc_8262FC30;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FC30:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fce4
	if (!cr6.eq) goto loc_8262FCE4;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262fce8
	goto loc_8262FCE8;
loc_8262FC40:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262fcf0
	goto loc_8262FCF0;
loc_8262FC54:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262fc74
	if (cr6.eq) goto loc_8262FC74;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262fcf0
	goto loc_8262FCF0;
loc_8262FC74:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fca0
	if (!cr0.lt) goto loc_8262FCA0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FCA0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fce8
	if (!cr6.eq) goto loc_8262FCE8;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fcd4
	if (!cr0.lt) goto loc_8262FCD4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FCD4:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fce4
	if (!cr6.eq) goto loc_8262FCE4;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262fce8
	goto loc_8262FCE8;
loc_8262FCE4:
	// li r29,0
	r29.s64 = 0;
loc_8262FCE8:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262FCF0:
	// stbx r11,r27,r14
	PPC_STORE_U8(r27.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262fd40
	if (cr6.eq) goto loc_8262FD40;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262FD40:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8263021c
	if (cr6.eq) goto loc_8263021C;
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x82630208
	goto loc_82630208;
loc_8262FD7C:
	// cmpwi cr6,r16,2
	cr6.compare<int32_t>(r16.s32, 2, xer);
	// bne cr6,0x8262ff70
	if (!cr6.eq) goto loc_8262FF70;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r29,1
	r29.s64 = 1;
	// li r28,1
	r28.s64 = 1;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,3372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8262fe48
	if (cr6.eq) goto loc_8262FE48;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bne cr6,0x8262fe34
	if (!cr6.eq) goto loc_8262FE34;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8262fe34
	if (!cr6.eq) goto loc_8262FE34;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fdf0
	if (!cr0.lt) goto loc_8262FDF0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FDF0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fedc
	if (!cr6.eq) goto loc_8262FEDC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fe24
	if (!cr0.lt) goto loc_8262FE24;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FE24:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fed8
	if (!cr6.eq) goto loc_8262FED8;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262fedc
	goto loc_8262FEDC;
loc_8262FE34:
	// clrlwi r11,r17,24
	r11.u64 = r17.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262fee4
	goto loc_8262FEE4;
loc_8262FE48:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8262fe68
	if (cr6.eq) goto loc_8262FE68;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,12,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r29,r10,0,30,30
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// clrlwi r28,r10,31
	r28.u64 = ctx.r10.u32 & 0x1;
	// b 0x8262fee4
	goto loc_8262FEE4;
loc_8262FE68:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fe94
	if (!cr0.lt) goto loc_8262FE94;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FE94:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fedc
	if (!cr6.eq) goto loc_8262FEDC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8262fec8
	if (!cr0.lt) goto loc_8262FEC8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8262FEC8:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8262fed8
	if (!cr6.eq) goto loc_8262FED8;
	// li r28,0
	r28.s64 = 0;
	// b 0x8262fedc
	goto loc_8262FEDC;
loc_8262FED8:
	// li r29,0
	r29.s64 = 0;
loc_8262FEDC:
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r28
	r11.u64 = r11.u64 | r28.u64;
loc_8262FEE4:
	// stbx r11,r27,r14
	PPC_STORE_U8(r27.u32 + r14.u32, r11.u8);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8262ff34
	if (cr6.eq) goto loc_8262FF34;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8262FF34:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8263021c
	if (cr6.eq) goto loc_8263021C;
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r6,1
	ctx.r6.s64 = 1;
	// b 0x82630208
	goto loc_82630208;
loc_8262FF70:
	// cmpwi cr6,r16,4
	cr6.compare<int32_t>(r16.s32, 4, xer);
	// bne cr6,0x8263021c
	if (!cr6.eq) goto loc_8263021C;
	// lwz r26,1764(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,2476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2476);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// extsh r29,r10
	r29.s64 = ctx.r10.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x82630080
	if (cr6.lt) goto loc_82630080;
	// clrlwi r10,r29,28
	ctx.r10.u64 = r29.u32 & 0xF;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// bge cr6,0x82630078
	if (!cr6.lt) goto loc_82630078;
loc_8262FFE0:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8263000c
	if (cr6.lt) goto loc_8263000C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8262ffe0
	if (cr6.eq) goto loc_8262FFE0;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826300bc
	goto loc_826300BC;
loc_8263000C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
loc_82630078:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x826300bc
	goto loc_826300BC;
loc_82630080:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82630088:
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r15
	r11.u64 = r29.u64 + r15.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x82630088
	if (cr6.lt) goto loc_82630088;
loc_826300BC:
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r30,r29,1
	r30.s64 = r29.s64 + 1;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8262ee3c
	if (!cr6.eq) goto loc_8262EE3C;
	// rlwinm r10,r30,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x8;
	// lwz r28,116(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r29,112(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stbx r30,r27,r14
	PPC_STORE_U8(r27.u32 + r14.u32, r30.u8);
	// beq cr6,0x8263012c
	if (cr6.eq) goto loc_8263012C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8263012C:
	// rlwinm r11,r30,0,29,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263017c
	if (cr6.eq) goto loc_8263017C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8263017C:
	// rlwinm r11,r30,0,30,30
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826301cc
	if (cr6.eq) goto loc_826301CC;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826301CC:
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263021c
	if (cr6.eq) goto loc_8263021C;
	// lwz r5,1860(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1860);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630298
	if (!cr6.eq) goto loc_82630298;
	// lwz r11,3176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3176);
	// li r6,3
	ctx.r6.s64 = 3;
loc_82630208:
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r4,8
	ctx.r4.s64 = 8;
loc_82630210:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8263021C:
	// lis r11,-32157
	r11.s64 = -2107441152;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// addi r11,r11,5376
	r11.s64 = r11.s64 + 5376;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82630258
	if (!cr6.eq) goto loc_82630258;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// li r11,64
	r11.s64 = 64;
loc_8263023C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// bne cr6,0x8263023c
	if (!cr6.eq) goto loc_8263023C;
loc_82630258:
	// lwz r4,388(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82630288
	goto loc_82630288;
loc_8263027C:
	// li r11,0
	r11.s64 = 0;
	// lwz r23,120(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stb r11,11(r24)
	PPC_STORE_U8(r24.u32 + 11, r11.u8);
loc_82630288:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r23,31,0,0
	r11.u64 = (__builtin_rotateleft32(r23.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
loc_82630298:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826302A0"))) PPC_WEAK_FUNC(sub_826302A0);
PPC_FUNC_IMPL(__imp__sub_826302A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-544(r1)
	ea = -544 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r1,188
	r11.s64 = ctx.r1.s64 + 188;
	// addi r8,r1,240
	ctx.r8.s64 = ctx.r1.s64 + 240;
	// addi r7,r1,260
	ctx.r7.s64 = ctx.r1.s64 + 260;
	// lwz r27,3720(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// addi r6,r1,280
	ctx.r6.s64 = ctx.r1.s64 + 280;
	// lwz r28,220(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// addi r5,r1,300
	ctx.r5.s64 = ctx.r1.s64 + 300;
	// lwz r9,228(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// addi r4,r1,320
	ctx.r4.s64 = ctx.r1.s64 + 320;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// stw r29,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r29.u32);
	// addi r3,r1,340
	ctx.r3.s64 = ctx.r1.s64 + 340;
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// addi r30,r1,360
	r30.s64 = ctx.r1.s64 + 360;
	// lwz r25,3724(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// li r17,1
	r17.s64 = 1;
	// stw r9,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r9.u32);
	// mr r15,r29
	r15.u64 = r29.u64;
	// stw r28,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r28.u32);
	// mr r16,r17
	r16.u64 = r17.u64;
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// lwz r10,232(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// stw r29,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r29.u32);
	// lwz r26,3728(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r27,3736(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// stw r8,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r8.u32);
	// add r8,r25,r11
	ctx.r8.u64 = r25.u64 + r11.u64;
	// stw r10,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r10.u32);
	// lwz r28,3740(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// stw r8,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r8.u32);
	// addi r8,r1,196
	ctx.r8.s64 = ctx.r1.s64 + 196;
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// stw r8,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, ctx.r8.u32);
	// add r8,r11,r26
	ctx.r8.u64 = r11.u64 + r26.u64;
	// stw r8,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r8.u32);
	// lwz r8,3756(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// stw r29,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r29.u32);
	// stw r9,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, ctx.r9.u32);
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// stw r29,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r29.u32);
	// stw r8,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, ctx.r8.u32);
	// addi r8,r1,200
	ctx.r8.s64 = ctx.r1.s64 + 200;
	// stw r8,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, ctx.r8.u32);
	// stw r29,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r29.u32);
	// stw r9,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r9.u32);
	// add r9,r27,r11
	ctx.r9.u64 = r27.u64 + r11.u64;
	// add r11,r28,r11
	r11.u64 = r28.u64 + r11.u64;
	// stw r29,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r29.u32);
	// stw r10,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r10.u32);
	// stw r9,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, ctx.r9.u32);
	// addi r9,r1,204
	ctx.r9.s64 = ctx.r1.s64 + 204;
	// stw r29,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r29.u32);
	// stw r11,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r11.u32);
	// lwz r11,268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// stw r29,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, r29.u32);
	// stw r9,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r9.u32);
	// stw r10,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, ctx.r10.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r29.u32);
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r11.u32);
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// stw r29,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, r29.u32);
	// stw r11,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r11.u32);
	// li r11,20
	r11.s64 = 20;
	// stw r11,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r11.u32);
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// lwz r11,2928(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2928);
	// stw r29,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r29.u32);
	// stw r29,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r29.u32);
	// addi r9,r1,376
	ctx.r9.s64 = ctx.r1.s64 + 376;
	// stw r29,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r29.u32);
	// addi r8,r11,726
	ctx.r8.s64 = r11.s64 + 726;
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2088);
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// lwz r7,3960(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// std r29,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, r29.u64);
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,2880(r31)
	PPC_STORE_U32(r31.u32 + 2880, ctx.r9.u32);
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// stw r9,2892(r31)
	PPC_STORE_U32(r31.u32 + 2892, ctx.r9.u32);
	// lwz r11,2100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(r31.u32 + 2092, r11.u32);
	// lwzx r11,r10,r31
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(r31.u32 + 2096, r11.u32);
	// bne cr6,0x82630438
	if (!cr6.eq) goto loc_82630438;
	// stw r29,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r29.u32);
	// b 0x8263043c
	goto loc_8263043C;
loc_82630438:
	// stw r17,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r17.u32);
loc_8263043C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// bl 0x825ebc08
	sub_825EBC08(ctx, base);
	// lwz r11,3960(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82630460
	if (cr6.eq) goto loc_82630460;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// bne cr6,0x82630464
	if (!cr6.eq) goto loc_82630464;
loc_82630460:
	// mr r11,r17
	r11.u64 = r17.u64;
loc_82630464:
	// lwz r10,1972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r11,76(r10)
	PPC_STORE_U32(ctx.r10.u32 + 76, r11.u32);
	// lwz r4,248(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1972);
	// bl 0x8265b9d0
	sub_8265B9D0(ctx, base);
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bge cr6,0x82630498
	if (!cr6.lt) goto loc_82630498;
	// addi r11,r31,2464
	r11.s64 = r31.s64 + 2464;
	// addi r10,r31,2480
	ctx.r10.s64 = r31.s64 + 2480;
	// addi r9,r31,2520
	ctx.r9.s64 = r31.s64 + 2520;
	// b 0x826304bc
	goto loc_826304BC;
loc_82630498:
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bge cr6,0x826304b0
	if (!cr6.lt) goto loc_826304B0;
	// addi r11,r31,2452
	r11.s64 = r31.s64 + 2452;
	// addi r10,r31,2492
	ctx.r10.s64 = r31.s64 + 2492;
	// addi r9,r31,2532
	ctx.r9.s64 = r31.s64 + 2532;
	// b 0x826304bc
	goto loc_826304BC;
loc_826304B0:
	// addi r11,r31,2440
	r11.s64 = r31.s64 + 2440;
	// addi r10,r31,2504
	ctx.r10.s64 = r31.s64 + 2504;
	// addi r9,r31,2544
	ctx.r9.s64 = r31.s64 + 2544;
loc_826304BC:
	// lwz r6,3756(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// mr r26,r29
	r26.u64 = r29.u64;
	// stw r10,2516(r31)
	PPC_STORE_U32(r31.u32 + 2516, ctx.r10.u32);
	// mr r21,r29
	r21.u64 = r29.u64;
	// stw r9,2556(r31)
	PPC_STORE_U32(r31.u32 + 2556, ctx.r9.u32);
	// lwz r9,3720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// stw r11,2476(r31)
	PPC_STORE_U32(r31.u32 + 2476, r11.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r6,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r6.u32);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r6,268(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// lwz r7,3724(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// lwz r8,3728(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r9,3736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// stw r10,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r10.u32);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// lwz r10,3740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// stw r6,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r6.u32);
	// lwz r6,136(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r7,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r7.u32);
	// lwz r7,140(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r26,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r26.u32);
	// stw r21,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r21.u32);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r8,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r8.u32);
	// stw r9,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r9.u32);
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// stw r6,340(r31)
	PPC_STORE_U32(r31.u32 + 340, ctx.r6.u32);
	// ble cr6,0x82630bb4
	if (!cr6.gt) goto loc_82630BB4;
	// lis r14,16384
	r14.s64 = 1073741824;
loc_82630544:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// lwz r10,340(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 340);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r9,21236(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21236);
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// lwz r24,188(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// subf r11,r26,r11
	r11.s64 = r11.s64 - r26.s64;
	// lwz r23,192(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r22,196(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r25,200(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// lwz r20,208(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// lwz r19,204(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// rlwinm r18,r11,27,31,31
	r18.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r10,340(r31)
	PPC_STORE_U32(r31.u32 + 340, ctx.r10.u32);
	// beq cr6,0x82630780
	if (cr6.eq) goto loc_82630780;
	// lwz r11,21264(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r27,r26,2,0,29
	r27.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630768
	if (cr6.eq) goto loc_82630768;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// lwz r30,84(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630634
	if (cr6.eq) goto loc_82630634;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r28,1
	r28.s64 = 1;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263060c
	if (!cr6.lt) goto loc_8263060C;
loc_826305CC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263060c
	if (cr6.eq) goto loc_8263060C;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r28,r11,r28
	r28.s64 = r28.s64 - r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// bge 0x826305fc
	if (!cr0.lt) goto loc_826305FC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826305FC:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x826305cc
	if (cr6.gt) goto loc_826305CC;
loc_8263060C:
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r10,r28,32
	ctx.r10.u64 = r28.u64 & 0xFFFFFFFF;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// subf. r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r10.u64);
	// bge 0x82630634
	if (!cr0.lt) goto loc_82630634;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82630634:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = r11.u32 & 0x7;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82639b10
	sub_82639B10(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,1944(r31)
	PPC_STORE_U32(r31.u32 + 1944, r11.u32);
	// beq cr6,0x826306d4
	if (cr6.eq) goto loc_826306D4;
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stw r29,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r29.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r29,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r29.u32);
	// addi r8,r1,184
	ctx.r8.s64 = ctx.r1.s64 + 184;
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// addi r7,r1,180
	ctx.r7.s64 = ctx.r1.s64 + 180;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630bd8
	if (!cr6.eq) goto loc_82630BD8;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// beq cr6,0x826306ac
	if (cr6.eq) goto loc_826306AC;
	// cmpwi cr6,r15,4
	cr6.compare<int32_t>(r15.s32, 4, xer);
	// bne cr6,0x826306b0
	if (!cr6.eq) goto loc_826306B0;
loc_826306AC:
	// mr r15,r30
	r15.u64 = r30.u64;
loc_826306B0:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x82630bb4
	if (cr6.eq) goto loc_82630BB4;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r17,r29
	r17.u64 = r29.u64;
	// lwz r26,180(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r21,184(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x82630b98
	goto loc_82630B98;
loc_826306D4:
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19976);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82630700
	if (!cr6.eq) goto loc_82630700;
	// lwz r10,19980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82630700
	if (!cr6.eq) goto loc_82630700;
	// lwz r10,284(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 284);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x82630700
	if (!cr6.eq) goto loc_82630700;
	// mr r17,r11
	r17.u64 = r11.u64;
	// b 0x8263076c
	goto loc_8263076C;
loc_82630700:
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stw r29,19976(r31)
	PPC_STORE_U32(r31.u32 + 19976, r29.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r29,19980(r31)
	PPC_STORE_U32(r31.u32 + 19980, r29.u32);
	// addi r8,r1,184
	ctx.r8.s64 = ctx.r1.s64 + 184;
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// addi r7,r1,180
	ctx.r7.s64 = ctx.r1.s64 + 180;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630be4
	if (!cr6.eq) goto loc_82630BE4;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x82630744
	if (!cr6.eq) goto loc_82630744;
	// li r15,4
	r15.s64 = 4;
loc_82630744:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// beq cr6,0x82630bb4
	if (cr6.eq) goto loc_82630BB4;
	// lwz r11,21272(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21272);
	// mr r17,r29
	r17.u64 = r29.u64;
	// lwz r26,180(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r21,184(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// stw r11,21272(r31)
	PPC_STORE_U32(r31.u32 + 21272, r11.u32);
	// b 0x82630b98
	goto loc_82630B98;
loc_82630768:
	// li r11,1
	r11.s64 = 1;
loc_8263076C:
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// lwzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r27.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82630780
	if (cr6.eq) goto loc_82630780;
	// mr r16,r11
	r16.u64 = r11.u64;
loc_82630780:
	// lwz r11,3932(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3932);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826307a0
	if (cr6.eq) goto loc_826307A0;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82639ef0
	sub_82639EF0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630d54
	if (!cr6.eq) goto loc_82630D54;
loc_826307A0:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r28,r29
	r28.u64 = r29.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x82630abc
	if (!cr6.gt) goto loc_82630ABC;
	// lwz r4,176(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
loc_826307B8:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// clrlwi r10,r30,29
	ctx.r10.u64 = r30.u32 & 0x7;
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// dcbt r9,r25
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// dcbt r9,r25
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// dcbt r9,r25
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// dcbt r11,r25
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// dcbt r11,r20
	// dcbt r11,r19
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r29,324(r31)
	PPC_STORE_U32(r31.u32 + 324, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aba8
	sub_8263ABA8(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82630a6c
	if (!cr6.eq) goto loc_82630A6C;
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r27,r11,27,31,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// bl 0x82651758
	sub_82651758(ctx, base);
	// lwz r4,176(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826309f4
	if (cr6.eq) goto loc_826309F4;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmplw cr6,r11,r14
	cr6.compare<uint32_t>(r11.u32, r14.u32, xer);
	// bne cr6,0x826309c0
	if (!cr6.eq) goto loc_826309C0;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,1780(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// mullw r11,r11,r26
	r11.s64 = int64_t(r11.s32) * int64_t(r26.s32);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// beq cr6,0x826309c0
	if (cr6.eq) goto loc_826309C0;
	// addi r11,r4,12
	r11.s64 = ctx.r4.s64 + 12;
	// srawi r8,r21,1
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x1) != 0);
	ctx.r8.s64 = r21.s32 >> 1;
	// srawi r7,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r7.s64 = r28.s32 >> 1;
	// stw r29,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r29.u32);
	// sth r29,4(r11)
	PPC_STORE_U16(r11.u32 + 4, r29.u16);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,6(r11)
	PPC_STORE_U8(r11.u32 + 6, r29.u8);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,7(r11)
	PPC_STORE_U8(r11.u32 + 7, r29.u8);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,8(r11)
	PPC_STORE_U8(r11.u32 + 8, r29.u8);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,9(r11)
	PPC_STORE_U8(r11.u32 + 9, r29.u8);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,10(r11)
	PPC_STORE_U8(r11.u32 + 10, r29.u8);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// stb r29,11(r11)
	PPC_STORE_U8(r11.u32 + 11, r29.u8);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,19700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// mullw r9,r10,r26
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r26.s32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// lwz r6,1772(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// lhzx r8,r6,r9
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r6.u32 + ctx.r9.u32);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82630990
	if (!cr6.eq) goto loc_82630990;
	// lwz r8,1776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// lhzx r9,r8,r9
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r9.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82630990
	if (!cr6.eq) goto loc_82630990;
	// lwz r8,3740(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// lwz r7,3736(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// lwz r6,3756(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3756);
	// mullw r11,r9,r21
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r21.s32);
	// lwz r27,3120(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3120);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// mtctr r27
	ctr.u64 = r27.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// oris r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 131072;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x82630a30
	goto loc_82630A30;
loc_82630990:
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82629528
	sub_82629528(ctx, base);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r10,r10,1
	ctx.r10.u64 = ctx.r10.u32 & 0x7FFFFFFF;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x82630a30
	goto loc_82630A30;
loc_826309C0:
	// rlwinm r11,r21,1,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8262de90
	sub_8262DE90(ctx, base);
	// b 0x82630a24
	goto loc_82630A24;
loc_826309F4:
	// rlwinm r11,r21,1,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8262af58
	sub_8262AF58(ctx, base);
loc_82630A24:
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82630a74
	if (!cr6.eq) goto loc_82630A74;
loc_82630A30:
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r24,r24,16
	r24.s64 = r24.s64 + 16;
	// addi r4,r10,20
	ctx.r4.s64 = ctx.r10.s64 + 20;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// addi r25,r25,16
	r25.s64 = r25.s64 + 16;
	// addi r20,r20,8
	r20.s64 = r20.s64 + 8;
	// stw r4,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r4.u32);
	// addi r19,r19,8
	r19.s64 = r19.s64 + 8;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x826307b8
	if (cr6.lt) goto loc_826307B8;
	// b 0x82630abc
	goto loc_82630ABC;
loc_82630A6C:
	// li r5,-1
	ctx.r5.s64 = -1;
	// b 0x82630a7c
	goto loc_82630A7C;
loc_82630A74:
	// li r5,-2
	ctx.r5.s64 = -2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82630A7C:
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r8,r1,184
	ctx.r8.s64 = ctx.r1.s64 + 184;
	// addi r7,r1,180
	ctx.r7.s64 = ctx.r1.s64 + 180;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x826289e8
	sub_826289E8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82630bf0
	if (!cr6.eq) goto loc_82630BF0;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// beq cr6,0x82630ab0
	if (cr6.eq) goto loc_82630AB0;
	// cmpwi cr6,r15,4
	cr6.compare<int32_t>(r15.s32, 4, xer);
	// bne cr6,0x82630ab4
	if (!cr6.eq) goto loc_82630AB4;
loc_82630AB0:
	// mr r15,r27
	r15.u64 = r27.u64;
loc_82630AB4:
	// lwz r26,180(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r21,184(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
loc_82630ABC:
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630aec
	if (cr6.eq) goto loc_82630AEC;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r7,196(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// lwz r6,192(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r5,188(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653338
	sub_82653338(ctx, base);
loc_82630AEC:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r16,r29
	r16.u64 = r29.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// bge cr6,0x82630b1c
	if (!cr6.lt) goto loc_82630B1C;
	// addi r11,r26,1
	r11.s64 = r26.s64 + 1;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21264);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630b1c
	if (cr6.eq) goto loc_82630B1C;
	// li r18,1
	r18.s64 = 1;
loc_82630B1C:
	// lwz r11,232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 232);
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// lwz r9,192(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,188(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,196(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,200(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// stw r6,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r6.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r5,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r5.u32);
	// stw r7,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r7.u32);
	// stw r10,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r10.u32);
	// lwz r10,208(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r10.u32);
	// lwz r10,204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// beq cr6,0x82630b98
	if (cr6.eq) goto loc_82630B98;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630b98
	if (cr6.eq) goto loc_82630B98;
	// li r9,1
	ctx.r9.s64 = 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82653338
	sub_82653338(ctx, base);
loc_82630B98:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r21,r21,16
	r21.s64 = r21.s64 + 16;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// stw r26,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r26.u32);
	// stw r21,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r21.u32);
	// blt cr6,0x82630544
	if (cr6.lt) goto loc_82630544;
loc_82630BB4:
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630c8c
	if (cr6.eq) goto loc_82630C8C;
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82630bfc
	if (!cr6.eq) goto loc_82630BFC;
	// bl 0x8262ada0
	sub_8262ADA0(ctx, base);
	// b 0x82630c00
	goto loc_82630C00;
loc_82630BD8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// b 0x8239bd10
	return;
loc_82630BE4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// b 0x8239bd10
	return;
loc_82630BF0:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// b 0x8239bd10
	return;
loc_82630BFC:
	// bl 0x8262acd8
	sub_8262ACD8(ctx, base);
loc_82630C00:
	// lwz r30,3812(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3812);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r28,3916(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3916);
	// lwz r27,15760(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 15760);
	// lwz r26,15744(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 15744);
	// lwz r25,15728(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 15728);
	// lwz r24,15752(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 15752);
	// lwz r23,15736(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 15736);
	// lwz r22,15720(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 15720);
	// lwz r21,15712(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 15712);
	// lwz r20,15696(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 15696);
	// lwz r11,224(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// add r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r10,15680(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15680);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// lwz r9,15704(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15704);
	// lwz r8,15688(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15688);
	// lwz r7,15672(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15672);
	// stw r30,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r30.u32);
	// stw r29,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r29.u32);
	// stw r28,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r28.u32);
	// stw r27,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r27.u32);
	// stw r26,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r26.u32);
	// stw r25,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r25.u32);
	// stw r24,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r24.u32);
	// stw r23,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r23.u32);
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r20.u32);
	// bl 0x8261be68
	sub_8261BE68(ctx, base);
loc_82630C8C:
	// lwz r11,14788(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82630d14
	if (cr6.eq) goto loc_82630D14;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r9,276(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 276);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82630d14
	if (!cr6.gt) goto loc_82630D14;
loc_82630CAC:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82630d04
	if (!cr6.gt) goto loc_82630D04;
loc_82630CBC:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r7,1780(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r7
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r7.u32);
	// cmplwi cr6,r10,16384
	cr6.compare<uint32_t>(ctx.r10.u32, 16384, xer);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// bne cr6,0x82630ce8
	if (!cr6.eq) goto loc_82630CE8;
	// rlwinm r10,r10,0,15,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// b 0x82630cec
	goto loc_82630CEC;
loc_82630CE8:
	// oris r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 131072;
loc_82630CEC:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82630cbc
	if (cr6.lt) goto loc_82630CBC;
loc_82630D04:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// blt cr6,0x82630cac
	if (cr6.lt) goto loc_82630CAC;
loc_82630D14:
	// lwz r11,3892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3892);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82630d3c
	if (!cr6.eq) goto loc_82630D3C;
	// lwz r11,14824(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82630d3c
	if (!cr6.eq) goto loc_82630D3C;
	// lwz r11,15196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15196);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// beq cr6,0x82630d40
	if (cr6.eq) goto loc_82630D40;
loc_82630D3C:
	// li r11,1
	r11.s64 = 1;
loc_82630D40:
	// stw r11,15560(r31)
	PPC_STORE_U32(r31.u32 + 15560, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
	// stw r29,15564(r31)
	PPC_STORE_U32(r31.u32 + 15564, r29.u32);
	// stw r11,15536(r31)
	PPC_STORE_U32(r31.u32 + 15536, r11.u32);
loc_82630D54:
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82630D5C"))) PPC_WEAK_FUNC(sub_82630D5C);
PPC_FUNC_IMPL(__imp__sub_82630D5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82630D60"))) PPC_WEAK_FUNC(sub_82630D60);
PPC_FUNC_IMPL(__imp__sub_82630D60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bccc
	// addi r11,r1,-376
	r11.s64 = ctx.r1.s64 + -376;
	// li r10,8
	ctx.r10.s64 = 8;
loc_82630D70:
	// lwz r31,0(r5)
	r31.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// lwz r8,28(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 28);
	// rlwinm r28,r31,11,0,20
	r28.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 11) & 0xFFFFF800;
	// lwz r7,12(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// mulli r26,r9,2276
	r26.s64 = ctx.r9.s64 * 2276;
	// lwz r6,20(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 20);
	// lwz r29,16(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// lwz r31,8(r5)
	r31.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// lwz r30,24(r5)
	r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 24);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r24,r6,r7
	r24.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r28,r28,128
	r28.s64 = r28.s64 + 128;
	// rlwinm r29,r29,11,0,20
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 11) & 0xFFFFF800;
	// add r22,r30,r31
	r22.u64 = r30.u64 + r31.u64;
	// mulli r27,r31,1568
	r27.s64 = r31.s64 * 1568;
	// mulli r23,r7,4017
	r23.s64 = ctx.r7.s64 * 4017;
	// mulli r21,r30,3784
	r21.s64 = r30.s64 * 3784;
	// add r31,r28,r29
	r31.u64 = r28.u64 + r29.u64;
	// subf r30,r29,r28
	r30.s64 = r28.s64 - r29.s64;
	// mulli r9,r9,565
	ctx.r9.s64 = ctx.r9.s64 * 565;
	// mulli r6,r6,799
	ctx.r6.s64 = ctx.r6.s64 * 799;
	// mulli r7,r24,2408
	ctx.r7.s64 = r24.s64 * 2408;
	// mulli r29,r22,1108
	r29.s64 = r22.s64 * 1108;
	// mulli r25,r8,3406
	r25.s64 = ctx.r8.s64 * 3406;
	// add r8,r26,r9
	ctx.r8.u64 = r26.u64 + ctx.r9.u64;
	// subf r6,r6,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r28,r21,r29
	r28.s64 = r29.s64 - r21.s64;
	// subf r9,r25,r9
	ctx.r9.s64 = ctx.r9.s64 - r25.s64;
	// subf r7,r23,r7
	ctx.r7.s64 = ctx.r7.s64 - r23.s64;
	// add r29,r27,r29
	r29.u64 = r27.u64 + r29.u64;
	// add r27,r6,r8
	r27.u64 = ctx.r6.u64 + ctx.r8.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r6,r9,r7
	ctx.r6.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r7,r31,r29
	ctx.r7.u64 = r31.u64 + r29.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// add r29,r30,r28
	r29.u64 = r30.u64 + r28.u64;
	// subf r30,r28,r30
	r30.s64 = r30.s64 - r28.s64;
	// add r28,r9,r8
	r28.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// mulli r8,r28,181
	ctx.r8.s64 = r28.s64 * 181;
	// mulli r9,r9,181
	ctx.r9.s64 = ctx.r9.s64 * 181;
	// addi r8,r8,128
	ctx.r8.s64 = ctx.r8.s64 + 128;
	// addi r28,r9,128
	r28.s64 = ctx.r9.s64 + 128;
	// srawi r9,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 8;
	// srawi r8,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	ctx.r8.s64 = r28.s32 >> 8;
	// add r28,r27,r7
	r28.u64 = r27.u64 + ctx.r7.u64;
	// add r26,r31,r6
	r26.u64 = r31.u64 + ctx.r6.u64;
	// srawi r28,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	r28.s64 = r28.s32 >> 8;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r31,r9,r29
	r31.u64 = ctx.r9.u64 + r29.u64;
	// subf r9,r9,r29
	ctx.r9.s64 = r29.s64 - ctx.r9.s64;
	// srawi r31,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	r31.s64 = r31.s32 >> 8;
	// subf r7,r27,r7
	ctx.r7.s64 = ctx.r7.s64 - r27.s64;
	// stw r28,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r28.u32);
	// add r28,r30,r8
	r28.u64 = r30.u64 + ctx.r8.u64;
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// srawi r28,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	r28.s64 = r28.s32 >> 8;
	// srawi r30,r26,8
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xFF) != 0);
	r30.s64 = r26.s32 >> 8;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stw r31,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r31.u32);
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r6,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r6.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// stw r7,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r7.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne cr6,0x82630d70
	if (!cr6.eq) goto loc_82630D70;
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r29,r1,-288
	r29.s64 = ctx.r1.s64 + -288;
	// add r27,r10,r4
	r27.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r11,r1,-256
	r11.s64 = ctx.r1.s64 + -256;
	// add r26,r27,r4
	r26.u64 = r27.u64 + ctx.r4.u64;
	// li r22,8
	r22.s64 = 8;
	// add r25,r26,r4
	r25.u64 = r26.u64 + ctx.r4.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// add r24,r25,r4
	r24.u64 = r25.u64 + ctx.r4.u64;
	// subf r20,r10,r3
	r20.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r23,r24,r4
	r23.u64 = r24.u64 + ctx.r4.u64;
	// add r21,r23,r4
	r21.u64 = r23.u64 + ctx.r4.u64;
loc_82630ED0:
	// lwz r10,-96(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -96);
	// lwz r9,96(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// mulli r30,r10,2276
	r30.s64 = ctx.r10.s64 * 2276;
	// lwz r8,32(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r6,64(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r5,-64(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -64);
	// lwz r4,-128(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -128);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r31,r8,r7
	r31.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mulli r10,r10,565
	ctx.r10.s64 = ctx.r10.s64 * 565;
	// mulli r19,r9,3406
	r19.s64 = ctx.r9.s64 * 3406;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// mulli r9,r31,2408
	ctx.r9.s64 = r31.s64 * 2408;
	// add r31,r6,r5
	r31.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mulli r18,r8,799
	r18.s64 = ctx.r8.s64 * 799;
	// mulli r17,r6,3784
	r17.s64 = ctx.r6.s64 * 3784;
	// add r8,r30,r10
	ctx.r8.u64 = r30.u64 + ctx.r10.u64;
	// subf r6,r19,r10
	ctx.r6.s64 = ctx.r10.s64 - r19.s64;
	// addi r10,r9,4
	ctx.r10.s64 = ctx.r9.s64 + 4;
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// mulli r31,r31,1108
	r31.s64 = r31.s64 * 1108;
	// mulli r30,r5,1568
	r30.s64 = ctx.r5.s64 * 1568;
	// mulli r7,r7,4017
	ctx.r7.s64 = ctx.r7.s64 * 4017;
	// srawi r9,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 3;
	// subf r5,r18,r10
	ctx.r5.s64 = ctx.r10.s64 - r18.s64;
	// rlwinm r3,r3,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r4,r4,8,0,23
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r8,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 3;
	// subf r19,r7,r10
	r19.s64 = ctx.r10.s64 - ctx.r7.s64;
	// addi r6,r31,4
	ctx.r6.s64 = r31.s64 + 4;
	// srawi r5,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 3;
	// add r10,r4,r3
	ctx.r10.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r7,r3,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r31,r30,r6
	r31.u64 = r30.u64 + ctx.r6.u64;
	// subf r3,r17,r6
	ctx.r3.s64 = ctx.r6.s64 - r17.s64;
	// srawi r4,r19,3
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7) != 0);
	ctx.r4.s64 = r19.s32 >> 3;
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// add r5,r8,r4
	ctx.r5.u64 = ctx.r8.u64 + ctx.r4.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// srawi r4,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 3;
	// srawi r3,r31,3
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7) != 0);
	ctx.r3.s64 = r31.s32 >> 3;
	// add r31,r8,r9
	r31.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r30,r8,r9
	r30.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mulli r8,r31,181
	ctx.r8.s64 = r31.s64 * 181;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// mulli r31,r30,181
	r31.s64 = r30.s64 * 181;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// addi r3,r8,128
	ctx.r3.s64 = ctx.r8.s64 + 128;
	// add r8,r7,r4
	ctx.r8.u64 = ctx.r7.u64 + ctx.r4.u64;
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// srawi r4,r3,8
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 8;
	// srawi r3,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	ctx.r3.s64 = r31.s32 >> 8;
	// add r31,r6,r9
	r31.u64 = ctx.r6.u64 + ctx.r9.u64;
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// srawi r6,r31,14
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3FFF) != 0);
	ctx.r6.s64 = r31.s32 >> 14;
	// srawi r31,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	r31.s64 = ctx.r9.s32 >> 14;
	// add r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 + ctx.r5.u64;
	// subf r30,r5,r10
	r30.s64 = ctx.r10.s64 - ctx.r5.s64;
	// add r10,r4,r8
	ctx.r10.u64 = ctx.r4.u64 + ctx.r8.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// add r4,r7,r3
	ctx.r4.u64 = ctx.r7.u64 + ctx.r3.u64;
	// subf r3,r3,r7
	ctx.r3.s64 = ctx.r7.s64 - ctx.r3.s64;
	// srawi r10,r10,14
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 14;
	// srawi r5,r8,14
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 14;
	// srawi r7,r4,14
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFF) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 14;
	// srawi r8,r3,14
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3FFF) != 0);
	ctx.r8.s64 = ctx.r3.s32 >> 14;
	// srawi r4,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 14;
	// srawi r9,r30,14
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3FFF) != 0);
	ctx.r9.s64 = r30.s32 >> 14;
	// or r3,r4,r6
	ctx.r3.u64 = ctx.r4.u64 | ctx.r6.u64;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// or r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 | ctx.r5.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// or r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 | ctx.r9.u64;
	// or r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 | r31.u64;
	// or r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 | ctx.r10.u64;
	// or r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 | ctx.r7.u64;
	// or r3,r3,r8
	ctx.r3.u64 = ctx.r3.u64 | ctx.r8.u64;
	// rlwinm r3,r3,0,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFF00;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82631100
	if (cr6.eq) goto loc_82631100;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bge cr6,0x82631030
	if (!cr6.lt) goto loc_82631030;
	// li r4,0
	ctx.r4.s64 = 0;
	// b 0x8263103c
	goto loc_8263103C;
loc_82631030:
	// cmpwi cr6,r4,255
	cr6.compare<int32_t>(ctx.r4.s32, 255, xer);
	// ble cr6,0x8263103c
	if (!cr6.gt) goto loc_8263103C;
	// li r4,255
	ctx.r4.s64 = 255;
loc_8263103C:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x8263104c
	if (!cr6.lt) goto loc_8263104C;
	// li r31,0
	r31.s64 = 0;
	// b 0x82631058
	goto loc_82631058;
loc_8263104C:
	// cmpwi cr6,r31,255
	cr6.compare<int32_t>(r31.s32, 255, xer);
	// ble cr6,0x82631058
	if (!cr6.gt) goto loc_82631058;
	// li r31,255
	r31.s64 = 255;
loc_82631058:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x82631068
	if (!cr6.lt) goto loc_82631068;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x82631074
	goto loc_82631074;
loc_82631068:
	// cmpwi cr6,r5,255
	cr6.compare<int32_t>(ctx.r5.s32, 255, xer);
	// ble cr6,0x82631074
	if (!cr6.gt) goto loc_82631074;
	// li r5,255
	ctx.r5.s64 = 255;
loc_82631074:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x82631084
	if (!cr6.lt) goto loc_82631084;
	// li r7,0
	ctx.r7.s64 = 0;
	// b 0x82631090
	goto loc_82631090;
loc_82631084:
	// cmpwi cr6,r7,255
	cr6.compare<int32_t>(ctx.r7.s32, 255, xer);
	// ble cr6,0x82631090
	if (!cr6.gt) goto loc_82631090;
	// li r7,255
	ctx.r7.s64 = 255;
loc_82631090:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x826310a0
	if (!cr6.lt) goto loc_826310A0;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x826310ac
	goto loc_826310AC;
loc_826310A0:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x826310ac
	if (!cr6.gt) goto loc_826310AC;
	// li r8,255
	ctx.r8.s64 = 255;
loc_826310AC:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bge cr6,0x826310bc
	if (!cr6.lt) goto loc_826310BC;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x826310c8
	goto loc_826310C8;
loc_826310BC:
	// cmpwi cr6,r6,255
	cr6.compare<int32_t>(ctx.r6.s32, 255, xer);
	// ble cr6,0x826310c8
	if (!cr6.gt) goto loc_826310C8;
	// li r6,255
	ctx.r6.s64 = 255;
loc_826310C8:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x826310d8
	if (!cr6.lt) goto loc_826310D8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x826310e4
	goto loc_826310E4;
loc_826310D8:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x826310e4
	if (!cr6.gt) goto loc_826310E4;
	// li r9,255
	ctx.r9.s64 = 255;
loc_826310E4:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x826310f4
	if (!cr6.lt) goto loc_826310F4;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82631100
	goto loc_82631100;
loc_826310F4:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82631100
	if (!cr6.gt) goto loc_82631100;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82631100:
	// stbx r6,r20,r28
	PPC_STORE_U8(r20.u32 + r28.u32, ctx.r6.u8);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// stb r10,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r10.u8);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// stb r7,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r7.u8);
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// stb r4,0(r26)
	PPC_STORE_U8(r26.u32 + 0, ctx.r4.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// stb r9,0(r25)
	PPC_STORE_U8(r25.u32 + 0, ctx.r9.u8);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// stb r8,0(r24)
	PPC_STORE_U8(r24.u32 + 0, ctx.r8.u8);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// stb r5,0(r23)
	PPC_STORE_U8(r23.u32 + 0, ctx.r5.u8);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// stb r31,0(r21)
	PPC_STORE_U8(r21.u32 + 0, r31.u8);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// bne cr6,0x82630ed0
	if (!cr6.eq) goto loc_82630ED0;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_8263114C"))) PPC_WEAK_FUNC(sub_8263114C);
PPC_FUNC_IMPL(__imp__sub_8263114C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82631150"))) PPC_WEAK_FUNC(sub_82631150);
PPC_FUNC_IMPL(__imp__sub_82631150) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd0
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// li r26,0
	r26.s64 = 0;
	// rlwinm r3,r5,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r4,4
	ctx.r9.s64 = ctx.r4.s64 + 4;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// li r25,1
	r25.s64 = 1;
loc_82631174:
	// slw. r8,r25,r26
	ctx.r8.u64 = r26.u8 & 0x20 ? 0 : (r25.u32 << (r26.u8 & 0x3F));
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x826311b8
	if (!cr0.eq) goto loc_826311B8;
	// lhz r8,-4(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + -4);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x826312f8
	if (cr6.eq) goto loc_826312F8;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r23,r26
	r23.u64 = r26.u64;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// sth r8,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r8.u16);
	// sth r8,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r8.u16);
	// sth r8,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r8.u16);
	// sth r8,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r8.u16);
	// sth r8,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r8.u16);
	// sth r8,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r8.u16);
	// sth r8,-4(r10)
	PPC_STORE_U16(ctx.r10.u32 + -4, ctx.r8.u16);
	// b 0x826312f4
	goto loc_826312F4;
loc_826311B8:
	// lhz r4,-4(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + -4);
	// mr r23,r26
	r23.u64 = r26.u64;
	// lhz r31,4(r9)
	r31.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r8,-2(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + -2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r30,0(r9)
	r30.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r27,r31
	r27.s64 = r31.s16;
	// lhz r7,10(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r6,2(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// rlwinm r29,r4,11,0,20
	r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 11) & 0xFFFFF800;
	// lhz r5,6(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r28,8(r9)
	r28.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// extsh r4,r30
	ctx.r4.s64 = r30.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r31,r28
	r31.s64 = r28.s16;
	// rlwinm r30,r27,11,0,20
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 11) & 0xFFFFF800;
	// mulli r27,r8,2276
	r27.s64 = ctx.r8.s64 * 2276;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r29,r29,128
	r29.s64 = r29.s64 + 128;
	// add r22,r5,r6
	r22.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r20,r31,r4
	r20.u64 = r31.u64 + ctx.r4.u64;
	// mulli r28,r4,1568
	r28.s64 = ctx.r4.s64 * 1568;
	// mulli r19,r31,3784
	r19.s64 = r31.s64 * 3784;
	// mulli r21,r6,4017
	r21.s64 = ctx.r6.s64 * 4017;
	// add r4,r29,r30
	ctx.r4.u64 = r29.u64 + r30.u64;
	// subf r31,r30,r29
	r31.s64 = r29.s64 - r30.s64;
	// mulli r8,r8,565
	ctx.r8.s64 = ctx.r8.s64 * 565;
	// mulli r5,r5,799
	ctx.r5.s64 = ctx.r5.s64 * 799;
	// mulli r6,r22,2408
	ctx.r6.s64 = r22.s64 * 2408;
	// mulli r30,r20,1108
	r30.s64 = r20.s64 * 1108;
	// mulli r24,r7,3406
	r24.s64 = ctx.r7.s64 * 3406;
	// add r7,r27,r8
	ctx.r7.u64 = r27.u64 + ctx.r8.u64;
	// subf r5,r5,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r29,r19,r30
	r29.s64 = r30.s64 - r19.s64;
	// subf r8,r24,r8
	ctx.r8.s64 = ctx.r8.s64 - r24.s64;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// subf r6,r21,r6
	ctx.r6.s64 = ctx.r6.s64 - r21.s64;
	// add r28,r5,r7
	r28.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r6,r4,r30
	ctx.r6.u64 = ctx.r4.u64 + r30.u64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// add r30,r31,r29
	r30.u64 = r31.u64 + r29.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// add r29,r8,r7
	r29.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mulli r7,r29,181
	ctx.r7.s64 = r29.s64 * 181;
	// mulli r8,r8,181
	ctx.r8.s64 = ctx.r8.s64 * 181;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// addi r29,r8,128
	r29.s64 = ctx.r8.s64 + 128;
	// srawi r8,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 8;
	// srawi r7,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	ctx.r7.s64 = r29.s32 >> 8;
	// add r29,r28,r6
	r29.u64 = r28.u64 + ctx.r6.u64;
	// add r27,r4,r5
	r27.u64 = ctx.r4.u64 + ctx.r5.u64;
	// srawi r29,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r29.s64 = r29.s32 >> 8;
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r4,r8,r30
	ctx.r4.u64 = ctx.r8.u64 + r30.u64;
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// sth r29,-4(r10)
	PPC_STORE_U16(ctx.r10.u32 + -4, r29.u16);
	// add r29,r31,r7
	r29.u64 = r31.u64 + ctx.r7.u64;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// srawi r29,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r29.s64 = r29.s32 >> 8;
	// srawi r31,r27,8
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xFF) != 0);
	r31.s64 = r27.s32 >> 8;
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// sth r4,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r4.u16);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// sth r29,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r29.u16);
	// sth r31,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r31.u16);
	// sth r5,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r5.u16);
	// sth r7,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r7.u16);
	// subf r7,r28,r6
	ctx.r7.s64 = ctx.r6.s64 - r28.s64;
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// sth r7,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r7.u16);
loc_826312F4:
	// sth r8,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r8.u16);
loc_826312F8:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmpwi cr6,r26,8
	cr6.compare<int32_t>(r26.s32, 8, xer);
	// blt cr6,0x82631174
	if (cr6.lt) goto loc_82631174;
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// li r22,8
	r22.s64 = 8;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r31,r11,r10
	r31.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r30,r11,r9
	r30.s64 = ctx.r9.s64 - r11.s64;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r28,r11,r9
	r28.s64 = ctx.r9.s64 - r11.s64;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r26,r11,r9
	r26.s64 = ctx.r9.s64 - r11.s64;
	// subf r25,r11,r10
	r25.s64 = ctx.r10.s64 - r11.s64;
loc_82631348:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// bne cr6,0x82631390
	if (!cr6.eq) goto loc_82631390;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826314e8
	if (cr6.eq) goto loc_826314E8;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sthx r10,r25,r11
	PPC_STORE_U16(r25.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r26,r11
	PPC_STORE_U16(r26.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r27,r11
	PPC_STORE_U16(r27.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r28,r11
	PPC_STORE_U16(r28.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r29,r11
	PPC_STORE_U16(r29.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r30,r11
	PPC_STORE_U16(r30.u32 + r11.u32, ctx.r10.u16);
	// sthx r10,r31,r11
	PPC_STORE_U16(r31.u32 + r11.u32, ctx.r10.u16);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// b 0x826314e8
	goto loc_826314E8;
loc_82631390:
	// lhzx r9,r31,r11
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + r11.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lhzx r8,r25,r11
	ctx.r8.u64 = PPC_LOAD_U16(r25.u32 + r11.u32);
	// lhzx r7,r27,r11
	ctx.r7.u64 = PPC_LOAD_U16(r27.u32 + r11.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhzx r6,r29,r11
	ctx.r6.u64 = PPC_LOAD_U16(r29.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhzx r5,r26,r11
	ctx.r5.u64 = PPC_LOAD_U16(r26.u32 + r11.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhzx r4,r30,r11
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + r11.u32);
	// mulli r24,r9,2276
	r24.s64 = ctx.r9.s64 * 2276;
	// lhzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U16(r28.u32 + r11.u32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r21,r7,r6
	r21.u64 = ctx.r7.u64 + ctx.r6.u64;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// mulli r20,r8,3406
	r20.s64 = ctx.r8.s64 * 3406;
	// mulli r9,r9,565
	ctx.r9.s64 = ctx.r9.s64 * 565;
	// mulli r8,r21,2408
	ctx.r8.s64 = r21.s64 * 2408;
	// add r21,r5,r4
	r21.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// mulli r18,r6,4017
	r18.s64 = ctx.r6.s64 * 4017;
	// mulli r19,r7,799
	r19.s64 = ctx.r7.s64 * 799;
	// mulli r6,r21,1108
	ctx.r6.s64 = r21.s64 * 1108;
	// add r7,r24,r9
	ctx.r7.u64 = r24.u64 + ctx.r9.u64;
	// mulli r21,r5,3784
	r21.s64 = ctx.r5.s64 * 3784;
	// subf r5,r20,r9
	ctx.r5.s64 = ctx.r9.s64 - r20.s64;
	// addi r9,r8,4
	ctx.r9.s64 = ctx.r8.s64 + 4;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// srawi r8,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 3;
	// srawi r7,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 3;
	// rlwinm r3,r3,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// subf r5,r19,r9
	ctx.r5.s64 = ctx.r9.s64 - r19.s64;
	// mulli r24,r4,1568
	r24.s64 = ctx.r4.s64 * 1568;
	// subf r4,r18,r9
	ctx.r4.s64 = ctx.r9.s64 - r18.s64;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// srawi r5,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 3;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// subf r3,r21,r6
	ctx.r3.s64 = ctx.r6.s64 - r21.s64;
	// add r24,r24,r6
	r24.u64 = r24.u64 + ctx.r6.u64;
	// srawi r4,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 3;
	// add r6,r5,r8
	ctx.r6.u64 = ctx.r5.u64 + ctx.r8.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r5,r7,r4
	ctx.r5.u64 = ctx.r7.u64 + ctx.r4.u64;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// srawi r4,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 3;
	// srawi r3,r24,3
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7) != 0);
	ctx.r3.s64 = r24.s32 >> 3;
	// add r24,r7,r8
	r24.u64 = ctx.r7.u64 + ctx.r8.u64;
	// subf r21,r7,r8
	r21.s64 = ctx.r8.s64 - ctx.r7.s64;
	// mulli r7,r24,181
	ctx.r7.s64 = r24.s64 * 181;
	// add r8,r9,r3
	ctx.r8.u64 = ctx.r9.u64 + ctx.r3.u64;
	// subf r9,r3,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r3.s64;
	// addi r3,r7,128
	ctx.r3.s64 = ctx.r7.s64 + 128;
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// srawi r4,r3,8
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 8;
	// mulli r3,r21,181
	ctx.r3.s64 = r21.s64 * 181;
	// addi r3,r3,128
	ctx.r3.s64 = ctx.r3.s64 + 128;
	// add r24,r6,r8
	r24.u64 = ctx.r6.u64 + ctx.r8.u64;
	// add r21,r4,r7
	r21.u64 = ctx.r4.u64 + ctx.r7.u64;
	// srawi r3,r3,8
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 8;
	// srawi r24,r24,14
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3FFF) != 0);
	r24.s64 = r24.s32 >> 14;
	// srawi r21,r21,14
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3FFF) != 0);
	r21.s64 = r21.s32 >> 14;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// sth r24,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r24.u16);
	// add r24,r10,r3
	r24.u64 = ctx.r10.u64 + ctx.r3.u64;
	// sthx r21,r31,r11
	PPC_STORE_U16(r31.u32 + r11.u32, r21.u16);
	// add r21,r9,r5
	r21.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r24,r24,14
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3FFF) != 0);
	r24.s64 = r24.s32 >> 14;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// srawi r5,r21,14
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3FFF) != 0);
	ctx.r5.s64 = r21.s32 >> 14;
	// srawi r9,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 14;
	// srawi r10,r10,14
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 14;
	// sthx r24,r30,r11
	PPC_STORE_U16(r30.u32 + r11.u32, r24.u16);
	// srawi r7,r7,14
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 14;
	// srawi r8,r8,14
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 14;
	// sthx r5,r29,r11
	PPC_STORE_U16(r29.u32 + r11.u32, ctx.r5.u16);
	// sthx r9,r28,r11
	PPC_STORE_U16(r28.u32 + r11.u32, ctx.r9.u16);
	// sthx r10,r27,r11
	PPC_STORE_U16(r27.u32 + r11.u32, ctx.r10.u16);
	// sthx r7,r26,r11
	PPC_STORE_U16(r26.u32 + r11.u32, ctx.r7.u16);
	// sthx r8,r25,r11
	PPC_STORE_U16(r25.u32 + r11.u32, ctx.r8.u16);
loc_826314E8:
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82631348
	if (!cr6.eq) goto loc_82631348;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_826314FC"))) PPC_WEAK_FUNC(sub_826314FC);
PPC_FUNC_IMPL(__imp__sub_826314FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82631500"))) PPC_WEAK_FUNC(sub_82631500);
PPC_FUNC_IMPL(__imp__sub_82631500) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd0
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// li r26,0
	r26.s64 = 0;
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r4,8
	ctx.r9.s64 = ctx.r4.s64 + 8;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// li r25,1
	r25.s64 = 1;
loc_82631524:
	// slw. r8,r25,r26
	ctx.r8.u64 = r26.u8 & 0x20 ? 0 : (r25.u32 << (r26.u8 & 0x3F));
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x82631560
	if (!cr0.eq) goto loc_82631560;
	// lwz r8,-8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82631680
	if (cr6.eq) goto loc_82631680;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r23,r26
	r23.u64 = r26.u64;
	// stw r8,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r8.u32);
	// stw r8,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r8.u32);
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r8,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r8.u32);
	// stw r8,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, ctx.r8.u32);
	// b 0x8263167c
	goto loc_8263167C;
loc_82631560:
	// lwz r4,-8(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// mr r23,r26
	r23.u64 = r26.u64;
	// lwz r8,-4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// lwz r7,20(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// rlwinm r29,r4,11,0,20
	r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 11) & 0xFFFFF800;
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mulli r27,r8,2276
	r27.s64 = ctx.r8.s64 * 2276;
	// lwz r5,12(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r30,8(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r31,16(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r22,r5,r6
	r22.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r29,r29,128
	r29.s64 = r29.s64 + 128;
	// rlwinm r30,r30,11,0,20
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 11) & 0xFFFFF800;
	// add r20,r31,r4
	r20.u64 = r31.u64 + ctx.r4.u64;
	// mulli r28,r4,1568
	r28.s64 = ctx.r4.s64 * 1568;
	// mulli r21,r6,4017
	r21.s64 = ctx.r6.s64 * 4017;
	// mulli r19,r31,3784
	r19.s64 = r31.s64 * 3784;
	// add r4,r29,r30
	ctx.r4.u64 = r29.u64 + r30.u64;
	// subf r31,r30,r29
	r31.s64 = r29.s64 - r30.s64;
	// mulli r8,r8,565
	ctx.r8.s64 = ctx.r8.s64 * 565;
	// mulli r5,r5,799
	ctx.r5.s64 = ctx.r5.s64 * 799;
	// mulli r6,r22,2408
	ctx.r6.s64 = r22.s64 * 2408;
	// mulli r30,r20,1108
	r30.s64 = r20.s64 * 1108;
	// mulli r24,r7,3406
	r24.s64 = ctx.r7.s64 * 3406;
	// add r7,r27,r8
	ctx.r7.u64 = r27.u64 + ctx.r8.u64;
	// subf r5,r5,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r29,r19,r30
	r29.s64 = r30.s64 - r19.s64;
	// subf r8,r24,r8
	ctx.r8.s64 = ctx.r8.s64 - r24.s64;
	// subf r6,r21,r6
	ctx.r6.s64 = ctx.r6.s64 - r21.s64;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// add r28,r5,r7
	r28.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r6,r4,r30
	ctx.r6.u64 = ctx.r4.u64 + r30.u64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// add r30,r31,r29
	r30.u64 = r31.u64 + r29.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// add r29,r8,r7
	r29.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mulli r7,r29,181
	ctx.r7.s64 = r29.s64 * 181;
	// mulli r8,r8,181
	ctx.r8.s64 = ctx.r8.s64 * 181;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// addi r29,r8,128
	r29.s64 = ctx.r8.s64 + 128;
	// srawi r8,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 8;
	// srawi r7,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	ctx.r7.s64 = r29.s32 >> 8;
	// add r29,r28,r6
	r29.u64 = r28.u64 + ctx.r6.u64;
	// add r27,r4,r5
	r27.u64 = ctx.r4.u64 + ctx.r5.u64;
	// srawi r29,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r29.s64 = r29.s32 >> 8;
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r4,r8,r30
	ctx.r4.u64 = ctx.r8.u64 + r30.u64;
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// subf r6,r28,r6
	ctx.r6.s64 = ctx.r6.s64 - r28.s64;
	// stw r29,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r29.u32);
	// add r29,r31,r7
	r29.u64 = r31.u64 + ctx.r7.u64;
	// subf r7,r7,r31
	ctx.r7.s64 = r31.s64 - ctx.r7.s64;
	// srawi r29,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r29.s64 = r29.s32 >> 8;
	// srawi r31,r27,8
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xFF) != 0);
	r31.s64 = r27.s32 >> 8;
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// stw r4,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r4.u32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stw r29,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r29.u32);
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
	// stw r7,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r7.u32);
	// stw r6,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r6.u32);
loc_8263167C:
	// stw r8,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r8.u32);
loc_82631680:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// cmpwi cr6,r26,8
	cr6.compare<int32_t>(r26.s32, 8, xer);
	// blt cr6,0x82631524
	if (cr6.lt) goto loc_82631524;
	// add r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 + r11.u64;
	// li r22,8
	r22.s64 = 8;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r31,r11,r10
	r31.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r30,r11,r9
	r30.s64 = ctx.r9.s64 - r11.s64;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r28,r11,r9
	r28.s64 = ctx.r9.s64 - r11.s64;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// subf r27,r11,r10
	r27.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r3,r9
	ctx.r10.u64 = ctx.r3.u64 + ctx.r9.u64;
	// subf r26,r11,r9
	r26.s64 = ctx.r9.s64 - r11.s64;
	// subf r25,r11,r10
	r25.s64 = ctx.r10.s64 - r11.s64;
loc_826316D0:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x82631710
	if (!cr6.eq) goto loc_82631710;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263184c
	if (cr6.eq) goto loc_8263184C;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// stwx r10,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r10.u32);
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
	// stwx r10,r11,r27
	PPC_STORE_U32(r11.u32 + r27.u32, ctx.r10.u32);
	// stwx r10,r11,r28
	PPC_STORE_U32(r11.u32 + r28.u32, ctx.r10.u32);
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
	// stwx r10,r11,r30
	PPC_STORE_U32(r11.u32 + r30.u32, ctx.r10.u32);
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x8263184c
	goto loc_8263184C;
loc_82631710:
	// lwzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwzx r8,r11,r25
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + r25.u32);
	// lwzx r7,r11,r27
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// mulli r24,r9,2276
	r24.s64 = ctx.r9.s64 * 2276;
	// lwzx r6,r11,r29
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// lwzx r5,r11,r30
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// lwzx r4,r11,r26
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// lwzx r3,r11,r28
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r21,r7,r6
	r21.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mulli r20,r8,3406
	r20.s64 = ctx.r8.s64 * 3406;
	// mulli r9,r9,565
	ctx.r9.s64 = ctx.r9.s64 * 565;
	// mulli r8,r21,2408
	ctx.r8.s64 = r21.s64 * 2408;
	// add r21,r4,r5
	r21.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// mulli r18,r6,4017
	r18.s64 = ctx.r6.s64 * 4017;
	// mulli r6,r21,1108
	ctx.r6.s64 = r21.s64 * 1108;
	// mulli r21,r4,3784
	r21.s64 = ctx.r4.s64 * 3784;
	// mulli r19,r7,799
	r19.s64 = ctx.r7.s64 * 799;
	// add r7,r24,r9
	ctx.r7.u64 = r24.u64 + ctx.r9.u64;
	// subf r4,r20,r9
	ctx.r4.s64 = ctx.r9.s64 - r20.s64;
	// addi r9,r8,4
	ctx.r9.s64 = ctx.r8.s64 + 4;
	// mulli r24,r5,1568
	r24.s64 = ctx.r5.s64 * 1568;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r3,r3,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFFFFFF00;
	// subf r5,r19,r9
	ctx.r5.s64 = ctx.r9.s64 - r19.s64;
	// srawi r8,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 3;
	// srawi r7,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r4.s32 >> 3;
	// subf r4,r18,r9
	ctx.r4.s64 = ctx.r9.s64 - r18.s64;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// add r9,r10,r3
	ctx.r9.u64 = ctx.r10.u64 + ctx.r3.u64;
	// srawi r5,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 3;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// add r24,r24,r6
	r24.u64 = r24.u64 + ctx.r6.u64;
	// srawi r4,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 3;
	// subf r3,r21,r6
	ctx.r3.s64 = ctx.r6.s64 - r21.s64;
	// add r6,r5,r8
	ctx.r6.u64 = ctx.r5.u64 + ctx.r8.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r5,r7,r4
	ctx.r5.u64 = ctx.r7.u64 + ctx.r4.u64;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// srawi r4,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 3;
	// srawi r3,r24,3
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7) != 0);
	ctx.r3.s64 = r24.s32 >> 3;
	// add r24,r7,r8
	r24.u64 = ctx.r7.u64 + ctx.r8.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// mulli r7,r24,181
	ctx.r7.s64 = r24.s64 * 181;
	// mulli r24,r8,181
	r24.s64 = ctx.r8.s64 * 181;
	// add r8,r9,r3
	ctx.r8.u64 = ctx.r9.u64 + ctx.r3.u64;
	// subf r9,r3,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r3.s64;
	// addi r3,r7,128
	ctx.r3.s64 = ctx.r7.s64 + 128;
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r24,r24,128
	r24.s64 = r24.s64 + 128;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// srawi r4,r3,8
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 8;
	// srawi r3,r24,8
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xFF) != 0);
	ctx.r3.s64 = r24.s32 >> 8;
	// add r24,r6,r8
	r24.u64 = ctx.r6.u64 + ctx.r8.u64;
	// add r21,r9,r5
	r21.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r24,r24,14
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x3FFF) != 0);
	r24.s64 = r24.s32 >> 14;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// add r5,r4,r7
	ctx.r5.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r20,r10,r3
	r20.u64 = ctx.r10.u64 + ctx.r3.u64;
	// srawi r5,r5,14
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3FFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 14;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// stw r24,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r24.u32);
	// srawi r24,r20,14
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x3FFF) != 0);
	r24.s64 = r20.s32 >> 14;
	// srawi r3,r21,14
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3FFF) != 0);
	ctx.r3.s64 = r21.s32 >> 14;
	// srawi r9,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 14;
	// srawi r10,r10,14
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 14;
	// stwx r5,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r5.u32);
	// stwx r24,r11,r30
	PPC_STORE_U32(r11.u32 + r30.u32, r24.u32);
	// stwx r3,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r3.u32);
	// stwx r9,r11,r28
	PPC_STORE_U32(r11.u32 + r28.u32, ctx.r9.u32);
	// subf r9,r6,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stwx r10,r11,r27
	PPC_STORE_U32(r11.u32 + r27.u32, ctx.r10.u32);
	// subf r10,r4,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r4.s64;
	// srawi r10,r10,14
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 14;
	// srawi r9,r9,14
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 14;
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
	// stwx r9,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.r9.u32);
loc_8263184C:
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x826316d0
	if (!cr6.eq) goto loc_826316D0;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_82631860"))) PPC_WEAK_FUNC(sub_82631860);
PPC_FUNC_IMPL(__imp__sub_82631860) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcdc
	// rlwinm r11,r6,7,0,24
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r27,r4,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r11,r3
	r26.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r10,r26,8
	ctx.r10.s64 = r26.s64 + 8;
loc_82631880:
	// lwz r4,-8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r7,20(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// rlwinm r30,r4,11,0,20
	r30.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 11) & 0xFFFFF800;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mulli r28,r8,2276
	r28.s64 = ctx.r8.s64 * 2276;
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r3,16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r24,r5,r6
	r24.u64 = ctx.r5.u64 + ctx.r6.u64;
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// rlwinm r31,r31,11,0,20
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 11) & 0xFFFFF800;
	// add r22,r3,r4
	r22.u64 = ctx.r3.u64 + ctx.r4.u64;
	// mulli r29,r4,1568
	r29.s64 = ctx.r4.s64 * 1568;
	// mulli r23,r6,4017
	r23.s64 = ctx.r6.s64 * 4017;
	// mulli r21,r3,3784
	r21.s64 = ctx.r3.s64 * 3784;
	// add r4,r30,r31
	ctx.r4.u64 = r30.u64 + r31.u64;
	// subf r3,r31,r30
	ctx.r3.s64 = r30.s64 - r31.s64;
	// mulli r8,r8,565
	ctx.r8.s64 = ctx.r8.s64 * 565;
	// mulli r5,r5,799
	ctx.r5.s64 = ctx.r5.s64 * 799;
	// mulli r6,r24,2408
	ctx.r6.s64 = r24.s64 * 2408;
	// mulli r31,r22,1108
	r31.s64 = r22.s64 * 1108;
	// mulli r25,r7,3406
	r25.s64 = ctx.r7.s64 * 3406;
	// add r7,r28,r8
	ctx.r7.u64 = r28.u64 + ctx.r8.u64;
	// subf r5,r5,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r30,r21,r31
	r30.s64 = r31.s64 - r21.s64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// subf r6,r23,r6
	ctx.r6.s64 = ctx.r6.s64 - r23.s64;
	// add r31,r29,r31
	r31.u64 = r29.u64 + r31.u64;
	// add r29,r5,r7
	r29.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r6,r4,r31
	ctx.r6.u64 = ctx.r4.u64 + r31.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// add r31,r3,r30
	r31.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r30,r3
	ctx.r3.s64 = ctx.r3.s64 - r30.s64;
	// add r30,r8,r7
	r30.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mulli r7,r30,181
	ctx.r7.s64 = r30.s64 * 181;
	// mulli r8,r8,181
	ctx.r8.s64 = ctx.r8.s64 * 181;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// addi r30,r8,128
	r30.s64 = ctx.r8.s64 + 128;
	// srawi r8,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 8;
	// srawi r7,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	ctx.r7.s64 = r30.s32 >> 8;
	// add r30,r29,r6
	r30.u64 = r29.u64 + ctx.r6.u64;
	// add r28,r4,r5
	r28.u64 = ctx.r4.u64 + ctx.r5.u64;
	// srawi r30,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	r30.s64 = r30.s32 >> 8;
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r4,r8,r31
	ctx.r4.u64 = ctx.r8.u64 + r31.u64;
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// subf r6,r29,r6
	ctx.r6.s64 = ctx.r6.s64 - r29.s64;
	// stw r30,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r30.u32);
	// add r30,r3,r7
	r30.u64 = ctx.r3.u64 + ctx.r7.u64;
	// subf r7,r7,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r7.s64;
	// srawi r30,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	r30.s64 = r30.s32 >> 8;
	// srawi r3,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	ctx.r3.s64 = r28.s32 >> 8;
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// stw r4,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r4.u32);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// stw r3,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r3.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
	// stw r7,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r7.u32);
	// stw r8,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r8.u32);
	// stw r6,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r6.u32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// bne cr6,0x82631880
	if (!cr6.eq) goto loc_82631880;
	// add r11,r27,r26
	r11.u64 = r27.u64 + r26.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// add r10,r27,r11
	ctx.r10.u64 = r27.u64 + r11.u64;
	// subf r31,r11,r26
	r31.s64 = r26.s64 - r11.s64;
	// add r8,r27,r10
	ctx.r8.u64 = r27.u64 + ctx.r10.u64;
	// subf r28,r11,r10
	r28.s64 = ctx.r10.s64 - r11.s64;
	// lis r10,0
	ctx.r10.s64 = 0;
	// subf r27,r11,r8
	r27.s64 = ctx.r8.s64 - r11.s64;
	// ori r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 32768;
loc_826319D4:
	// add r7,r28,r11
	ctx.r7.u64 = r28.u64 + r11.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r6,r27,r11
	ctx.r6.u64 = r27.u64 + r11.u64;
	// lwzx r5,r31,r11
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// mulli r30,r8,1892
	r30.s64 = ctx.r8.s64 * 1892;
	// lwz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mulli r26,r8,784
	r26.s64 = ctx.r8.s64 * 784;
	// add r8,r4,r5
	ctx.r8.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mulli r29,r3,784
	r29.s64 = ctx.r3.s64 * 784;
	// subf r4,r4,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mulli r8,r8,1448
	ctx.r8.s64 = ctx.r8.s64 * 1448;
	// add r5,r30,r29
	ctx.r5.u64 = r30.u64 + r29.u64;
	// mulli r3,r3,1892
	ctx.r3.s64 = ctx.r3.s64 * 1892;
	// add r30,r8,r5
	r30.u64 = ctx.r8.u64 + ctx.r5.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r5,r30,r10
	ctx.r5.u64 = r30.u64 + ctx.r10.u64;
	// mulli r4,r4,1448
	ctx.r4.s64 = ctx.r4.s64 * 1448;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// subf r3,r3,r26
	ctx.r3.s64 = r26.s64 - ctx.r3.s64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r30,r4,r3
	r30.u64 = ctx.r4.u64 + ctx.r3.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stwx r5,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, ctx.r5.u32);
	// subf r5,r3,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r4,r30,r10
	ctx.r4.u64 = r30.u64 + ctx.r10.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r4,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 16;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// srawi r8,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// stw r8,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r8.u32);
	// bne cr6,0x826319d4
	if (!cr6.eq) goto loc_826319D4;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82631A68"))) PPC_WEAK_FUNC(sub_82631A68);
PPC_FUNC_IMPL(__imp__sub_82631A68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc8
	// rlwinm r11,r6,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// li r8,8
	ctx.r8.s64 = 8;
	// addi r10,r30,8
	ctx.r10.s64 = r30.s64 + 8;
loc_82631A88:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r6,-4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mulli r3,r6,1892
	ctx.r3.s64 = ctx.r6.s64 * 1892;
	// add r29,r5,r7
	r29.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mulli r31,r4,784
	r31.s64 = ctx.r4.s64 * 784;
	// subf r5,r5,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mulli r28,r6,784
	r28.s64 = ctx.r6.s64 * 784;
	// add r6,r3,r31
	ctx.r6.u64 = ctx.r3.u64 + r31.u64;
	// mulli r7,r29,1448
	ctx.r7.s64 = r29.s64 * 1448;
	// add r3,r6,r7
	ctx.r3.u64 = ctx.r6.u64 + ctx.r7.u64;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// addi r6,r3,64
	ctx.r6.s64 = ctx.r3.s64 + 64;
	// addi r3,r7,64
	ctx.r3.s64 = ctx.r7.s64 + 64;
	// srawi r7,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 7;
	// mulli r4,r4,1892
	ctx.r4.s64 = ctx.r4.s64 * 1892;
	// stw r7,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, ctx.r7.u32);
	// mulli r5,r5,1448
	ctx.r5.s64 = ctx.r5.s64 * 1448;
	// subf r4,r4,r28
	ctx.r4.s64 = r28.s64 - ctx.r4.s64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// add r6,r4,r5
	ctx.r6.u64 = ctx.r4.u64 + ctx.r5.u64;
	// subf r7,r4,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r4.s64;
	// addi r6,r6,64
	ctx.r6.s64 = ctx.r6.s64 + 64;
	// addi r7,r7,64
	ctx.r7.s64 = ctx.r7.s64 + 64;
	// srawi r6,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 7;
	// srawi r7,r7,7
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 7;
	// srawi r5,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 7;
	// stw r6,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r6.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// stw r5,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r5.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bne cr6,0x82631a88
	if (!cr6.eq) goto loc_82631A88;
	// add r11,r9,r30
	r11.u64 = ctx.r9.u64 + r30.u64;
	// li r10,4
	ctx.r10.s64 = 4;
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + r11.u64;
	// subf r28,r11,r30
	r28.s64 = r30.s64 - r11.s64;
	// add r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r27,r11,r8
	r27.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r9,r7
	ctx.r8.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r26,r11,r7
	r26.s64 = ctx.r7.s64 - r11.s64;
	// add r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r25,r11,r8
	r25.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r9,r7
	ctx.r8.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r24,r11,r7
	r24.s64 = ctx.r7.s64 - r11.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r20,r11,r8
	r20.s64 = ctx.r8.s64 - r11.s64;
	// subf r19,r11,r9
	r19.s64 = ctx.r9.s64 - r11.s64;
loc_82631B50:
	// add r9,r19,r11
	ctx.r9.u64 = r19.u64 + r11.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r7,r20,r11
	ctx.r7.u64 = r20.u64 + r11.u64;
	// lwzx r5,r24,r11
	ctx.r5.u64 = PPC_LOAD_U32(r24.u32 + r11.u32);
	// mulli r21,r8,2276
	r21.s64 = ctx.r8.s64 * 2276;
	// lwzx r4,r26,r11
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + r11.u32);
	// lwzx r30,r28,r11
	r30.u64 = PPC_LOAD_U32(r28.u32 + r11.u32);
	// lwzx r29,r25,r11
	r29.u64 = PPC_LOAD_U32(r25.u32 + r11.u32);
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwzx r3,r27,r11
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// lwz r31,0(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// mulli r18,r6,3406
	r18.s64 = ctx.r6.s64 * 3406;
	// mulli r8,r8,565
	ctx.r8.s64 = ctx.r8.s64 * 565;
	// add r6,r4,r5
	ctx.r6.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r23,r30,32
	r23.s64 = r30.s64 + 32;
	// rlwinm r30,r29,8,0,23
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFFFFFF00;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// rlwinm r29,r23,8,0,23
	r29.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 8) & 0xFFFFFF00;
	// mulli r6,r6,2408
	ctx.r6.s64 = ctx.r6.s64 * 2408;
	// add r23,r3,r31
	r23.u64 = ctx.r3.u64 + r31.u64;
	// mulli r17,r5,799
	r17.s64 = ctx.r5.s64 * 799;
	// mulli r22,r3,1568
	r22.s64 = ctx.r3.s64 * 1568;
	// add r5,r21,r8
	ctx.r5.u64 = r21.u64 + ctx.r8.u64;
	// subf r3,r18,r8
	ctx.r3.s64 = ctx.r8.s64 - r18.s64;
	// addi r8,r6,4
	ctx.r8.s64 = ctx.r6.s64 + 4;
	// mulli r23,r23,1108
	r23.s64 = r23.s64 * 1108;
	// mulli r16,r31,3784
	r16.s64 = r31.s64 * 3784;
	// mulli r4,r4,4017
	ctx.r4.s64 = ctx.r4.s64 * 4017;
	// subf r31,r17,r8
	r31.s64 = ctx.r8.s64 - r17.s64;
	// srawi r6,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 3;
	// srawi r5,r3,3
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 3;
	// subf r21,r4,r8
	r21.s64 = ctx.r8.s64 - ctx.r4.s64;
	// addi r3,r23,4
	ctx.r3.s64 = r23.s64 + 4;
	// srawi r31,r31,3
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7) != 0);
	r31.s64 = r31.s32 >> 3;
	// add r8,r30,r29
	ctx.r8.u64 = r30.u64 + r29.u64;
	// subf r4,r30,r29
	ctx.r4.s64 = r29.s64 - r30.s64;
	// srawi r30,r21,3
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7) != 0);
	r30.s64 = r21.s32 >> 3;
	// add r23,r22,r3
	r23.u64 = r22.u64 + ctx.r3.u64;
	// subf r29,r16,r3
	r29.s64 = ctx.r3.s64 - r16.s64;
	// add r3,r31,r6
	ctx.r3.u64 = r31.u64 + ctx.r6.u64;
	// subf r6,r31,r6
	ctx.r6.s64 = ctx.r6.s64 - r31.s64;
	// add r31,r30,r5
	r31.u64 = r30.u64 + ctx.r5.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// srawi r30,r29,3
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7) != 0);
	r30.s64 = r29.s32 >> 3;
	// srawi r29,r23,3
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7) != 0);
	r29.s64 = r23.s32 >> 3;
	// add r23,r5,r6
	r23.u64 = ctx.r5.u64 + ctx.r6.u64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// mulli r5,r23,181
	ctx.r5.s64 = r23.s64 * 181;
	// mulli r23,r6,181
	r23.s64 = ctx.r6.s64 * 181;
	// add r6,r8,r29
	ctx.r6.u64 = ctx.r8.u64 + r29.u64;
	// subf r8,r29,r8
	ctx.r8.s64 = ctx.r8.s64 - r29.s64;
	// addi r29,r5,128
	r29.s64 = ctx.r5.s64 + 128;
	// add r5,r30,r4
	ctx.r5.u64 = r30.u64 + ctx.r4.u64;
	// addi r23,r23,128
	r23.s64 = r23.s64 + 128;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// srawi r30,r29,8
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xFF) != 0);
	r30.s64 = r29.s32 >> 8;
	// srawi r29,r23,8
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFF) != 0);
	r29.s64 = r23.s32 >> 8;
	// add r23,r3,r6
	r23.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r22,r8,r31
	r22.u64 = ctx.r8.u64 + r31.u64;
	// srawi r23,r23,14
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3FFF) != 0);
	r23.s64 = r23.s32 >> 14;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// add r31,r5,r30
	r31.u64 = ctx.r5.u64 + r30.u64;
	// add r21,r4,r29
	r21.u64 = ctx.r4.u64 + r29.u64;
	// srawi r31,r31,14
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3FFF) != 0);
	r31.s64 = r31.s32 >> 14;
	// subf r4,r29,r4
	ctx.r4.s64 = ctx.r4.s64 - r29.s64;
	// stwx r23,r28,r11
	PPC_STORE_U32(r28.u32 + r11.u32, r23.u32);
	// srawi r23,r21,14
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x3FFF) != 0);
	r23.s64 = r21.s32 >> 14;
	// srawi r29,r22,14
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3FFF) != 0);
	r29.s64 = r22.s32 >> 14;
	// srawi r8,r8,14
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 14;
	// srawi r4,r4,14
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 14;
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// subf r6,r3,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r3.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwx r23,r27,r11
	PPC_STORE_U32(r27.u32 + r11.u32, r23.u32);
	// stwx r29,r26,r11
	PPC_STORE_U32(r26.u32 + r11.u32, r29.u32);
	// stwx r8,r25,r11
	PPC_STORE_U32(r25.u32 + r11.u32, ctx.r8.u32);
	// subf r8,r30,r5
	ctx.r8.s64 = ctx.r5.s64 - r30.s64;
	// stwx r4,r24,r11
	PPC_STORE_U32(r24.u32 + r11.u32, ctx.r4.u32);
	// srawi r8,r8,14
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 14;
	// srawi r6,r6,14
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 14;
	// stw r8,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82631b50
	if (!cr6.eq) goto loc_82631B50;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_82631CAC"))) PPC_WEAK_FUNC(sub_82631CAC);
PPC_FUNC_IMPL(__imp__sub_82631CAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82631CB0"))) PPC_WEAK_FUNC(sub_82631CB0);
PPC_FUNC_IMPL(__imp__sub_82631CB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcdc
	// rlwinm r11,r6,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r27,r4,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r26,r11,r3
	r26.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r10,r26,4
	ctx.r10.s64 = r26.s64 + 4;
loc_82631CD0:
	// lhz r4,-4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r28,r3
	r28.s64 = ctx.r3.s16;
	// lhz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// rlwinm r30,r4,11,0,20
	r30.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 11) & 0xFFFFF800;
	// lhz r5,6(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r29,8(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r4,r31
	ctx.r4.s64 = r31.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r3,r29
	ctx.r3.s64 = r29.s16;
	// rlwinm r31,r28,11,0,20
	r31.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 11) & 0xFFFFF800;
	// mulli r28,r8,2276
	r28.s64 = ctx.r8.s64 * 2276;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// add r24,r5,r6
	r24.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r22,r3,r4
	r22.u64 = ctx.r3.u64 + ctx.r4.u64;
	// mulli r29,r4,1568
	r29.s64 = ctx.r4.s64 * 1568;
	// mulli r21,r3,3784
	r21.s64 = ctx.r3.s64 * 3784;
	// mulli r23,r6,4017
	r23.s64 = ctx.r6.s64 * 4017;
	// add r4,r30,r31
	ctx.r4.u64 = r30.u64 + r31.u64;
	// subf r3,r31,r30
	ctx.r3.s64 = r30.s64 - r31.s64;
	// mulli r8,r8,565
	ctx.r8.s64 = ctx.r8.s64 * 565;
	// mulli r5,r5,799
	ctx.r5.s64 = ctx.r5.s64 * 799;
	// mulli r6,r24,2408
	ctx.r6.s64 = r24.s64 * 2408;
	// mulli r31,r22,1108
	r31.s64 = r22.s64 * 1108;
	// mulli r25,r7,3406
	r25.s64 = ctx.r7.s64 * 3406;
	// add r7,r28,r8
	ctx.r7.u64 = r28.u64 + ctx.r8.u64;
	// subf r5,r5,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r30,r21,r31
	r30.s64 = r31.s64 - r21.s64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// add r31,r29,r31
	r31.u64 = r29.u64 + r31.u64;
	// subf r6,r23,r6
	ctx.r6.s64 = ctx.r6.s64 - r23.s64;
	// add r29,r5,r7
	r29.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r6,r4,r31
	ctx.r6.u64 = ctx.r4.u64 + r31.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// add r31,r3,r30
	r31.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r30,r3
	ctx.r3.s64 = ctx.r3.s64 - r30.s64;
	// add r30,r8,r7
	r30.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mulli r7,r30,181
	ctx.r7.s64 = r30.s64 * 181;
	// mulli r8,r8,181
	ctx.r8.s64 = ctx.r8.s64 * 181;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// addi r30,r8,128
	r30.s64 = ctx.r8.s64 + 128;
	// srawi r8,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 8;
	// srawi r7,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	ctx.r7.s64 = r30.s32 >> 8;
	// add r30,r29,r6
	r30.u64 = r29.u64 + ctx.r6.u64;
	// add r28,r4,r5
	r28.u64 = ctx.r4.u64 + ctx.r5.u64;
	// srawi r30,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	r30.s64 = r30.s32 >> 8;
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r4,r8,r31
	ctx.r4.u64 = ctx.r8.u64 + r31.u64;
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// sth r30,-4(r10)
	PPC_STORE_U16(ctx.r10.u32 + -4, r30.u16);
	// add r30,r3,r7
	r30.u64 = ctx.r3.u64 + ctx.r7.u64;
	// subf r7,r7,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r7.s64;
	// srawi r30,r30,8
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFF) != 0);
	r30.s64 = r30.s32 >> 8;
	// srawi r3,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	ctx.r3.s64 = r28.s32 >> 8;
	// srawi r5,r5,8
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 8;
	// sth r4,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r4.u16);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// srawi r8,r8,8
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 8;
	// sth r30,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r30.u16);
	// sth r3,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r3.u16);
	// sth r5,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r5.u16);
	// sth r7,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r7.u16);
	// subf r7,r29,r6
	ctx.r7.s64 = ctx.r6.s64 - r29.s64;
	// sth r8,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r8.u16);
	// srawi r7,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 8;
	// sth r7,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r7.u16);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82631cd0
	if (!cr6.eq) goto loc_82631CD0;
	// add r11,r27,r26
	r11.u64 = r27.u64 + r26.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// add r10,r27,r11
	ctx.r10.u64 = r27.u64 + r11.u64;
	// subf r31,r11,r26
	r31.s64 = r26.s64 - r11.s64;
	// add r8,r27,r10
	ctx.r8.u64 = r27.u64 + ctx.r10.u64;
	// subf r28,r11,r10
	r28.s64 = ctx.r10.s64 - r11.s64;
	// lis r10,0
	ctx.r10.s64 = 0;
	// subf r27,r11,r8
	r27.s64 = ctx.r8.s64 - r11.s64;
	// ori r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 32768;
loc_82631E44:
	// add r8,r28,r11
	ctx.r8.u64 = r28.u64 + r11.u64;
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r7,r27,r11
	ctx.r7.u64 = r27.u64 + r11.u64;
	// lhzx r5,r31,r11
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + r11.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// mulli r30,r6,1892
	r30.s64 = ctx.r6.s64 * 1892;
	// lhz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// lhz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mulli r26,r6,784
	r26.s64 = ctx.r6.s64 * 784;
	// add r6,r4,r5
	ctx.r6.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mulli r29,r3,784
	r29.s64 = ctx.r3.s64 * 784;
	// subf r4,r4,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mulli r6,r6,1448
	ctx.r6.s64 = ctx.r6.s64 * 1448;
	// add r5,r30,r29
	ctx.r5.u64 = r30.u64 + r29.u64;
	// mulli r3,r3,1892
	ctx.r3.s64 = ctx.r3.s64 * 1892;
	// add r30,r6,r5
	r30.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// add r5,r30,r10
	ctx.r5.u64 = r30.u64 + ctx.r10.u64;
	// mulli r4,r4,1448
	ctx.r4.s64 = ctx.r4.s64 * 1448;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// subf r3,r3,r26
	ctx.r3.s64 = r26.s64 - ctx.r3.s64;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r30,r4,r3
	r30.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r3,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r3,r30,r10
	ctx.r3.u64 = r30.u64 + ctx.r10.u64;
	// sthx r5,r31,r11
	PPC_STORE_U16(r31.u32 + r11.u32, ctx.r5.u16);
	// add r5,r4,r10
	ctx.r5.u64 = ctx.r4.u64 + ctx.r10.u64;
	// srawi r4,r3,16
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xFFFF) != 0);
	ctx.r4.s64 = ctx.r3.s32 >> 16;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// sth r4,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r4.u16);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// sth r5,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r5.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// bne cr6,0x82631e44
	if (!cr6.eq) goto loc_82631E44;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82631EE8"))) PPC_WEAK_FUNC(sub_82631EE8);
PPC_FUNC_IMPL(__imp__sub_82631EE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc4
	// rlwinm r11,r6,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// li r8,8
	ctx.r8.s64 = 8;
	// addi r10,r30,4
	ctx.r10.s64 = r30.s64 + 4;
loc_82631F08:
	// lhz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lhz r6,-2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// add r29,r5,r7
	r29.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mulli r3,r6,1892
	ctx.r3.s64 = ctx.r6.s64 * 1892;
	// mulli r31,r4,784
	r31.s64 = ctx.r4.s64 * 784;
	// subf r5,r5,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mulli r28,r6,784
	r28.s64 = ctx.r6.s64 * 784;
	// add r6,r3,r31
	ctx.r6.u64 = ctx.r3.u64 + r31.u64;
	// mulli r7,r29,1448
	ctx.r7.s64 = r29.s64 * 1448;
	// add r3,r6,r7
	ctx.r3.u64 = ctx.r6.u64 + ctx.r7.u64;
	// mulli r4,r4,1892
	ctx.r4.s64 = ctx.r4.s64 * 1892;
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// addi r6,r3,64
	ctx.r6.s64 = ctx.r3.s64 + 64;
	// mulli r5,r5,1448
	ctx.r5.s64 = ctx.r5.s64 * 1448;
	// subf r4,r4,r28
	ctx.r4.s64 = r28.s64 - ctx.r4.s64;
	// srawi r31,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	r31.s64 = ctx.r6.s32 >> 7;
	// addi r3,r7,64
	ctx.r3.s64 = ctx.r7.s64 + 64;
	// add r6,r4,r5
	ctx.r6.u64 = ctx.r4.u64 + ctx.r5.u64;
	// subf r7,r4,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r4.s64;
	// addi r6,r6,64
	ctx.r6.s64 = ctx.r6.s64 + 64;
	// addi r7,r7,64
	ctx.r7.s64 = ctx.r7.s64 + 64;
	// srawi r6,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 7;
	// sth r31,-4(r10)
	PPC_STORE_U16(ctx.r10.u32 + -4, r31.u16);
	// srawi r7,r7,7
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 7;
	// srawi r5,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 7;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sth r6,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, ctx.r6.u16);
	// sth r7,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r7.u16);
	// sth r5,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r5.u16);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bne cr6,0x82631f08
	if (!cr6.eq) goto loc_82631F08;
	// add r11,r9,r30
	r11.u64 = ctx.r9.u64 + r30.u64;
	// li r10,4
	ctx.r10.s64 = 4;
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + r11.u64;
	// subf r27,r11,r30
	r27.s64 = r30.s64 - r11.s64;
	// add r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r26,r11,r8
	r26.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r9,r7
	ctx.r8.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r25,r11,r7
	r25.s64 = ctx.r7.s64 - r11.s64;
	// add r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r24,r11,r8
	r24.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r9,r7
	ctx.r8.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r20,r11,r7
	r20.s64 = ctx.r7.s64 - r11.s64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r19,r11,r8
	r19.s64 = ctx.r8.s64 - r11.s64;
	// subf r18,r11,r9
	r18.s64 = ctx.r9.s64 - r11.s64;
loc_82631FE0:
	// add r9,r18,r11
	ctx.r9.u64 = r18.u64 + r11.u64;
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r8,r20,r11
	ctx.r8.u64 = r20.u64 + r11.u64;
	// lhzx r3,r25,r11
	ctx.r3.u64 = PPC_LOAD_U16(r25.u32 + r11.u32);
	// add r6,r19,r11
	ctx.r6.u64 = r19.u64 + r11.u64;
	// lhzx r31,r27,r11
	r31.u64 = PPC_LOAD_U16(r27.u32 + r11.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhzx r28,r26,r11
	r28.u64 = PPC_LOAD_U16(r26.u32 + r11.u32);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lhzx r23,r24,r11
	r23.u64 = PPC_LOAD_U16(r24.u32 + r11.u32);
	// lhz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// mulli r21,r7,2276
	r21.s64 = ctx.r7.s64 * 2276;
	// lhz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// lhz r30,0(r6)
	r30.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// add r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mulli r17,r5,3406
	r17.s64 = ctx.r5.s64 * 3406;
	// extsh r29,r31
	r29.s64 = r31.s16;
	// mulli r7,r7,565
	ctx.r7.s64 = ctx.r7.s64 * 565;
	// add r5,r3,r4
	ctx.r5.u64 = ctx.r3.u64 + ctx.r4.u64;
	// extsh r31,r30
	r31.s64 = r30.s16;
	// extsh r30,r28
	r30.s64 = r28.s16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// extsh r28,r23
	r28.s64 = r23.s16;
	// mulli r5,r5,2408
	ctx.r5.s64 = ctx.r5.s64 * 2408;
	// add r23,r30,r31
	r23.u64 = r30.u64 + r31.u64;
	// mulli r16,r4,799
	r16.s64 = ctx.r4.s64 * 799;
	// mulli r15,r31,3784
	r15.s64 = r31.s64 * 3784;
	// add r4,r21,r7
	ctx.r4.u64 = r21.u64 + ctx.r7.u64;
	// subf r31,r17,r7
	r31.s64 = ctx.r7.s64 - r17.s64;
	// addi r7,r5,4
	ctx.r7.s64 = ctx.r5.s64 + 4;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// mulli r23,r23,1108
	r23.s64 = r23.s64 * 1108;
	// mulli r22,r30,1568
	r22.s64 = r30.s64 * 1568;
	// mulli r3,r3,4017
	ctx.r3.s64 = ctx.r3.s64 * 4017;
	// srawi r5,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r5.s64 = ctx.r4.s32 >> 3;
	// subf r30,r16,r7
	r30.s64 = ctx.r7.s64 - r16.s64;
	// rlwinm r28,r28,8,0,23
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r29,r29,8,0,23
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r4,r31,3
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7) != 0);
	ctx.r4.s64 = r31.s32 >> 3;
	// addi r31,r23,4
	r31.s64 = r23.s64 + 4;
	// subf r21,r3,r7
	r21.s64 = ctx.r7.s64 - ctx.r3.s64;
	// srawi r30,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	r30.s64 = r30.s32 >> 3;
	// add r7,r28,r29
	ctx.r7.u64 = r28.u64 + r29.u64;
	// subf r3,r28,r29
	ctx.r3.s64 = r29.s64 - r28.s64;
	// add r23,r22,r31
	r23.u64 = r22.u64 + r31.u64;
	// srawi r29,r21,3
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7) != 0);
	r29.s64 = r21.s32 >> 3;
	// subf r28,r15,r31
	r28.s64 = r31.s64 - r15.s64;
	// add r31,r30,r5
	r31.u64 = r30.u64 + ctx.r5.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// add r30,r29,r4
	r30.u64 = r29.u64 + ctx.r4.u64;
	// subf r4,r29,r4
	ctx.r4.s64 = ctx.r4.s64 - r29.s64;
	// srawi r29,r28,3
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7) != 0);
	r29.s64 = r28.s32 >> 3;
	// srawi r28,r23,3
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7) != 0);
	r28.s64 = r23.s32 >> 3;
	// add r23,r4,r5
	r23.u64 = ctx.r4.u64 + ctx.r5.u64;
	// subf r22,r4,r5
	r22.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mulli r4,r23,181
	ctx.r4.s64 = r23.s64 * 181;
	// add r5,r7,r28
	ctx.r5.u64 = ctx.r7.u64 + r28.u64;
	// subf r7,r28,r7
	ctx.r7.s64 = ctx.r7.s64 - r28.s64;
	// addi r28,r4,128
	r28.s64 = ctx.r4.s64 + 128;
	// add r4,r29,r3
	ctx.r4.u64 = r29.u64 + ctx.r3.u64;
	// subf r3,r29,r3
	ctx.r3.s64 = ctx.r3.s64 - r29.s64;
	// srawi r29,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	r29.s64 = r28.s32 >> 8;
	// mulli r28,r22,181
	r28.s64 = r22.s64 * 181;
	// addi r28,r28,128
	r28.s64 = r28.s64 + 128;
	// add r23,r31,r5
	r23.u64 = r31.u64 + ctx.r5.u64;
	// add r22,r4,r29
	r22.u64 = ctx.r4.u64 + r29.u64;
	// srawi r28,r28,8
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFF) != 0);
	r28.s64 = r28.s32 >> 8;
	// srawi r23,r23,14
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3FFF) != 0);
	r23.s64 = r23.s32 >> 14;
	// srawi r22,r22,14
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3FFF) != 0);
	r22.s64 = r22.s32 >> 14;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// subf r4,r29,r4
	ctx.r4.s64 = ctx.r4.s64 - r29.s64;
	// sthx r23,r27,r11
	PPC_STORE_U16(r27.u32 + r11.u32, r23.u16);
	// add r23,r3,r28
	r23.u64 = ctx.r3.u64 + r28.u64;
	// sth r22,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r22.u16);
	// add r22,r7,r30
	r22.u64 = ctx.r7.u64 + r30.u64;
	// srawi r23,r23,14
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3FFF) != 0);
	r23.s64 = r23.s32 >> 14;
	// subf r7,r30,r7
	ctx.r7.s64 = ctx.r7.s64 - r30.s64;
	// subf r3,r28,r3
	ctx.r3.s64 = ctx.r3.s64 - r28.s64;
	// srawi r30,r22,14
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x3FFF) != 0);
	r30.s64 = r22.s32 >> 14;
	// srawi r7,r7,14
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 14;
	// sthx r23,r26,r11
	PPC_STORE_U16(r26.u32 + r11.u32, r23.u16);
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - r31.s64;
	// sthx r30,r25,r11
	PPC_STORE_U16(r25.u32 + r11.u32, r30.u16);
	// srawi r3,r3,14
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3FFF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 14;
	// srawi r4,r4,14
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3FFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 14;
	// srawi r5,r5,14
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3FFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 14;
	// sthx r7,r24,r11
	PPC_STORE_U16(r24.u32 + r11.u32, ctx.r7.u16);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r3,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r3.u16);
	// sth r4,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r4.u16);
	// sth r5,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r5.u16);
	// bne cr6,0x82631fe0
	if (!cr6.eq) goto loc_82631FE0;
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_82632160"))) PPC_WEAK_FUNC(sub_82632160);
PPC_FUNC_IMPL(__imp__sub_82632160) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf0
	// rlwinm r11,r6,2,28,28
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x8;
	// clrlwi r10,r6,31
	ctx.r10.u64 = ctx.r6.u32 & 0x1;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// li r9,4
	ctx.r9.s64 = 4;
	// add r30,r10,r3
	r30.u64 = ctx.r10.u64 + ctx.r3.u64;
	// addi r10,r30,8
	ctx.r10.s64 = r30.s64 + 8;
loc_8263218C:
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mulli r3,r7,1892
	ctx.r3.s64 = ctx.r7.s64 * 1892;
	// add r29,r6,r8
	r29.u64 = ctx.r6.u64 + ctx.r8.u64;
	// mulli r31,r5,784
	r31.s64 = ctx.r5.s64 * 784;
	// subf r6,r6,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r6.s64;
	// mulli r28,r7,784
	r28.s64 = ctx.r7.s64 * 784;
	// add r7,r3,r31
	ctx.r7.u64 = ctx.r3.u64 + r31.u64;
	// mulli r8,r29,1448
	ctx.r8.s64 = r29.s64 * 1448;
	// add r3,r7,r8
	ctx.r3.u64 = ctx.r7.u64 + ctx.r8.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// addi r7,r3,64
	ctx.r7.s64 = ctx.r3.s64 + 64;
	// addi r3,r8,64
	ctx.r3.s64 = ctx.r8.s64 + 64;
	// srawi r8,r7,7
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 7;
	// mulli r5,r5,1892
	ctx.r5.s64 = ctx.r5.s64 * 1892;
	// stw r8,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, ctx.r8.u32);
	// mulli r6,r6,1448
	ctx.r6.s64 = ctx.r6.s64 * 1448;
	// subf r5,r5,r28
	ctx.r5.s64 = r28.s64 - ctx.r5.s64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// add r7,r5,r6
	ctx.r7.u64 = ctx.r5.u64 + ctx.r6.u64;
	// subf r8,r5,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r5.s64;
	// addi r7,r7,64
	ctx.r7.s64 = ctx.r7.s64 + 64;
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// srawi r7,r7,7
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 7;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// srawi r6,r3,7
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7F) != 0);
	ctx.r6.s64 = ctx.r3.s32 >> 7;
	// stw r7,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r7.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// bne cr6,0x8263218c
	if (!cr6.eq) goto loc_8263218C;
	// add r11,r4,r30
	r11.u64 = ctx.r4.u64 + r30.u64;
	// li r9,4
	ctx.r9.s64 = 4;
	// add r10,r4,r11
	ctx.r10.u64 = ctx.r4.u64 + r11.u64;
	// subf r31,r11,r30
	r31.s64 = r30.s64 - r11.s64;
	// add r8,r4,r10
	ctx.r8.u64 = ctx.r4.u64 + ctx.r10.u64;
	// subf r28,r11,r10
	r28.s64 = ctx.r10.s64 - r11.s64;
	// lis r10,0
	ctx.r10.s64 = 0;
	// subf r27,r11,r8
	r27.s64 = ctx.r8.s64 - r11.s64;
	// ori r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 32768;
loc_8263223C:
	// add r7,r28,r11
	ctx.r7.u64 = r28.u64 + r11.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r6,r27,r11
	ctx.r6.u64 = r27.u64 + r11.u64;
	// lwzx r5,r31,r11
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// mulli r30,r8,1892
	r30.s64 = ctx.r8.s64 * 1892;
	// lwz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mulli r26,r8,784
	r26.s64 = ctx.r8.s64 * 784;
	// add r8,r4,r5
	ctx.r8.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mulli r29,r3,784
	r29.s64 = ctx.r3.s64 * 784;
	// subf r4,r4,r5
	ctx.r4.s64 = ctx.r5.s64 - ctx.r4.s64;
	// mulli r8,r8,1448
	ctx.r8.s64 = ctx.r8.s64 * 1448;
	// add r5,r30,r29
	ctx.r5.u64 = r30.u64 + r29.u64;
	// mulli r3,r3,1892
	ctx.r3.s64 = ctx.r3.s64 * 1892;
	// add r30,r5,r8
	r30.u64 = ctx.r5.u64 + ctx.r8.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// add r5,r30,r10
	ctx.r5.u64 = r30.u64 + ctx.r10.u64;
	// mulli r4,r4,1448
	ctx.r4.s64 = ctx.r4.s64 * 1448;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// subf r3,r3,r26
	ctx.r3.s64 = r26.s64 - ctx.r3.s64;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r30,r3,r4
	r30.u64 = ctx.r3.u64 + ctx.r4.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stwx r5,r31,r11
	PPC_STORE_U32(r31.u32 + r11.u32, ctx.r5.u32);
	// subf r5,r3,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r3.s64;
	// add r4,r30,r10
	ctx.r4.u64 = r30.u64 + ctx.r10.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// srawi r4,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 16;
	// srawi r5,r5,16
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 16;
	// srawi r8,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// stw r8,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r8.u32);
	// bne cr6,0x8263223c
	if (!cr6.eq) goto loc_8263223C;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_826322D0"))) PPC_WEAK_FUNC(sub_826322D0);
PPC_FUNC_IMPL(__imp__sub_826322D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r21,0
	r21.s64 = 0;
	// li r30,5
	r30.s64 = 5;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,144(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// lwz r11,19984(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 19984);
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// lwz r10,268(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 268);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x82632378
	if (!cr6.lt) goto loc_82632378;
loc_82632320:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632378
	if (cr6.eq) goto loc_82632378;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632368
	if (!cr0.lt) goto loc_82632368;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632368:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632320
	if (cr6.gt) goto loc_82632320;
loc_82632378:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826323b4
	if (!cr0.lt) goto loc_826323B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826323B4:
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// stw r30,3952(r26)
	PPC_STORE_U32(r26.u32 + 3952, r30.u32);
	// li r22,1
	r22.s64 = 1;
	// bgt cr6,0x8263247c
	if (cr6.gt) goto loc_8263247C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632438
	if (!cr6.lt) goto loc_82632438;
loc_826323E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632438
	if (cr6.eq) goto loc_82632438;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632428
	if (!cr0.lt) goto loc_82632428;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632428:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826323e0
	if (cr6.gt) goto loc_826323E0;
loc_82632438:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632474
	if (!cr0.lt) goto loc_82632474;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632474:
	// stw r30,252(r26)
	PPC_STORE_U32(r26.u32 + 252, r30.u32);
	// b 0x82632480
	goto loc_82632480;
loc_8263247C:
	// stw r21,252(r26)
	PPC_STORE_U32(r26.u32 + 252, r21.u32);
loc_82632480:
	// lwz r11,3440(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632540
	if (cr6.eq) goto loc_82632540;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632500
	if (!cr6.lt) goto loc_82632500;
loc_826324A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632500
	if (cr6.eq) goto loc_82632500;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826324f0
	if (!cr0.lt) goto loc_826324F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826324F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826324a8
	if (cr6.gt) goto loc_826324A8;
loc_82632500:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263253c
	if (!cr0.lt) goto loc_8263253C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263253C:
	// stw r30,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r30.u32);
loc_82632540:
	// lwz r11,3432(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3432);
	// lwz r8,3952(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + 3952);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263255c
	if (!cr6.eq) goto loc_8263255C;
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// bgt cr6,0x82632590
	if (cr6.gt) goto loc_82632590;
	// stw r22,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r22.u32);
loc_8263255C:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_82632560:
	// lwz r11,2972(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2972);
	// stw r10,248(r26)
	PPC_STORE_U32(r26.u32 + 248, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r21,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r21.u32);
	// beq cr6,0x826325c4
	if (cr6.eq) goto loc_826325C4;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826325c4
	if (cr6.eq) goto loc_826325C4;
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// blt cr6,0x826325ac
	if (cr6.lt) goto loc_826325AC;
	// stw r22,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r22.u32);
	// b 0x826325c4
	goto loc_826325C4;
loc_82632590:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// stw r21,3428(r26)
	PPC_STORE_U32(r26.u32 + 3428, r21.u32);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,4872
	r11.s64 = r11.s64 + 4872;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// b 0x82632560
	goto loc_82632560;
loc_826325AC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826325bc
	if (cr6.eq) goto loc_826325BC;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826325c4
	if (!cr6.eq) goto loc_826325C4;
loc_826325BC:
	// li r11,7
	r11.s64 = 7;
	// stw r11,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r11.u32);
loc_826325C4:
	// lwz r9,3428(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 3428);
	// addi r11,r26,3988
	r11.s64 = r26.s64 + 3988;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x826325d8
	if (!cr6.eq) goto loc_826325D8;
	// addi r11,r26,5268
	r11.s64 = r26.s64 + 5268;
loc_826325D8:
	// stw r11,6548(r26)
	PPC_STORE_U32(r26.u32 + 6548, r11.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r11,r26,6560
	r11.s64 = r26.s64 + 6560;
	// bne cr6,0x826325ec
	if (!cr6.eq) goto loc_826325EC;
	// addi r11,r26,10656
	r11.s64 = r26.s64 + 10656;
loc_826325EC:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// stw r11,14752(r26)
	PPC_STORE_U32(r26.u32 + 14752, r11.u32);
	// stw r10,248(r26)
	PPC_STORE_U32(r26.u32 + 248, ctx.r10.u32);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263263c
	if (!cr6.eq) goto loc_8263263C;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82632648
	if (cr6.eq) goto loc_82632648;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82632648
	if (cr6.eq) goto loc_82632648;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632628
	if (cr6.eq) goto loc_82632628;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x8263263c
	if (!cr6.eq) goto loc_8263263C;
loc_82632628:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,248(r26)
	PPC_STORE_U32(r26.u32 + 248, ctx.r10.u32);
	// ble cr6,0x8263263c
	if (!cr6.gt) goto loc_8263263C;
	// cmpwi cr6,r10,31
	cr6.compare<int32_t>(ctx.r10.s32, 31, xer);
	// ble cr6,0x8263265c
	if (!cr6.gt) goto loc_8263265C;
loc_8263263C:
	// li r3,4
	ctx.r3.s64 = 4;
loc_82632640:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_82632648:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,248(r26)
	PPC_STORE_U32(r26.u32 + 248, ctx.r10.u32);
	// ble cr6,0x8263263c
	if (!cr6.gt) goto loc_8263263C;
	// cmpwi cr6,r10,31
	cr6.compare<int32_t>(ctx.r10.s32, 31, xer);
	// bgt cr6,0x8263263c
	if (cr6.gt) goto loc_8263263C;
loc_8263265C:
	// cmpwi cr6,r8,8
	cr6.compare<int32_t>(ctx.r8.s32, 8, xer);
	// bgt cr6,0x82632670
	if (cr6.gt) goto loc_82632670;
	// addi r11,r26,2840
	r11.s64 = r26.s64 + 2840;
	// addi r10,r26,2800
	ctx.r10.s64 = r26.s64 + 2800;
	// b 0x82632678
	goto loc_82632678;
loc_82632670:
	// addi r11,r26,2640
	r11.s64 = r26.s64 + 2640;
	// addi r10,r26,2680
	ctx.r10.s64 = r26.s64 + 2680;
loc_82632678:
	// stw r11,2904(r26)
	PPC_STORE_U32(r26.u32 + 2904, r11.u32);
	// li r23,2
	r23.s64 = 2;
	// lwz r11,20868(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20868);
	// stw r10,2916(r26)
	PPC_STORE_U32(r26.u32 + 2916, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632740
	if (cr6.eq) goto loc_82632740;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r23
	r30.u64 = r23.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82632700
	if (!cr6.lt) goto loc_82632700;
loc_826326A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632700
	if (cr6.eq) goto loc_82632700;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826326f0
	if (!cr0.lt) goto loc_826326F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826326F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826326a8
	if (cr6.gt) goto loc_826326A8;
loc_82632700:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263273c
	if (!cr0.lt) goto loc_8263273C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263273C:
	// stw r30,20872(r26)
	PPC_STORE_U32(r26.u32 + 20872, r30.u32);
loc_82632740:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634494
	if (cr6.eq) goto loc_82634494;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82634494
	if (cr6.eq) goto loc_82634494;
	// mr r27,r21
	r27.u64 = r21.u64;
	// mr r28,r21
	r28.u64 = r21.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x8263281c
	if (cr6.eq) goto loc_8263281C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826327d8
	if (!cr6.lt) goto loc_826327D8;
loc_82632780:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826327d8
	if (cr6.eq) goto loc_826327D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826327c8
	if (!cr0.lt) goto loc_826327C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826327C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632780
	if (cr6.gt) goto loc_82632780;
loc_826327D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632814
	if (!cr0.lt) goto loc_82632814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632814:
	// stw r30,20996(r26)
	PPC_STORE_U32(r26.u32 + 20996, r30.u32);
	// b 0x82632820
	goto loc_82632820;
loc_8263281C:
	// stw r22,20996(r26)
	PPC_STORE_U32(r26.u32 + 20996, r22.u32);
loc_82632820:
	// lwz r11,20996(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20996);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632834
	if (cr6.eq) goto loc_82632834;
	// stw r22,20992(r26)
	PPC_STORE_U32(r26.u32 + 20992, r22.u32);
	// b 0x82632908
	goto loc_82632908;
loc_82632834:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826328a8
	if (!cr6.lt) goto loc_826328A8;
loc_82632850:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826328a8
	if (cr6.eq) goto loc_826328A8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632898
	if (!cr0.lt) goto loc_82632898;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632898:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632850
	if (cr6.gt) goto loc_82632850;
loc_826328A8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826328e4
	if (!cr0.lt) goto loc_826328E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826328E4:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,20984(r26)
	PPC_STORE_U32(r26.u32 + 20984, r11.u32);
	// beq cr6,0x82632904
	if (cr6.eq) goto loc_82632904;
	// stw r21,20988(r26)
	PPC_STORE_U32(r26.u32 + 20988, r21.u32);
	// stw r22,20992(r26)
	PPC_STORE_U32(r26.u32 + 20992, r22.u32);
	// b 0x8263290c
	goto loc_8263290C;
loc_82632904:
	// stw r21,20992(r26)
	PPC_STORE_U32(r26.u32 + 20992, r21.u32);
loc_82632908:
	// stw r22,20988(r26)
	PPC_STORE_U32(r26.u32 + 20988, r22.u32);
loc_8263290C:
	// addi r7,r26,20448
	ctx.r7.s64 = r26.s64 + 20448;
	// lwz r11,20996(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20996);
	// addi r6,r26,20460
	ctx.r6.s64 = r26.s64 + 20460;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r7,2412(r26)
	PPC_STORE_U32(r26.u32 + 2412, ctx.r7.u32);
	// stw r6,2416(r26)
	PPC_STORE_U32(r26.u32 + 2416, ctx.r6.u32);
	// beq cr6,0x8263293c
	if (cr6.eq) goto loc_8263293C;
	// addi r11,r26,20400
	r11.s64 = r26.s64 + 20400;
	// addi r10,r26,20412
	ctx.r10.s64 = r26.s64 + 20412;
	// addi r9,r26,20424
	ctx.r9.s64 = r26.s64 + 20424;
	// addi r8,r26,20436
	ctx.r8.s64 = r26.s64 + 20436;
	// b 0x8263294c
	goto loc_8263294C;
loc_8263293C:
	// addi r11,r26,20496
	r11.s64 = r26.s64 + 20496;
	// addi r10,r26,20508
	ctx.r10.s64 = r26.s64 + 20508;
	// addi r9,r26,20520
	ctx.r9.s64 = r26.s64 + 20520;
	// addi r8,r26,20532
	ctx.r8.s64 = r26.s64 + 20532;
loc_8263294C:
	// stw r11,2396(r26)
	PPC_STORE_U32(r26.u32 + 2396, r11.u32);
	// addi r11,r26,20472
	r11.s64 = r26.s64 + 20472;
	// stw r10,2400(r26)
	PPC_STORE_U32(r26.u32 + 2400, ctx.r10.u32);
	// addi r10,r26,20484
	ctx.r10.s64 = r26.s64 + 20484;
	// stw r8,2408(r26)
	PPC_STORE_U32(r26.u32 + 2408, ctx.r8.u32);
	// stw r9,2404(r26)
	PPC_STORE_U32(r26.u32 + 2404, ctx.r9.u32);
	// stw r11,2420(r26)
	PPC_STORE_U32(r26.u32 + 2420, r11.u32);
	// lwz r11,20864(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20864);
	// stw r10,2424(r26)
	PPC_STORE_U32(r26.u32 + 2424, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632ba0
	if (cr6.eq) goto loc_82632BA0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826329ec
	if (!cr6.lt) goto loc_826329EC;
loc_82632994:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826329ec
	if (cr6.eq) goto loc_826329EC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826329dc
	if (!cr0.lt) goto loc_826329DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826329DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632994
	if (cr6.gt) goto loc_82632994;
loc_826329EC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632a28
	if (!cr0.lt) goto loc_82632A28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632A28:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x82632ae4
	if (cr6.eq) goto loc_82632AE4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632aa4
	if (!cr6.lt) goto loc_82632AA4;
loc_82632A4C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632aa4
	if (cr6.eq) goto loc_82632AA4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632a94
	if (!cr0.lt) goto loc_82632A94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632A94:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632a4c
	if (cr6.gt) goto loc_82632A4C;
loc_82632AA4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632ae0
	if (!cr0.lt) goto loc_82632AE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632AE0:
	// add r28,r30,r28
	r28.u64 = r30.u64 + r28.u64;
loc_82632AE4:
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// bne cr6,0x82632ba0
	if (!cr6.eq) goto loc_82632BA0;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632b60
	if (!cr6.lt) goto loc_82632B60;
loc_82632B08:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632b60
	if (cr6.eq) goto loc_82632B60;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632b50
	if (!cr0.lt) goto loc_82632B50;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632B50:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632b08
	if (cr6.gt) goto loc_82632B08;
loc_82632B60:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632b9c
	if (!cr0.lt) goto loc_82632B9C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632B9C:
	// addi r28,r30,2
	r28.s64 = r30.s64 + 2;
loc_82632BA0:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82603a38
	sub_82603A38(ctx, base);
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lwz r11,19984(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 19984);
	// bne cr6,0x82632bd8
	if (!cr6.eq) goto loc_82632BD8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,404(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 404);
	// bne cr6,0x82632bd0
	if (!cr6.eq) goto loc_82632BD0;
	// stw r11,21532(r26)
	PPC_STORE_U32(r26.u32 + 21532, r11.u32);
	// b 0x82632c08
	goto loc_82632C08;
loc_82632BD0:
	// stw r11,21536(r26)
	PPC_STORE_U32(r26.u32 + 21536, r11.u32);
	// b 0x82632c08
	goto loc_82632C08;
loc_82632BD8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,404(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 404);
	// bne cr6,0x82632bfc
	if (!cr6.eq) goto loc_82632BFC;
	// lwz r10,21532(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 21532);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82632c08
	if (!cr6.lt) goto loc_82632C08;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
loc_82632BFC:
	// lwz r10,21536(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 21536);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263263c
	if (cr6.lt) goto loc_8263263C;
loc_82632C08:
	// lwz r11,20996(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20996);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82632c30
	if (!cr6.eq) goto loc_82632C30;
	// lwz r11,428(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 428);
	// lwz r10,420(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 420);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r10,420(r26)
	PPC_STORE_U32(r26.u32 + 420, ctx.r10.u32);
	// stw r11,428(r26)
	PPC_STORE_U32(r26.u32 + 428, r11.u32);
loc_82632C30:
	// lwz r11,20956(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82632e94
	if (cr6.eq) goto loc_82632E94;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632cb0
	if (!cr6.lt) goto loc_82632CB0;
loc_82632C58:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632cb0
	if (cr6.eq) goto loc_82632CB0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632ca0
	if (!cr0.lt) goto loc_82632CA0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632CA0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632c58
	if (cr6.gt) goto loc_82632C58;
loc_82632CB0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632cec
	if (!cr0.lt) goto loc_82632CEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632CEC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,20960(r26)
	PPC_STORE_U32(r26.u32 + 20960, r30.u32);
	// beq cr6,0x82632db4
	if (cr6.eq) goto loc_82632DB4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632d6c
	if (!cr6.lt) goto loc_82632D6C;
loc_82632D14:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632d6c
	if (cr6.eq) goto loc_82632D6C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632d5c
	if (!cr0.lt) goto loc_82632D5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632D5C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632d14
	if (cr6.gt) goto loc_82632D14;
loc_82632D6C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632da8
	if (!cr0.lt) goto loc_82632DA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632DA8:
	// lwz r11,20960(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20960);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,20960(r26)
	PPC_STORE_U32(r26.u32 + 20960, r11.u32);
loc_82632DB4:
	// lwz r11,20960(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20960);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82632e7c
	if (!cr6.eq) goto loc_82632E7C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632e34
	if (!cr6.lt) goto loc_82632E34;
loc_82632DDC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632e34
	if (cr6.eq) goto loc_82632E34;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632e24
	if (!cr0.lt) goto loc_82632E24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632E24:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632ddc
	if (cr6.gt) goto loc_82632DDC;
loc_82632E34:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632e70
	if (!cr0.lt) goto loc_82632E70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632E70:
	// lwz r11,20960(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20960);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,20960(r26)
	PPC_STORE_U32(r26.u32 + 20960, r11.u32);
loc_82632E7C:
	// lwz r11,20960(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20960);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// stw r10,20964(r26)
	PPC_STORE_U32(r26.u32 + 20964, ctx.r10.u32);
	// stw r11,20968(r26)
	PPC_STORE_U32(r26.u32 + 20968, r11.u32);
loc_82632E94:
	// stw r21,3964(r26)
	PPC_STORE_U32(r26.u32 + 3964, r21.u32);
	// stw r21,20028(r26)
	PPC_STORE_U32(r26.u32 + 20028, r21.u32);
	// stw r21,20024(r26)
	PPC_STORE_U32(r26.u32 + 20024, r21.u32);
loc_82632EA0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r25,3
	r25.s64 = 3;
	// lwz r11,248(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 248);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// ble cr6,0x826331b0
	if (!cr6.gt) goto loc_826331B0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632f24
	if (!cr6.lt) goto loc_82632F24;
loc_82632ECC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632f24
	if (cr6.eq) goto loc_82632F24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632f14
	if (!cr0.lt) goto loc_82632F14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632F14:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632ecc
	if (cr6.gt) goto loc_82632ECC;
loc_82632F24:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82632f60
	if (!cr0.lt) goto loc_82632F60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632F60:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826334b4
	if (!cr6.eq) goto loc_826334B4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82632fdc
	if (!cr6.lt) goto loc_82632FDC;
loc_82632F84:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82632fdc
	if (cr6.eq) goto loc_82632FDC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82632fcc
	if (!cr0.lt) goto loc_82632FCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82632FCC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82632f84
	if (cr6.gt) goto loc_82632F84;
loc_82632FDC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633018
	if (!cr0.lt) goto loc_82633018;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633018:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8263349c
	if (!cr6.eq) goto loc_8263349C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82633094
	if (!cr6.lt) goto loc_82633094;
loc_8263303C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633094
	if (cr6.eq) goto loc_82633094;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633084
	if (!cr0.lt) goto loc_82633084;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633084:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263303c
	if (cr6.gt) goto loc_8263303C;
loc_82633094:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826330d0
	if (!cr0.lt) goto loc_826330D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826330D0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826334ac
	if (!cr6.eq) goto loc_826334AC;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826334a4
	if (cr6.eq) goto loc_826334A4;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x826334a4
	if (!cr6.eq) goto loc_826334A4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82633160
	if (!cr6.lt) goto loc_82633160;
loc_82633108:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633160
	if (cr6.eq) goto loc_82633160;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633150
	if (!cr0.lt) goto loc_82633150;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633150:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633108
	if (cr6.gt) goto loc_82633108;
loc_82633160:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263319c
	if (!cr0.lt) goto loc_8263319C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263319C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826334a4
	if (cr6.eq) goto loc_826334A4;
	// mr r27,r22
	r27.u64 = r22.u64;
	// stw r22,3964(r26)
	PPC_STORE_U32(r26.u32 + 3964, r22.u32);
	// b 0x82632ea0
	goto loc_82632EA0;
loc_826331B0:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82633210
	if (!cr6.lt) goto loc_82633210;
loc_826331B8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633210
	if (cr6.eq) goto loc_82633210;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633200
	if (!cr0.lt) goto loc_82633200;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633200:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826331b8
	if (cr6.gt) goto loc_826331B8;
loc_82633210:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263324c
	if (!cr0.lt) goto loc_8263324C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263324C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8263349c
	if (!cr6.eq) goto loc_8263349C;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826332c8
	if (!cr6.lt) goto loc_826332C8;
loc_82633270:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826332c8
	if (cr6.eq) goto loc_826332C8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826332b8
	if (!cr0.lt) goto loc_826332B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826332B8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633270
	if (cr6.gt) goto loc_82633270;
loc_826332C8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633304
	if (!cr0.lt) goto loc_82633304;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633304:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826334a4
	if (!cr6.eq) goto loc_826334A4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82633380
	if (!cr6.lt) goto loc_82633380;
loc_82633328:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633380
	if (cr6.eq) goto loc_82633380;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633370
	if (!cr0.lt) goto loc_82633370;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633370:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633328
	if (cr6.gt) goto loc_82633328;
loc_82633380:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826333bc
	if (!cr0.lt) goto loc_826333BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826333BC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826334ac
	if (!cr6.eq) goto loc_826334AC;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826334b4
	if (cr6.eq) goto loc_826334B4;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x826334b4
	if (!cr6.eq) goto loc_826334B4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263344c
	if (!cr6.lt) goto loc_8263344C;
loc_826333F4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263344c
	if (cr6.eq) goto loc_8263344C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263343c
	if (!cr0.lt) goto loc_8263343C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263343C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826333f4
	if (cr6.gt) goto loc_826333F4;
loc_8263344C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633488
	if (!cr0.lt) goto loc_82633488;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633488:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826334b4
	if (cr6.eq) goto loc_826334B4;
	// mr r27,r22
	r27.u64 = r22.u64;
	// stw r22,3964(r26)
	PPC_STORE_U32(r26.u32 + 3964, r22.u32);
	// b 0x82632ea0
	goto loc_82632EA0;
loc_8263349C:
	// stw r22,3960(r26)
	PPC_STORE_U32(r26.u32 + 3960, r22.u32);
	// b 0x826334b8
	goto loc_826334B8;
loc_826334A4:
	// stw r21,3960(r26)
	PPC_STORE_U32(r26.u32 + 3960, r21.u32);
	// b 0x826334b8
	goto loc_826334B8;
loc_826334AC:
	// stw r23,3960(r26)
	PPC_STORE_U32(r26.u32 + 3960, r23.u32);
	// b 0x826334b8
	goto loc_826334B8;
loc_826334B4:
	// stw r25,3960(r26)
	PPC_STORE_U32(r26.u32 + 3960, r25.u32);
loc_826334B8:
	// lwz r11,3964(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3964);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82633bd8
	if (cr6.eq) goto loc_82633BD8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82633538
	if (!cr6.lt) goto loc_82633538;
loc_826334E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633538
	if (cr6.eq) goto loc_82633538;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633528
	if (!cr0.lt) goto loc_82633528;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633528:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826334e0
	if (cr6.gt) goto loc_826334E0;
loc_82633538:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633574
	if (!cr0.lt) goto loc_82633574;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633574:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r29,r21
	r29.u64 = r21.u64;
	// beq cr6,0x82633858
	if (cr6.eq) goto loc_82633858;
	// stw r22,20028(r26)
	PPC_STORE_U32(r26.u32 + 20028, r22.u32);
	// li r30,6
	r30.s64 = 6;
	// stw r22,20024(r26)
	PPC_STORE_U32(r26.u32 + 20024, r22.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x826335f8
	if (!cr6.lt) goto loc_826335F8;
loc_826335A0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826335f8
	if (cr6.eq) goto loc_826335F8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826335e8
	if (!cr0.lt) goto loc_826335E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826335E8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826335a0
	if (cr6.gt) goto loc_826335A0;
loc_826335F8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633634
	if (!cr0.lt) goto loc_82633634;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633634:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20032(r26)
	PPC_STORE_U32(r26.u32 + 20032, r28.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x826336ac
	if (!cr6.lt) goto loc_826336AC;
loc_82633654:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826336ac
	if (cr6.eq) goto loc_826336AC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263369c
	if (!cr0.lt) goto loc_8263369C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263369C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633654
	if (cr6.gt) goto loc_82633654;
loc_826336AC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826336e8
	if (!cr0.lt) goto loc_826336E8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826336E8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20036(r26)
	PPC_STORE_U32(r26.u32 + 20036, r28.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633760
	if (!cr6.lt) goto loc_82633760;
loc_82633708:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633760
	if (cr6.eq) goto loc_82633760;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633750
	if (!cr0.lt) goto loc_82633750;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633750:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633708
	if (cr6.gt) goto loc_82633708;
loc_82633760:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263379c
	if (!cr0.lt) goto loc_8263379C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263379C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20040(r26)
	PPC_STORE_U32(r26.u32 + 20040, r28.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633814
	if (!cr6.lt) goto loc_82633814;
loc_826337BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633814
	if (cr6.eq) goto loc_82633814;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633804
	if (!cr0.lt) goto loc_82633804;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633804:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826337bc
	if (cr6.gt) goto loc_826337BC;
loc_82633814:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633a70
	if (!cr0.lt) goto loc_82633A70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// stw r30,20044(r26)
	PPC_STORE_U32(r26.u32 + 20044, r30.u32);
	// b 0x82633bd8
	goto loc_82633BD8;
loc_82633858:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r30,r22
	r30.u64 = r22.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826338c4
	if (!cr6.lt) goto loc_826338C4;
loc_8263386C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826338c4
	if (cr6.eq) goto loc_826338C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826338b4
	if (!cr0.lt) goto loc_826338B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826338B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263386c
	if (cr6.gt) goto loc_8263386C;
loc_826338C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633900
	if (!cr0.lt) goto loc_82633900;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633900:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r21
	r29.u64 = r21.u64;
	// beq cr6,0x82633a78
	if (cr6.eq) goto loc_82633A78;
	// stw r22,20028(r26)
	PPC_STORE_U32(r26.u32 + 20028, r22.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633980
	if (!cr6.lt) goto loc_82633980;
loc_82633928:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633980
	if (cr6.eq) goto loc_82633980;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633970
	if (!cr0.lt) goto loc_82633970;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633970:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633928
	if (cr6.gt) goto loc_82633928;
loc_82633980:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826339bc
	if (!cr0.lt) goto loc_826339BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826339BC:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20040(r26)
	PPC_STORE_U32(r26.u32 + 20040, r28.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633a34
	if (!cr6.lt) goto loc_82633A34;
loc_826339DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633a34
	if (cr6.eq) goto loc_82633A34;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633a24
	if (!cr0.lt) goto loc_82633A24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633A24:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826339dc
	if (cr6.gt) goto loc_826339DC;
loc_82633A34:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633a70
	if (!cr0.lt) goto loc_82633A70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633A70:
	// stw r30,20044(r26)
	PPC_STORE_U32(r26.u32 + 20044, r30.u32);
	// b 0x82633bd8
	goto loc_82633BD8;
loc_82633A78:
	// stw r22,20024(r26)
	PPC_STORE_U32(r26.u32 + 20024, r22.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633ae4
	if (!cr6.lt) goto loc_82633AE4;
loc_82633A8C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633ae4
	if (cr6.eq) goto loc_82633AE4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633ad4
	if (!cr0.lt) goto loc_82633AD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633AD4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633a8c
	if (cr6.gt) goto loc_82633A8C;
loc_82633AE4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633b20
	if (!cr0.lt) goto loc_82633B20;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633B20:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// stw r28,20032(r26)
	PPC_STORE_U32(r26.u32 + 20032, r28.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82633b98
	if (!cr6.lt) goto loc_82633B98;
loc_82633B40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633b98
	if (cr6.eq) goto loc_82633B98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633b88
	if (!cr0.lt) goto loc_82633B88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633B88:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633b40
	if (cr6.gt) goto loc_82633B40;
loc_82633B98:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633bd4
	if (!cr0.lt) goto loc_82633BD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633BD4:
	// stw r30,20036(r26)
	PPC_STORE_U32(r26.u32 + 20036, r30.u32);
loc_82633BD8:
	// lwz r11,14788(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 14788);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82633c58
	if (cr6.eq) goto loc_82633C58;
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82633c58
	if (!cr6.eq) goto loc_82633C58;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82632640
	if (!cr6.eq) goto loc_82632640;
	// lwz r11,14804(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 14804);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82633c58
	if (cr6.eq) goto loc_82633C58;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82633c58
	if (!cr6.gt) goto loc_82633C58;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_82633C24:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82633c3c
	if (cr6.eq) goto loc_82633C3C;
	// rlwimi r11,r22,7,24,26
	r11.u64 = (__builtin_rotateleft32(r22.u32, 7) & 0xE0) | (r11.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x82633c40
	goto loc_82633C40;
loc_82633C3C:
	// rlwinm r11,r11,0,27,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFF1F;
loc_82633C40:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x82633c24
	if (cr6.lt) goto loc_82633C24;
loc_82633C58:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r25
	r30.u64 = r25.u64;
	// lwz r11,3960(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3960);
	// mr r29,r21
	r29.u64 = r21.u64;
	// stw r22,448(r26)
	PPC_STORE_U32(r26.u32 + 448, r22.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82633d20
	if (!cr6.eq) goto loc_82633D20;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82633cdc
	if (!cr6.lt) goto loc_82633CDC;
loc_82633C84:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633cdc
	if (cr6.eq) goto loc_82633CDC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633ccc
	if (!cr0.lt) goto loc_82633CCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633CCC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633c84
	if (cr6.gt) goto loc_82633C84;
loc_82633CDC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633d18
	if (!cr0.lt) goto loc_82633D18;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633D18:
	// addi r11,r30,5050
	r11.s64 = r30.s64 + 5050;
	// b 0x82633dc0
	goto loc_82633DC0;
loc_82633D20:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82633d80
	if (!cr6.lt) goto loc_82633D80;
loc_82633D28:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633d80
	if (cr6.eq) goto loc_82633D80;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633d70
	if (!cr0.lt) goto loc_82633D70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633D70:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633d28
	if (cr6.gt) goto loc_82633D28;
loc_82633D80:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633dbc
	if (!cr0.lt) goto loc_82633DBC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633DBC:
	// addi r11,r30,5058
	r11.s64 = r30.s64 + 5058;
loc_82633DC0:
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,20196(r26)
	PPC_STORE_U32(r26.u32 + 20196, r11.u32);
	// lwz r11,20996(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20996);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x82633e50
	if (cr6.eq) goto loc_82633E50;
	// mr r30,r25
	r30.u64 = r25.u64;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82633eb4
	if (!cr6.lt) goto loc_82633EB4;
loc_82633DF4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633eb4
	if (cr6.eq) goto loc_82633EB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633e3c
	if (!cr0.lt) goto loc_82633E3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633E3C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633df4
	if (cr6.gt) goto loc_82633DF4;
	// b 0x82633eb4
	goto loc_82633EB4;
loc_82633E50:
	// mr r30,r23
	r30.u64 = r23.u64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82633eb4
	if (!cr6.lt) goto loc_82633EB4;
loc_82633E5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633eb4
	if (cr6.eq) goto loc_82633EB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633ea4
	if (!cr0.lt) goto loc_82633EA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633EA4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633e5c
	if (cr6.gt) goto loc_82633E5C;
loc_82633EB4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// bge 0x82633ef0
	if (!cr0.lt) goto loc_82633EF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633EF0:
	// addi r11,r30,599
	r11.s64 = r30.s64 + 599;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r25
	r30.u64 = r25.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,2376(r26)
	PPC_STORE_U32(r26.u32 + 2376, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82633f74
	if (!cr6.lt) goto loc_82633F74;
loc_82633F1C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82633f74
	if (cr6.eq) goto loc_82633F74;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82633f64
	if (!cr0.lt) goto loc_82633F64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633F64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633f1c
	if (cr6.gt) goto loc_82633F1C;
loc_82633F74:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82633fb0
	if (!cr0.lt) goto loc_82633FB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82633FB0:
	// addi r11,r30,5254
	r11.s64 = r30.s64 + 5254;
	// lwz r10,3960(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 3960);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,21012(r26)
	PPC_STORE_U32(r26.u32 + 21012, r11.u32);
	// stw r11,21008(r26)
	PPC_STORE_U32(r26.u32 + 21008, r11.u32);
	// bne cr6,0x82634090
	if (!cr6.eq) goto loc_82634090;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r23
	r30.u64 = r23.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82634044
	if (!cr6.lt) goto loc_82634044;
loc_82633FEC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634044
	if (cr6.eq) goto loc_82634044;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634034
	if (!cr0.lt) goto loc_82634034;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634034:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82633fec
	if (cr6.gt) goto loc_82633FEC;
loc_82634044:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634080
	if (!cr0.lt) goto loc_82634080;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634080:
	// addi r11,r30,5067
	r11.s64 = r30.s64 + 5067;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	r11.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// stw r11,20264(r26)
	PPC_STORE_U32(r26.u32 + 20264, r11.u32);
loc_82634090:
	// lwz r11,3980(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3980);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826340ac
	if (cr6.eq) goto loc_826340AC;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
	// b 0x826340b0
	goto loc_826340B0;
loc_826340AC:
	// bl 0x826179f8
	sub_826179F8(ctx, base);
loc_826340B0:
	// lwz r11,436(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634248
	if (cr6.eq) goto loc_82634248;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634130
	if (!cr6.lt) goto loc_82634130;
loc_826340D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634130
	if (cr6.eq) goto loc_82634130;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634120
	if (!cr0.lt) goto loc_82634120;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634120:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826340d8
	if (cr6.gt) goto loc_826340D8;
loc_82634130:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263416c
	if (!cr0.lt) goto loc_8263416C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263416C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x82634240
	if (!cr6.eq) goto loc_82634240;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r23
	r30.u64 = r23.u64;
	// stw r21,328(r26)
	PPC_STORE_U32(r26.u32 + 328, r21.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x826341ec
	if (!cr6.lt) goto loc_826341EC;
loc_82634194:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826341ec
	if (cr6.eq) goto loc_826341EC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826341dc
	if (!cr0.lt) goto loc_826341DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826341DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82634194
	if (cr6.gt) goto loc_82634194;
loc_826341EC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634228
	if (!cr0.lt) goto loc_82634228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634228:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,15136
	r11.s64 = r11.s64 + 15136;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// stw r11,336(r26)
	PPC_STORE_U32(r26.u32 + 336, r11.u32);
	// b 0x8263424c
	goto loc_8263424C;
loc_82634240:
	// stw r22,328(r26)
	PPC_STORE_U32(r26.u32 + 328, r22.u32);
	// b 0x8263424c
	goto loc_8263424C;
loc_82634248:
	// stw r21,328(r26)
	PPC_STORE_U32(r26.u32 + 328, r21.u32);
loc_8263424C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826342c0
	if (!cr6.lt) goto loc_826342C0;
loc_82634268:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826342c0
	if (cr6.eq) goto loc_826342C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826342b0
	if (!cr0.lt) goto loc_826342B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826342B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82634268
	if (cr6.gt) goto loc_82634268;
loc_826342C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826342fc
	if (!cr0.lt) goto loc_826342FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826342FC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r30.u32);
	// beq cr6,0x826343c4
	if (cr6.eq) goto loc_826343C4;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263437c
	if (!cr6.lt) goto loc_8263437C;
loc_82634324:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263437c
	if (cr6.eq) goto loc_8263437C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263436c
	if (!cr0.lt) goto loc_8263436C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263436C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82634324
	if (cr6.gt) goto loc_82634324;
loc_8263437C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826343b8
	if (!cr0.lt) goto loc_826343B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826343B8:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r11.u32);
loc_826343C4:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// mr r30,r22
	r30.u64 = r22.u64;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r21
	r29.u64 = r21.u64;
	// stw r11,2936(r26)
	PPC_STORE_U32(r26.u32 + 2936, r11.u32);
	// stw r11,2932(r26)
	PPC_STORE_U32(r26.u32 + 2932, r11.u32);
	// stw r11,2948(r26)
	PPC_STORE_U32(r26.u32 + 2948, r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(r26.u32 + 2944, r11.u32);
	// stw r11,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634450
	if (!cr6.lt) goto loc_82634450;
loc_826343F8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634450
	if (cr6.eq) goto loc_82634450;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634440
	if (!cr0.lt) goto loc_82634440;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634440:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826343f8
	if (cr6.gt) goto loc_826343F8;
loc_82634450:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263448c
	if (!cr0.lt) goto loc_8263448C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263448C:
	// stw r30,2088(r26)
	PPC_STORE_U32(r26.u32 + 2088, r30.u32);
	// b 0x82634acc
	goto loc_82634ACC;
loc_82634494:
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82632640
	if (!cr6.eq) goto loc_82632640;
	// lwz r11,20004(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20004);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826344f0
	if (cr6.eq) goto loc_826344F0;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826344f0
	if (!cr6.gt) goto loc_826344F0;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_826344C8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// rlwimi r8,r9,4,28,28
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 4) & 0x8) | (ctx.r8.u64 & 0xFFFFFFFFFFFFFFF7);
	// rlwinm r9,r8,0,28,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFFFEF;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r9,144(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x826344c8
	if (cr6.lt) goto loc_826344C8;
loc_826344F0:
	// lwz r11,2968(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2968);
	// rlwinm r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826346d8
	if (cr6.eq) goto loc_826346D8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634574
	if (!cr6.lt) goto loc_82634574;
loc_8263451C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634574
	if (cr6.eq) goto loc_82634574;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634564
	if (!cr0.lt) goto loc_82634564;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634564:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263451c
	if (cr6.gt) goto loc_8263451C;
loc_82634574:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826345b0
	if (!cr0.lt) goto loc_826345B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826345B0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826345c0
	if (!cr6.eq) goto loc_826345C0;
	// stw r21,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r21.u32);
	// b 0x826346d8
	goto loc_826346D8;
loc_826345C0:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634634
	if (!cr6.lt) goto loc_82634634;
loc_826345DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634634
	if (cr6.eq) goto loc_82634634;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634624
	if (!cr0.lt) goto loc_82634624;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634624:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826345dc
	if (cr6.gt) goto loc_826345DC;
loc_82634634:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634670
	if (!cr0.lt) goto loc_82634670;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634670:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82634680
	if (!cr6.eq) goto loc_82634680;
	// stw r22,2968(r26)
	PPC_STORE_U32(r26.u32 + 2968, r22.u32);
	// b 0x826346d8
	goto loc_826346D8;
loc_82634680:
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825edb20
	sub_825EDB20(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82632640
	if (!cr6.eq) goto loc_82632640;
	// lwz r11,20940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20940);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826346d8
	if (cr6.eq) goto loc_826346D8;
	// lwz r11,144(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826346d8
	if (!cr6.gt) goto loc_826346D8;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_826346B4:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// rlwimi r8,r9,12,20,20
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 12) & 0x800) | (ctx.r8.u64 & 0xFFFFFFFFFFFFF7FF);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwz r9,144(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 144);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x826346b4
	if (cr6.lt) goto loc_826346B4;
loc_826346D8:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263474c
	if (!cr6.lt) goto loc_8263474C;
loc_826346F4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263474c
	if (cr6.eq) goto loc_8263474C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263473c
	if (!cr0.lt) goto loc_8263473C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263473C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826346f4
	if (cr6.gt) goto loc_826346F4;
loc_8263474C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634788
	if (!cr0.lt) goto loc_82634788;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634788:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r30.u32);
	// beq cr6,0x82634850
	if (cr6.eq) goto loc_82634850;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634808
	if (!cr6.lt) goto loc_82634808;
loc_826347B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634808
	if (cr6.eq) goto loc_82634808;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826347f8
	if (!cr0.lt) goto loc_826347F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826347F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826347b0
	if (cr6.gt) goto loc_826347B0;
loc_82634808:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634844
	if (!cr0.lt) goto loc_82634844;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634844:
	// lwz r11,2928(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2928);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(r26.u32 + 2928, r11.u32);
loc_82634850:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826348c4
	if (!cr6.lt) goto loc_826348C4;
loc_8263486C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826348c4
	if (cr6.eq) goto loc_826348C4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826348b4
	if (!cr0.lt) goto loc_826348B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826348B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263486c
	if (cr6.gt) goto loc_8263486C;
loc_826348C4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634900
	if (!cr0.lt) goto loc_82634900;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634900:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r30.u32);
	// beq cr6,0x826349c8
	if (cr6.eq) goto loc_826349C8;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634980
	if (!cr6.lt) goto loc_82634980;
loc_82634928:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634980
	if (cr6.eq) goto loc_82634980;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634970
	if (!cr0.lt) goto loc_82634970;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634970:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82634928
	if (cr6.gt) goto loc_82634928;
loc_82634980:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826349bc
	if (!cr0.lt) goto loc_826349BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826349BC:
	// lwz r11,2940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2940);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r11,2940(r26)
	PPC_STORE_U32(r26.u32 + 2940, r11.u32);
loc_826349C8:
	// lwz r11,2940(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2940);
	// mr r30,r22
	r30.u64 = r22.u64;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r29,r21
	r29.u64 = r21.u64;
	// stw r11,2948(r26)
	PPC_STORE_U32(r26.u32 + 2948, r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(r26.u32 + 2944, r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82634a48
	if (!cr6.lt) goto loc_82634A48;
loc_826349F0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82634a48
	if (cr6.eq) goto loc_82634A48;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82634a38
	if (!cr0.lt) goto loc_82634A38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634A38:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826349f0
	if (cr6.gt) goto loc_826349F0;
loc_82634A48:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82634a84
	if (!cr0.lt) goto loc_82634A84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82634A84:
	// lwz r11,3980(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 3980);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r30,2088(r26)
	PPC_STORE_U32(r26.u32 + 2088, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634aa4
	if (cr6.eq) goto loc_82634AA4;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825ee6e8
	sub_825EE6E8(ctx, base);
	// b 0x82634aa8
	goto loc_82634AA8;
loc_82634AA4:
	// bl 0x826179f8
	sub_826179F8(ctx, base);
loc_82634AA8:
	// lwz r11,284(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82634acc
	if (!cr6.eq) goto loc_82634ACC;
	// lwz r11,19984(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 19984);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82634ac8
	if (!cr6.eq) goto loc_82634AC8;
	// stw r21,21532(r26)
	PPC_STORE_U32(r26.u32 + 21532, r21.u32);
	// b 0x82634acc
	goto loc_82634ACC;
loc_82634AC8:
	// stw r21,21536(r26)
	PPC_STORE_U32(r26.u32 + 21536, r21.u32);
loc_82634ACC:
	// lwz r11,2968(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 2968);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634b00
	if (cr6.eq) goto loc_82634B00;
	// lwz r11,1900(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r21,16(r11)
	PPC_STORE_U16(r11.u32 + 16, r21.u16);
	// lwz r11,1900(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r21,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r21.u16);
	// lwz r11,1904(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r21,16(r11)
	PPC_STORE_U16(r11.u32 + 16, r21.u16);
	// lwz r11,1904(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r21,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r21.u16);
	// b 0x82634b24
	goto loc_82634B24;
loc_82634B00:
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// li r11,128
	r11.s64 = 128;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
loc_82634B24:
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r3,r11,0,29,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82634B40"))) PPC_WEAK_FUNC(sub_82634B40);
PPC_FUNC_IMPL(__imp__sub_82634B40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// ble cr6,0x82634b4c
	if (!cr6.gt) goto loc_82634B4C;
	// li r4,3
	ctx.r4.s64 = 3;
loc_82634B4C:
	// lwz r11,21000(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21000);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mulli r11,r4,28
	r11.s64 = ctx.r4.s64 * 28;
	// addi r10,r10,15400
	ctx.r10.s64 = ctx.r10.s64 + 15400;
	// bne cr6,0x82634bd4
	if (!cr6.eq) goto loc_82634BD4;
	// addi r6,r10,-112
	ctx.r6.s64 = ctx.r10.s64 + -112;
	// addi r7,r10,-112
	ctx.r7.s64 = ctx.r10.s64 + -112;
	// addi r8,r10,-112
	ctx.r8.s64 = ctx.r10.s64 + -112;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r5,r8,8
	ctx.r5.s64 = ctx.r8.s64 + 8;
	// lwzx r6,r11,r6
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// addi r9,r10,-112
	ctx.r9.s64 = ctx.r10.s64 + -112;
	// addi r8,r10,-112
	ctx.r8.s64 = ctx.r10.s64 + -112;
	// addi r4,r9,12
	ctx.r4.s64 = ctx.r9.s64 + 12;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r9,r10,-112
	ctx.r9.s64 = ctx.r10.s64 + -112;
	// stw r6,21088(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21088, ctx.r6.u32);
	// addi r10,r10,-112
	ctx.r10.s64 = ctx.r10.s64 + -112;
	// lwzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// stw r7,21092(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21092, ctx.r7.u32);
	// lwzx r7,r11,r5
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// stw r7,21096(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21096, ctx.r7.u32);
	// lwzx r7,r11,r4
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r7,21100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21100, ctx.r7.u32);
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// stw r8,21104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21104, ctx.r8.u32);
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r9,21108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21108, ctx.r9.u32);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r11,21112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21112, r11.u32);
	// blr 
	return;
loc_82634BD4:
	// addi r4,r10,24
	ctx.r4.s64 = ctx.r10.s64 + 24;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,8
	ctx.r8.s64 = ctx.r10.s64 + 8;
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// addi r5,r10,20
	ctx.r5.s64 = ctx.r10.s64 + 20;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r10,21088(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21088, ctx.r10.u32);
	// lwzx r10,r11,r9
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r10,21092(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21092, ctx.r10.u32);
	// lwzx r10,r11,r8
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// stw r10,21096(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21096, ctx.r10.u32);
	// lwzx r10,r11,r7
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// stw r10,21100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21100, ctx.r10.u32);
	// lwzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// stw r10,21104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21104, ctx.r10.u32);
	// lwzx r10,r11,r5
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// stw r10,21108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21108, ctx.r10.u32);
	// lwzx r11,r11,r4
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r11,21112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21112, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82634C28"))) PPC_WEAK_FUNC(sub_82634C28);
PPC_FUNC_IMPL(__imp__sub_82634C28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,3
	cr6.compare<int32_t>(ctx.r4.s32, 3, xer);
	// ble cr6,0x82634c34
	if (!cr6.gt) goto loc_82634C34;
	// li r4,3
	ctx.r4.s64 = 3;
loc_82634C34:
	// lwz r11,21000(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21000);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mulli r11,r4,28
	r11.s64 = ctx.r4.s64 * 28;
	// addi r10,r10,15624
	ctx.r10.s64 = ctx.r10.s64 + 15624;
	// bne cr6,0x82634cbc
	if (!cr6.eq) goto loc_82634CBC;
	// addi r6,r10,-112
	ctx.r6.s64 = ctx.r10.s64 + -112;
	// addi r7,r10,-112
	ctx.r7.s64 = ctx.r10.s64 + -112;
	// addi r8,r10,-112
	ctx.r8.s64 = ctx.r10.s64 + -112;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r5,r8,8
	ctx.r5.s64 = ctx.r8.s64 + 8;
	// lwzx r6,r11,r6
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// addi r9,r10,-112
	ctx.r9.s64 = ctx.r10.s64 + -112;
	// addi r8,r10,-112
	ctx.r8.s64 = ctx.r10.s64 + -112;
	// addi r4,r9,12
	ctx.r4.s64 = ctx.r9.s64 + 12;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r9,r10,-112
	ctx.r9.s64 = ctx.r10.s64 + -112;
	// stw r6,21116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21116, ctx.r6.u32);
	// addi r10,r10,-112
	ctx.r10.s64 = ctx.r10.s64 + -112;
	// lwzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// stw r7,21120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21120, ctx.r7.u32);
	// lwzx r7,r11,r5
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// stw r7,21124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21124, ctx.r7.u32);
	// lwzx r7,r11,r4
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r7,21128(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21128, ctx.r7.u32);
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// stw r8,21132(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21132, ctx.r8.u32);
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r9,21136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21136, ctx.r9.u32);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r11,21140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21140, r11.u32);
	// blr 
	return;
loc_82634CBC:
	// addi r4,r10,24
	ctx.r4.s64 = ctx.r10.s64 + 24;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// addi r8,r10,8
	ctx.r8.s64 = ctx.r10.s64 + 8;
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// addi r5,r10,20
	ctx.r5.s64 = ctx.r10.s64 + 20;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r10,21116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21116, ctx.r10.u32);
	// lwzx r10,r11,r9
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stw r10,21120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21120, ctx.r10.u32);
	// lwzx r10,r11,r8
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// stw r10,21124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21124, ctx.r10.u32);
	// lwzx r10,r11,r7
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// stw r10,21128(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21128, ctx.r10.u32);
	// lwzx r10,r11,r6
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// stw r10,21132(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21132, ctx.r10.u32);
	// lwzx r10,r11,r5
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// stw r10,21136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21136, ctx.r10.u32);
	// lwzx r11,r11,r4
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// stw r11,21140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21140, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82634D10"))) PPC_WEAK_FUNC(sub_82634D10);
PPC_FUNC_IMPL(__imp__sub_82634D10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,14668
	ctx.r3.s64 = 14668;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82634d54
	if (!cr6.eq) goto loc_82634D54;
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_82634D54:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266e908
	sub_8266E908(ctx, base);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8266e770
	sub_8266E770(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634d8c
	if (cr6.eq) goto loc_82634D8C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_82634D8C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82634D94"))) PPC_WEAK_FUNC(sub_82634D94);
PPC_FUNC_IMPL(__imp__sub_82634D94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82634D98"))) PPC_WEAK_FUNC(sub_82634D98);
PPC_FUNC_IMPL(__imp__sub_82634D98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// mr r20,r9
	r20.u64 = ctx.r9.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// lwz r25,276(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x82635028
	if (cr6.lt) goto loc_82635028;
	// lwz r9,4(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// add r11,r24,r28
	r11.u64 = r24.u64 + r28.u64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x82635028
	if (cr6.gt) goto loc_82635028;
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// add r11,r22,r25
	r11.u64 = r22.u64 + r25.u64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x82635028
	if (cr6.gt) goto loc_82635028;
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// add r11,r21,r28
	r11.u64 = r21.u64 + r28.u64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x82635028
	if (cr6.gt) goto loc_82635028;
	// lwz r9,8(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// add r11,r20,r25
	r11.u64 = r20.u64 + r25.u64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x82635028
	if (cr6.gt) goto loc_82635028;
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82634e74
	if (!cr6.eq) goto loc_82634E74;
	// lhz r9,14(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 14);
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// bne cr6,0x82634e74
	if (!cr6.eq) goto loc_82634E74;
	// li r10,1024
	ctx.r10.s64 = 1024;
	// b 0x82634e80
	goto loc_82634E80;
loc_82634E74:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x82634e80
	if (!cr6.eq) goto loc_82634E80;
	// li r10,12
	ctx.r10.s64 = 12;
loc_82634E80:
	// addi r31,r10,40
	r31.s64 = ctx.r10.s64 + 40;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82634eac
	if (!cr6.eq) goto loc_82634EAC;
	// li r11,2
	r11.s64 = 2;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82634EAC:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82634edc
	if (!cr6.eq) goto loc_82634EDC;
	// lhz r10,14(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x82634edc
	if (!cr6.eq) goto loc_82634EDC;
	// li r11,1024
	r11.s64 = 1024;
	// b 0x82634eec
	goto loc_82634EEC;
loc_82634EDC:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// li r11,12
	r11.s64 = 12;
	// beq cr6,0x82634eec
	if (cr6.eq) goto loc_82634EEC;
	// li r11,0
	r11.s64 = 0;
loc_82634EEC:
	// addi r31,r11,40
	r31.s64 = r11.s64 + 40;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82634f24
	if (!cr6.eq) goto loc_82634F24;
	// li r11,2
	r11.s64 = 2;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82634F24:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// stw r28,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r28.u32);
	// stw r28,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r28.u32);
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r25
	r11.u64 = r25.u64;
	// bgt cr6,0x82634f50
	if (cr6.gt) goto loc_82634F50;
	// neg r11,r25
	r11.s64 = -r25.s64;
loc_82634F50:
	// stw r11,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r11.u32);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r25
	r11.u64 = r25.u64;
	// bgt cr6,0x82634f68
	if (cr6.gt) goto loc_82634F68;
	// neg r11,r25
	r11.s64 = -r25.s64;
loc_82634F68:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,284(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82634d10
	sub_82634D10(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263500c
	if (!cr6.eq) goto loc_8263500C;
	// li r11,1
	r11.s64 = 1;
	// lwz r4,292(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// stw r24,14604(r31)
	PPC_STORE_U32(r31.u32 + 14604, r24.u32);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r22,14608(r31)
	PPC_STORE_U32(r31.u32 + 14608, r22.u32);
	// stw r21,14612(r31)
	PPC_STORE_U32(r31.u32 + 14612, r21.u32);
	// stw r11,14584(r31)
	PPC_STORE_U32(r31.u32 + 14584, r11.u32);
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// stw r20,14616(r31)
	PPC_STORE_U32(r31.u32 + 14616, r20.u32);
	// stw r4,14624(r31)
	PPC_STORE_U32(r31.u32 + 14624, ctx.r4.u32);
	// stw r10,14632(r31)
	PPC_STORE_U32(r31.u32 + 14632, ctx.r10.u32);
	// stw r11,14628(r31)
	PPC_STORE_U32(r31.u32 + 14628, r11.u32);
	// beq cr6,0x82634fe8
	if (cr6.eq) goto loc_82634FE8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82634fe8
	if (cr6.eq) goto loc_82634FE8;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82634fec
	if (!cr6.eq) goto loc_82634FEC;
loc_82634FE8:
	// lwz r4,4(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 4);
loc_82634FEC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,8(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// lwz r6,4(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r5,8(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// bl 0x8266f4f0
	sub_8266F4F0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_8263500C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82635030
	if (cr6.eq) goto loc_82635030;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
loc_82635028:
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
loc_82635030:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_8263503C"))) PPC_WEAK_FUNC(sub_8263503C);
PPC_FUNC_IMPL(__imp__sub_8263503C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82635040"))) PPC_WEAK_FUNC(sub_82635040);
PPC_FUNC_IMPL(__imp__sub_82635040) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82635070
	if (cr6.eq) goto loc_82635070;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_82635070:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82635084
	if (cr6.eq) goto loc_82635084;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
loc_82635084:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826350A8"))) PPC_WEAK_FUNC(sub_826350A8);
PPC_FUNC_IMPL(__imp__sub_826350A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826350cc
	if (cr6.eq) goto loc_826350CC;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826350cc
	if (cr6.eq) goto loc_826350CC;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// b 0x82670b88
	sub_82670B88(ctx, base);
	return;
loc_826350CC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826350D4"))) PPC_WEAK_FUNC(sub_826350D4);
PPC_FUNC_IMPL(__imp__sub_826350D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826350D8"))) PPC_WEAK_FUNC(sub_826350D8);
PPC_FUNC_IMPL(__imp__sub_826350D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x826354b0
	if (cr6.eq) goto loc_826354B0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x826354b0
	if (cr6.eq) goto loc_826354B0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x8263511c
	if (cr6.eq) goto loc_8263511C;
	// lwz r3,4(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r30,8(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lhz r29,14(r4)
	r29.u64 = PPC_LOAD_U16(ctx.r4.u32 + 14);
	// lwz r4,16(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// b 0x82635130
	goto loc_82635130;
loc_8263511C:
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r3,14588(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// lwz r30,14592(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14592);
	// lhz r29,14(r5)
	r29.u64 = PPC_LOAD_U16(ctx.r5.u32 + 14);
	// lwz r4,16(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
loc_82635130:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// lwz r5,228(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x826354b0
	if (cr6.lt) goto loc_826354B0;
	// add r27,r6,r10
	r27.u64 = ctx.r6.u64 + ctx.r10.u64;
	// cmpw cr6,r27,r3
	cr6.compare<int32_t>(r27.s32, ctx.r3.s32, xer);
	// bgt cr6,0x826354b0
	if (cr6.gt) goto loc_826354B0;
	// mr r27,r30
	r27.u64 = r30.u64;
	// add r26,r7,r5
	r26.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r25,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r27.s32 >> 31;
	// xor r27,r27,r25
	r27.u64 = r27.u64 ^ r25.u64;
	// subf r27,r25,r27
	r27.s64 = r27.s64 - r25.s64;
	// cmpw cr6,r26,r27
	cr6.compare<int32_t>(r26.s32, r27.s32, xer);
	// bgt cr6,0x826354b0
	if (cr6.gt) goto loc_826354B0;
	// lwz r26,4(r28)
	r26.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// add r27,r8,r10
	r27.u64 = ctx.r8.u64 + ctx.r10.u64;
	// cmpw cr6,r27,r26
	cr6.compare<int32_t>(r27.s32, r26.s32, xer);
	// bgt cr6,0x826354b0
	if (cr6.gt) goto loc_826354B0;
	// lwz r26,8(r28)
	r26.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// add r27,r9,r5
	r27.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r25,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r26.s32 >> 31;
	// xor r26,r26,r25
	r26.u64 = r26.u64 ^ r25.u64;
	// subf r26,r25,r26
	r26.s64 = r26.s64 - r25.s64;
	// cmpw cr6,r27,r26
	cr6.compare<int32_t>(r27.s32, r26.s32, xer);
	// bgt cr6,0x826354b0
	if (cr6.gt) goto loc_826354B0;
	// lwz r27,14604(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 14604);
	// cmpw cr6,r27,r6
	cr6.compare<int32_t>(r27.s32, ctx.r6.s32, xer);
	// beq cr6,0x826351cc
	if (cr6.eq) goto loc_826351CC;
	// li r11,2
	r11.s64 = 2;
	// stw r6,14604(r31)
	PPC_STORE_U32(r31.u32 + 14604, ctx.r6.u32);
loc_826351CC:
	// lwz r6,14608(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 14608);
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// beq cr6,0x826351e0
	if (cr6.eq) goto loc_826351E0;
	// li r11,2
	r11.s64 = 2;
	// stw r7,14608(r31)
	PPC_STORE_U32(r31.u32 + 14608, ctx.r7.u32);
loc_826351E0:
	// lwz r7,14612(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 14612);
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// beq cr6,0x826351f4
	if (cr6.eq) goto loc_826351F4;
	// li r11,2
	r11.s64 = 2;
	// stw r8,14612(r31)
	PPC_STORE_U32(r31.u32 + 14612, ctx.r8.u32);
loc_826351F4:
	// lwz r8,14616(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 14616);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// beq cr6,0x82635208
	if (cr6.eq) goto loc_82635208;
	// li r11,2
	r11.s64 = 2;
	// stw r9,14616(r31)
	PPC_STORE_U32(r31.u32 + 14616, ctx.r9.u32);
loc_82635208:
	// lwz r9,14516(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14516);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x82635224
	if (cr6.eq) goto loc_82635224;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r11,2
	r11.s64 = 2;
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// stw r10,14516(r31)
	PPC_STORE_U32(r31.u32 + 14516, ctx.r10.u32);
loc_82635224:
	// lwz r9,14520(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14520);
	// cmpw cr6,r9,r5
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, xer);
	// beq cr6,0x82635240
	if (cr6.eq) goto loc_82635240;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r11,2
	r11.s64 = 2;
	// stw r5,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r5.u32);
	// stw r5,14520(r31)
	PPC_STORE_U32(r31.u32 + 14520, ctx.r5.u32);
loc_82635240:
	// lwz r9,14480(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14480);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x8263525c
	if (cr6.eq) goto loc_8263525C;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,2
	r11.s64 = 2;
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// stw r10,14480(r31)
	PPC_STORE_U32(r31.u32 + 14480, ctx.r10.u32);
loc_8263525C:
	// lwz r10,14484(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14484);
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// beq cr6,0x82635278
	if (cr6.eq) goto loc_82635278;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,2
	r11.s64 = 2;
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
	// stw r5,14484(r31)
	PPC_STORE_U32(r31.u32 + 14484, ctx.r5.u32);
loc_82635278:
	// lwz r10,14588(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// cmpw cr6,r10,r3
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r3.s32, xer);
	// beq cr6,0x82635298
	if (cr6.eq) goto loc_82635298;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r3,14588(r31)
	PPC_STORE_U32(r31.u32 + 14588, ctx.r3.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82635298:
	// lwz r10,14592(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14592);
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// beq cr6,0x826352ac
	if (cr6.eq) goto loc_826352AC;
	// li r11,2
	r11.s64 = 2;
	// stw r30,14592(r31)
	PPC_STORE_U32(r31.u32 + 14592, r30.u32);
loc_826352AC:
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r9,14596(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x826352d0
	if (cr6.eq) goto loc_826352D0;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r10,14596(r31)
	PPC_STORE_U32(r31.u32 + 14596, ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826352D0:
	// lwz r10,8(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r9,14600(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14600);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x826352e8
	if (cr6.eq) goto loc_826352E8;
	// li r11,2
	r11.s64 = 2;
	// stw r10,14600(r31)
	PPC_STORE_U32(r31.u32 + 14600, ctx.r10.u32);
loc_826352E8:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// cmplw cr6,r9,r4
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r4.u32, xer);
	// beq cr6,0x82635300
	if (cr6.eq) goto loc_82635300;
	// li r11,2
	r11.s64 = 2;
	// stw r4,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r4.u32);
loc_82635300:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,16(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// lwz r8,16(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// beq cr6,0x8263531c
	if (cr6.eq) goto loc_8263531C;
	// li r11,2
	r11.s64 = 2;
	// stw r9,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r9.u32);
loc_8263531C:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lhz r9,14(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// beq cr6,0x82635334
	if (cr6.eq) goto loc_82635334;
	// li r11,2
	r11.s64 = 2;
	// sth r29,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, r29.u16);
loc_82635334:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lhz r9,14(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 14);
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lhz r8,14(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 14);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// beq cr6,0x82635354
	if (cr6.eq) goto loc_82635354;
	// li r11,2
	r11.s64 = 2;
	// sth r9,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, ctx.r9.u16);
loc_82635354:
	// lwz r9,14624(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14624);
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x82635378
	if (cr6.eq) goto loc_82635378;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r10,14624(r31)
	PPC_STORE_U32(r31.u32 + 14624, ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82635378:
	// lwz r9,14628(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14628);
	// lwz r10,252(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x8263539c
	if (cr6.eq) goto loc_8263539C;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r10,14628(r31)
	PPC_STORE_U32(r31.u32 + 14628, ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8263539C:
	// lwz r9,14632(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14632);
	// lwz r10,260(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x826353c0
	if (cr6.eq) goto loc_826353C0;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r10,14632(r31)
	PPC_STORE_U32(r31.u32 + 14632, ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826353C0:
	// lwz r9,14620(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14620);
	// lwz r10,236(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// beq cr6,0x826353d8
	if (cr6.eq) goto loc_826353D8;
	// stw r10,14620(r31)
	PPC_STORE_U32(r31.u32 + 14620, ctx.r10.u32);
	// b 0x826353e0
	goto loc_826353E0;
loc_826353D8:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82635484
	if (!cr6.eq) goto loc_82635484;
loc_826353E0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266d578
	sub_8266D578(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826354b4
	if (!cr6.eq) goto loc_826354B4;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82635410
	if (!cr6.eq) goto loc_82635410;
	// lhz r10,14(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 14);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x82635410
	if (!cr6.eq) goto loc_82635410;
	// li r5,1024
	ctx.r5.s64 = 1024;
	// b 0x8263541c
	goto loc_8263541C;
loc_82635410:
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8263542c
	if (!cr6.eq) goto loc_8263542C;
	// li r5,12
	ctx.r5.s64 = 12;
loc_8263541C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r4,r28,40
	ctx.r4.s64 = r28.s64 + 40;
	// addi r3,r11,40
	ctx.r3.s64 = r11.s64 + 40;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_8263542C:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82635440
	if (!cr6.lt) goto loc_82635440;
	// li r10,-1
	ctx.r10.s64 = -1;
loc_82635440:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// bl 0x82670c70
	sub_82670C70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826354b4
	if (!cr6.eq) goto loc_826354B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266e5a0
	sub_8266E5A0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826354a4
	if (cr6.eq) goto loc_826354A4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82635484:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826354a4
	if (!cr6.eq) goto loc_826354A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,14600(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 14600);
	// lwz r6,14596(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 14596);
	// lwz r5,14592(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 14592);
	// lwz r4,14588(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 14588);
	// bl 0x8266f4f0
	sub_8266F4F0(ctx, base);
loc_826354A4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_826354B0:
	// li r3,1
	ctx.r3.s64 = 1;
loc_826354B4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_826354BC"))) PPC_WEAK_FUNC(sub_826354BC);
PPC_FUNC_IMPL(__imp__sub_826354BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826354C0"))) PPC_WEAK_FUNC(sub_826354C0);
PPC_FUNC_IMPL(__imp__sub_826354C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r16,r11,28
	r16.u64 = r11.u32 & 0xF;
	// addi r11,r11,15
	r11.s64 = r11.s64 + 15;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// addi r9,r10,15
	ctx.r9.s64 = ctx.r10.s64 + 15;
	// addze r18,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r18.s64 = temp.s64;
	// srawi r11,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	r11.s64 = ctx.r9.s32 >> 4;
	// clrlwi r15,r10,28
	r15.u64 = ctx.r10.u32 & 0xF;
	// addze r20,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r20.s64 = temp.s64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// bne cr6,0x82635514
	if (!cr6.eq) goto loc_82635514;
	// li r16,16
	r16.s64 = 16;
loc_82635514:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x82635520
	if (!cr6.eq) goto loc_82635520;
	// li r15,16
	r15.s64 = 16;
loc_82635520:
	// lwz r7,15628(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15628);
	// li r24,0
	r24.s64 = 0;
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// add r22,r7,r4
	r22.u64 = ctx.r7.u64 + ctx.r4.u64;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lwz r3,96(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mullw r30,r10,r7
	r30.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r8,224(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r10,r3,r10
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r10.s32);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r3,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 1;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r23,r6,r9
	r23.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r25,r9,r11
	r25.u64 = ctx.r9.u64 + r11.u64;
	// add r21,r10,r11
	r21.u64 = ctx.r10.u64 + r11.u64;
	// beq cr6,0x82635688
	if (cr6.eq) goto loc_82635688;
loc_82635598:
	// mr r29,r23
	r29.u64 = r23.u64;
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r28,r22
	r28.u64 = r22.u64;
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x82635660
	if (cr6.eq) goto loc_82635660;
	// addi r17,r18,-1
	r17.s64 = r18.s64 + -1;
	// addi r19,r20,-1
	r19.s64 = r20.s64 + -1;
	// subf r26,r25,r21
	r26.s64 = r21.s64 - r25.s64;
loc_826355BC:
	// cmplw cr6,r27,r17
	cr6.compare<uint32_t>(r27.u32, r17.u32, xer);
	// beq cr6,0x82635604
	if (cr6.eq) goto loc_82635604;
	// cmplw cr6,r24,r19
	cr6.compare<uint32_t>(r24.u32, r19.u32, xer);
	// beq cr6,0x826355fc
	if (cr6.eq) goto loc_826355FC;
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// add r7,r26,r30
	ctx.r7.u64 = r26.u64 + r30.u64;
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,15872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15872);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82635644
	goto loc_82635644;
loc_826355FC:
	// li r10,16
	ctx.r10.s64 = 16;
	// b 0x82635608
	goto loc_82635608;
loc_82635604:
	// mr r10,r16
	ctx.r10.u64 = r16.u64;
loc_82635608:
	// cmplw cr6,r24,r19
	cr6.compare<uint32_t>(r24.u32, r19.u32, xer);
	// li r11,16
	r11.s64 = 16;
	// bne cr6,0x82635618
	if (!cr6.eq) goto loc_82635618;
	// mr r11,r15
	r11.u64 = r15.u64;
loc_82635618:
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r7,r26,r30
	ctx.r7.u64 = r26.u64 + r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r14,15876(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 15876);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r14
	ctr.u64 = r14.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82635644:
	// lwz r11,15632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15632);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplw cr6,r27,r18
	cr6.compare<uint32_t>(r27.u32, r18.u32, xer);
	// blt cr6,0x826355bc
	if (cr6.lt) goto loc_826355BC;
loc_82635660:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,15644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r23,r23,r9
	r23.u64 = r23.u64 + ctx.r9.u64;
	// add r22,r10,r22
	r22.u64 = ctx.r10.u64 + r22.u64;
	// add r21,r11,r21
	r21.u64 = r11.u64 + r21.u64;
	// cmplw cr6,r24,r20
	cr6.compare<uint32_t>(r24.u32, r20.u32, xer);
	// blt cr6,0x82635598
	if (cr6.lt) goto loc_82635598;
loc_82635688:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82635694"))) PPC_WEAK_FUNC(sub_82635694);
PPC_FUNC_IMPL(__imp__sub_82635694) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82635698"))) PPC_WEAK_FUNC(sub_82635698);
PPC_FUNC_IMPL(__imp__sub_82635698) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r3,256(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// srawi r28,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r28.s64 = r11.s32 >> 2;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stw r25,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, r25.u32);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r29,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, r29.u32);
	// stw r28,-464(r1)
	PPC_STORE_U32(ctx.r1.u32 + -464, r28.u32);
	// stw r24,68(r1)
	PPC_STORE_U32(ctx.r1.u32 + 68, r24.u32);
	// stw r8,-476(r1)
	PPC_STORE_U32(ctx.r1.u32 + -476, ctx.r8.u32);
	// ble cr6,0x82636284
	if (!cr6.gt) goto loc_82636284;
	// lwz r27,116(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// srawi r22,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r22.s64 = ctx.r10.s32 >> 2;
	// lis r4,154
	ctx.r4.s64 = 10092544;
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r27,r11
	r11.u64 = r27.u64 + r11.u64;
	// stw r22,-440(r1)
	PPC_STORE_U32(ctx.r1.u32 + -440, r22.u32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-460(r1)
	PPC_STORE_U32(ctx.r1.u32 + -460, r11.u32);
	// lis r11,0
	r11.s64 = 0;
	// ori r11,r11,32768
	r11.u64 = r11.u64 | 32768;
loc_8263570C:
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82635730
	if (cr6.eq) goto loc_82635730;
	// addi r10,r28,-1
	ctx.r10.s64 = r28.s64 + -1;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// bne cr6,0x82635730
	if (!cr6.eq) goto loc_82635730;
	// li r21,1
	r21.s64 = 1;
	// stw r21,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, r21.u32);
	// b 0x8263573c
	goto loc_8263573C;
loc_82635730:
	// li r10,0
	ctx.r10.s64 = 0;
	// rotlwi r21,r10,0
	r21.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r10,-496(r1)
	PPC_STORE_U32(ctx.r1.u32 + -496, ctx.r10.u32);
loc_8263573C:
	// li r23,0
	r23.s64 = 0;
	// stw r29,-492(r1)
	PPC_STORE_U32(ctx.r1.u32 + -492, r29.u32);
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// stw r23,-472(r1)
	PPC_STORE_U32(ctx.r1.u32 + -472, r23.u32);
	// ble cr6,0x82636248
	if (!cr6.gt) goto loc_82636248;
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// rlwinm r28,r25,2,0,29
	r28.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r24,1,0,30
	r29.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r8,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r8.s64 = r27.s32 >> 1;
	// rlwinm r26,r10,1,0,30
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r10,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r10,r27
	ctx.r6.u64 = ctx.r10.u64 + r27.u64;
	// add r7,r25,r28
	ctx.r7.u64 = r25.u64 + r28.u64;
	// stw r8,-468(r1)
	PPC_STORE_U32(ctx.r1.u32 + -468, ctx.r8.u32);
	// add r8,r10,r26
	ctx.r8.u64 = ctx.r10.u64 + r26.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r5,-444(r1)
	PPC_STORE_U32(ctx.r1.u32 + -444, ctx.r5.u32);
	// stw r7,-452(r1)
	PPC_STORE_U32(ctx.r1.u32 + -452, ctx.r7.u32);
	// add r7,r24,r29
	ctx.r7.u64 = r24.u64 + r29.u64;
	// stw r10,-436(r1)
	PPC_STORE_U32(ctx.r1.u32 + -436, ctx.r10.u32);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r7,-428(r1)
	PPC_STORE_U32(ctx.r1.u32 + -428, ctx.r7.u32);
	// stw r10,-424(r1)
	PPC_STORE_U32(ctx.r1.u32 + -424, ctx.r10.u32);
	// b 0x826357c0
	goto loc_826357C0;
loc_826357B8:
	// lwz r7,-428(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -428);
	// lwz r21,-496(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -496);
loc_826357C0:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826357dc
	if (cr6.eq) goto loc_826357DC;
	// addi r10,r22,-1
	ctx.r10.s64 = r22.s64 + -1;
	// li r6,1
	ctx.r6.s64 = 1;
	// cmpw cr6,r23,r10
	cr6.compare<int32_t>(r23.s32, ctx.r10.s32, xer);
	// beq cr6,0x826357e0
	if (cr6.eq) goto loc_826357E0;
loc_826357DC:
	// li r6,0
	ctx.r6.s64 = 0;
loc_826357E0:
	// stw r6,-500(r1)
	PPC_STORE_U32(ctx.r1.u32 + -500, ctx.r6.u32);
	// addi r10,r1,-527
	ctx.r10.s64 = ctx.r1.s64 + -527;
	// li r29,5
	r29.s64 = 5;
loc_826357EC:
	// lbz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// lbz r28,1(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// lbz r27,2(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 2);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lbz r26,3(r31)
	r26.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// lbz r23,4(r31)
	r23.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// add r31,r31,r25
	r31.u64 = r31.u64 + r25.u64;
	// stb r5,-1(r10)
	PPC_STORE_U8(ctx.r10.u32 + -1, ctx.r5.u8);
	// stb r28,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r28.u8);
	// stb r27,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, r27.u8);
	// stb r26,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, r26.u8);
	// stb r23,3(r10)
	PPC_STORE_U8(ctx.r10.u32 + 3, r23.u8);
	// addi r10,r10,5
	ctx.r10.s64 = ctx.r10.s64 + 5;
	// bne cr6,0x826357ec
	if (!cr6.eq) goto loc_826357EC;
	// lwz r10,-452(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -452);
	// subf r10,r10,r31
	ctx.r10.s64 = r31.s64 - ctx.r10.s64;
	// stw r10,-456(r1)
	PPC_STORE_U32(ctx.r1.u32 + -456, ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
loc_82635838:
	// lbz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r31,r1,-544
	r31.s64 = ctx.r1.s64 + -544;
	// lbz r29,0(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r28,r1,-560
	r28.s64 = ctx.r1.s64 + -560;
	// lbz r27,1(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// addi r26,r1,-543
	r26.s64 = ctx.r1.s64 + -543;
	// lbz r25,1(r30)
	r25.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// addi r23,r1,-559
	r23.s64 = ctx.r1.s64 + -559;
	// lbz r22,2(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// addi r20,r1,-542
	r20.s64 = ctx.r1.s64 + -542;
	// lbz r19,2(r30)
	r19.u64 = PPC_LOAD_U8(r30.u32 + 2);
	// addi r18,r1,-558
	r18.s64 = ctx.r1.s64 + -558;
	// stbx r5,r10,r31
	PPC_STORE_U8(ctx.r10.u32 + r31.u32, ctx.r5.u8);
	// add r9,r9,r24
	ctx.r9.u64 = ctx.r9.u64 + r24.u64;
	// stbx r29,r10,r28
	PPC_STORE_U8(ctx.r10.u32 + r28.u32, r29.u8);
	// add r30,r30,r24
	r30.u64 = r30.u64 + r24.u64;
	// stbx r27,r10,r26
	PPC_STORE_U8(ctx.r10.u32 + r26.u32, r27.u8);
	// stbx r25,r10,r23
	PPC_STORE_U8(ctx.r10.u32 + r23.u32, r25.u8);
	// stbx r22,r10,r20
	PPC_STORE_U8(ctx.r10.u32 + r20.u32, r22.u8);
	// stbx r19,r10,r18
	PPC_STORE_U8(ctx.r10.u32 + r18.u32, r19.u8);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// cmpwi cr6,r10,9
	cr6.compare<int32_t>(ctx.r10.s32, 9, xer);
	// blt cr6,0x82635838
	if (cr6.lt) goto loc_82635838;
	// subf r10,r7,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lbz r5,-540(r1)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + -540);
	// lbz r9,-510(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -510);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lbz r31,-556(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -556);
	// stw r10,-432(r1)
	PPC_STORE_U32(ctx.r1.u32 + -432, ctx.r10.u32);
	// subf r10,r7,r30
	ctx.r10.s64 = r30.s64 - ctx.r7.s64;
	// stw r10,-448(r1)
	PPC_STORE_U32(ctx.r1.u32 + -448, ctx.r10.u32);
	// beq cr6,0x82635908
	if (cr6.eq) goto loc_82635908;
	// lbz r10,-520(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -520);
	// lbz r6,-525(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + -525);
	// stb r9,-509(r1)
	PPC_STORE_U8(ctx.r1.u32 + -509, ctx.r9.u8);
	// stb r5,-539(r1)
	PPC_STORE_U8(ctx.r1.u32 + -539, ctx.r5.u8);
	// stb r31,-555(r1)
	PPC_STORE_U8(ctx.r1.u32 + -555, r31.u8);
	// stb r10,-519(r1)
	PPC_STORE_U8(ctx.r1.u32 + -519, ctx.r10.u8);
	// lbz r10,-515(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -515);
	// stb r6,-524(r1)
	PPC_STORE_U8(ctx.r1.u32 + -524, ctx.r6.u8);
	// stb r10,-514(r1)
	PPC_STORE_U8(ctx.r1.u32 + -514, ctx.r10.u8);
	// lbz r10,-505(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -505);
	// stb r10,-504(r1)
	PPC_STORE_U8(ctx.r1.u32 + -504, ctx.r10.u8);
	// lbz r10,-543(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -543);
	// stb r10,-542(r1)
	PPC_STORE_U8(ctx.r1.u32 + -542, ctx.r10.u8);
	// lbz r10,-559(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -559);
	// stb r10,-558(r1)
	PPC_STORE_U8(ctx.r1.u32 + -558, ctx.r10.u8);
	// lbz r10,-537(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -537);
	// stb r10,-536(r1)
	PPC_STORE_U8(ctx.r1.u32 + -536, ctx.r10.u8);
	// lbz r10,-553(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -553);
	// stb r10,-552(r1)
	PPC_STORE_U8(ctx.r1.u32 + -552, ctx.r10.u8);
	// b 0x8263590c
	goto loc_8263590C;
loc_82635908:
	// lbz r6,-524(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + -524);
loc_8263590C:
	// lbz r10,-511(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -511);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// lbz r30,-512(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -512);
	// lbz r28,-513(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + -513);
	// beq cr6,0x82635968
	if (cr6.eq) goto loc_82635968;
	// stb r9,-505(r1)
	PPC_STORE_U8(ctx.r1.u32 + -505, ctx.r9.u8);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// lbz r9,-509(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -509);
	// stb r28,-508(r1)
	PPC_STORE_U8(ctx.r1.u32 + -508, r28.u8);
	// stb r30,-507(r1)
	PPC_STORE_U8(ctx.r1.u32 + -507, r30.u8);
	// stb r5,-537(r1)
	PPC_STORE_U8(ctx.r1.u32 + -537, ctx.r5.u8);
	// stb r7,-506(r1)
	PPC_STORE_U8(ctx.r1.u32 + -506, ctx.r7.u8);
	// stb r9,-504(r1)
	PPC_STORE_U8(ctx.r1.u32 + -504, ctx.r9.u8);
	// lbz r9,-541(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -541);
	// stb r31,-553(r1)
	PPC_STORE_U8(ctx.r1.u32 + -553, r31.u8);
	// stb r9,-538(r1)
	PPC_STORE_U8(ctx.r1.u32 + -538, ctx.r9.u8);
	// lbz r9,-557(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -557);
	// stb r9,-554(r1)
	PPC_STORE_U8(ctx.r1.u32 + -554, ctx.r9.u8);
	// lbz r9,-539(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -539);
	// stb r9,-536(r1)
	PPC_STORE_U8(ctx.r1.u32 + -536, ctx.r9.u8);
	// lbz r9,-555(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -555);
	// stb r9,-552(r1)
	PPC_STORE_U8(ctx.r1.u32 + -552, ctx.r9.u8);
	// b 0x8263596c
	goto loc_8263596C;
loc_82635968:
	// lbz r7,-506(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -506);
loc_8263596C:
	// lbz r9,-522(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -522);
	// clrlwi r25,r28,24
	r25.u64 = r28.u32 & 0xFF;
	// lbz r27,-523(r1)
	r27.u64 = PPC_LOAD_U8(ctx.r1.u32 + -523);
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// rotlwi r29,r9,1
	r29.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// lbz r31,-517(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -517);
	// lbz r18,-521(r1)
	r18.u64 = PPC_LOAD_U8(ctx.r1.u32 + -521);
	// rlwinm r24,r30,1,23,30
	r24.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0x1FE;
	// add r14,r29,r27
	r14.u64 = r29.u64 + r27.u64;
	// lbz r28,-527(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + -527);
	// add r21,r31,r29
	r21.u64 = r31.u64 + r29.u64;
	// lbz r30,-528(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -528);
	// add r29,r18,r29
	r29.u64 = r18.u64 + r29.u64;
	// lbz r23,-526(r1)
	r23.u64 = PPC_LOAD_U8(ctx.r1.u32 + -526);
	// add r5,r14,r28
	ctx.r5.u64 = r14.u64 + r28.u64;
	// lbz r26,-516(r1)
	r26.u64 = PPC_LOAD_U8(ctx.r1.u32 + -516);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// rotlwi r20,r28,1
	r20.u64 = __builtin_rotateleft32(r28.u32, 1);
	// rlwinm r28,r5,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r29,1,0,30
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r28,r30
	ctx.r5.u64 = r28.u64 + r30.u64;
	// rotlwi r28,r30,8
	r28.u64 = __builtin_rotateleft32(r30.u32, 8);
	// mulli r5,r5,28
	ctx.r5.s64 = ctx.r5.s64 * 28;
	// stw r28,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r28.u32);
	// stw r5,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, ctx.r5.u32);
	// add r29,r29,r23
	r29.u64 = r29.u64 + r23.u64;
	// add r16,r24,r25
	r16.u64 = r24.u64 + r25.u64;
	// mulli r5,r29,28
	ctx.r5.s64 = r29.s64 * 28;
	// stw r5,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r5.u32);
	// add r29,r21,r27
	r29.u64 = r21.u64 + r27.u64;
	// add r28,r21,r18
	r28.u64 = r21.u64 + r18.u64;
	// add r21,r16,r31
	r21.u64 = r16.u64 + r31.u64;
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// rotlwi r19,r27,1
	r19.u64 = __builtin_rotateleft32(r27.u32, 1);
	// lbz r10,-518(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -518);
	// rlwinm r27,r29,1,0,30
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r21,1,0,30
	r29.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// add r21,r20,r30
	r21.u64 = r20.u64 + r30.u64;
	// rotlwi r5,r10,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 8);
	// add r30,r19,r30
	r30.u64 = r19.u64 + r30.u64;
	// rotlwi r22,r31,1
	r22.u64 = __builtin_rotateleft32(r31.u32, 1);
	// mulli r30,r30,85
	r30.s64 = r30.s64 * 85;
	// stw r5,-248(r1)
	PPC_STORE_U32(ctx.r1.u32 + -248, ctx.r5.u32);
	// stw r30,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r30.u32);
	// add r5,r22,r10
	ctx.r5.u64 = r22.u64 + ctx.r10.u64;
	// add r19,r10,r19
	r19.u64 = ctx.r10.u64 + r19.u64;
	// mulli r5,r5,85
	ctx.r5.s64 = ctx.r5.s64 * 85;
	// stw r5,-244(r1)
	PPC_STORE_U32(ctx.r1.u32 + -244, ctx.r5.u32);
	// mulli r30,r19,85
	r30.s64 = r19.s64 * 85;
	// stw r30,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r30.u32);
	// rlwinm r17,r25,1,0,30
	r17.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// add r15,r9,r24
	r15.u64 = ctx.r9.u64 + r24.u64;
	// add r27,r27,r10
	r27.u64 = r27.u64 + ctx.r10.u64;
	// add r14,r17,r10
	r14.u64 = r17.u64 + ctx.r10.u64;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// add r22,r22,r26
	r22.u64 = r22.u64 + r26.u64;
	// add r31,r15,r31
	r31.u64 = r15.u64 + r31.u64;
	// mulli r29,r21,85
	r29.s64 = r21.s64 * 85;
	// stw r29,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r29.u32);
	// mulli r30,r27,28
	r30.s64 = r27.s64 * 28;
	// stw r30,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, r30.u32);
	// mulli r5,r22,85
	ctx.r5.s64 = r22.s64 * 85;
	// stw r5,-240(r1)
	PPC_STORE_U32(ctx.r1.u32 + -240, ctx.r5.u32);
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
	// stw r10,-220(r1)
	PPC_STORE_U32(ctx.r1.u32 + -220, ctx.r10.u32);
	// rlwinm r28,r28,1,0,30
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r20,r23,r20
	r20.u64 = r23.u64 + r20.u64;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// mulli r29,r20,85
	r29.s64 = r20.s64 * 85;
	// stw r29,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r29.u32);
	// mulli r30,r28,28
	r30.s64 = r28.s64 * 28;
	// stw r30,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, r30.u32);
	// mulli r5,r14,85
	ctx.r5.s64 = r14.s64 * 85;
	// stw r5,-224(r1)
	PPC_STORE_U32(ctx.r1.u32 + -224, ctx.r5.u32);
	// mulli r10,r31,28
	ctx.r10.s64 = r31.s64 * 28;
	// stw r10,-216(r1)
	PPC_STORE_U32(ctx.r1.u32 + -216, ctx.r10.u32);
	// beq cr6,0x82635ac0
	if (cr6.eq) goto loc_82635AC0;
	// rlwinm r10,r25,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 8) & 0xFFFFFF00;
	// stw r10,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r10.u32);
	// mulli r10,r16,85
	ctx.r10.s64 = r16.s64 * 85;
	// stw r10,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, ctx.r10.u32);
	// mulli r10,r15,85
	ctx.r10.s64 = r15.s64 * 85;
	// b 0x82635b00
	goto loc_82635B00;
loc_82635AC0:
	// lbz r31,-507(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -507);
	// clrlwi r30,r7,24
	r30.u64 = ctx.r7.u32 & 0xFF;
	// lbz r10,-508(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -508);
	// add r31,r31,r24
	r31.u64 = r31.u64 + r24.u64;
	// add r5,r10,r17
	ctx.r5.u64 = ctx.r10.u64 + r17.u64;
	// add r29,r31,r25
	r29.u64 = r31.u64 + r25.u64;
	// add r31,r31,r9
	r31.u64 = r31.u64 + ctx.r9.u64;
	// rlwinm r29,r29,1,0,30
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// mulli r5,r5,85
	ctx.r5.s64 = ctx.r5.s64 * 85;
	// stw r5,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r5.u32);
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
	// stw r10,-196(r1)
	PPC_STORE_U32(ctx.r1.u32 + -196, ctx.r10.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// mulli r10,r31,28
	ctx.r10.s64 = r31.s64 * 28;
loc_82635B00:
	// stw r10,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r10.u32);
	// rlwinm r10,r23,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 8) & 0xFFFFFF00;
	// lbz r31,-525(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -525);
	// lwz r25,-500(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -500);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stw r10,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, ctx.r10.u32);
	// rotlwi r10,r31,1
	ctx.r10.u64 = __builtin_rotateleft32(r31.u32, 1);
	// add r5,r10,r23
	ctx.r5.u64 = ctx.r10.u64 + r23.u64;
	// mulli r5,r5,85
	ctx.r5.s64 = ctx.r5.s64 * 85;
	// stw r5,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r5.u32);
	// beq cr6,0x82635b34
	if (cr6.eq) goto loc_82635B34;
	// rlwinm r10,r31,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635b40
	goto loc_82635B40;
loc_82635B34:
	// clrlwi r30,r6,24
	r30.u64 = ctx.r6.u32 & 0xFF;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
loc_82635B40:
	// rlwinm r29,r18,1,0,30
	r29.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r10.u32);
	// lbz r10,-520(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -520);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// add r5,r29,r23
	ctx.r5.u64 = r29.u64 + r23.u64;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// mulli r5,r5,85
	ctx.r5.s64 = ctx.r5.s64 * 85;
	// stw r5,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r5.u32);
	// add r30,r10,r31
	r30.u64 = ctx.r10.u64 + r31.u64;
	// add r5,r30,r18
	ctx.r5.u64 = r30.u64 + r18.u64;
	// rlwinm r28,r5,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r28,r23
	ctx.r5.u64 = r28.u64 + r23.u64;
	// mulli r5,r5,28
	ctx.r5.s64 = ctx.r5.s64 * 28;
	// stw r5,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r5.u32);
	// beq cr6,0x82635b84
	if (cr6.eq) goto loc_82635B84;
	// mulli r6,r30,85
	ctx.r6.s64 = r30.s64 * 85;
	// b 0x82635ba0
	goto loc_82635BA0;
loc_82635B84:
	// lbz r28,-519(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + -519);
	// clrlwi r30,r6,24
	r30.u64 = ctx.r6.u32 & 0xFF;
	// add r28,r28,r10
	r28.u64 = r28.u64 + ctx.r10.u64;
	// add r6,r28,r31
	ctx.r6.u64 = r28.u64 + r31.u64;
	// rlwinm r31,r6,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r31,r30
	ctx.r6.u64 = r31.u64 + r30.u64;
	// mulli r6,r6,28
	ctx.r6.s64 = ctx.r6.s64 * 28;
loc_82635BA0:
	// stw r6,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r6.u32);
	// add r6,r29,r26
	ctx.r6.u64 = r29.u64 + r26.u64;
	// lbz r31,-515(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -515);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// mulli r6,r6,85
	ctx.r6.s64 = ctx.r6.s64 * 85;
	// lbz r5,-514(r1)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + -514);
	// stw r6,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r6.u32);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// add r6,r10,r18
	ctx.r6.u64 = ctx.r10.u64 + r18.u64;
	// rlwinm r30,r6,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r30,r26
	ctx.r6.u64 = r30.u64 + r26.u64;
	// mulli r6,r6,28
	ctx.r6.s64 = ctx.r6.s64 * 28;
	// stw r6,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, ctx.r6.u32);
	// beq cr6,0x82635be0
	if (cr6.eq) goto loc_82635BE0;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635bf8
	goto loc_82635BF8;
loc_82635BE0:
	// lbz r29,-519(r1)
	r29.u64 = PPC_LOAD_U8(ctx.r1.u32 + -519);
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
loc_82635BF8:
	// rlwinm r6,r26,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 8) & 0xFFFFFF00;
	// stw r10,-252(r1)
	PPC_STORE_U32(ctx.r1.u32 + -252, ctx.r10.u32);
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stw r6,-236(r1)
	PPC_STORE_U32(ctx.r1.u32 + -236, ctx.r6.u32);
	// add r6,r10,r26
	ctx.r6.u64 = ctx.r10.u64 + r26.u64;
	// mulli r6,r6,85
	ctx.r6.s64 = ctx.r6.s64 * 85;
	// stw r6,-232(r1)
	PPC_STORE_U32(ctx.r1.u32 + -232, ctx.r6.u32);
	// beq cr6,0x82635c24
	if (cr6.eq) goto loc_82635C24;
	// rlwinm r10,r31,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635c30
	goto loc_82635C30;
loc_82635C24:
	// clrlwi r30,r5,24
	r30.u64 = ctx.r5.u32 & 0xFF;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
loc_82635C30:
	// rlwinm r29,r9,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r27,-510(r1)
	r27.u64 = PPC_LOAD_U8(ctx.r1.u32 + -510);
	// stw r10,-228(r1)
	PPC_STORE_U32(ctx.r1.u32 + -228, ctx.r10.u32);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// add r6,r29,r26
	ctx.r6.u64 = r29.u64 + r26.u64;
	// rotlwi r10,r27,1
	ctx.r10.u64 = __builtin_rotateleft32(r27.u32, 1);
	// mulli r6,r6,85
	ctx.r6.s64 = ctx.r6.s64 * 85;
	// stw r6,-212(r1)
	PPC_STORE_U32(ctx.r1.u32 + -212, ctx.r6.u32);
	// add r30,r10,r31
	r30.u64 = ctx.r10.u64 + r31.u64;
	// add r6,r30,r9
	ctx.r6.u64 = r30.u64 + ctx.r9.u64;
	// rlwinm r28,r6,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r28,r26
	ctx.r6.u64 = r28.u64 + r26.u64;
	// mulli r6,r6,28
	ctx.r6.s64 = ctx.r6.s64 * 28;
	// stw r6,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r6.u32);
	// beq cr6,0x82635c7c
	if (cr6.eq) goto loc_82635C7C;
	// mulli r6,r30,85
	ctx.r6.s64 = r30.s64 * 85;
	// stw r6,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r6.u32);
	// lbz r6,-509(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + -509);
	// b 0x82635ca0
	goto loc_82635CA0;
loc_82635C7C:
	// lbz r6,-509(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + -509);
	// clrlwi r30,r5,24
	r30.u64 = ctx.r5.u32 & 0xFF;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// add r28,r28,r10
	r28.u64 = r28.u64 + ctx.r10.u64;
	// add r5,r28,r31
	ctx.r5.u64 = r28.u64 + r31.u64;
	// rlwinm r31,r5,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r31,r30
	ctx.r5.u64 = r31.u64 + r30.u64;
	// mulli r5,r5,28
	ctx.r5.s64 = ctx.r5.s64 * 28;
	// stw r5,-204(r1)
	PPC_STORE_U32(ctx.r1.u32 + -204, ctx.r5.u32);
loc_82635CA0:
	// lwz r5,-496(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -496);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82635ce0
	if (cr6.eq) goto loc_82635CE0;
	// add r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stw r9,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r9.u32);
	// mulli r9,r7,85
	ctx.r9.s64 = ctx.r7.s64 * 85;
	// stw r9,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r9.u32);
	// beq cr6,0x82635cd0
	if (cr6.eq) goto loc_82635CD0;
	// rlwinm r10,r27,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635d34
	goto loc_82635D34;
loc_82635CD0:
	// clrlwi r9,r6,24
	ctx.r9.u64 = ctx.r6.u32 & 0xFF;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635d34
	goto loc_82635D34;
loc_82635CE0:
	// lbz r30,-505(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -505);
	// clrlwi r31,r7,24
	r31.u64 = ctx.r7.u32 & 0xFF;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// add r7,r29,r31
	ctx.r7.u64 = r29.u64 + r31.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r7,r7,85
	ctx.r7.s64 = ctx.r7.s64 * 85;
	// stw r7,-188(r1)
	PPC_STORE_U32(ctx.r1.u32 + -188, ctx.r7.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// mulli r9,r9,28
	ctx.r9.s64 = ctx.r9.s64 * 28;
	// stw r9,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r9.u32);
	// beq cr6,0x82635d1c
	if (cr6.eq) goto loc_82635D1C;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635d34
	goto loc_82635D34;
loc_82635D1C:
	// clrlwi r31,r6,24
	r31.u64 = ctx.r6.u32 & 0xFF;
	// lbz r9,-504(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -504);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
loc_82635D34:
	// lbz r31,-544(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -544);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r30,-543(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -543);
	// rotlwi r9,r31,8
	ctx.r9.u64 = __builtin_rotateleft32(r31.u32, 8);
	// stw r10,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r10.u32);
	// rotlwi r10,r30,1
	ctx.r10.u64 = __builtin_rotateleft32(r30.u32, 1);
	// stw r9,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, ctx.r9.u32);
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// mulli r9,r9,85
	ctx.r9.s64 = ctx.r9.s64 * 85;
	// stw r9,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, ctx.r9.u32);
	// beq cr6,0x82635d68
	if (cr6.eq) goto loc_82635D68;
	// rlwinm r10,r30,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635d74
	goto loc_82635D74;
loc_82635D68:
	// lbz r9,-542(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -542);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
loc_82635D74:
	// lbz r9,-541(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -541);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r26,-540(r1)
	r26.u64 = PPC_LOAD_U8(ctx.r1.u32 + -540);
	// rotlwi r28,r9,1
	r28.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// stw r10,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, ctx.r10.u32);
	// rotlwi r10,r26,1
	ctx.r10.u64 = __builtin_rotateleft32(r26.u32, 1);
	// add r7,r28,r31
	ctx.r7.u64 = r28.u64 + r31.u64;
	// add r27,r10,r9
	r27.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r7,r7,85
	ctx.r7.s64 = ctx.r7.s64 * 85;
	// stw r7,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, ctx.r7.u32);
	// add r7,r27,r30
	ctx.r7.u64 = r27.u64 + r30.u64;
	// rlwinm r29,r7,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r29,r31
	ctx.r7.u64 = r29.u64 + r31.u64;
	// mulli r7,r7,28
	ctx.r7.s64 = ctx.r7.s64 * 28;
	// stw r7,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, ctx.r7.u32);
	// beq cr6,0x82635dc8
	if (cr6.eq) goto loc_82635DC8;
	// add r7,r10,r30
	ctx.r7.u64 = ctx.r10.u64 + r30.u64;
	// mulli r7,r7,85
	ctx.r7.s64 = ctx.r7.s64 * 85;
	// stw r7,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r7.u32);
	// lbz r7,-539(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -539);
	// b 0x82635dec
	goto loc_82635DEC;
loc_82635DC8:
	// lbz r7,-539(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -539);
	// lbz r29,-542(r1)
	r29.u64 = PPC_LOAD_U8(ctx.r1.u32 + -542);
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// add r31,r31,r10
	r31.u64 = r31.u64 + ctx.r10.u64;
	// add r6,r31,r30
	ctx.r6.u64 = r31.u64 + r30.u64;
	// rlwinm r31,r6,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r31,r29
	ctx.r6.u64 = r31.u64 + r29.u64;
	// mulli r6,r6,28
	ctx.r6.s64 = ctx.r6.s64 * 28;
	// stw r6,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, ctx.r6.u32);
loc_82635DEC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82635e24
	if (cr6.eq) goto loc_82635E24;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stw r9,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r9.u32);
	// mulli r9,r27,85
	ctx.r9.s64 = r27.s64 * 85;
	// stw r9,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, ctx.r9.u32);
	// beq cr6,0x82635e14
	if (cr6.eq) goto loc_82635E14;
	// rlwinm r10,r26,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635e78
	goto loc_82635E78;
loc_82635E14:
	// clrlwi r9,r7,24
	ctx.r9.u64 = ctx.r7.u32 & 0xFF;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635e78
	goto loc_82635E78;
loc_82635E24:
	// lbz r30,-537(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -537);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r31,-538(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -538);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// add r6,r31,r28
	ctx.r6.u64 = r31.u64 + r28.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r6,r6,85
	ctx.r6.s64 = ctx.r6.s64 * 85;
	// stw r6,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r6.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// mulli r9,r9,28
	ctx.r9.s64 = ctx.r9.s64 * 28;
	// stw r9,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, ctx.r9.u32);
	// beq cr6,0x82635e60
	if (cr6.eq) goto loc_82635E60;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635e78
	goto loc_82635E78;
loc_82635E60:
	// clrlwi r31,r7,24
	r31.u64 = ctx.r7.u32 & 0xFF;
	// lbz r9,-536(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -536);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
loc_82635E78:
	// lbz r31,-560(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -560);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r30,-559(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -559);
	// rotlwi r9,r31,8
	ctx.r9.u64 = __builtin_rotateleft32(r31.u32, 8);
	// stw r10,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, ctx.r10.u32);
	// rotlwi r10,r30,1
	ctx.r10.u64 = __builtin_rotateleft32(r30.u32, 1);
	// stw r9,-416(r1)
	PPC_STORE_U32(ctx.r1.u32 + -416, ctx.r9.u32);
	// add r9,r10,r31
	ctx.r9.u64 = ctx.r10.u64 + r31.u64;
	// mulli r9,r9,85
	ctx.r9.s64 = ctx.r9.s64 * 85;
	// stw r9,-412(r1)
	PPC_STORE_U32(ctx.r1.u32 + -412, ctx.r9.u32);
	// beq cr6,0x82635eac
	if (cr6.eq) goto loc_82635EAC;
	// rlwinm r10,r30,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635eb8
	goto loc_82635EB8;
loc_82635EAC:
	// lbz r9,-558(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -558);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
loc_82635EB8:
	// lbz r9,-557(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -557);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r26,-556(r1)
	r26.u64 = PPC_LOAD_U8(ctx.r1.u32 + -556);
	// rotlwi r28,r9,1
	r28.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// stw r10,-408(r1)
	PPC_STORE_U32(ctx.r1.u32 + -408, ctx.r10.u32);
	// rotlwi r10,r26,1
	ctx.r10.u64 = __builtin_rotateleft32(r26.u32, 1);
	// add r7,r28,r31
	ctx.r7.u64 = r28.u64 + r31.u64;
	// add r27,r10,r9
	r27.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r7,r7,85
	ctx.r7.s64 = ctx.r7.s64 * 85;
	// stw r7,-404(r1)
	PPC_STORE_U32(ctx.r1.u32 + -404, ctx.r7.u32);
	// add r7,r27,r30
	ctx.r7.u64 = r27.u64 + r30.u64;
	// rlwinm r29,r7,1,0,30
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r29,r31
	ctx.r7.u64 = r29.u64 + r31.u64;
	// mulli r7,r7,28
	ctx.r7.s64 = ctx.r7.s64 * 28;
	// stw r7,-400(r1)
	PPC_STORE_U32(ctx.r1.u32 + -400, ctx.r7.u32);
	// beq cr6,0x82635f0c
	if (cr6.eq) goto loc_82635F0C;
	// add r7,r10,r30
	ctx.r7.u64 = ctx.r10.u64 + r30.u64;
	// mulli r7,r7,85
	ctx.r7.s64 = ctx.r7.s64 * 85;
	// stw r7,-396(r1)
	PPC_STORE_U32(ctx.r1.u32 + -396, ctx.r7.u32);
	// lbz r7,-555(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -555);
	// b 0x82635f30
	goto loc_82635F30;
loc_82635F0C:
	// lbz r7,-555(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -555);
	// lbz r29,-558(r1)
	r29.u64 = PPC_LOAD_U8(ctx.r1.u32 + -558);
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// add r31,r31,r10
	r31.u64 = r31.u64 + ctx.r10.u64;
	// add r6,r31,r30
	ctx.r6.u64 = r31.u64 + r30.u64;
	// rlwinm r31,r6,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r31,r29
	ctx.r6.u64 = r31.u64 + r29.u64;
	// mulli r6,r6,28
	ctx.r6.s64 = ctx.r6.s64 * 28;
	// stw r6,-396(r1)
	PPC_STORE_U32(ctx.r1.u32 + -396, ctx.r6.u32);
loc_82635F30:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82635f68
	if (cr6.eq) goto loc_82635F68;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// stw r9,-392(r1)
	PPC_STORE_U32(ctx.r1.u32 + -392, ctx.r9.u32);
	// mulli r9,r27,85
	ctx.r9.s64 = r27.s64 * 85;
	// stw r9,-388(r1)
	PPC_STORE_U32(ctx.r1.u32 + -388, ctx.r9.u32);
	// beq cr6,0x82635f58
	if (cr6.eq) goto loc_82635F58;
	// rlwinm r10,r26,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82635fbc
	goto loc_82635FBC;
loc_82635F58:
	// clrlwi r9,r7,24
	ctx.r9.u64 = ctx.r7.u32 & 0xFF;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635fbc
	goto loc_82635FBC;
loc_82635F68:
	// lbz r30,-553(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + -553);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// lbz r31,-554(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + -554);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// add r6,r31,r28
	ctx.r6.u64 = r31.u64 + r28.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r6,r6,85
	ctx.r6.s64 = ctx.r6.s64 * 85;
	// stw r6,-392(r1)
	PPC_STORE_U32(ctx.r1.u32 + -392, ctx.r6.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// mulli r9,r9,28
	ctx.r9.s64 = ctx.r9.s64 * 28;
	// stw r9,-388(r1)
	PPC_STORE_U32(ctx.r1.u32 + -388, ctx.r9.u32);
	// beq cr6,0x82635fa4
	if (cr6.eq) goto loc_82635FA4;
	// mulli r10,r10,85
	ctx.r10.s64 = ctx.r10.s64 * 85;
	// b 0x82635fbc
	goto loc_82635FBC;
loc_82635FA4:
	// clrlwi r31,r7,24
	r31.u64 = ctx.r7.u32 & 0xFF;
	// lbz r9,-552(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -552);
	// add r10,r31,r10
	ctx.r10.u64 = r31.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
loc_82635FBC:
	// stw r10,-384(r1)
	PPC_STORE_U32(ctx.r1.u32 + -384, ctx.r10.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r28,-492(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -492);
	// addi r31,r1,-316
	r31.s64 = ctx.r1.s64 + -316;
	// lwz r10,-444(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -444);
	// add r27,r10,r28
	r27.u64 = ctx.r10.u64 + r28.u64;
	// lwz r10,-436(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -436);
	// stw r9,-500(r1)
	PPC_STORE_U32(ctx.r1.u32 + -500, ctx.r9.u32);
	// add r26,r10,r28
	r26.u64 = ctx.r10.u64 + r28.u64;
	// stw r28,-480(r1)
	PPC_STORE_U32(ctx.r1.u32 + -480, r28.u32);
	// stw r27,-488(r1)
	PPC_STORE_U32(ctx.r1.u32 + -488, r27.u32);
	// stw r26,-484(r1)
	PPC_STORE_U32(ctx.r1.u32 + -484, r26.u32);
loc_82635FEC:
	// srawi r30,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r30.s64 = ctx.r9.s32 >> 1;
	// li r10,2
	ctx.r10.s64 = 2;
	// rlwinm r29,r30,1,0,30
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r25,r30,r29
	r25.u64 = r30.u64 + r29.u64;
loc_82635FFC:
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// lwz r7,-4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + -4);
	// addi r6,r1,-416
	ctx.r6.s64 = ctx.r1.s64 + -416;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// srawi r30,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r30.s64 = ctx.r9.s32 >> 1;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r22,r1,-368
	r22.s64 = ctx.r1.s64 + -368;
	// add r29,r30,r25
	r29.u64 = r30.u64 + r25.u64;
	// mulli r30,r7,297
	r30.s64 = ctx.r7.s64 * 297;
	// rlwinm r24,r29,2,0,29
	r24.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// addi r21,r1,-416
	r21.s64 = ctx.r1.s64 + -416;
	// addi r20,r1,-368
	r20.s64 = ctx.r1.s64 + -368;
	// mulli r29,r5,297
	r29.s64 = ctx.r5.s64 * 297;
	// lwzx r23,r24,r6
	r23.u64 = PPC_LOAD_U32(r24.u32 + ctx.r6.u32);
	// lwzx r24,r24,r22
	r24.u64 = PPC_LOAD_U32(r24.u32 + r22.u32);
	// mulli r22,r24,100
	r22.s64 = r24.s64 * 100;
	// mulli r6,r23,-208
	ctx.r6.s64 = r23.s64 * -208;
	// subf r22,r22,r6
	r22.s64 = ctx.r6.s64 - r22.s64;
	// subf r6,r11,r23
	ctx.r6.s64 = r23.s64 - r11.s64;
	// add r22,r22,r30
	r22.u64 = r22.u64 + r30.u64;
	// subf r24,r11,r24
	r24.s64 = r24.s64 - r11.s64;
	// mulli r23,r6,408
	r23.s64 = ctx.r6.s64 * 408;
	// rlwinm r24,r24,9,0,22
	r24.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 9) & 0xFFFFFE00;
	// add r6,r22,r4
	ctx.r6.u64 = r22.u64 + ctx.r4.u64;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r30,r24,r30
	r30.u64 = r24.u64 + r30.u64;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// srawi r24,r23,16
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xFFFF) != 0);
	r24.s64 = r23.s32 >> 16;
	// srawi r23,r30,16
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFFF) != 0);
	r23.s64 = r30.s32 >> 16;
	// srawi r30,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r30.s64 = ctx.r7.s32 >> 1;
	// addi r5,r1,-416
	ctx.r5.s64 = ctx.r1.s64 + -416;
	// addi r19,r1,-368
	r19.s64 = ctx.r1.s64 + -368;
	// lbzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// add r7,r30,r25
	ctx.r7.u64 = r30.u64 + r25.u64;
	// lbzx r24,r24,r3
	r24.u64 = PPC_LOAD_U8(r24.u32 + ctx.r3.u32);
	// rlwinm r30,r7,2,0,29
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lbzx r7,r23,r3
	ctx.r7.u64 = PPC_LOAD_U8(r23.u32 + ctx.r3.u32);
	// rlwimi r6,r24,5,11,23
	ctx.r6.u64 = (__builtin_rotateleft32(r24.u32, 5) & 0x1FFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFE000FF);
	// rlwinm r7,r7,29,3,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r6,r6,3,8,26
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFE0;
	// clrlwi r6,r6,16
	ctx.r6.u64 = ctx.r6.u32 & 0xFFFF;
	// lwzx r24,r30,r21
	r24.u64 = PPC_LOAD_U32(r30.u32 + r21.u32);
	// lwzx r30,r30,r20
	r30.u64 = PPC_LOAD_U32(r30.u32 + r20.u32);
	// or r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 | ctx.r7.u64;
	// mulli r6,r30,100
	ctx.r6.s64 = r30.s64 * 100;
	// sth r7,0(r28)
	PPC_STORE_U16(r28.u32 + 0, ctx.r7.u16);
	// mulli r7,r24,-208
	ctx.r7.s64 = r24.s64 * -208;
	// subf r23,r6,r7
	r23.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r7,r11,r24
	ctx.r7.s64 = r24.s64 - r11.s64;
	// subf r6,r11,r30
	ctx.r6.s64 = r30.s64 - r11.s64;
	// add r23,r23,r29
	r23.u64 = r23.u64 + r29.u64;
	// mulli r24,r7,408
	r24.s64 = ctx.r7.s64 * 408;
	// rlwinm r30,r6,9,0,22
	r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 9) & 0xFFFFFE00;
	// add r7,r23,r4
	ctx.r7.u64 = r23.u64 + ctx.r4.u64;
	// add r6,r24,r29
	ctx.r6.u64 = r24.u64 + r29.u64;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// add r4,r30,r29
	ctx.r4.u64 = r30.u64 + r29.u64;
	// srawi r4,r4,16
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 16;
	// lbzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r3.u32);
	// srawi r30,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r30.s64 = ctx.r10.s32 >> 1;
	// lbzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// add r30,r30,r25
	r30.u64 = r30.u64 + r25.u64;
	// rlwimi r7,r6,5,11,23
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r6.u32, 5) & 0x1FFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFE000FF);
	// lbzx r4,r4,r3
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r3.u32);
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,3,8,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFE0;
	// rlwinm r6,r4,29,3,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 29) & 0x1FFFFFFF;
	// clrlwi r7,r7,16
	ctx.r7.u64 = ctx.r7.u32 & 0xFFFF;
	// lis r4,154
	ctx.r4.s64 = 10092544;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lwzx r29,r30,r5
	r29.u64 = PPC_LOAD_U32(r30.u32 + ctx.r5.u32);
	// lwzx r24,r30,r19
	r24.u64 = PPC_LOAD_U32(r30.u32 + r19.u32);
	// mulli r30,r9,297
	r30.s64 = ctx.r9.s64 * 297;
	// sth r7,0(r27)
	PPC_STORE_U16(r27.u32 + 0, ctx.r7.u16);
	// mulli r9,r24,100
	ctx.r9.s64 = r24.s64 * 100;
	// mulli r7,r29,-208
	ctx.r7.s64 = r29.s64 * -208;
	// subf r22,r9,r7
	r22.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r9,r11,r29
	ctx.r9.s64 = r29.s64 - r11.s64;
	// mulli r23,r9,408
	r23.s64 = ctx.r9.s64 * 408;
	// subf r9,r11,r24
	ctx.r9.s64 = r24.s64 - r11.s64;
	// add r7,r23,r30
	ctx.r7.u64 = r23.u64 + r30.u64;
	// rlwinm r29,r9,9,0,22
	r29.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 9) & 0xFFFFFE00;
	// add r9,r22,r30
	ctx.r9.u64 = r22.u64 + r30.u64;
	// add r6,r29,r30
	ctx.r6.u64 = r29.u64 + r30.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// addi r5,r10,-2
	ctx.r5.s64 = ctx.r10.s64 + -2;
	// srawi r9,r9,16
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 16;
	// srawi r7,r7,16
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 16;
	// srawi r6,r6,16
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 16;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// add r28,r8,r28
	r28.u64 = ctx.r8.u64 + r28.u64;
	// add r27,r8,r27
	r27.u64 = ctx.r8.u64 + r27.u64;
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// cmpwi cr6,r5,6
	cr6.compare<int32_t>(ctx.r5.s32, 6, xer);
	// lbzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r3.u32);
	// lbzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// rlwimi r9,r7,5,11,23
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r7.u32, 5) & 0x1FFF00) | (ctx.r9.u64 & 0xFFFFFFFFFFE000FF);
	// rlwinm r7,r6,29,3,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r9,r9,3,8,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFE0;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// sth r9,0(r26)
	PPC_STORE_U16(r26.u32 + 0, ctx.r9.u16);
	// add r26,r8,r26
	r26.u64 = ctx.r8.u64 + r26.u64;
	// blt cr6,0x82635ffc
	if (cr6.lt) goto loc_82635FFC;
	// lwz r10,-500(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -500);
	// lwz r7,-480(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -480);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// lwz r10,-468(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -468);
	// add r28,r10,r7
	r28.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lwz r7,-484(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -484);
	// cmpwi cr6,r9,6
	cr6.compare<int32_t>(ctx.r9.s32, 6, xer);
	// add r26,r10,r7
	r26.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lwz r7,-488(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -488);
	// stw r9,-500(r1)
	PPC_STORE_U32(ctx.r1.u32 + -500, ctx.r9.u32);
	// add r27,r10,r7
	r27.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r28,-480(r1)
	PPC_STORE_U32(ctx.r1.u32 + -480, r28.u32);
	// stw r26,-484(r1)
	PPC_STORE_U32(ctx.r1.u32 + -484, r26.u32);
	// stw r27,-488(r1)
	PPC_STORE_U32(ctx.r1.u32 + -488, r27.u32);
	// blt cr6,0x82635fec
	if (cr6.lt) goto loc_82635FEC;
	// lwz r10,-472(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -472);
	// lwz r7,-492(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -492);
	// addi r23,r10,1
	r23.s64 = ctx.r10.s64 + 1;
	// lwz r10,-456(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -456);
	// lwz r22,-440(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -440);
	// addi r31,r10,4
	r31.s64 = ctx.r10.s64 + 4;
	// lwz r10,-432(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -432);
	// lwz r24,68(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 68);
	// cmpw cr6,r23,r22
	cr6.compare<int32_t>(r23.s32, r22.s32, xer);
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// lwz r10,-448(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -448);
	// lwz r25,60(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// addi r30,r10,2
	r30.s64 = ctx.r10.s64 + 2;
	// lwz r10,-424(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -424);
	// stw r23,-472(r1)
	PPC_STORE_U32(ctx.r1.u32 + -472, r23.u32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r10,-492(r1)
	PPC_STORE_U32(ctx.r1.u32 + -492, ctx.r10.u32);
	// blt cr6,0x826357b8
	if (cr6.lt) goto loc_826357B8;
	// lwz r27,116(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r7,52(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// lwz r6,44(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// lwz r5,36(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r29,28(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r8,-476(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -476);
	// lwz r28,-464(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -464);
loc_82636248:
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lwz r10,-460(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -460);
	// stw r8,-476(r1)
	PPC_STORE_U32(ctx.r1.u32 + -476, ctx.r8.u32);
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// stw r7,52(r1)
	PPC_STORE_U32(ctx.r1.u32 + 52, ctx.r7.u32);
	// stw r29,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, r29.u32);
	// blt cr6,0x8263570c
	if (cr6.lt) goto loc_8263570C;
loc_82636284:
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82636288"))) PPC_WEAK_FUNC(sub_82636288);
PPC_FUNC_IMPL(__imp__sub_82636288) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// lwz r9,15628(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15628);
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r24,r9,r4
	r24.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r21,128(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// add r25,r8,r7
	r25.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r8,15892(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15892);
	// add r26,r9,r10
	r26.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r17,132(r31)
	r17.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// add r22,r11,r10
	r22.u64 = r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8263635c
	if (cr6.eq) goto loc_8263635C;
	// lwz r28,156(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r29,160(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826362f4
	if (cr6.eq) goto loc_826362F4;
	// lwz r28,15308(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r29,15312(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15312);
loc_826362F4:
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826364e0
	if (cr6.eq) goto loc_826364E0;
loc_82636300:
	// lwz r11,15892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15892);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// clrlwi r8,r30,31
	ctx.r8.u64 = r30.u32 & 0x1;
	// lwz r9,96(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// add r25,r25,r9
	r25.u64 = r25.u64 + ctx.r9.u64;
	// add r24,r10,r24
	r24.u64 = ctx.r10.u64 + r24.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// blt cr6,0x82636300
	if (cr6.lt) goto loc_82636300;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
loc_8263635C:
	// li r20,0
	r20.s64 = 0;
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// beq cr6,0x826364e0
	if (cr6.eq) goto loc_826364E0;
loc_82636368:
	// mr r29,r25
	r29.u64 = r25.u64;
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x826364b8
	if (cr6.eq) goto loc_826364B8;
	// addi r19,r21,-1
	r19.s64 = r21.s64 + -1;
	// addi r18,r17,-1
	r18.s64 = r17.s64 + -1;
	// subf r23,r26,r22
	r23.s64 = r22.s64 - r26.s64;
loc_8263638C:
	// cmplw cr6,r27,r19
	cr6.compare<uint32_t>(r27.u32, r19.u32, xer);
	// beq cr6,0x826363cc
	if (cr6.eq) goto loc_826363CC;
	// cmplw cr6,r20,r18
	cr6.compare<uint32_t>(r20.u32, r18.u32, xer);
	// beq cr6,0x826363cc
	if (cr6.eq) goto loc_826363CC;
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// add r7,r23,r30
	ctx.r7.u64 = r23.u64 + r30.u64;
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,15872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15872);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8263649c
	goto loc_8263649C;
loc_826363CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82636418
	if (cr6.eq) goto loc_82636418;
	// cmplw cr6,r27,r19
	cr6.compare<uint32_t>(r27.u32, r19.u32, xer);
	// beq cr6,0x826363ec
	if (cr6.eq) goto loc_826363EC;
	// li r10,16
	ctx.r10.s64 = 16;
	// b 0x826363fc
	goto loc_826363FC;
loc_826363EC:
	// lwz r10,15316(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15316);
	// lwz r11,15308(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
loc_826363FC:
	// cmplw cr6,r20,r18
	cr6.compare<uint32_t>(r20.u32, r18.u32, xer);
	// beq cr6,0x8263640c
	if (cr6.eq) goto loc_8263640C;
	// li r11,16
	r11.s64 = 16;
	// b 0x82636470
	goto loc_82636470;
loc_8263640C:
	// lwz r11,15320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15320);
	// lwz r9,15312(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// b 0x82636450
	goto loc_82636450;
loc_82636418:
	// cmplw cr6,r27,r19
	cr6.compare<uint32_t>(r27.u32, r19.u32, xer);
	// beq cr6,0x82636428
	if (cr6.eq) goto loc_82636428;
	// li r10,16
	ctx.r10.s64 = 16;
	// b 0x82636438
	goto loc_82636438;
loc_82636428:
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
loc_82636438:
	// cmplw cr6,r20,r18
	cr6.compare<uint32_t>(r20.u32, r18.u32, xer);
	// beq cr6,0x82636448
	if (cr6.eq) goto loc_82636448;
	// li r11,16
	r11.s64 = 16;
	// b 0x82636470
	goto loc_82636470;
loc_82636448:
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r9,160(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 160);
loc_82636450:
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_82636470:
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r7,r23,r30
	ctx.r7.u64 = r23.u64 + r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r16,15876(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 15876);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r16
	ctr.u64 = r16.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8263649C:
	// lwz r11,15632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15632);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplw cr6,r27,r21
	cr6.compare<uint32_t>(r27.u32, r21.u32, xer);
	// blt cr6,0x8263638c
	if (cr6.lt) goto loc_8263638C;
loc_826364B8:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,15644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// add r25,r9,r25
	r25.u64 = ctx.r9.u64 + r25.u64;
	// add r24,r10,r24
	r24.u64 = ctx.r10.u64 + r24.u64;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// cmplw cr6,r20,r17
	cr6.compare<uint32_t>(r20.u32, r17.u32, xer);
	// blt cr6,0x82636368
	if (cr6.lt) goto loc_82636368;
loc_826364E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_826364EC"))) PPC_WEAK_FUNC(sub_826364EC);
PPC_FUNC_IMPL(__imp__sub_826364EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826364F0"))) PPC_WEAK_FUNC(sub_826364F0);
PPC_FUNC_IMPL(__imp__sub_826364F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// lwz r9,15628(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15628);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// add r21,r9,r4
	r21.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lwz r19,132(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// add r22,r8,r7
	r22.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r24,128(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// add r25,r9,r10
	r25.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r20,r11,r10
	r20.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x82636668
	if (cr6.eq) goto loc_82636668;
loc_82636540:
	// mr r29,r22
	r29.u64 = r22.u64;
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r28,r21
	r28.u64 = r21.u64;
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x8263663c
	if (cr6.eq) goto loc_8263663C;
	// addi r17,r24,-1
	r17.s64 = r24.s64 + -1;
	// addi r18,r19,-1
	r18.s64 = r19.s64 + -1;
	// subf r26,r25,r20
	r26.s64 = r20.s64 - r25.s64;
loc_82636564:
	// cmplw cr6,r27,r17
	cr6.compare<uint32_t>(r27.u32, r17.u32, xer);
	// beq cr6,0x826365ac
	if (cr6.eq) goto loc_826365AC;
	// cmplw cr6,r23,r18
	cr6.compare<uint32_t>(r23.u32, r18.u32, xer);
	// beq cr6,0x826365a4
	if (cr6.eq) goto loc_826365A4;
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// add r7,r26,r30
	ctx.r7.u64 = r26.u64 + r30.u64;
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r11,15872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15872);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82636620
	goto loc_82636620;
loc_826365A4:
	// li r10,16
	ctx.r10.s64 = 16;
	// b 0x826365bc
	goto loc_826365BC;
loc_826365AC:
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
loc_826365BC:
	// cmplw cr6,r23,r18
	cr6.compare<uint32_t>(r23.u32, r18.u32, xer);
	// beq cr6,0x826365cc
	if (cr6.eq) goto loc_826365CC;
	// li r11,16
	r11.s64 = 16;
	// b 0x826365f4
	goto loc_826365F4;
loc_826365CC:
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r9,160(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r11,r11,r8
	r11.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_826365F4:
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r7,r26,r30
	ctx.r7.u64 = r26.u64 + r30.u64;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r16,15876(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 15876);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r16
	ctr.u64 = r16.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82636620:
	// lwz r11,15632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15632);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// add r28,r28,r11
	r28.u64 = r28.u64 + r11.u64;
	// cmplw cr6,r27,r24
	cr6.compare<uint32_t>(r27.u32, r24.u32, xer);
	// blt cr6,0x82636564
	if (cr6.lt) goto loc_82636564;
loc_8263663C:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,15644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r22,r9,r22
	r22.u64 = ctx.r9.u64 + r22.u64;
	// add r21,r10,r21
	r21.u64 = ctx.r10.u64 + r21.u64;
	// cmplw cr6,r23,r19
	cr6.compare<uint32_t>(r23.u32, r19.u32, xer);
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// add r20,r11,r20
	r20.u64 = r11.u64 + r20.u64;
	// blt cr6,0x82636540
	if (cr6.lt) goto loc_82636540;
loc_82636668:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_82636674"))) PPC_WEAK_FUNC(sub_82636674);
PPC_FUNC_IMPL(__imp__sub_82636674) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82636678"))) PPC_WEAK_FUNC(sub_82636678);
PPC_FUNC_IMPL(__imp__sub_82636678) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r15,r11,28
	r15.u64 = r11.u32 & 0xF;
	// addi r11,r11,15
	r11.s64 = r11.s64 + 15;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// addi r9,r10,15
	ctx.r9.s64 = ctx.r10.s64 + 15;
	// addze r17,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r17.s64 = temp.s64;
	// stw r15,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r15.u32);
	// srawi r11,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	r11.s64 = ctx.r9.s32 >> 4;
	// clrlwi r14,r10,28
	r14.u64 = ctx.r10.u32 & 0xF;
	// addze r16,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r16.s64 = temp.s64;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// stw r14,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r14.u32);
	// stw r16,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r16.u32);
	// bne cr6,0x826366dc
	if (!cr6.eq) goto loc_826366DC;
	// li r15,16
	r15.s64 = 16;
	// stw r15,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r15.u32);
loc_826366DC:
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// bne cr6,0x826366ec
	if (!cr6.eq) goto loc_826366EC;
	// li r14,16
	r14.s64 = 16;
	// stw r14,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r14.u32);
loc_826366EC:
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// li r22,0
	r22.s64 = 0;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lwz r3,96(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mullw r30,r10,r7
	r30.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r8,224(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// lwz r20,21444(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 21444);
	// lwz r19,21448(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 21448);
	// lwz r18,21452(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + 21452);
	// mullw r10,r3,r10
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r10.s32);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r3,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 1;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r21,r6,r9
	r21.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r23,r9,r11
	r23.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// beq cr6,0x826368e0
	if (cr6.eq) goto loc_826368E0;
loc_8263676C:
	// mr r29,r21
	r29.u64 = r21.u64;
	// mr r30,r23
	r30.u64 = r23.u64;
	// mr r28,r20
	r28.u64 = r20.u64;
	// mr r27,r19
	r27.u64 = r19.u64;
	// mr r26,r18
	r26.u64 = r18.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// beq cr6,0x826368a4
	if (cr6.eq) goto loc_826368A4;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// subf r24,r23,r11
	r24.s64 = r11.s64 - r23.s64;
loc_82636794:
	// addi r11,r17,-1
	r11.s64 = r17.s64 + -1;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// beq cr6,0x82636804
	if (cr6.eq) goto loc_82636804;
	// addi r11,r16,-1
	r11.s64 = r16.s64 + -1;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// beq cr6,0x826367fc
	if (cr6.eq) goto loc_826367FC;
	// lwz r6,15884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15884);
	// add r9,r24,r30
	ctx.r9.u64 = r24.u64 + r30.u64;
	// lwz r11,15624(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15624);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r4,15620(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r3,108(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// stw r6,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r6.u32);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8263687c
	goto loc_8263687C;
loc_826367FC:
	// li r3,16
	ctx.r3.s64 = 16;
	// b 0x82636808
	goto loc_82636808;
loc_82636804:
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
loc_82636808:
	// addi r11,r16,-1
	r11.s64 = r16.s64 + -1;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// li r11,16
	r11.s64 = 16;
	// bne cr6,0x8263681c
	if (!cr6.eq) goto loc_8263681C;
	// mr r11,r14
	r11.u64 = r14.u64;
loc_8263681C:
	// lwz r5,15888(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 15888);
	// add r9,r24,r30
	ctx.r9.u64 = r24.u64 + r30.u64;
	// lwz r16,15624(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 15624);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r15,15620(r31)
	r15.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r14,108(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r5,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r5.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r16,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r16.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r15,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r15.u32);
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r14.u32);
	// lwz r16,140(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// mtctr r16
	ctr.u64 = r16.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r14,132(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r15,136(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r16,144(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_8263687C:
	// lwz r11,15640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15640);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// lwz r10,15632(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15632);
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// add r26,r11,r26
	r26.u64 = r11.u64 + r26.u64;
	// cmplw cr6,r25,r17
	cr6.compare<uint32_t>(r25.u32, r17.u32, xer);
	// blt cr6,0x82636794
	if (cr6.lt) goto loc_82636794;
loc_826368A4:
	// lwz r9,15644(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// add r20,r9,r20
	r20.u64 = ctx.r9.u64 + r20.u64;
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r23,r11,r23
	r23.u64 = r11.u64 + r23.u64;
	// lwz r10,15652(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15652);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r8,100(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// add r19,r10,r19
	r19.u64 = ctx.r10.u64 + r19.u64;
	// add r21,r8,r21
	r21.u64 = ctx.r8.u64 + r21.u64;
	// add r18,r10,r18
	r18.u64 = ctx.r10.u64 + r18.u64;
	// cmplw cr6,r22,r16
	cr6.compare<uint32_t>(r22.u32, r16.u32, xer);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// blt cr6,0x8263676c
	if (cr6.lt) goto loc_8263676C;
loc_826368E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826368EC"))) PPC_WEAK_FUNC(sub_826368EC);
PPC_FUNC_IMPL(__imp__sub_826368EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826368F0"))) PPC_WEAK_FUNC(sub_826368F0);
PPC_FUNC_IMPL(__imp__sub_826368F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// lwz r9,21440(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 21440);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// lwz r15,128(r31)
	r15.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// lwz r14,132(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// add r25,r9,r10
	r25.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r16,r11,r10
	r16.u64 = r11.u64 + ctx.r10.u64;
	// lwz r29,156(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r26,160(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 160);
	// add r18,r8,r7
	r18.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r15,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r15.u32);
	// stw r14,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r14.u32);
	// stw r16,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r16.u32);
	// bne cr6,0x8263697c
	if (!cr6.eq) goto loc_8263697C;
	// lwz r30,21456(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// lwz r28,21460(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// rlwinm r11,r30,4,0,27
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r27,21464(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// lwz r23,21444(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 21444);
	// lwz r22,21448(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 21448);
	// lwz r21,21452(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 21452);
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// rlwinm r11,r28,3,0,28
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// rlwinm r11,r27,3,0,28
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// b 0x826369bc
	goto loc_826369BC;
loc_8263697C:
	// lwz r8,15644(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// lwz r10,15628(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15628);
	// lwz r11,15652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15652);
	// add r23,r10,r4
	r23.u64 = ctx.r10.u64 + ctx.r4.u64;
	// lwz r10,15576(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15576);
	// lwz r9,15580(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15580);
	// stw r8,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r8.u32);
	// add r22,r10,r23
	r22.u64 = ctx.r10.u64 + r23.u64;
	// lwz r8,15620(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// add r21,r9,r23
	r21.u64 = ctx.r9.u64 + r23.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
loc_826369BC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826369d8
	if (cr6.eq) goto loc_826369D8;
	// lwz r29,15308(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r26,15312(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 15312);
loc_826369D8:
	// lwz r11,15896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15896);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82636a60
	if (cr6.eq) goto loc_82636A60;
	// lwz r7,96(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r30,r29,31,1,31
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r29,r26,31,1,31
	r29.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r11,15896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15896);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lwz r11,15896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15896);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd10
	return;
loc_82636A60:
	// li r20,0
	r20.s64 = 0;
	// cmplwi cr6,r14,0
	cr6.compare<uint32_t>(r14.u32, 0, xer);
	// beq cr6,0x82636c54
	if (cr6.eq) goto loc_82636C54;
loc_82636A6C:
	// mr r29,r18
	r29.u64 = r18.u64;
	// li r19,0
	r19.s64 = 0;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x82636c18
	if (cr6.eq) goto loc_82636C18;
	// addi r17,r15,-1
	r17.s64 = r15.s64 + -1;
	// mr r30,r25
	r30.u64 = r25.u64;
	// subf r28,r25,r16
	r28.s64 = r16.s64 - r25.s64;
	// subf r27,r25,r21
	r27.s64 = r21.s64 - r25.s64;
	// subf r26,r25,r22
	r26.s64 = r22.s64 - r25.s64;
	// subf r24,r18,r23
	r24.s64 = r23.s64 - r18.s64;
loc_82636A94:
	// cmplw cr6,r19,r17
	cr6.compare<uint32_t>(r19.u32, r17.u32, xer);
	// beq cr6,0x82636af8
	if (cr6.eq) goto loc_82636AF8;
	// addi r11,r14,-1
	r11.s64 = r14.s64 + -1;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x82636af8
	if (cr6.eq) goto loc_82636AF8;
	// lwz r6,15884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15884);
	// add r9,r28,r30
	ctx.r9.u64 = r28.u64 + r30.u64;
	// lwz r11,15624(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15624);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r4,15620(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r3,108(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r5,r26,r30
	ctx.r5.u64 = r26.u64 + r30.u64;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// stw r6,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r6.u32);
	// add r6,r27,r30
	ctx.r6.u64 = r27.u64 + r30.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r4,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
	// add r4,r24,r29
	ctx.r4.u64 = r24.u64 + r29.u64;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82636c04
	goto loc_82636C04;
loc_82636AF8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2e70
	sub_825E2E70(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82636b48
	if (cr6.eq) goto loc_82636B48;
	// cmplw cr6,r19,r17
	cr6.compare<uint32_t>(r19.u32, r17.u32, xer);
	// beq cr6,0x82636b18
	if (cr6.eq) goto loc_82636B18;
	// li r3,16
	ctx.r3.s64 = 16;
	// b 0x82636b28
	goto loc_82636B28;
loc_82636B18:
	// lwz r11,15308(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r10,15316(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15316);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
loc_82636B28:
	// addi r11,r14,-1
	r11.s64 = r14.s64 + -1;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x82636b3c
	if (cr6.eq) goto loc_82636B3C;
	// li r11,16
	r11.s64 = 16;
	// b 0x82636ba4
	goto loc_82636BA4;
loc_82636B3C:
	// lwz r11,15320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15320);
	// lwz r10,15312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// b 0x82636b84
	goto loc_82636B84;
loc_82636B48:
	// cmplw cr6,r19,r17
	cr6.compare<uint32_t>(r19.u32, r17.u32, xer);
	// beq cr6,0x82636b58
	if (cr6.eq) goto loc_82636B58;
	// li r3,16
	ctx.r3.s64 = 16;
	// b 0x82636b68
	goto loc_82636B68;
loc_82636B58:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
loc_82636B68:
	// addi r11,r14,-1
	r11.s64 = r14.s64 + -1;
	// cmplw cr6,r20,r11
	cr6.compare<uint32_t>(r20.u32, r11.u32, xer);
	// beq cr6,0x82636b7c
	if (cr6.eq) goto loc_82636B7C;
	// li r11,16
	r11.s64 = 16;
	// b 0x82636ba4
	goto loc_82636BA4;
loc_82636B7C:
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// lwz r10,160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 160);
loc_82636B84:
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subfic r11,r11,16
	xer.ca = r11.u32 <= 16;
	r11.s64 = 16 - r11.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_82636BA4:
	// lwz r5,15888(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 15888);
	// add r9,r28,r30
	ctx.r9.u64 = r28.u64 + r30.u64;
	// lwz r16,15624(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 15624);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r15,15620(r31)
	r15.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r14,108(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// add r6,r27,r30
	ctx.r6.u64 = r27.u64 + r30.u64;
	// stw r3,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r3.u32);
	// add r4,r24,r29
	ctx.r4.u64 = r24.u64 + r29.u64;
	// stw r5,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r5.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r16,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r16.u32);
	// add r5,r26,r30
	ctx.r5.u64 = r26.u64 + r30.u64;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r15,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r15.u32);
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r14.u32);
	// lwz r16,144(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mtctr r16
	ctr.u64 = r16.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r16,140(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r14,148(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r15,152(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
loc_82636C04:
	// addi r19,r19,1
	r19.s64 = r19.s64 + 1;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplw cr6,r19,r15
	cr6.compare<uint32_t>(r19.u32, r15.u32, xer);
	// blt cr6,0x82636a94
	if (cr6.lt) goto loc_82636A94;
loc_82636C18:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r16,r11,r16
	r16.u64 = r11.u64 + r16.u64;
	// add r18,r18,r10
	r18.u64 = r18.u64 + ctx.r10.u64;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r23,r9,r23
	r23.u64 = ctx.r9.u64 + r23.u64;
	// lwz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r21,r10,r21
	r21.u64 = ctx.r10.u64 + r21.u64;
	// add r22,r9,r22
	r22.u64 = ctx.r9.u64 + r22.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// stw r16,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r16.u32);
	// cmplw cr6,r20,r14
	cr6.compare<uint32_t>(r20.u32, r14.u32, xer);
	// blt cr6,0x82636a6c
	if (cr6.lt) goto loc_82636A6C;
loc_82636C54:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82636C60"))) PPC_WEAK_FUNC(sub_82636C60);
PPC_FUNC_IMPL(__imp__sub_82636C60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// addi r28,r5,2
	r28.s64 = ctx.r5.s64 + 2;
	// addi r29,r4,4
	r29.s64 = ctx.r4.s64 + 4;
	// li r27,16
	r27.s64 = 16;
loc_82636C74:
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// subf r30,r6,r7
	r30.s64 = ctx.r7.s64 - ctx.r6.s64;
	// li r3,4
	ctx.r3.s64 = 4;
loc_82636C88:
	// lbz r31,-2(r5)
	r31.u64 = PPC_LOAD_U8(ctx.r5.u32 + -2);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stb r31,-4(r11)
	PPC_STORE_U8(r11.u32 + -4, r31.u8);
	// lbz r31,-1(r5)
	r31.u64 = PPC_LOAD_U8(ctx.r5.u32 + -1);
	// stb r31,-2(r11)
	PPC_STORE_U8(r11.u32 + -2, r31.u8);
	// lbz r31,0(r5)
	r31.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// lbz r31,1(r5)
	r31.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// stb r31,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r31.u8);
	// lbz r31,0(r4)
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stb r31,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r31.u8);
	// stb r31,-3(r11)
	PPC_STORE_U8(r11.u32 + -3, r31.u8);
	// lbzx r31,r30,r4
	r31.u64 = PPC_LOAD_U8(r30.u32 + ctx.r4.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stb r31,3(r11)
	PPC_STORE_U8(r11.u32 + 3, r31.u8);
	// stb r31,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, r31.u8);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne cr6,0x82636c88
	if (!cr6.eq) goto loc_82636C88;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r29,r29,r10
	r29.u64 = r29.u64 + ctx.r10.u64;
	// add r28,r28,r8
	r28.u64 = r28.u64 + ctx.r8.u64;
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82636c74
	if (!cr6.eq) goto loc_82636C74;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82636CF8"))) PPC_WEAK_FUNC(sub_82636CF8);
PPC_FUNC_IMPL(__imp__sub_82636CF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// li r3,8
	ctx.r3.s64 = 8;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82636D0C:
	// lbz r31,0(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r11,r5,r8
	r11.u64 = ctx.r5.u64 + ctx.r8.u64;
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// add r10,r30,r4
	ctx.r10.u64 = r30.u64 + ctx.r4.u64;
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,1(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// lbz r27,0(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r29.u32);
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,0(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r31.u32);
	// lbz r31,1(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,3(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 3);
	// lbz r27,2(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r29.u32);
	// lbz r29,3(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r28,2(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// lbz r31,2(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// lbz r29,2(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,5(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 5);
	// lbz r27,4(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r29.u32);
	// lbz r29,5(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r28,4(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r31.u32);
	// lbz r31,3(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// lbz r29,3(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,7(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 7);
	// lbz r27,6(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 6);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r29.u32);
	// lbz r29,7(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lbz r28,6(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, r31.u32);
	// lbz r31,4(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 4);
	// lbz r29,4(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,9(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 9);
	// lbz r27,8(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 8);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, r29.u32);
	// lbz r29,9(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 9);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lbz r28,8(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r31.u32);
	// lbz r31,5(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 5);
	// lbz r29,5(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,11(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 11);
	// lbz r27,10(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 10);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, r29.u32);
	// lbz r29,11(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 11);
	// lbz r28,10(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 10);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r31.u32);
	// lbz r31,6(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 6);
	// lbz r29,6(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,13(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 13);
	// lbz r27,12(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 12);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r29.u32);
	// lbz r29,13(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 13);
	// lbz r28,12(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, r31.u32);
	// lbz r31,7(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 7);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// lbz r29,7(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,15(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 15);
	// lbz r27,14(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 14);
	// add r5,r11,r8
	ctx.r5.u64 = r11.u64 + ctx.r8.u64;
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, r29.u32);
	// add r4,r30,r10
	ctx.r4.u64 = r30.u64 + ctx.r10.u64;
	// lbz r29,15(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 15);
	// lbz r11,14(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 14);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r11,r29,r11
	r11.u64 = r29.u64 | r11.u64;
	// or r11,r11,r31
	r11.u64 = r11.u64 | r31.u64;
	// stw r11,28(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28, r11.u32);
	// bne cr6,0x82636d0c
	if (!cr6.eq) goto loc_82636D0C;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82636F54"))) PPC_WEAK_FUNC(sub_82636F54);
PPC_FUNC_IMPL(__imp__sub_82636F54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82636F58"))) PPC_WEAK_FUNC(sub_82636F58);
PPC_FUNC_IMPL(__imp__sub_82636F58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// li r3,4
	ctx.r3.s64 = 4;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82636F6C:
	// lbz r31,0(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// add r11,r5,r8
	r11.u64 = ctx.r5.u64 + ctx.r8.u64;
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// add r10,r30,r4
	ctx.r10.u64 = r30.u64 + ctx.r4.u64;
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,1(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// lbz r27,0(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r29.u32);
	// lbz r29,1(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r28,0(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r31.u32);
	// lbz r31,1(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,3(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 3);
	// lbz r27,2(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r29.u32);
	// lbz r29,3(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r28,2(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// lbz r31,2(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// lbz r29,2(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,5(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 5);
	// lbz r27,4(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r29.u32);
	// lbz r29,5(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r28,4(r11)
	r28.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// or r31,r29,r31
	r31.u64 = r29.u64 | r31.u64;
	// stw r31,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r31.u32);
	// lbz r31,3(r7)
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// lbz r29,3(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rotlwi r31,r31,16
	r31.u64 = __builtin_rotateleft32(r31.u32, 16);
	// lbz r28,7(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 7);
	// lbz r27,6(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 6);
	// add r5,r11,r8
	ctx.r5.u64 = r11.u64 + ctx.r8.u64;
	// or r31,r31,r29
	r31.u64 = r31.u64 | r29.u64;
	// rotlwi r29,r28,16
	r29.u64 = __builtin_rotateleft32(r28.u32, 16);
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// or r29,r29,r27
	r29.u64 = r29.u64 | r27.u64;
	// or r29,r29,r31
	r29.u64 = r29.u64 | r31.u64;
	// stw r29,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r29.u32);
	// add r4,r30,r10
	ctx.r4.u64 = r30.u64 + ctx.r10.u64;
	// lbz r29,7(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lbz r11,6(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// rotlwi r29,r29,16
	r29.u64 = __builtin_rotateleft32(r29.u32, 16);
	// or r11,r29,r11
	r11.u64 = r29.u64 | r11.u64;
	// or r11,r11,r31
	r11.u64 = r11.u64 | r31.u64;
	// stw r11,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, r11.u32);
	// bne cr6,0x82636f6c
	if (!cr6.eq) goto loc_82636F6C;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_826370A4"))) PPC_WEAK_FUNC(sub_826370A4);
PPC_FUNC_IMPL(__imp__sub_826370A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

