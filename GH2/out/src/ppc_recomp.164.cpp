#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_825CA354"))) PPC_WEAK_FUNC(sub_825CA354);
PPC_FUNC_IMPL(__imp__sub_825CA354) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CA358"))) PPC_WEAK_FUNC(sub_825CA358);
PPC_FUNC_IMPL(__imp__sub_825CA358) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,548(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// lhz r25,34(r29)
	r25.u64 = PPC_LOAD_U16(r29.u32 + 34);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ca420
	if (cr6.eq) goto loc_825CA420;
	// li r26,1
	r26.s64 = 1;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r25,1
	cr6.compare<int32_t>(r25.s32, 1, xer);
	// blt cr6,0x825ca40c
	if (cr6.lt) goto loc_825CA40C;
	// li r30,4
	r30.s64 = 4;
loc_825CA38C:
	// cmpwi cr6,r26,6
	cr6.compare<int32_t>(r26.s32, 6, xer);
	// ble cr6,0x825ca3dc
	if (!cr6.gt) goto loc_825CA3DC;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x825ca3dc
	if (!cr6.gt) goto loc_825CA3DC;
	// mr r31,r27
	r31.u64 = r27.u64;
	// mr r28,r26
	r28.u64 = r26.u64;
loc_825CA3A4:
	// lwz r11,548(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// lwzx r11,r30,r11
	r11.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ca3cc
	if (cr6.eq) goto loc_825CA3CC;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// lwz r11,548(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// lwzx r11,r30,r11
	r11.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// stwx r27,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, r27.u32);
loc_825CA3CC:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x825ca3a4
	if (!cr6.eq) goto loc_825CA3A4;
loc_825CA3DC:
	// lwz r11,548(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// lwzx r10,r30,r11
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ca3fc
	if (cr6.eq) goto loc_825CA3FC;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// lwz r11,548(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// stwx r27,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, r27.u32);
loc_825CA3FC:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r26,r25
	cr6.compare<int32_t>(r26.s32, r25.s32, xer);
	// ble cr6,0x825ca38c
	if (!cr6.gt) goto loc_825CA38C;
loc_825CA40C:
	// lwz r3,548(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 548);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca420
	if (cr6.eq) goto loc_825CA420;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r27,548(r29)
	PPC_STORE_U32(r29.u32 + 548, r27.u32);
loc_825CA420:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825CA428"))) PPC_WEAK_FUNC(sub_825CA428);
PPC_FUNC_IMPL(__imp__sub_825CA428) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,-1
	r11.s64 = -1;
	// and r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 & ctx.r4.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r11,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, r11.u16);
	// beqlr cr6
	if (cr6.eq) return;
	// li r11,1
	r11.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
loc_825CA444:
	// and r10,r11,r3
	ctx.r10.u64 = r11.u64 & ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ca45c
	if (cr6.eq) goto loc_825CA45C;
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sth r10,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r10.u16);
loc_825CA45C:
	// and r10,r11,r4
	ctx.r10.u64 = r11.u64 & ctx.r4.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// blt cr6,0x825ca444
	if (cr6.lt) goto loc_825CA444;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CA47C"))) PPC_WEAK_FUNC(sub_825CA47C);
PPC_FUNC_IMPL(__imp__sub_825CA47C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CA480"))) PPC_WEAK_FUNC(sub_825CA480);
PPC_FUNC_IMPL(__imp__sub_825CA480) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,348(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ca600
	if (cr6.eq) goto loc_825CA600;
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ca4fc
	if (!cr6.gt) goto loc_825CA4FC;
loc_825CA4AC:
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ca4ec
	if (!cr6.gt) goto loc_825CA4EC;
	// rlwinm r28,r27,2,0,29
	r28.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// li r30,0
	r30.s64 = 0;
loc_825CA4C0:
	// lwz r11,348(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// li r5,28
	ctx.r5.s64 = 28;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwzx r11,r28,r11
	r11.u64 = PPC_LOAD_U32(r28.u32 + r11.u32);
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825ca4c0
	if (cr6.lt) goto loc_825CA4C0;
loc_825CA4EC:
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825ca4ac
	if (cr6.lt) goto loc_825CA4AC;
loc_825CA4FC:
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ca600
	if (!cr6.gt) goto loc_825CA600;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r26,0
	r26.s64 = 0;
	// li r29,1
	r29.s64 = 1;
loc_825CA518:
	// lwz r9,340(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 340);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,344(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 344);
	// add r10,r26,r10
	ctx.r10.u64 = r26.u64 + ctx.r10.u64;
	// lwzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ca5e8
	if (!cr6.gt) goto loc_825CA5E8;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// slw r28,r29,r27
	r28.u64 = r27.u8 & 0x20 ? 0 : (r29.u32 << (r27.u8 & 0x3F));
loc_825CA53C:
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mullw r9,r9,r28
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r28.s32);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addze r6,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r6.s64 = temp.s64;
	// ble cr6,0x825ca5d0
	if (!cr6.gt) goto loc_825CA5D0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825CA56C:
	// lwz r9,344(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 344);
	// li r11,0
	r11.s64 = 0;
	// add r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 + ctx.r9.u64;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r25,0(r8)
	r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// slw r9,r29,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// mullw r25,r9,r25
	r25.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// cmpw cr6,r25,r6
	cr6.compare<int32_t>(r25.s32, ctx.r6.s32, xer);
	// bge cr6,0x825ca5a8
	if (!cr6.lt) goto loc_825CA5A8;
loc_825CA590:
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r25,0(r8)
	r25.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mullw r25,r25,r9
	r25.s64 = int64_t(r25.s32) * int64_t(ctx.r9.s32);
	// cmpw cr6,r25,r6
	cr6.compare<int32_t>(r25.s32, ctx.r6.s32, xer);
	// blt cr6,0x825ca590
	if (cr6.lt) goto loc_825CA590;
loc_825CA5A8:
	// lwz r9,348(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r7,r7,116
	ctx.r7.s64 = ctx.r7.s64 + 116;
	// lwzx r9,r3,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r9.u32);
	// lwzx r9,r9,r5
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// stbx r11,r9,r4
	PPC_STORE_U8(ctx.r9.u32 + ctx.r4.u32, r11.u8);
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825ca56c
	if (cr6.lt) goto loc_825CA56C;
loc_825CA5D0:
	// lwz r10,340(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 340);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ca53c
	if (cr6.lt) goto loc_825CA53C;
loc_825CA5E8:
	// lwz r11,244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 244);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r26,r26,116
	r26.s64 = r26.s64 + 116;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825ca518
	if (cr6.lt) goto loc_825CA518;
loc_825CA600:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825CA608"))) PPC_WEAK_FUNC(sub_825CA608);
PPC_FUNC_IMPL(__imp__sub_825CA608) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,256(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// li r30,0
	r30.s64 = 0;
	// rotlwi r31,r11,1
	r31.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r6,r11,r5
	ctx.r6.s32 = r11.s32 / ctx.r5.s32;
	// addi r7,r31,-1
	ctx.r7.s64 = r31.s64 + -1;
	// twllei r5,0
	// andc r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 & ~ctx.r7.u64;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// twlgei r7,-1
	// ble cr6,0x825ca64c
	if (!cr6.gt) goto loc_825CA64C;
loc_825CA63C:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srw r7,r6,r30
	ctx.r7.u64 = r30.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r30.u8 & 0x3F));
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bgt cr6,0x825ca63c
	if (cr6.gt) goto loc_825CA63C;
loc_825CA64C:
	// divw r5,r11,r8
	ctx.r5.s32 = r11.s32 / ctx.r8.s32;
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// twllei r8,0
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r6,0
	ctx.r6.s64 = 0;
	// andc r11,r8,r11
	r11.u64 = ctx.r8.u64 & ~r11.u64;
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// twlgei r11,-1
	// ble cr6,0x825ca680
	if (!cr6.gt) goto loc_825CA680;
loc_825CA670:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// srw r11,r5,r6
	r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r6.u8 & 0x3F));
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825ca670
	if (cr6.gt) goto loc_825CA670;
loc_825CA680:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ca6c0
	if (!cr6.gt) goto loc_825CA6C0;
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r30,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
loc_825CA694:
	// lwz r7,348(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	// lwzx r7,r7,r8
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	// lwzx r7,r7,r6
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r6.u32);
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rotlwi r7,r7,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// lwzx r7,r7,r4
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x825ca694
	if (cr6.lt) goto loc_825CA694;
loc_825CA6C0:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CA6D0"))) PPC_WEAK_FUNC(sub_825CA6D0);
PPC_FUNC_IMPL(__imp__sub_825CA6D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// mulli r30,r31,152
	r30.s64 = r31.s64 * 152;
	// stw r29,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r29.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r25,r29
	r25.u64 = r29.u64;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r3.u32);
	// bne cr6,0x825ca718
	if (!cr6.eq) goto loc_825CA718;
loc_825CA708:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_825CA718:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r28,r29
	r28.u64 = r29.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825ca7bc
	if (!cr6.gt) goto loc_825CA7BC;
	// rlwinm r26,r31,2,0,29
	r26.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
loc_825CA734:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r3.u32);
	// beq cr6,0x825ca708
	if (cr6.eq) goto loc_825CA708;
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// ble cr6,0x825ca78c
	if (!cr6.gt) goto loc_825CA78C;
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r3,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r3.s64 = temp.s64;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,136(r30)
	PPC_STORE_U32(r30.u32 + 136, ctx.r3.u32);
	// beq cr6,0x825ca708
	if (cr6.eq) goto loc_825CA708;
	// addi r11,r31,-1
	r11.s64 = r31.s64 + -1;
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r31
	r11.s64 = int64_t(r11.s32) * int64_t(r31.s32);
	// rlwinm r5,r11,31,1,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825CA78C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,140(r30)
	PPC_STORE_U32(r30.u32 + 140, ctx.r3.u32);
	// beq cr6,0x825ca708
	if (cr6.eq) goto loc_825CA708;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r29,r29,152
	r29.s64 = r29.s64 + 152;
	// cmpw cr6,r28,r31
	cr6.compare<int32_t>(r28.s32, r31.s32, xer);
	// blt cr6,0x825ca734
	if (cr6.lt) goto loc_825CA734;
loc_825CA7BC:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825CA7C8"))) PPC_WEAK_FUNC(sub_825CA7C8);
PPC_FUNC_IMPL(__imp__sub_825CA7C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x825ca890
	if (cr6.eq) goto loc_825CA890;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ca890
	if (cr6.eq) goto loc_825CA890;
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825ca87c
	if (!cr6.gt) goto loc_825CA87C;
	// mr r29,r30
	r29.u64 = r30.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
loc_825CA800:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca81c
	if (cr6.eq) goto loc_825CA81C;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
loc_825CA81C:
	// lwz r3,136(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca830
	if (cr6.eq) goto loc_825CA830;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,136(r31)
	PPC_STORE_U32(r31.u32 + 136, r30.u32);
loc_825CA830:
	// lwz r3,140(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca844
	if (cr6.eq) goto loc_825CA844;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r30.u32);
loc_825CA844:
	// lwz r3,144(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 144);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca858
	if (cr6.eq) goto loc_825CA858;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,144(r31)
	PPC_STORE_U32(r31.u32 + 144, r30.u32);
loc_825CA858:
	// lwz r3,148(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca86c
	if (cr6.eq) goto loc_825CA86C;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r30.u32);
loc_825CA86C:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r29,r29,152
	r29.s64 = r29.s64 + 152;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x825ca800
	if (!cr6.eq) goto loc_825CA800;
loc_825CA87C:
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825ca890
	if (cr6.eq) goto loc_825CA890;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r30.u32);
loc_825CA890:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825CA898"))) PPC_WEAK_FUNC(sub_825CA898);
PPC_FUNC_IMPL(__imp__sub_825CA898) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// ble cr6,0x825ca9e4
	if (!cr6.gt) goto loc_825CA9E4;
	// addi r19,r28,-1
	r19.s64 = r28.s64 + -1;
	// li r23,0
	r23.s64 = 0;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x825ca9e4
	if (!cr6.gt) goto loc_825CA9E4;
	// mullw r11,r19,r6
	r11.s64 = int64_t(r19.s32) * int64_t(ctx.r6.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r24,r28,2,0,29
	r24.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r11,r4
	r26.u64 = r11.u64 + ctx.r4.u64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r18,r6,2,0,29
	r18.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r22,r11,19824
	r22.s64 = r11.s64 + 19824;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r21,r11,19568
	r21.s64 = r11.s64 + 19568;
loc_825CA8F4:
	// lbzx r11,r23,r20
	r11.u64 = PPC_LOAD_U8(r23.u32 + r20.u32);
	// addi r10,r21,128
	ctx.r10.s64 = r21.s64 + 128;
	// addi r9,r22,128
	ctx.r9.s64 = r22.s64 + 128;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwzx r30,r11,r10
	r30.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r31,r11,r9
	r31.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825ca9d4
	if (!cr6.gt) goto loc_825CA9D4;
	// neg r11,r31
	r11.s64 = -r31.s64;
	// extsw r3,r31
	ctx.r3.s64 = r31.s32;
	// extsw r31,r11
	r31.s64 = r11.s32;
	// extsw r7,r30
	ctx.r7.s64 = r30.s32;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// subf r6,r27,r25
	ctx.r6.s64 = r25.s64 - r27.s64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_825CA95C:
	// lwzx r4,r6,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// mulld r4,r4,r7
	ctx.r4.s64 = ctx.r4.s64 * ctx.r7.s64;
	// mulld r5,r5,r3
	ctx.r5.s64 = ctx.r5.s64 * ctx.r3.s64;
	// sradi r4,r4,30
	xer.ca = (ctx.r4.s64 < 0) & ((ctx.r4.u64 & 0x3FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s64 >> 30;
	// sradi r30,r5,30
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0x3FFFFFFF) != 0);
	r30.s64 = ctx.r5.s64 >> 30;
	// extsw r5,r4
	ctx.r5.s64 = ctx.r4.s32;
	// extsw r4,r30
	ctx.r4.s64 = r30.s32;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// stw r5,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r5,r6,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// mulld r5,r5,r31
	ctx.r5.s64 = ctx.r5.s64 * r31.s64;
	// mulld r4,r4,r7
	ctx.r4.s64 = ctx.r4.s64 * ctx.r7.s64;
	// sradi r5,r5,30
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0x3FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r5.s64 >> 30;
	// sradi r4,r4,30
	xer.ca = (ctx.r4.s64 < 0) & ((ctx.r4.u64 & 0x3FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s64 >> 30;
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne cr6,0x825ca95c
	if (!cr6.eq) goto loc_825CA95C;
loc_825CA9D4:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// add r29,r18,r29
	r29.u64 = r18.u64 + r29.u64;
	// cmpw cr6,r23,r19
	cr6.compare<int32_t>(r23.s32, r19.s32, xer);
	// blt cr6,0x825ca8f4
	if (cr6.lt) goto loc_825CA8F4;
loc_825CA9E4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_825CA9F0"))) PPC_WEAK_FUNC(sub_825CA9F0);
PPC_FUNC_IMPL(__imp__sub_825CA9F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// li r22,0
	r22.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825cac38
	if (cr6.eq) goto loc_825CAC38;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825cac38
	if (cr6.eq) goto loc_825CAC38;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825cac38
	if (cr6.eq) goto loc_825CAC38;
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// blt cr6,0x825cac38
	if (cr6.lt) goto loc_825CAC38;
	// cmpw cr6,r31,r6
	cr6.compare<int32_t>(r31.s32, ctx.r6.s32, xer);
	// bgt cr6,0x825cac38
	if (cr6.gt) goto loc_825CAC38;
	// mullw r23,r31,r31
	r23.s64 = int64_t(r31.s32) * int64_t(r31.s32);
	// rlwinm r5,r23,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825caa94
	if (!cr6.gt) goto loc_825CAA94;
	// addi r9,r31,1
	ctx.r9.s64 = r31.s64 + 1;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825CAA6C:
	// lbzx r9,r11,r30
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// beq cr6,0x825caa80
	if (cr6.eq) goto loc_825CAA80;
	// lis r9,16384
	ctx.r9.s64 = 1073741824;
loc_825CAA80:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// blt cr6,0x825caa6c
	if (cr6.lt) goto loc_825CAA6C;
loc_825CAA94:
	// li r28,0
	r28.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825caae0
	if (!cr6.gt) goto loc_825CAAE0;
loc_825CAAA4:
	// addi r29,r30,1
	r29.s64 = r30.s64 + 1;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// add r3,r28,r27
	ctx.r3.u64 = r28.u64 + r27.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x825ca898
	sub_825CA898(ctx, base);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x825cac2c
	if (cr6.lt) goto loc_825CAC2C;
	// add r28,r30,r28
	r28.u64 = r30.u64 + r28.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmpw cr6,r30,r31
	cr6.compare<int32_t>(r30.s32, r31.s32, xer);
	// blt cr6,0x825caaa4
	if (cr6.lt) goto loc_825CAAA4;
loc_825CAAE0:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x825cab2c
	if (!cr6.gt) goto loc_825CAB2C;
	// lis r11,31
	r11.s64 = 2031616;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// lis r7,32
	ctx.r7.s64 = 2097152;
	// ori r8,r11,65535
	ctx.r8.u64 = r11.u64 | 65535;
loc_825CAAFC:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825cab10
	if (cr6.lt) goto loc_825CAB10;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// b 0x825cab14
	goto loc_825CAB14;
loc_825CAB10:
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
loc_825CAB14:
	// rlwinm r11,r11,0,0,9
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFC00000;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x825caafc
	if (!cr6.eq) goto loc_825CAAFC;
loc_825CAB2C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r23,4
	cr6.compare<int32_t>(r23.s32, 4, xer);
	// lfs f0,-31836(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -31836);
	f0.f64 = double(temp.f32);
	// blt cr6,0x825cabe8
	if (cr6.lt) goto loc_825CABE8;
	// addi r10,r23,-4
	ctx.r10.s64 = r23.s64 + -4;
	// addi r11,r24,8
	r11.s64 = r24.s64 + 8;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_825CAB54:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f13,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// std r7,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r7.u64);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// bne cr6,0x825cab54
	if (!cr6.eq) goto loc_825CAB54;
loc_825CABE8:
	// cmpw cr6,r8,r23
	cr6.compare<int32_t>(ctx.r8.s32, r23.s32, xer);
	// bge cr6,0x825cac2c
	if (!cr6.lt) goto loc_825CAC2C;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r8,r23
	ctx.r10.s64 = r23.s64 - ctx.r8.s64;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
loc_825CABFC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// lfd f13,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825cabfc
	if (!cr6.eq) goto loc_825CABFC;
loc_825CAC2C:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
loc_825CAC38:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825CAC48"))) PPC_WEAK_FUNC(sub_825CAC48);
PPC_FUNC_IMPL(__imp__sub_825CAC48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r11,1
	r11.s64 = 1;
	// li r28,0
	r28.s64 = 0;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// stw r11,648(r29)
	PPC_STORE_U32(r29.u32 + 648, r11.u32);
	// lhz r11,580(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// stw r28,716(r29)
	PPC_STORE_U32(r29.u32 + 716, r28.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cad14
	if (!cr6.gt) goto loc_825CAD14;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_825CAC80:
	// lwz r11,584(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 584);
	// rlwinm r10,r27,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// li r5,160
	ctx.r5.s64 = 160;
	// li r4,0
	ctx.r4.s64 = 0;
	// lhzx r11,r10,r11
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r26
	r31.u64 = r11.u64 + r26.u64;
	// addi r3,r31,1616
	ctx.r3.s64 = r31.s64 + 1616;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r28,468(r31)
	PPC_STORE_U32(r31.u32 + 468, r28.u32);
	// stw r28,472(r31)
	PPC_STORE_U32(r31.u32 + 472, r28.u32);
	// stw r28,476(r31)
	PPC_STORE_U32(r31.u32 + 476, r28.u32);
	// stw r28,480(r31)
	PPC_STORE_U32(r31.u32 + 480, r28.u32);
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cacf8
	if (!cr6.gt) goto loc_825CACF8;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_825CACCC:
	// mulli r11,r30,56
	r11.s64 = r30.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r4,r11,200
	ctx.r4.s64 = r11.s64 + 200;
	// bl 0x825dba68
	sub_825DBA68(ctx, base);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825caccc
	if (cr6.lt) goto loc_825CACCC;
loc_825CACF8:
	// stw r28,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r28.u32);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// lhz r10,580(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cac80
	if (cr6.lt) goto loc_825CAC80;
loc_825CAD14:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825CAD20"))) PPC_WEAK_FUNC(sub_825CAD20);
PPC_FUNC_IMPL(__imp__sub_825CAD20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,580(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// li r8,0
	ctx.r8.s64 = 0;
loc_825CAD34:
	// lwz r11,584(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 584);
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r7,424(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lhz r10,118(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// ble cr6,0x825cada0
	if (!cr6.gt) goto loc_825CADA0;
loc_825CAD7C:
	// lhz r9,208(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 208);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825cad7c
	if (!cr6.eq) goto loc_825CAD7C;
loc_825CADA0:
	// lhz r10,580(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// addi r11,r8,1
	r11.s64 = ctx.r8.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cad34
	if (cr6.lt) goto loc_825CAD34;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CADBC"))) PPC_WEAK_FUNC(sub_825CADBC);
PPC_FUNC_IMPL(__imp__sub_825CADBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CADC0"))) PPC_WEAK_FUNC(sub_825CADC0);
PPC_FUNC_IMPL(__imp__sub_825CADC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf8
	// lhz r11,580(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cb104
	if (!cr6.gt) goto loc_825CB104;
	// li r29,0
	r29.s64 = 0;
loc_825CADDC:
	// lwz r9,584(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 584);
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r7,648(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,424(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// lhz r9,118(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 118);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x825cb074
	if (cr6.eq) goto loc_825CB074;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// beq cr6,0x825caf94
	if (cr6.eq) goto loc_825CAF94;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x825caecc
	if (!cr6.gt) goto loc_825CAECC;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
loc_825CAE40:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x825caea0
	if (!cr6.gt) goto loc_825CAEA0;
	// li r11,0
	r11.s64 = 0;
loc_825CAE50:
	// addi r4,r11,158
	ctx.r4.s64 = r11.s64 + 158;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r4,r3
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	// ble cr6,0x825cae78
	if (!cr6.gt) goto loc_825CAE78;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r5
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r5.u32);
	// b 0x825cae88
	goto loc_825CAE88;
loc_825CAE78:
	// subf r8,r9,r11
	ctx.r8.s64 = r11.s64 - ctx.r9.s64;
	// addi r8,r8,117
	ctx.r8.s64 = ctx.r8.s64 + 117;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
loc_825CAE88:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x825cae50
	if (cr6.lt) goto loc_825CAE50;
loc_825CAEA0:
	// lhz r11,582(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r8,0(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// sraw r11,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	r11.s64 = ctx.r10.s32 >> temp.u32;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// lwz r7,648(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// blt cr6,0x825cae40
	if (cr6.lt) goto loc_825CAE40;
loc_825CAECC:
	// lwz r9,648(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// bge cr6,0x825caf50
	if (!cr6.lt) goto loc_825CAF50;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r5
	ctx.r4.u64 = r11.u64 + ctx.r5.u64;
loc_825CAEE0:
	// lwz r6,648(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x825caf28
	if (!cr6.gt) goto loc_825CAF28;
	// li r11,0
	r11.s64 = 0;
loc_825CAEF4:
	// subf r7,r11,r9
	ctx.r7.s64 = ctx.r9.s64 - r11.s64;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// addi r11,r11,158
	r11.s64 = r11.s64 + 158;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r3
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// lwzx r7,r7,r5
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r5.u32);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r8
	r11.s64 = ctx.r8.s16;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// blt cr6,0x825caef4
	if (cr6.lt) goto loc_825CAEF4;
loc_825CAF28:
	// lhz r11,582(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// sraw r11,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	r11.s64 = ctx.r10.s32 >> temp.u32;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// blt cr6,0x825caee0
	if (cr6.lt) goto loc_825CAEE0;
loc_825CAF50:
	// lwz r11,648(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cb0ec
	if (!cr6.gt) goto loc_825CB0EC;
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r31,468
	ctx.r10.s64 = r31.s64 + 468;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
loc_825CAF70:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r8,648(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 648);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x825caf70
	if (cr6.lt) goto loc_825CAF70;
	// b 0x825cb0ec
	goto loc_825CB0EC;
loc_825CAF94:
	// lwz r10,632(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// lwz r7,472(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 472);
	// lwz r11,636(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// lwz r8,468(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// lhz r6,582(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r10,r8
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// extsh r8,r6
	ctx.r8.s64 = ctx.r6.s16;
	// sraw r11,r11,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// lwz r11,632(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// lwz r9,636(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// lwz r8,468(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// lhz r6,582(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// sraw r11,r11,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r11.u32);
	// ble cr6,0x825cb050
	if (!cr6.gt) goto loc_825CB050;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// addi r10,r30,-2
	ctx.r10.s64 = r30.s64 + -2;
loc_825CB00C:
	// lwz r9,636(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 636);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,632(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// lwz r4,-8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwz r28,-4(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lhz r6,582(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r7,r28,r7
	ctx.r7.s64 = int64_t(r28.s32) * int64_t(ctx.r7.s32);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// sraw r9,r9,r6
	temp.u32 = ctx.r6.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825cb00c
	if (!cr6.eq) goto loc_825CB00C;
loc_825CB050:
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r30,-2
	ctx.r10.s64 = r30.s64 + -2;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,468(r31)
	PPC_STORE_U32(r31.u32 + 468, r11.u32);
	// lwzx r11,r10,r5
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
	// stw r11,472(r31)
	PPC_STORE_U32(r31.u32 + 472, r11.u32);
	// b 0x825cb0ec
	goto loc_825CB0EC;
loc_825CB074:
	// lwz r9,468(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// lwz r11,632(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// lhz r8,582(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// extsh r9,r8
	ctx.r9.s64 = ctx.r8.s16;
	// sraw r11,r11,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// ble cr6,0x825cb0dc
	if (!cr6.gt) goto loc_825CB0DC;
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
loc_825CB0A8:
	// lwz r8,632(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lhz r7,582(r3)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 582);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// sraw r9,r9,r7
	temp.u32 = ctx.r7.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825cb0a8
	if (!cr6.eq) goto loc_825CB0A8;
loc_825CB0DC:
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,468(r31)
	PPC_STORE_U32(r31.u32 + 468, r11.u32);
loc_825CB0EC:
	// lhz r10,580(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 580);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825caddc
	if (cr6.lt) goto loc_825CADDC;
loc_825CB104:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825CB10C"))) PPC_WEAK_FUNC(sub_825CB10C);
PPC_FUNC_IMPL(__imp__sub_825CB10C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CB110"))) PPC_WEAK_FUNC(sub_825CB110);
PPC_FUNC_IMPL(__imp__sub_825CB110) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x825cb2a8
	if (!cr6.gt) goto loc_825CB2A8;
	// extsh r28,r9
	r28.s64 = ctx.r9.s16;
loc_825CB138:
	// li r4,0
	ctx.r4.s64 = 0;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// bge cr6,0x825cb228
	if (!cr6.lt) goto loc_825CB228;
	// lwz r11,192(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cb1b4
	if (!cr6.eq) goto loc_825CB1B4;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x825cb164
	if (!cr6.eq) goto loc_825CB164;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// b 0x825cb29c
	goto loc_825CB29C;
loc_825CB164:
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// bne cr6,0x825cb180
	if (!cr6.eq) goto loc_825CB180;
	// lwz r11,4(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r11.u32);
	// b 0x825cb29c
	goto loc_825CB29C;
loc_825CB180:
	// ble cr6,0x825cb29c
	if (!cr6.gt) goto loc_825CB29C;
	// addi r10,r31,-2
	ctx.r10.s64 = r31.s64 + -2;
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// add r10,r11,r29
	ctx.r10.u64 = r11.u64 + r29.u64;
	// lwzx r11,r11,r6
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,-4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x825cb29c
	goto loc_825CB29C;
loc_825CB1B4:
	// lhz r11,168(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 168);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x825cb218
	if (!cr6.gt) goto loc_825CB218;
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// add r5,r10,r29
	ctx.r5.u64 = ctx.r10.u64 + r29.u64;
	// subf r9,r10,r27
	ctx.r9.s64 = r27.s64 - ctx.r10.s64;
	// addi r10,r5,-4
	ctx.r10.s64 = ctx.r5.s64 + -4;
loc_825CB1E0:
	// lwz r25,0(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r31,r4
	cr6.compare<int32_t>(r31.s32, ctx.r4.s32, xer);
	// ble cr6,0x825cb1f4
	if (!cr6.gt) goto loc_825CB1F4;
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// b 0x825cb1f8
	goto loc_825CB1F8;
loc_825CB1F4:
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
loc_825CB1F8:
	// mullw r5,r5,r25
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r25.s32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r4,r30
	cr6.compare<int32_t>(ctx.r4.s32, r30.s32, xer);
	// blt cr6,0x825cb1e0
	if (cr6.lt) goto loc_825CB1E0;
loc_825CB218:
	// lhz r10,170(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 170);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sraw r10,r8,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	ctx.r10.s64 = ctx.r8.s32 >> temp.u32;
	// b 0x825cb28c
	goto loc_825CB28C;
loc_825CB228:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825cb280
	if (!cr6.gt) goto loc_825CB280;
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addi r8,r31,-1
	ctx.r8.s64 = r31.s64 + -1;
	// addi r9,r11,-4
	ctx.r9.s64 = r11.s64 + -4;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_825CB248:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x825cb258
	if (cr6.lt) goto loc_825CB258;
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// b 0x825cb25c
	goto loc_825CB25C;
loc_825CB258:
	// li r5,0
	ctx.r5.s64 = 0;
loc_825CB25C:
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// mullw r5,r5,r30
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r30.s32);
	// add r4,r5,r4
	ctx.r4.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cb248
	if (!cr6.eq) goto loc_825CB248;
loc_825CB280:
	// lhz r10,170(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 170);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sraw r10,r4,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r4.s32 < 0) & (((ctx.r4.s32 >> temp.u32) << temp.u32) != ctx.r4.s32);
	ctx.r10.s64 = ctx.r4.s32 >> temp.u32;
loc_825CB28C:
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stwx r10,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r10.u32);
loc_825CB29C:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmpw cr6,r31,r26
	cr6.compare<int32_t>(r31.s32, r26.s32, xer);
	// blt cr6,0x825cb138
	if (cr6.lt) goto loc_825CB138;
loc_825CB2A8:
	// li r5,160
	ctx.r5.s64 = 160;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// addi r11,r26,-1
	r11.s64 = r26.s64 + -1;
	// addi r9,r26,-40
	ctx.r9.s64 = r26.s64 + -40;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cb2f8
	if (cr6.lt) goto loc_825CB2F8;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_825CB2DC:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825cb2dc
	if (!cr6.eq) goto loc_825CB2DC;
loc_825CB2F8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825CB300"))) PPC_WEAK_FUNC(sub_825CB300);
PPC_FUNC_IMPL(__imp__sub_825CB300) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// lhz r20,30(r30)
	r20.u64 = PPC_LOAD_U16(r30.u32 + 30);
	// lwz r27,0(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r28,48(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r26,40(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// lwz r29,36(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// lwz r24,4(r30)
	r24.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r4,32(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// sth r20,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r20.u16);
	// ble cr6,0x825cb5c4
	if (!cr6.gt) goto loc_825CB5C4;
	// rlwinm r21,r11,1,0,30
	r21.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
loc_825CB360:
	// lwz r5,0(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// ble cr6,0x825cb46c
	if (!cr6.gt) goto loc_825CB46C;
	// addi r10,r4,6
	ctx.r10.s64 = ctx.r4.s64 + 6;
	// addi r11,r28,12
	r11.s64 = r28.s64 + 12;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
loc_825CB384:
	// lhz r7,-12(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + -12);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r6,-12(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + -12);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r3,-10(r10)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r10.u32 + -10);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r31,-10(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -10);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// lhz r23,-8(r10)
	r23.u64 = PPC_LOAD_U16(ctx.r10.u32 + -8);
	// mullw r7,r7,r6
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r22,-8(r11)
	r22.u64 = PPC_LOAD_U16(r11.u32 + -8);
	// lhz r21,-6(r10)
	r21.u64 = PPC_LOAD_U16(ctx.r10.u32 + -6);
	// lhz r20,-6(r11)
	r20.u64 = PPC_LOAD_U16(r11.u32 + -6);
	// lhz r19,-4(r10)
	r19.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// sth r6,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r6.u16);
	// lhz r18,-4(r11)
	r18.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// lhz r17,-2(r10)
	r17.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// lhz r16,-2(r11)
	r16.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// lhz r15,2(r10)
	r15.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// lhz r14,2(r11)
	r14.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// mullw r6,r3,r31
	ctx.r6.s64 = int64_t(ctx.r3.s32) * int64_t(r31.s32);
	// lhz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r31,r23
	r31.s64 = r23.s16;
	// extsh r23,r22
	r23.s64 = r22.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r31,r23
	ctx.r6.s64 = int64_t(r31.s32) * int64_t(r23.s32);
	// extsh r22,r21
	r22.s64 = r21.s16;
	// extsh r31,r20
	r31.s64 = r20.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r22,r31
	ctx.r6.s64 = int64_t(r22.s32) * int64_t(r31.s32);
	// extsh r23,r19
	r23.s64 = r19.s16;
	// extsh r31,r18
	r31.s64 = r18.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r23,r31
	ctx.r6.s64 = int64_t(r23.s32) * int64_t(r31.s32);
	// extsh r22,r17
	r22.s64 = r17.s16;
	// extsh r31,r16
	r31.s64 = r16.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r22,r31
	ctx.r6.s64 = int64_t(r22.s32) * int64_t(r31.s32);
	// lhz r22,80(r1)
	r22.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r23,r15
	r23.s64 = r15.s16;
	// extsh r31,r14
	r31.s64 = r14.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r23,r31
	ctx.r6.s64 = int64_t(r23.s32) * int64_t(r31.s32);
	// extsh r22,r22
	r22.s64 = r22.s16;
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r22,r3
	ctx.r6.s64 = int64_t(r22.s32) * int64_t(ctx.r3.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// bne cr6,0x825cb384
	if (!cr6.eq) goto loc_825CB384;
	// lhz r20,82(r1)
	r20.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lwz r21,92(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r22,84(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_825CB46C:
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// sraw r11,r8,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	r11.s64 = ctx.r8.s32 >> temp.u32;
	// add r31,r11,r5
	r31.u64 = r11.u64 + ctx.r5.u64;
	// ble cr6,0x825cb4c0
	if (!cr6.gt) goto loc_825CB4C0;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x825cb4f8
	if (!cr6.gt) goto loc_825CB4F8;
	// subf r7,r28,r10
	ctx.r7.s64 = ctx.r10.s64 - r28.s64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_825CB49C:
	// lhzx r9,r7,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x825cb49c
	if (!cr6.eq) goto loc_825CB49C;
	// b 0x825cb4f8
	goto loc_825CB4F8;
loc_825CB4C0:
	// bge cr6,0x825cb4f8
	if (!cr6.lt) goto loc_825CB4F8;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x825cb4f8
	if (!cr6.gt) goto loc_825CB4F8;
	// subf r9,r28,r10
	ctx.r9.s64 = ctx.r10.s64 - r28.s64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_825CB4D8:
	// lhzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x825cb4d8
	if (!cr6.eq) goto loc_825CB4D8;
loc_825CB4F8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825cb52c
	if (!cr6.eq) goto loc_825CB52C;
	// rlwinm r23,r27,1,0,30
	r23.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// add r3,r23,r29
	ctx.r3.u64 = r23.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// add r3,r23,r26
	ctx.r3.u64 = r23.u64 + r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r27,-1
	ctx.r4.s64 = r27.s64 + -1;
	// b 0x825cb530
	goto loc_825CB530;
loc_825CB52C:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
loc_825CB530:
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// add r11,r10,r26
	r11.u64 = ctx.r10.u64 + r26.u64;
	// sthx r31,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, r31.u16);
	// ble cr6,0x825cb55c
	if (!cr6.gt) goto loc_825CB55C;
	// cmpwi cr6,r31,32767
	cr6.compare<int32_t>(r31.s32, 32767, xer);
	// sth r20,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r20.u16);
	// ble cr6,0x825cb588
	if (!cr6.gt) goto loc_825CB588;
	// li r9,32767
	ctx.r9.s64 = 32767;
	// sthx r9,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, ctx.r9.u16);
	// b 0x825cb588
	goto loc_825CB588;
loc_825CB55C:
	// bge cr6,0x825cb580
	if (!cr6.lt) goto loc_825CB580;
	// extsh r9,r20
	ctx.r9.s64 = r20.s16;
	// cmpwi cr6,r31,-32768
	cr6.compare<int32_t>(r31.s32, -32768, xer);
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// bge cr6,0x825cb588
	if (!cr6.lt) goto loc_825CB588;
	// li r9,-32768
	ctx.r9.s64 = -32768;
	// sthx r9,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, ctx.r9.u16);
	// b 0x825cb588
	goto loc_825CB588;
loc_825CB580:
	// li r10,0
	ctx.r10.s64 = 0;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825CB588:
	// lhzx r10,r21,r11
	ctx.r10.u64 = PPC_LOAD_U16(r21.u32 + r11.u32);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// sthx r10,r21,r11
	PPC_STORE_U16(r21.u32 + r11.u32, ctx.r10.u16);
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// sthx r9,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r9.u16);
	// stw r31,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r31.u32);
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// bne cr6,0x825cb360
	if (!cr6.eq) goto loc_825CB360;
loc_825CB5C4:
	// stw r4,32(r30)
	PPC_STORE_U32(r30.u32 + 32, ctx.r4.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825CB5D0"))) PPC_WEAK_FUNC(sub_825CB5D0);
PPC_FUNC_IMPL(__imp__sub_825CB5D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// lhz r19,30(r30)
	r19.u64 = PPC_LOAD_U16(r30.u32 + 30);
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r26,48(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r27,40(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// lwz r29,36(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// lwz r24,4(r30)
	r24.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r4,32(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// sth r19,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r19.u16);
	// ble cr6,0x825cb880
	if (!cr6.gt) goto loc_825CB880;
	// rlwinm r21,r11,1,0,30
	r21.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// lis r11,127
	r11.s64 = 8323072;
	// lis r18,-128
	r18.s64 = -8388608;
	// ori r20,r11,65535
	r20.u64 = r11.u64 | 65535;
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
loc_825CB63C:
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,0(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// ble cr6,0x825cb728
	if (!cr6.gt) goto loc_825CB728;
	// addi r10,r26,12
	ctx.r10.s64 = r26.s64 + 12;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
loc_825CB65C:
	// lhz r7,2(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r6,-2(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lwz r3,28(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r31,20(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r23,-4(r10)
	r23.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// mullw r7,r7,r3
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r3.s32);
	// lwz r3,16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lhz r22,-6(r10)
	r22.u64 = PPC_LOAD_U16(ctx.r10.u32 + -6);
	// lwz r21,12(r11)
	r21.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lhz r20,-8(r10)
	r20.u64 = PPC_LOAD_U16(ctx.r10.u32 + -8);
	// lwz r19,8(r11)
	r19.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lhz r18,-10(r10)
	r18.u64 = PPC_LOAD_U16(ctx.r10.u32 + -10);
	// lwz r17,4(r11)
	r17.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lhz r16,-12(r10)
	r16.u64 = PPC_LOAD_U16(ctx.r10.u32 + -12);
	// mullw r6,r6,r31
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r31.s32);
	// lhz r15,0(r10)
	r15.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r14,24(r11)
	r14.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// extsh r23,r23
	r23.s64 = r23.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r23,r3
	ctx.r6.s64 = int64_t(r23.s32) * int64_t(ctx.r3.s32);
	// extsh r22,r22
	r22.s64 = r22.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r22,r21
	ctx.r6.s64 = int64_t(r22.s32) * int64_t(r21.s32);
	// extsh r3,r20
	ctx.r3.s64 = r20.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r3,r19
	ctx.r6.s64 = int64_t(ctx.r3.s32) * int64_t(r19.s32);
	// extsh r23,r18
	r23.s64 = r18.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r23,r17
	ctx.r6.s64 = int64_t(r23.s32) * int64_t(r17.s32);
	// extsh r3,r16
	ctx.r3.s64 = r16.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r3,r31
	ctx.r6.s64 = int64_t(ctx.r3.s32) * int64_t(r31.s32);
	// extsh r23,r15
	r23.s64 = r15.s16;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mullw r6,r14,r23
	ctx.r6.s64 = int64_t(r14.s32) * int64_t(r23.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825cb65c
	if (!cr6.eq) goto loc_825CB65C;
	// lis r11,127
	r11.s64 = 8323072;
	// lhz r19,80(r1)
	r19.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// lwz r21,92(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lis r18,-128
	r18.s64 = -8388608;
	// lwz r22,84(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// ori r20,r11,65535
	r20.u64 = r11.u64 | 65535;
loc_825CB728:
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// add r10,r11,r27
	ctx.r10.u64 = r11.u64 + r27.u64;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// sraw r11,r8,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r8.s32 < 0) & (((ctx.r8.s32 >> temp.u32) << temp.u32) != ctx.r8.s32);
	r11.s64 = ctx.r8.s32 >> temp.u32;
	// add r31,r11,r5
	r31.u64 = r11.u64 + ctx.r5.u64;
	// ble cr6,0x825cb77c
	if (!cr6.gt) goto loc_825CB77C;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825cb7b4
	if (!cr6.gt) goto loc_825CB7B4;
	// subf r7,r26,r10
	ctx.r7.s64 = ctx.r10.s64 - r26.s64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_825CB758:
	// lhzx r9,r7,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x825cb758
	if (!cr6.eq) goto loc_825CB758;
	// b 0x825cb7b4
	goto loc_825CB7B4;
loc_825CB77C:
	// bge cr6,0x825cb7b4
	if (!cr6.lt) goto loc_825CB7B4;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825cb7b4
	if (!cr6.gt) goto loc_825CB7B4;
	// subf r9,r26,r10
	ctx.r9.s64 = ctx.r10.s64 - r26.s64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_825CB794:
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x825cb794
	if (!cr6.eq) goto loc_825CB794;
loc_825CB7B4:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825cb7ec
	if (!cr6.eq) goto loc_825CB7EC;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r23,r28,1,0,30
	r23.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r23,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r3,r23,r27
	ctx.r3.u64 = r23.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r28,-1
	ctx.r4.s64 = r28.s64 + -1;
	// b 0x825cb7f0
	goto loc_825CB7F0;
loc_825CB7EC:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
loc_825CB7F0:
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// stwx r31,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r31.u32);
	// ble cr6,0x825cb81c
	if (!cr6.gt) goto loc_825CB81C;
	// cmpw cr6,r31,r20
	cr6.compare<int32_t>(r31.s32, r20.s32, xer);
	// sth r19,0(r11)
	PPC_STORE_U16(r11.u32 + 0, r19.u16);
	// ble cr6,0x825cb844
	if (!cr6.gt) goto loc_825CB844;
	// stwx r20,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r20.u32);
	// b 0x825cb844
	goto loc_825CB844;
loc_825CB81C:
	// bge cr6,0x825cb83c
	if (!cr6.lt) goto loc_825CB83C;
	// extsh r9,r19
	ctx.r9.s64 = r19.s16;
	// cmpw cr6,r31,r18
	cr6.compare<int32_t>(r31.s32, r18.s32, xer);
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// bge cr6,0x825cb844
	if (!cr6.lt) goto loc_825CB844;
	// stwx r18,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r18.u32);
	// b 0x825cb844
	goto loc_825CB844;
loc_825CB83C:
	// li r10,0
	ctx.r10.s64 = 0;
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825CB844:
	// lhzx r10,r21,r11
	ctx.r10.u64 = PPC_LOAD_U16(r21.u32 + r11.u32);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// sthx r10,r21,r11
	PPC_STORE_U16(r21.u32 + r11.u32, ctx.r10.u16);
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// sthx r9,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r9.u16);
	// stw r31,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r31.u32);
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// bne cr6,0x825cb63c
	if (!cr6.eq) goto loc_825CB63C;
loc_825CB880:
	// stw r4,32(r30)
	PPC_STORE_U32(r30.u32 + 32, ctx.r4.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825CB88C"))) PPC_WEAK_FUNC(sub_825CB88C);
PPC_FUNC_IMPL(__imp__sub_825CB88C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CB890"))) PPC_WEAK_FUNC(sub_825CB890);
PPC_FUNC_IMPL(__imp__sub_825CB890) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// lwz r10,704(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 704);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cb8d4
	if (cr6.eq) goto loc_825CB8D4;
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
loc_825CB8D4:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CB8E4"))) PPC_WEAK_FUNC(sub_825CB8E4);
PPC_FUNC_IMPL(__imp__sub_825CB8E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CB8E8"))) PPC_WEAK_FUNC(sub_825CB8E8);
PPC_FUNC_IMPL(__imp__sub_825CB8E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cb9b8
	if (!cr6.eq) goto loc_825CB9B8;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bgt cr6,0x825cb9b8
	if (cr6.gt) goto loc_825CB9B8;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825cba18
	if (!cr6.gt) goto loc_825CBA18;
loc_825CB930:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bgt cr6,0x825cba18
	if (cr6.gt) goto loc_825CBA18;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// subfic r8,r30,8
	xer.ca = r30.u32 <= 8;
	ctx.r8.s64 = 8 - r30.s64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// clrlwi r9,r30,24
	ctx.r9.u64 = r30.u32 & 0xFF;
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// slw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// li r30,0
	r30.s64 = 0;
	// srw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r9.u8 & 0x3F));
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bgt cr6,0x825cb930
	if (cr6.gt) goto loc_825CB930;
	// b 0x825cba18
	goto loc_825CBA18;
loc_825CB9B8:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// lwz r7,44(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// subfic r9,r30,8
	xer.ca = r30.u32 <= 8;
	ctx.r9.s64 = 8 - r30.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// clrlwi r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// slw r11,r8,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r30.u8 & 0x3F));
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// srw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
loc_825CBA18:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBA30"))) PPC_WEAK_FUNC(sub_825CBA30);
PPC_FUNC_IMPL(__imp__sub_825CBA30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x825cba64
	if (cr6.gt) goto loc_825CBA64;
	// lwz r10,212(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cba5c
	if (cr6.eq) goto loc_825CBA5C;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r10,r10,11
	ctx.r10.s64 = ctx.r10.s64 + 11;
	// b 0x825cba80
	goto loc_825CBA80;
loc_825CBA5C:
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825cba84
	goto loc_825CBA84;
loc_825CBA64:
	// lwz r10,604(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 604);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// beq cr6,0x825cba7c
	if (cr6.eq) goto loc_825CBA7C;
	// addi r10,r10,17
	ctx.r10.s64 = ctx.r10.s64 + 17;
	// b 0x825cba80
	goto loc_825CBA80;
loc_825CBA7C:
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_825CBA80:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
loc_825CBA84:
	// lwz r9,60(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// srawi r9,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 3;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// bgt cr6,0x825cbac4
	if (cr6.gt) goto loc_825CBAC4;
	// lwz r10,212(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cbabc
	if (cr6.eq) goto loc_825CBABC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r10,r10,11
	ctx.r10.s64 = ctx.r10.s64 + 11;
	// b 0x825cbae0
	goto loc_825CBAE0;
loc_825CBABC:
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825cbae4
	goto loc_825CBAE4;
loc_825CBAC4:
	// lwz r10,604(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 604);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// beq cr6,0x825cbadc
	if (cr6.eq) goto loc_825CBADC;
	// addi r10,r10,17
	ctx.r10.s64 = ctx.r10.s64 + 17;
	// b 0x825cbae0
	goto loc_825CBAE0;
loc_825CBADC:
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_825CBAE0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
loc_825CBAE4:
	// rlwinm r7,r10,29,27,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1F;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r8,620(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 620);
	// add r11,r10,r9
	r11.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
	// ble cr6,0x825cbb1c
	if (!cr6.gt) goto loc_825CBB1C;
	// stw r10,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r10.u32);
	// b 0x825cbb2c
	goto loc_825CBB2C;
loc_825CBB1C:
	// li r9,1
	ctx.r9.s64 = 1;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r9.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, r11.u32);
loc_825CBB2C:
	// b 0x825cb8e8
	sub_825CB8E8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825CBB30"))) PPC_WEAK_FUNC(sub_825CBB30);
PPC_FUNC_IMPL(__imp__sub_825CBB30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x825cbb80
	if (cr6.gt) goto loc_825CBB80;
	// lwz r10,212(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cbb78
	if (cr6.eq) goto loc_825CBB78;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r11,r11,11
	r11.s64 = r11.s64 + 11;
	// b 0x825cbb9c
	goto loc_825CBB9C;
loc_825CBB78:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// b 0x825cbba0
	goto loc_825CBBA0;
loc_825CBB80:
	// lwz r10,604(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 604);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cbb98
	if (cr6.eq) goto loc_825CBB98;
	// addi r11,r11,17
	r11.s64 = r11.s64 + 17;
	// b 0x825cbb9c
	goto loc_825CBB9C;
loc_825CBB98:
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
loc_825CBB9C:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_825CBBA0:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r9,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r9.u32);
	// stw r9,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r9.u32);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r9,r11,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r7,84(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r9,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r9.u32);
	// srawi r9,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r9.s64 = r11.s32 >> 3;
	// lbz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r30,r9,r11
	r30.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r10,1
	r11.s64 = ctx.r10.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// clrlwi r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// subfic r9,r30,8
	xer.ca = r30.u32 <= 8;
	ctx.r9.s64 = 8 - r30.s64;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// slw r11,r11,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r11.u32 << (r30.u8 & 0x3F));
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// srw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBC34"))) PPC_WEAK_FUNC(sub_825CBC34);
PPC_FUNC_IMPL(__imp__sub_825CBC34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CBC38"))) PPC_WEAK_FUNC(sub_825CBC38);
PPC_FUNC_IMPL(__imp__sub_825CBC38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,76(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cbc7c
	if (cr6.eq) goto loc_825CBC7C;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,704(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cbc74
	if (cr6.eq) goto loc_825CBC74;
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r8,28(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x825cbc7c
	if (cr6.lt) goto loc_825CBC7C;
loc_825CBC74:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_825CBC7C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBC84"))) PPC_WEAK_FUNC(sub_825CBC84);
PPC_FUNC_IMPL(__imp__sub_825CBC84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CBC88"))) PPC_WEAK_FUNC(sub_825CBC88);
PPC_FUNC_IMPL(__imp__sub_825CBC88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r9,40(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r9,60(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bgt cr6,0x825cbccc
	if (cr6.gt) goto loc_825CBCCC;
	// lwz r9,212(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 212);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825cbcc4
	if (cr6.eq) goto loc_825CBCC4;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// addi r10,r10,11
	ctx.r10.s64 = ctx.r10.s64 + 11;
	// b 0x825cbce8
	goto loc_825CBCE8;
loc_825CBCC4:
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825cbcec
	goto loc_825CBCEC;
loc_825CBCCC:
	// lwz r9,604(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 604);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825cbce4
	if (cr6.eq) goto loc_825CBCE4;
	// addi r10,r10,17
	ctx.r10.s64 = ctx.r10.s64 + 17;
	// b 0x825cbce8
	goto loc_825CBCE8;
loc_825CBCE4:
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
loc_825CBCE8:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
loc_825CBCEC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// srawi r9,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 3;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// lwz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lwz r8,24(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// subf r7,r10,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// add r7,r6,r3
	ctx.r7.u64 = ctx.r6.u64 + ctx.r3.u64;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBD4C"))) PPC_WEAK_FUNC(sub_825CBD4C);
PPC_FUNC_IMPL(__imp__sub_825CBD4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CBD50"))) PPC_WEAK_FUNC(sub_825CBD50);
PPC_FUNC_IMPL(__imp__sub_825CBD50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r8,40(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// bgt cr6,0x825cbd94
	if (cr6.gt) goto loc_825CBD94;
	// lwz r7,212(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825cbd8c
	if (cr6.eq) goto loc_825CBD8C;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r11,r11,11
	r11.s64 = r11.s64 + 11;
	// b 0x825cbdb0
	goto loc_825CBDB0;
loc_825CBD8C:
	// li r11,0
	r11.s64 = 0;
	// b 0x825cbdb4
	goto loc_825CBDB4;
loc_825CBD94:
	// lwz r7,604(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 604);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825cbdac
	if (cr6.eq) goto loc_825CBDAC;
	// addi r11,r11,17
	r11.s64 = r11.s64 + 17;
	// b 0x825cbdb0
	goto loc_825CBDB0;
loc_825CBDAC:
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
loc_825CBDB0:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_825CBDB4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// srawi r7,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r7.s64 = r11.s32 >> 3;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// subf. r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beqlr 
	if (cr0.eq) return;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// xor r10,r11,r4
	ctx.r10.u64 = r11.u64 ^ ctx.r4.u64;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// stw r7,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r7.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// ble cr6,0x825cbe44
	if (!cr6.gt) goto loc_825CBE44;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// rlwinm r11,r11,29,3,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x825cbe10
	if (cr6.gt) goto loc_825CBE10;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// blr 
	return;
loc_825CBE10:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,36(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// stw r10,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r10.u32);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r7
	r11.s64 = ctx.r7.s64 - r11.s64;
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// subf r11,r10,r8
	r11.s64 = ctx.r8.s64 - ctx.r10.s64;
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// srw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// blr 
	return;
loc_825CBE44:
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwinm r11,r11,29,3,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// stw r10,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r10.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBE5C"))) PPC_WEAK_FUNC(sub_825CBE5C);
PPC_FUNC_IMPL(__imp__sub_825CBE5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CBE60"))) PPC_WEAK_FUNC(sub_825CBE60);
PPC_FUNC_IMPL(__imp__sub_825CBE60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r4,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r4.u32);
	// li r10,15
	ctx.r10.s64 = 15;
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// stw r10,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r10.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,212(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 212);
	// stw r9,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, ctx.r9.u32);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, r11.u32);
	// stw r10,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r10.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// lwz r10,704(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 704);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825cbee0
	if (cr6.eq) goto loc_825CBEE0;
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
loc_825CBEE0:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CBEF0"))) PPC_WEAK_FUNC(sub_825CBEF0);
PPC_FUNC_IMPL(__imp__sub_825CBEF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// mr r25,r26
	r25.u64 = r26.u64;
	// li r23,1
	r23.s64 = 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x825cbf30
	if (cr6.eq) goto loc_825CBF30;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825cbf34
	if (cr6.eq) goto loc_825CBF34;
loc_825CBF30:
	// stw r23,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r23.u32);
loc_825CBF34:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x825cc16c
	if (!cr6.gt) goto loc_825CC16C;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r29,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r29.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// beq cr6,0x825cbf54
	if (cr6.eq) goto loc_825CBF54;
	// stw r10,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r10.u32);
	// b 0x825cbf60
	goto loc_825CBF60;
loc_825CBF54:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
loc_825CBF60:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x825cbf70
	if (!cr6.eq) goto loc_825CBF70;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x825cbfd8
	if (cr6.eq) goto loc_825CBFD8;
loc_825CBF70:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// stw r29,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r29.u32);
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// beq cr6,0x825cbfd8
	if (cr6.eq) goto loc_825CBFD8;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x825b04e0
	sub_825B04E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// srawi r10,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r10.s64 = r11.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf. r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x825cbfac
	if (cr0.eq) goto loc_825CBFAC;
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_825CBFAC:
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cbfd8
	if (cr6.eq) goto loc_825CBFD8;
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x825cbfd0
	if (!cr6.gt) goto loc_825CBFD0;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r26,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r26.u32);
	// b 0x825cbfd8
	goto loc_825CBFD8;
loc_825CBFD0:
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
loc_825CBFD8:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cc000
	if (!cr6.eq) goto loc_825CC000;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825cc160
	if (cr6.eq) goto loc_825CC160;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,22
	r11.u64 = r11.u32 & 0x3FF;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// b 0x825cc160
	goto loc_825CC160;
loc_825CC000:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825cc160
	if (cr6.eq) goto loc_825CC160;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x825cc160
	if (!cr6.eq) goto loc_825CC160;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// stw r26,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r26.u32);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r30,r3,8,16,23
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFF00;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// lbz r3,1(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// lwz r9,84(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// or r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 | r30.u64;
	// lbz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// rlwinm r30,r10,8,0,23
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// or r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 | r30.u64;
	// rlwinm r30,r9,8,0,23
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lbz r3,3(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// or r30,r11,r30
	r30.u64 = r11.u64 | r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// bl 0x825b04e0
	sub_825B04E0(ctx, base);
	// lwz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// lwz r9,28(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r8,r30,4,28,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xF;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r10,r11,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// lwz r7,52(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r8,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r8.u32);
	// srawi r6,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r6.s64 = r11.s32 >> 3;
	// stw r9,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// addze r10,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r25,r10,r11
	r25.s64 = r11.s64 - ctx.r10.s64;
	// bne cr6,0x825cc15c
	if (!cr6.eq) goto loc_825CC15C;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// cmpwi cr6,r24,2
	cr6.compare<int32_t>(r24.s32, 2, xer);
	// ble cr6,0x825cc100
	if (!cr6.gt) goto loc_825CC100;
	// rlwinm r10,r30,0,5,5
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825cc100
	if (cr6.eq) goto loc_825CC100;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
loc_825CC100:
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// beq cr6,0x825cc118
	if (cr6.eq) goto loc_825CC118;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825cc128
	if (!cr6.eq) goto loc_825CC128;
loc_825CC118:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825cc128
	if (!cr6.eq) goto loc_825CC128;
	// stw r26,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r26.u32);
	// b 0x825cc160
	goto loc_825CC160;
loc_825CC128:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// stw r9,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// stw r23,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r23.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_825CC15C:
	// stw r26,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r26.u32);
loc_825CC160:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cb8e8
	sub_825CB8E8(ctx, base);
loc_825CC16C:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825CC178"))) PPC_WEAK_FUNC(sub_825CC178);
PPC_FUNC_IMPL(__imp__sub_825CC178) {
	PPC_FUNC_PROLOGUE();
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825cbef0
	sub_825CBEF0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825CC180"))) PPC_WEAK_FUNC(sub_825CC180);
PPC_FUNC_IMPL(__imp__sub_825CC180) {
	PPC_FUNC_PROLOGUE();
	// xoris r10,r10,43894
	ctx.r10.u64 = ctx.r10.u64 ^ 2876637184;
	// xori r10,r10,14558
	ctx.r10.u64 = ctx.r10.u64 ^ 14558;
	// b 0x825cbef0
	sub_825CBEF0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825CC18C"))) PPC_WEAK_FUNC(sub_825CC18C);
PPC_FUNC_IMPL(__imp__sub_825CC18C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CC190"))) PPC_WEAK_FUNC(sub_825CC190);
PPC_FUNC_IMPL(__imp__sub_825CC190) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r30
	r29.u64 = r30.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,704(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cc214
	if (cr6.eq) goto loc_825CC214;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x825cc214
	if (!cr6.lt) goto loc_825CC214;
	// bl 0x825cba30
	sub_825CBA30(ctx, base);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x825cc200
	if (!cr6.eq) goto loc_825CC200;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x825cc204
	goto loc_825CC204;
loc_825CC200:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825CC204:
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// ble cr6,0x825cc310
	if (!cr6.gt) goto loc_825CC310;
loc_825CC214:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cc2bc
	if (!cr6.eq) goto loc_825CC2BC;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825cc308
	if (cr6.eq) goto loc_825CC308;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// std r30,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r30.u64);
	// std r30,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r30.u64);
	// std r30,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r30.u64);
	// std r30,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r30.u64);
	// std r30,32(r11)
	PPC_STORE_U64(r11.u32 + 32, r30.u64);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x825cc310
	if (cr6.lt) goto loc_825CC310;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x825b12d8
	sub_825B12D8(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x825cc310
	if (cr6.lt) goto loc_825CC310;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x825cc2bc
	if (!cr6.lt) goto loc_825CC2BC;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r11,704(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cc2bc
	if (cr6.eq) goto loc_825CC2BC;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r8,28(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x825cc2bc
	if (!cr6.lt) goto loc_825CC2BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cba30
	sub_825CBA30(ctx, base);
loc_825CC2BC:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x825cc2d8
	if (!cr6.eq) goto loc_825CC2D8;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x825cc2dc
	goto loc_825CC2DC;
loc_825CC2D8:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825CC2DC:
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// ble cr6,0x825cc310
	if (!cr6.gt) goto loc_825CC310;
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cc308
	if (cr6.eq) goto loc_825CC308;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x825cc310
	if (cr6.eq) goto loc_825CC310;
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// beq cr6,0x825cc310
	if (cr6.eq) goto loc_825CC310;
loc_825CC308:
	// lis r29,-32764
	r29.s64 = -2147221504;
	// ori r29,r29,4
	r29.u64 = r29.u64 | 4;
loc_825CC310:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825CC31C"))) PPC_WEAK_FUNC(sub_825CC31C);
PPC_FUNC_IMPL(__imp__sub_825CC31C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CC320"))) PPC_WEAK_FUNC(sub_825CC320);
PPC_FUNC_IMPL(__imp__sub_825CC320) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825cc354
	if (!cr6.gt) goto loc_825CC354;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825CC354:
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bge cr6,0x825cc454
	if (!cr6.lt) goto loc_825CC454;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cc3b8
	if (cr6.eq) goto loc_825CC3B8;
	// subfic r10,r9,32
	xer.ca = ctx.r9.u32 <= 32;
	ctx.r10.s64 = 32 - ctx.r9.s64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x825cc37c
	if (cr6.lt) goto loc_825CC37C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_825CC37C:
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r7,36(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// slw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// slw r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// and r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r8,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r8.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_825CC3B8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bgt cr6,0x825cc42c
	if (cr6.gt) goto loc_825CC42C;
loc_825CC3C4:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825cc42c
	if (!cr6.gt) goto loc_825CC42C;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// ble cr6,0x825cc3c4
	if (!cr6.gt) goto loc_825CC3C4;
loc_825CC42C:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bge cr6,0x825cc454
	if (!cr6.lt) goto loc_825CC454;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x825cc480
	if (cr6.lt) goto loc_825CC480;
loc_825CC454:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// rlwinm r7,r30,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r11,-31832
	ctx.r10.s64 = r11.s64 + -31832;
	// subf r11,r30,r9
	r11.s64 = ctx.r9.s64 - r30.s64;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// srw r9,r8,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// lwzx r11,r7,r10
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_825CC480:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825CC48C"))) PPC_WEAK_FUNC(sub_825CC48C);
PPC_FUNC_IMPL(__imp__sub_825CC48C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CC490"))) PPC_WEAK_FUNC(sub_825CC490);
PPC_FUNC_IMPL(__imp__sub_825CC490) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r27
	r28.u64 = r27.u64;
	// li r29,1
	r29.s64 = 1;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825cc510
	if (!cr6.gt) goto loc_825CC510;
loc_825CC4BC:
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// mr r11,r27
	r11.u64 = r27.u64;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subfic r10,r10,32
	xer.ca = ctx.r10.u32 <= 32;
	ctx.r10.s64 = 32 - ctx.r10.s64;
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r9,r10,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825cc4f0
	if (cr6.eq) goto loc_825CC4F0;
loc_825CC4DC:
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r10,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825cc4dc
	if (!cr6.eq) goto loc_825CC4DC;
loc_825CC4F0:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bge 0x825cc600
	if (!cr0.lt) goto loc_825CC600;
loc_825CC510:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cc564
	if (cr6.eq) goto loc_825CC564;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// li r10,32
	ctx.r10.s64 = 32;
	// bgt cr6,0x825cc530
	if (cr6.gt) goto loc_825CC530;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_825CC530:
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lwz r7,36(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// stw r10,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r10.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// slw r8,r29,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// slw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// and r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r8,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r8.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_825CC564:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bgt cr6,0x825cc5d8
	if (cr6.gt) goto loc_825CC5D8;
loc_825CC570:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825cc5d8
	if (!cr6.gt) goto loc_825CC5D8;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// ble cr6,0x825cc570
	if (!cr6.gt) goto loc_825CC570;
loc_825CC5D8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825cc4bc
	if (!cr6.lt) goto loc_825CC4BC;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bge cr6,0x825cc4bc
	if (!cr6.lt) goto loc_825CC4BC;
loc_825CC600:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825CC60C"))) PPC_WEAK_FUNC(sub_825CC60C);
PPC_FUNC_IMPL(__imp__sub_825CC60C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CC610"))) PPC_WEAK_FUNC(sub_825CC610);
PPC_FUNC_IMPL(__imp__sub_825CC610) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bge cr6,0x825cc728
	if (!cr6.lt) goto loc_825CC728;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cc68c
	if (cr6.eq) goto loc_825CC68C;
	// subfic r10,r9,32
	xer.ca = ctx.r9.u32 <= 32;
	ctx.r10.s64 = 32 - ctx.r9.s64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x825cc650
	if (cr6.lt) goto loc_825CC650;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_825CC650:
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r7,36(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// slw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// slw r8,r8,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// and r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r8,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r8.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_825CC68C:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bgt cr6,0x825cc700
	if (cr6.gt) goto loc_825CC700;
loc_825CC698:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x825cc700
	if (!cr6.gt) goto loc_825CC700;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// lbz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// ble cr6,0x825cc698
	if (!cr6.gt) goto loc_825CC698;
loc_825CC700:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bge cr6,0x825cc728
	if (!cr6.lt) goto loc_825CC728;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x825cc734
	if (cr6.lt) goto loc_825CC734;
loc_825CC728:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
loc_825CC734:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825CC740"))) PPC_WEAK_FUNC(sub_825CC740);
PPC_FUNC_IMPL(__imp__sub_825CC740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825cc774
	if (cr6.lt) goto loc_825CC774;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// sth r11,16(r31)
	PPC_STORE_U16(r31.u32 + 16, r11.u16);
loc_825CC774:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CC788"))) PPC_WEAK_FUNC(sub_825CC788);
PPC_FUNC_IMPL(__imp__sub_825CC788) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// lhz r11,34(r8)
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 34);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// mulli r6,r4,28
	ctx.r6.s64 = ctx.r4.s64 * 28;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_825CC7A8:
	// lwz r9,320(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 320);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// lwz r9,424(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 424);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r5,12(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// sth r7,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r7.u16);
	// sth r7,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r7.u16);
	// lhz r9,34(r8)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + 34);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cc7a8
	if (cr6.lt) goto loc_825CC7A8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CC7DC"))) PPC_WEAK_FUNC(sub_825CC7DC);
PPC_FUNC_IMPL(__imp__sub_825CC7DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CC7E0"))) PPC_WEAK_FUNC(sub_825CC7E0);
PPC_FUNC_IMPL(__imp__sub_825CC7E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// li r21,0
	r21.s64 = 0;
	// mr r17,r4
	r17.u64 = ctx.r4.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r16,r21
	r16.u64 = r21.u64;
	// lwz r28,0(r14)
	r28.u64 = PPC_LOAD_U32(r14.u32 + 0);
	// mr r15,r21
	r15.u64 = r21.u64;
	// lhz r8,34(r28)
	ctx.r8.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// lwz r31,256(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// lwz r11,228(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 228);
	// mullw r25,r31,r8
	r25.s64 = int64_t(r31.s32) * int64_t(ctx.r8.s32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cc880
	if (!cr6.eq) goto loc_825CC880;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825cc874
	if (!cr6.gt) goto loc_825CC874;
	// mulli r8,r17,28
	ctx.r8.s64 = r17.s64 * 28;
	// mr r11,r21
	r11.u64 = r21.u64;
	// li r18,1
	r18.s64 = 1;
loc_825CC834:
	// lwz r9,320(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// mulli r10,r11,1776
	ctx.r10.s64 = r11.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r6,256(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// lwz r10,424(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 424);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// sth r21,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r21.u16);
	// sth r18,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r18.u16);
	// lhz r10,34(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cc834
	if (cr6.lt) goto loc_825CC834;
loc_825CC874:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825CC880:
	// lwz r10,176(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// li r18,1
	r18.s64 = 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825cc914
	if (!cr6.eq) goto loc_825CC914;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825cc8a8
	if (!cr6.eq) goto loc_825CC8A8;
	// mr r16,r18
	r16.u64 = r18.u64;
	// mr r15,r18
	r15.u64 = r18.u64;
	// add r19,r15,r16
	r19.u64 = r15.u64 + r16.u64;
	// b 0x825cc938
	goto loc_825CC938;
loc_825CC8A8:
	// cmpwi cr6,r11,16
	cr6.compare<int32_t>(r11.s32, 16, xer);
	// bne cr6,0x825cc8c0
	if (!cr6.eq) goto loc_825CC8C0;
	// mr r16,r18
	r16.u64 = r18.u64;
	// li r15,2
	r15.s64 = 2;
	// add r19,r15,r16
	r19.u64 = r15.u64 + r16.u64;
	// b 0x825cc938
	goto loc_825CC938;
loc_825CC8C0:
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825cc8dc
	if (!cr6.gt) goto loc_825CC8DC;
loc_825CC8CC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cc8cc
	if (cr6.gt) goto loc_825CC8CC;
loc_825CC8DC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mr r11,r21
	r11.u64 = r21.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825cc904
	if (!cr6.gt) goto loc_825CC904;
loc_825CC8F4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cc8f4
	if (cr6.gt) goto loc_825CC8F4;
loc_825CC904:
	// addi r15,r11,1
	r15.s64 = r11.s64 + 1;
	// mr r16,r21
	r16.u64 = r21.u64;
	// add r19,r15,r16
	r19.u64 = r15.u64 + r16.u64;
	// b 0x825cc938
	goto loc_825CC938;
loc_825CC914:
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// mr r11,r21
	r11.u64 = r21.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825cc934
	if (!cr6.gt) goto loc_825CC934;
loc_825CC924:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cc924
	if (cr6.gt) goto loc_825CC924;
loc_825CC934:
	// addi r19,r11,1
	r19.s64 = r11.s64 + 1;
loc_825CC938:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825cc99c
	if (!cr6.gt) goto loc_825CC99C;
	// lwz r7,320(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// mulli r6,r17,28
	ctx.r6.s64 = r17.s64 * 28;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
loc_825CC94C:
	// mulli r11,r9,1776
	r11.s64 = ctx.r9.s64 * 1776;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lwz r11,424(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cc98c
	if (!cr6.gt) goto loc_825CC98C;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
loc_825CC970:
	// lhz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// subf r25,r5,r25
	r25.s64 = r25.s64 - ctx.r5.s64;
	// bne cr6,0x825cc970
	if (!cr6.eq) goto loc_825CC970;
loc_825CC98C:
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x825cc94c
	if (cr6.lt) goto loc_825CC94C;
loc_825CC99C:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825ccd70
	if (!cr6.gt) goto loc_825CCD70;
	// addi r22,r14,224
	r22.s64 = r14.s64 + 224;
loc_825CC9A8:
	// rotlwi r10,r31,1
	ctx.r10.u64 = __builtin_rotateleft32(r31.u32, 1);
	// lwz r9,228(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 228);
	// add r11,r8,r19
	r11.u64 = ctx.r8.u64 + r19.u64;
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
	// divw r8,r31,r9
	ctx.r8.s32 = r31.s32 / ctx.r9.s32;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// twllei r9,0
	// mr r31,r21
	r31.u64 = r21.u64;
	// mr r26,r21
	r26.u64 = r21.u64;
	// twlgei r10,-1
	// extsh r27,r8
	r27.s64 = ctx.r8.s16;
	// mr r24,r21
	r24.u64 = r21.u64;
	// mr r20,r25
	r20.u64 = r25.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lhz r11,34(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// lwz r10,256(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bne cr6,0x825cca3c
	if (!cr6.eq) goto loc_825CCA3C;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cca38
	if (cr6.eq) goto loc_825CCA38;
	// stw r18,128(r14)
	PPC_STORE_U32(r14.u32 + 128, r18.u32);
	// b 0x825cca3c
	goto loc_825CCA3C;
loc_825CCA38:
	// stw r21,128(r14)
	PPC_STORE_U32(r14.u32 + 128, r21.u32);
loc_825CCA3C:
	// lhz r5,34(r28)
	ctx.r5.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// lwz r11,256(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// lwz r6,320(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// extsh r23,r11
	r23.s64 = r11.s16;
	// ble cr6,0x825cca98
	if (!cr6.gt) goto loc_825CCA98;
	// mulli r8,r17,28
	ctx.r8.s64 = r17.s64 * 28;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// addi r11,r6,424
	r11.s64 = ctx.r6.s64 + 424;
loc_825CCA60:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsh r7,r23
	ctx.r7.s64 = r23.s16;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lhz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// cmpw cr6,r7,r4
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r4.s32, xer);
	// ble cr6,0x825cca84
	if (!cr6.gt) goto loc_825CCA84;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
loc_825CCA84:
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// cmpw cr6,r9,r5
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, xer);
	// blt cr6,0x825cca60
	if (cr6.lt) goto loc_825CCA60;
loc_825CCA98:
	// lwz r11,128(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + 128);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ccb04
	if (!cr6.eq) goto loc_825CCB04;
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x825ccb08
	if (!cr6.gt) goto loc_825CCB08;
	// mulli r8,r17,28
	ctx.r8.s64 = r17.s64 * 28;
	// extsh r7,r23
	ctx.r7.s64 = r23.s16;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// addi r11,r6,424
	r11.s64 = ctx.r6.s64 + 424;
loc_825CCAC0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bne cr6,0x825ccae4
	if (!cr6.eq) goto loc_825CCAE4;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
loc_825CCAE4:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// blt cr6,0x825ccac0
	if (cr6.lt) goto loc_825CCAC0;
	// cmpwi cr6,r24,1
	cr6.compare<int32_t>(r24.s32, 1, xer);
	// bgt cr6,0x825ccb0c
	if (cr6.gt) goto loc_825CCB0C;
	// b 0x825ccb08
	goto loc_825CCB08;
loc_825CCB04:
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
loc_825CCB08:
	// mr r31,r18
	r31.u64 = r18.u64;
loc_825CCB0C:
	// rotlwi r11,r25,1
	r11.u64 = __builtin_rotateleft32(r25.u32, 1);
	// divw r10,r25,r24
	ctx.r10.s32 = r25.s32 / r24.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// extsh r9,r27
	ctx.r9.s64 = r27.s16;
	// andc r11,r24,r11
	r11.u64 = r24.u64 & ~r11.u64;
	// twllei r24,0
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// twlgei r11,-1
	// bne cr6,0x825ccb38
	if (!cr6.eq) goto loc_825CCB38;
	// mr r31,r18
	r31.u64 = r18.u64;
	// mr r26,r18
	r26.u64 = r18.u64;
loc_825CCB38:
	// li r29,-1
	r29.s64 = -1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x825ccba8
	if (!cr6.eq) goto loc_825CCBA8;
	// mr r30,r24
	r30.u64 = r24.u64;
	// cmpwi cr6,r24,24
	cr6.compare<int32_t>(r24.s32, 24, xer);
	// li r31,24
	r31.s64 = 24;
	// bgt cr6,0x825ccb58
	if (cr6.gt) goto loc_825CCB58;
	// mr r31,r24
	r31.u64 = r24.u64;
loc_825CCB58:
	// mr r29,r21
	r29.u64 = r21.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x825ccba8
	if (cr6.eq) goto loc_825CCBA8;
loc_825CCB64:
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// subf r30,r31,r30
	r30.s64 = r30.s64 - r31.s64;
	// li r31,24
	r31.s64 = 24;
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// cmpwi cr6,r30,24
	cr6.compare<int32_t>(r30.s32, 24, xer);
	// bgt cr6,0x825ccb98
	if (cr6.gt) goto loc_825CCB98;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_825CCB98:
	// extsw r10,r31
	ctx.r10.s64 = r31.s32;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// sld r29,r11,r10
	r29.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// bne cr6,0x825ccb64
	if (!cr6.eq) goto loc_825CCB64;
loc_825CCBA8:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x825ccc78
	if (!cr6.eq) goto loc_825CCC78;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bne cr6,0x825ccbd8
	if (!cr6.eq) goto loc_825CCBD8;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x825ccc1c
	goto loc_825CCC1C;
loc_825CCBD8:
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r16,16
	ctx.r10.u64 = r16.u32 & 0xFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x825ccc1c
	if (cr6.lt) goto loc_825CCC1C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ccdb4
	if (cr6.lt) goto loc_825CCDB4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825CCC1C:
	// lwz r10,176(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 176);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825ccc4c
	if (!cr6.eq) goto loc_825CCC4C;
	// lwz r9,256(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// slw r11,r18,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r18.u32 << (r11.u8 & 0x3F));
	// rotlwi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// divw r9,r9,r11
	ctx.r9.s32 = ctx.r9.s32 / r11.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// twllei r11,0
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// extsh r27,r9
	r27.s64 = ctx.r9.s16;
	// b 0x825ccc74
	goto loc_825CCC74;
loc_825CCC4C:
	// lwz r10,256(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwz r8,228(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 228);
	// rotlwi r11,r10,1
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// divw r10,r10,r8
	ctx.r10.s32 = ctx.r10.s32 / ctx.r8.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// twllei r8,0
	// andc r11,r8,r11
	r11.u64 = ctx.r8.u64 & ~r11.u64;
	// extsh r27,r10
	r27.s64 = ctx.r10.s16;
loc_825CCC74:
	// twlgei r11,-1
loc_825CCC78:
	// lwz r11,236(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 236);
	// extsh r6,r27
	ctx.r6.s64 = r27.s16;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// blt cr6,0x825ccdbc
	if (cr6.lt) goto loc_825CCDBC;
	// lwz r31,256(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// cmpw cr6,r6,r31
	cr6.compare<int32_t>(ctx.r6.s32, r31.s32, xer);
	// bgt cr6,0x825ccdbc
	if (cr6.gt) goto loc_825CCDBC;
	// lhz r10,34(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// lwz r11,320(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ccdbc
	if (cr6.eq) goto loc_825CCDBC;
	// mulli r4,r17,28
	ctx.r4.s64 = r17.s64 * 28;
	// addi r7,r11,424
	ctx.r7.s64 = r11.s64 + 424;
loc_825CCCB0:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bgt cr6,0x825ccdbc
	if (cr6.gt) goto loc_825CCDBC;
	// lhz r30,0(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r26,r23
	r26.s64 = r23.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// cmpw cr6,r26,r30
	cr6.compare<int32_t>(r26.s32, r30.s32, xer);
	// bne cr6,0x825ccd44
	if (!cr6.eq) goto loc_825CCD44;
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// extsw r30,r24
	r30.s64 = r24.s32;
	// sld r30,r18,r30
	r30.u64 = r30.u8 & 0x40 ? 0 : (r18.u64 << (r30.u8 & 0x7F));
	// and r30,r30,r29
	r30.u64 = r30.u64 & r29.u64;
	// cmpldi cr6,r30,0
	cr6.compare<uint64_t>(r30.u64, 0, xer);
	// beq cr6,0x825ccd44
	if (cr6.eq) goto loc_825CCD44;
	// cmpwi cr6,r9,32
	cr6.compare<int32_t>(ctx.r9.s32, 32, xer);
	// bge cr6,0x825ccdbc
	if (!cr6.lt) goto loc_825CCDBC;
	// rlwinm r31,r9,1,0,30
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r25,r6,r25
	r25.s64 = r25.s64 - ctx.r6.s64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// sthx r27,r31,r8
	PPC_STORE_U16(r31.u32 + ctx.r8.u32, r27.u16);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// blt cr6,0x825ccdbc
	if (cr6.lt) goto loc_825CCDBC;
	// lwz r31,256(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 256);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// bgt cr6,0x825ccdbc
	if (cr6.gt) goto loc_825CCDBC;
loc_825CCD44:
	// addi r11,r5,1
	r11.s64 = ctx.r5.s64 + 1;
	// lhz r8,34(r28)
	ctx.r8.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// addi r7,r7,1776
	ctx.r7.s64 = ctx.r7.s64 + 1776;
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x825cccb0
	if (cr6.lt) goto loc_825CCCB0;
	// cmpw cr6,r25,r20
	cr6.compare<int32_t>(r25.s32, r20.s32, xer);
	// bge cr6,0x825ccdbc
	if (!cr6.lt) goto loc_825CCDBC;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// bgt cr6,0x825cc9a8
	if (cr6.gt) goto loc_825CC9A8;
loc_825CCD70:
	// lhz r11,34(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ccdb4
	if (cr6.eq) goto loc_825CCDB4;
	// mulli r8,r17,28
	ctx.r8.s64 = r17.s64 * 28;
	// mr r11,r21
	r11.u64 = r21.u64;
loc_825CCD84:
	// lwz r9,320(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// mulli r10,r11,1776
	ctx.r10.s64 = r11.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// lwz r10,424(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 424);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// sth r21,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r21.u16);
	// lhz r10,34(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 34);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ccd84
	if (cr6.lt) goto loc_825CCD84;
loc_825CCDB4:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825CCDBC:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825CCDCC"))) PPC_WEAK_FUNC(sub_825CCDCC);
PPC_FUNC_IMPL(__imp__sub_825CCDCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CCDD0"))) PPC_WEAK_FUNC(sub_825CCDD0);
PPC_FUNC_IMPL(__imp__sub_825CCDD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// li r5,25
	ctx.r5.s64 = 25;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825ccdec
	if (!cr6.gt) goto loc_825CCDEC;
	// li r5,28
	ctx.r5.s64 = 28;
loc_825CCDEC:
	// stw r5,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r5.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825cce04
	if (cr6.gt) goto loc_825CCE04;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r10,r10,-4112
	ctx.r10.s64 = ctx.r10.s64 + -4112;
	// b 0x825cce0c
	goto loc_825CCE0C;
loc_825CCE04:
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r10,r10,18976
	ctx.r10.s64 = ctx.r10.s64 + 18976;
loc_825CCE0C:
	// stw r10,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r10.u32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// lwz r11,344(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r10.u64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfd f0,-200(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f0.f64 = double(temp.f32);
	// fdivs f11,f0,f13
	ctx.f11.f64 = double(float(f0.f64 / ctx.f13.f64));
	// bne cr6,0x825cceec
	if (!cr6.eq) goto loc_825CCEEC;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// ble cr6,0x825cd0fc
	if (!cr6.gt) goto loc_825CD0FC;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lwz r10,-208(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// lwz r7,252(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// lfs f0,5736(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5736);
	f0.f64 = double(temp.f32);
loc_825CCE64:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r4,r1,-208
	ctx.r4.s64 = ctx.r1.s64 + -208;
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r8,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r8.u64);
	// lfd f13,-192(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f13,f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + f0.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// lwz r8,-208(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r7,252(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// srawi r4,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r7.s32 >> 1;
	// addze r4,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r4.s64 = temp.s64;
	// cmpw cr6,r8,r4
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r4.s32, xer);
	// bgt cr6,0x825ccec4
	if (cr6.gt) goto loc_825CCEC4;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x825cce64
	if (cr6.lt) goto loc_825CCE64;
	// b 0x825cd0fc
	goto loc_825CD0FC;
loc_825CCEC4:
	// lwz r10,252(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// addi r9,r6,1
	ctx.r9.s64 = ctx.r6.s64 + 1;
	// addi r8,r6,1
	ctx.r8.s64 = ctx.r6.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// stwx r10,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r10.u32);
	// lwz r11,340(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x825cd0fc
	goto loc_825CD0FC;
loc_825CCEEC:
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r10,244(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 244);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r8,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r8.u32);
	// ble cr6,0x825cd0fc
	if (!cr6.gt) goto loc_825CD0FC;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// li r26,17
	r26.s64 = 17;
	// li r31,5
	r31.s64 = 5;
	// li r30,12
	r30.s64 = 12;
	// lfs f12,22976(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 22976);
	ctx.f12.f64 = double(temp.f32);
	// li r5,18
	ctx.r5.s64 = 18;
	// lfs f13,560(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 560);
	ctx.f13.f64 = double(temp.f32);
	// li r4,34
	ctx.r4.s64 = 34;
	// li r15,46
	r15.s64 = 46;
	// li r16,63
	r16.s64 = 63;
	// li r17,86
	r17.s64 = 86;
	// li r28,102
	r28.s64 = 102;
	// li r29,123
	r29.s64 = 123;
	// li r18,149
	r18.s64 = 149;
	// li r19,179
	r19.s64 = 179;
	// li r20,221
	r20.s64 = 221;
	// li r21,512
	r21.s64 = 512;
	// li r23,15
	r23.s64 = 15;
	// li r24,11
	r24.s64 = 11;
	// li r25,37
	r25.s64 = 37;
	// li r14,74
	r14.s64 = 74;
	// li r27,256
	r27.s64 = 256;
	// li r22,128
	r22.s64 = 128;
loc_825CCF60:
	// std r11,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, r11.u64);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// li r11,1
	r11.s64 = 1;
	// lwz r6,60(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// lwz r9,252(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// lwz r6,-200(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -200);
	// rotlwi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r6,r11,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r6.u8 & 0x3F));
	// andc r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 & ~ctx.r10.u64;
	// twllei r6,0
	// divw r6,r9,r6
	ctx.r6.s32 = ctx.r9.s32 / ctx.r6.s32;
	// twlgei r10,-1
	// ld r11,-184(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// bgt cr6,0x825cd3a8
	if (cr6.gt) goto loc_825CD3A8;
	// lis r9,0
	ctx.r9.s64 = 0;
	// lwz r10,80(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// ori r9,r9,44100
	ctx.r9.u64 = ctx.r9.u64 | 44100;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cd180
	if (cr6.lt) goto loc_825CD180;
	// cmpwi cr6,r6,1024
	cr6.compare<int32_t>(ctx.r6.s32, 1024, xer);
	// bne cr6,0x825cd024
	if (!cr6.eq) goto loc_825CD024;
	// lwz r6,340(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// addi r9,r11,60
	ctx.r9.s64 = r11.s64 + 60;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r26,r8,r6
	PPC_STORE_U32(ctx.r8.u32 + ctx.r6.u32, r26.u32);
	// li r6,25
	ctx.r6.s64 = 25;
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r5,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r5.u32);
	// stw r4,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r4.u32);
	// stw r15,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r15.u32);
	// stw r6,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r6.u32);
	// li r6,54
	ctx.r6.s64 = 54;
	// stw r16,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r16.u32);
	// stw r17,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r17.u32);
	// stw r28,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r28.u32);
	// stw r29,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r29.u32);
	// stw r6,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r6.u32);
	// li r6,279
	ctx.r6.s64 = 279;
	// stw r18,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r18.u32);
	// stw r19,52(r11)
	PPC_STORE_U32(r11.u32 + 52, r19.u32);
	// stw r20,56(r11)
	PPC_STORE_U32(r11.u32 + 56, r20.u32);
	// stw r21,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r21.u32);
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// li r6,360
	ctx.r6.s64 = 360;
	// stw r6,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r6.u32);
	// b 0x825cd094
	goto loc_825CD094;
loc_825CD024:
	// cmpwi cr6,r6,512
	cr6.compare<int32_t>(ctx.r6.s32, 512, xer);
	// bne cr6,0x825cd114
	if (!cr6.eq) goto loc_825CD114;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r23,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, r23.u32);
	// li r9,23
	ctx.r9.s64 = 23;
	// stw r24,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r24.u32);
	// stw r26,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r26.u32);
	// stw r25,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r25.u32);
	// stw r14,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r14.u32);
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,31
	ctx.r9.s64 = 31;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,43
	ctx.r9.s64 = 43;
	// stw r9,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r9.u32);
	// li r9,51
	ctx.r9.s64 = 51;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,62
	ctx.r9.s64 = 62;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// li r9,89
	ctx.r9.s64 = 89;
	// stw r9,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r9.u32);
	// li r9,110
	ctx.r9.s64 = 110;
	// stw r9,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r9.u32);
	// li r9,139
	ctx.r9.s64 = 139;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// li r9,180
	ctx.r9.s64 = 180;
loc_825CD08C:
	// stw r9,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r9.u32);
	// stw r27,60(r11)
	PPC_STORE_U32(r11.u32 + 60, r27.u32);
loc_825CD094:
	// stw r31,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r31.u32);
loc_825CD098:
	// lwz r6,340(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// lwzx r7,r8,r6
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x825cd0dc
	if (!cr6.gt) goto loc_825CD0DC;
loc_825CD0AC:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// srawi r7,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r7,340(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// lwzx r7,r8,r7
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// blt cr6,0x825cd0ac
	if (cr6.lt) goto loc_825CD0AC;
loc_825CD0DC:
	// lwz r10,-200(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -200);
	// addi r11,r11,116
	r11.s64 = r11.s64 + 116;
	// lwz r9,244(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 244);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// stw r10,-200(r1)
	PPC_STORE_U32(ctx.r1.u32 + -200, ctx.r10.u32);
	// blt cr6,0x825ccf60
	if (cr6.lt) goto loc_825CCF60;
loc_825CD0FC:
	// lwz r11,344(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	// lwz r10,340(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// stw r11,308(r3)
	PPC_STORE_U32(ctx.r3.u32 + 308, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,304(r3)
	PPC_STORE_U32(ctx.r3.u32 + 304, r11.u32);
	// b 0x8239bd10
	return;
loc_825CD114:
	// cmpwi cr6,r6,256
	cr6.compare<int32_t>(ctx.r6.s32, 256, xer);
	// bne cr6,0x825cd3a8
	if (!cr6.eq) goto loc_825CD3A8;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r30,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, r30.u32);
	// li r9,9
	ctx.r9.s64 = 9;
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
	// stw r25,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r25.u32);
	// stw r22,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r22.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// li r9,16
	ctx.r9.s64 = 16;
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,21
	ctx.r9.s64 = 21;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,26
	ctx.r9.s64 = 26;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// li r9,45
	ctx.r9.s64 = 45;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,55
	ctx.r9.s64 = 55;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// li r9,70
	ctx.r9.s64 = 70;
	// stw r9,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r9.u32);
	// li r9,90
	ctx.r9.s64 = 90;
	// stw r9,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r9.u32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// b 0x825cd098
	goto loc_825CD098;
loc_825CD180:
	// cmpwi cr6,r10,32000
	cr6.compare<int32_t>(ctx.r10.s32, 32000, xer);
	// blt cr6,0x825cd2f4
	if (cr6.lt) goto loc_825CD2F4;
	// cmpwi cr6,r6,1024
	cr6.compare<int32_t>(ctx.r6.s32, 1024, xer);
	// bne cr6,0x825cd21c
	if (!cr6.eq) goto loc_825CD21C;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r6,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r6.u32);
	// li r9,13
	ctx.r9.s64 = 13;
	// stw r14,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r14.u32);
	// stw r21,64(r11)
	PPC_STORE_U32(r11.u32 + 64, r21.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// li r9,20
	ctx.r9.s64 = 20;
	// stw r9,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r9.u32);
	// li r9,29
	ctx.r9.s64 = 29;
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,41
	ctx.r9.s64 = 41;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,55
	ctx.r9.s64 = 55;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// li r9,101
	ctx.r9.s64 = 101;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,141
	ctx.r9.s64 = 141;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// li r9,170
	ctx.r9.s64 = 170;
	// stw r9,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r9.u32);
	// li r9,205
	ctx.r9.s64 = 205;
	// stw r9,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r9.u32);
	// li r9,246
	ctx.r9.s64 = 246;
	// stw r9,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r9.u32);
	// li r9,304
	ctx.r9.s64 = 304;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// li r9,384
	ctx.r9.s64 = 384;
	// stw r9,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r9.u32);
	// li r9,496
	ctx.r9.s64 = 496;
	// stw r9,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r9.u32);
	// li r9,6
	ctx.r9.s64 = 6;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// b 0x825cd098
	goto loc_825CD098;
loc_825CD21C:
	// cmpwi cr6,r6,512
	cr6.compare<int32_t>(ctx.r6.s32, 512, xer);
	// bne cr6,0x825cd288
	if (!cr6.eq) goto loc_825CD288;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r23,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, r23.u32);
	// li r9,10
	ctx.r9.s64 = 10;
	// stw r23,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r23.u32);
	// stw r25,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r25.u32);
	// stw r28,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r28.u32);
	// stw r29,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r29.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// li r9,20
	ctx.r9.s64 = 20;
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,28
	ctx.r9.s64 = 28;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,50
	ctx.r9.s64 = 50;
	// stw r9,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r9.u32);
	// li r9,70
	ctx.r9.s64 = 70;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,85
	ctx.r9.s64 = 85;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// li r9,152
	ctx.r9.s64 = 152;
	// stw r9,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r9.u32);
	// li r9,192
	ctx.r9.s64 = 192;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// li r9,248
	ctx.r9.s64 = 248;
	// b 0x825cd08c
	goto loc_825CD08C;
loc_825CD288:
	// cmpwi cr6,r6,256
	cr6.compare<int32_t>(ctx.r6.s32, 256, xer);
	// bne cr6,0x825cd3a8
	if (!cr6.eq) goto loc_825CD3A8;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r24,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, r24.u32);
	// li r9,9
	ctx.r9.s64 = 9;
	// stw r22,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r22.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// li r9,14
	ctx.r9.s64 = 14;
	// stw r9,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r9.u32);
	// li r9,19
	ctx.r9.s64 = 19;
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,25
	ctx.r9.s64 = 25;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,35
	ctx.r9.s64 = 35;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// li r9,51
	ctx.r9.s64 = 51;
	// stw r9,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r9.u32);
	// li r9,76
	ctx.r9.s64 = 76;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,96
	ctx.r9.s64 = 96;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// li r9,124
	ctx.r9.s64 = 124;
	// stw r9,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r9.u32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// b 0x825cd098
	goto loc_825CD098;
loc_825CD2F4:
	// cmpwi cr6,r10,22050
	cr6.compare<int32_t>(ctx.r10.s32, 22050, xer);
	// blt cr6,0x825cd3a8
	if (cr6.lt) goto loc_825CD3A8;
	// cmpwi cr6,r6,512
	cr6.compare<int32_t>(ctx.r6.s32, 512, xer);
	// bne cr6,0x825cd350
	if (!cr6.eq) goto loc_825CD350;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// li r6,14
	ctx.r6.s64 = 14;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r6,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r6.u32);
	// li r9,25
	ctx.r9.s64 = 25;
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r5,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r5.u32);
	// stw r4,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r4.u32);
	// stw r15,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r15.u32);
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// stw r16,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r16.u32);
	// stw r17,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r17.u32);
	// stw r28,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r28.u32);
	// stw r29,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r29.u32);
	// stw r18,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r18.u32);
	// stw r19,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r19.u32);
	// stw r20,52(r11)
	PPC_STORE_U32(r11.u32 + 52, r20.u32);
	// stw r27,56(r11)
	PPC_STORE_U32(r11.u32 + 56, r27.u32);
	// b 0x825cd094
	goto loc_825CD094;
loc_825CD350:
	// cmpwi cr6,r6,256
	cr6.compare<int32_t>(ctx.r6.s32, 256, xer);
	// bne cr6,0x825cd3a8
	if (!cr6.eq) goto loc_825CD3A8;
	// lwz r9,340(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// li r6,10
	ctx.r6.s64 = 10;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stwx r6,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r6.u32);
	// li r9,23
	ctx.r9.s64 = 23;
	// stw r24,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r24.u32);
	// stw r26,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r26.u32);
	// stw r22,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r22.u32);
	// stw r9,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r9.u32);
	// li r9,31
	ctx.r9.s64 = 31;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// li r9,43
	ctx.r9.s64 = 43;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// li r9,62
	ctx.r9.s64 = 62;
	// stw r9,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r9.u32);
	// li r9,89
	ctx.r9.s64 = 89;
	// stw r9,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r9.u32);
	// li r9,110
	ctx.r9.s64 = 110;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// b 0x825cd094
	goto loc_825CD094;
loc_825CD3A8:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lwz r9,-208(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// std r10,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r10.u64);
	// lfd f0,-176(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f11
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
loc_825CD3CC:
	// lwz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bgt cr6,0x825cd414
	if (cr6.gt) goto loc_825CD414;
	// std r10,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r10.u64);
	// addi r14,r1,-204
	r14.s64 = ctx.r1.s64 + -204;
	// lfd f10,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmadds f10,f10,f0,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * f0.f64 + ctx.f13.f64));
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r14
	PPC_STORE_U32(r14.u32, ctx.f10.u32);
	// lwz r10,-204(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -204);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x825cd43c
	goto loc_825CD43C;
loc_825CD414:
	// lwz r14,80(r3)
	r14.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// divwu r10,r10,r14
	ctx.r10.u32 = ctx.r10.u32 / r14.u32;
	// twllei r14,0
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// srawi r14,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r14.s64 = ctx.r10.s32 >> 2;
	// addze r14,r14
	temp.s64 = r14.s64 + xer.ca;
	xer.ca = temp.u32 < r14.u32;
	r14.s64 = temp.s64;
	// rlwinm r14,r14,2,0,29
	r14.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r14,r14,r10
	r14.s64 = ctx.r10.s64 - r14.s64;
	// subf r10,r14,r10
	ctx.r10.s64 = ctx.r10.s64 - r14.s64;
loc_825CD43C:
	// lwz r14,0(r5)
	r14.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmpw cr6,r10,r14
	cr6.compare<int32_t>(ctx.r10.s32, r14.s32, xer);
	// ble cr6,0x825cd454
	if (!cr6.gt) goto loc_825CD454;
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
loc_825CD454:
	// lwz r10,-192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// bge cr6,0x825cd474
	if (!cr6.lt) goto loc_825CD474;
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// lwz r14,0(r5)
	r14.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// cmpw cr6,r14,r10
	cr6.compare<int32_t>(r14.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cd3cc
	if (cr6.lt) goto loc_825CD3CC;
loc_825CD474:
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 1;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// addi r7,r4,-1
	ctx.r7.s64 = ctx.r4.s64 + -1;
	// li r4,34
	ctx.r4.s64 = 34;
	// li r5,18
	ctx.r5.s64 = 18;
	// li r14,74
	r14.s64 = 74;
	// stw r9,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r9.u32);
	// lwz r10,340(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// stwx r7,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r7.u32);
	// b 0x825cd0dc
	goto loc_825CD0DC;
}

__attribute__((alias("__imp__sub_825CD4A4"))) PPC_WEAK_FUNC(sub_825CD4A4);
PPC_FUNC_IMPL(__imp__sub_825CD4A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CD4A8"))) PPC_WEAK_FUNC(sub_825CD4A8);
PPC_FUNC_IMPL(__imp__sub_825CD4A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cd4c8
	if (cr6.lt) goto loc_825CD4C8;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r11.u16);
	// sth r6,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r6.u16);
	// b 0x825cd4e8
	goto loc_825CD4E8;
loc_825CD4C8:
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// sth r10,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r10.u16);
	// sth r11,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, r11.u16);
loc_825CD4E8:
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// bne cr6,0x825cd504
	if (!cr6.eq) goto loc_825CD504;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r11,148(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// b 0x825cd510
	goto loc_825CD510;
loc_825CD504:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r11,156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
loc_825CD510:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lhz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// sth r11,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r11.u16);
	// sth r11,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CD544"))) PPC_WEAK_FUNC(sub_825CD544);
PPC_FUNC_IMPL(__imp__sub_825CD544) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CD548"))) PPC_WEAK_FUNC(sub_825CD548);
PPC_FUNC_IMPL(__imp__sub_825CD548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// extsh r10,r6
	ctx.r10.s64 = ctx.r6.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825cd564
	if (cr6.lt) goto loc_825CD564;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// b 0x825cd588
	goto loc_825CD588;
loc_825CD564:
	// subf r6,r10,r11
	ctx.r6.s64 = r11.s64 - ctx.r10.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r6,r11,r7
	ctx.r6.u64 = r11.u64 + ctx.r7.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// sth r6,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r6.u16);
loc_825CD588:
	// sth r11,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r11.u16);
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// bne cr6,0x825cd5a8
	if (!cr6.eq) goto loc_825CD5A8;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r11,156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// b 0x825cd5b4
	goto loc_825CD5B4;
loc_825CD5A8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lwz r11,148(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
loc_825CD5B4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lhz r11,0(r8)
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// sth r11,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, r11.u16);
	// sth r11,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CD5E8"))) PPC_WEAK_FUNC(sub_825CD5E8);
PPC_FUNC_IMPL(__imp__sub_825CD5E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// addi r12,r1,-40
	r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8239d5e0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lhz r11,580(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cd76c
	if (!cr6.gt) goto loc_825CD76C;
	// lis r7,-32251
	ctx.r7.s64 = -2113601536;
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f27,264(r7)
	ctx.fpscr.disableFlushMode();
	f27.u64 = PPC_LOAD_U64(ctx.r7.u32 + 264);
	// li r29,0
	r29.s64 = 0;
	// lfd f28,-28592(r8)
	f28.u64 = PPC_LOAD_U64(ctx.r8.u32 + -28592);
	// addi r28,r11,26480
	r28.s64 = r11.s64 + 26480;
	// lfd f29,-31856(r9)
	f29.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31856);
	// lfs f26,560(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 560);
	f26.f64 = double(temp.f32);
loc_825CD63C:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// rlwinm r9,r29,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,124(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 124);
	// lhz r10,122(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 122);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bge cr6,0x825cd674
	if (!cr6.lt) goto loc_825CD674;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_825CD674:
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x825cd6d4
	if (cr6.lt) goto loc_825CD6D4;
	// cmpwi cr6,r11,2048
	cr6.compare<int32_t>(r11.s32, 2048, xer);
	// bgt cr6,0x825cd6d4
	if (cr6.gt) goto loc_825CD6D4;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & r11.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825cd6d4
	if (!cr6.eq) goto loc_825CD6D4;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r28
	r11.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,72(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 72, temp.u32);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,76(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 76, temp.u32);
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
	// stfs f0,80(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 80, temp.u32);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,84(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 84, temp.u32);
	// lfs f0,32(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 32);
	f0.f64 = double(temp.f32);
	// fmuls f0,f0,f26
	f0.f64 = double(float(f0.f64 * f26.f64));
	// b 0x825cd750
	goto loc_825CD750;
loc_825CD6D4:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fdiv f30,f29,f0
	f30.f64 = f29.f64 / f0.f64;
	// fmul f31,f30,f28
	f31.f64 = f30.f64 * f28.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,72(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 72, temp.u32);
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,76(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 76, temp.u32);
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,80(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 80, temp.u32);
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,84(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 84, temp.u32);
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmul f0,f1,f27
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64 * f27.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
loc_825CD750:
	// stfs f0,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 88, temp.u32);
	// lhz r10,580(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 580);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cd63c
	if (cr6.lt) goto loc_825CD63C;
loc_825CD76C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// addi r12,r1,-40
	r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8239d62c
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825CD780"))) PPC_WEAK_FUNC(sub_825CD780);
PPC_FUNC_IMPL(__imp__sub_825CD780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// blt cr6,0x825cd7a4
	if (cr6.lt) goto loc_825CD7A4;
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// li r11,31
	r11.s64 = 31;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// stw r10,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r10.u32);
	// blr 
	return;
loc_825CD7A4:
	// cmpwi cr6,r4,5
	cr6.compare<int32_t>(ctx.r4.s32, 5, xer);
	// bge cr6,0x825cd7b4
	if (!cr6.lt) goto loc_825CD7B4;
loc_825CD7AC:
	// li r11,13
	r11.s64 = 13;
	// b 0x825cd7f4
	goto loc_825CD7F4;
loc_825CD7B4:
	// cmpwi cr6,r4,15
	cr6.compare<int32_t>(ctx.r4.s32, 15, xer);
	// blt cr6,0x825cd7ac
	if (cr6.lt) goto loc_825CD7AC;
	// cmpwi cr6,r4,32
	cr6.compare<int32_t>(ctx.r4.s32, 32, xer);
	// bge cr6,0x825cd7cc
	if (!cr6.lt) goto loc_825CD7CC;
	// li r11,12
	r11.s64 = 12;
	// b 0x825cd7f4
	goto loc_825CD7F4;
loc_825CD7CC:
	// cmpwi cr6,r4,40
	cr6.compare<int32_t>(ctx.r4.s32, 40, xer);
	// bge cr6,0x825cd7dc
	if (!cr6.lt) goto loc_825CD7DC;
	// li r11,11
	r11.s64 = 11;
	// b 0x825cd7f4
	goto loc_825CD7F4;
loc_825CD7DC:
	// cmpwi cr6,r4,45
	cr6.compare<int32_t>(ctx.r4.s32, 45, xer);
	// bge cr6,0x825cd7ec
	if (!cr6.lt) goto loc_825CD7EC;
	// li r11,10
	r11.s64 = 10;
	// b 0x825cd7f4
	goto loc_825CD7F4;
loc_825CD7EC:
	// cmpwi cr6,r4,55
	cr6.compare<int32_t>(ctx.r4.s32, 55, xer);
	// li r11,9
	r11.s64 = 9;
loc_825CD7F4:
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CD810"))) PPC_WEAK_FUNC(sub_825CD810);
PPC_FUNC_IMPL(__imp__sub_825CD810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// li r30,-5
	r30.s64 = -5;
	// addi r29,r11,-25280
	r29.s64 = r11.s64 + -25280;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// mr r31,r29
	r31.u64 = r29.u64;
	// lfd f31,32128(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + 32128);
loc_825CD83C:
	// extsw r11,r30
	r11.s64 = r30.s32;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f2,f0
	ctx.f2.f64 = double(f0.s64);
	// bl 0x8239e6a0
	sub_8239E6A0(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,0(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// addi r11,r29,80
	r11.s64 = r29.s64 + 80;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x825cd83c
	if (cr6.lt) goto loc_825CD83C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r31,r29,80
	r31.s64 = r29.s64 + 80;
	// li r30,0
	r30.s64 = 0;
	// lfs f30,3008(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3008);
	f30.f64 = double(temp.f32);
loc_825CD880:
	// extsw r11,r30
	r11.s64 = r30.s32;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f2,f0,f30
	ctx.f2.f64 = double(float(f0.f64 * f30.f64));
	// bl 0x8239e6a0
	sub_8239E6A0(ctx, base);
	// addi r11,r29,80
	r11.s64 = r29.s64 + 80;
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,0(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r11,r11,80
	r11.s64 = r11.s64 + 80;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x825cd880
	if (cr6.lt) goto loc_825CD880;
	// li r11,1
	r11.s64 = 1;
	// stw r11,160(r29)
	PPC_STORE_U32(r29.u32 + 160, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-48(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825CD8D8"))) PPC_WEAK_FUNC(sub_825CD8D8);
PPC_FUNC_IMPL(__imp__sub_825CD8D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r11,-25280
	r30.s64 = r11.s64 + -25280;
	// lwz r11,160(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cd908
	if (!cr6.eq) goto loc_825CD908;
	// bl 0x825cd810
	sub_825CD810(ctx, base);
loc_825CD908:
	// addi r11,r31,-20
	r11.s64 = r31.s64 + -20;
	// cmplwi cr6,r11,299
	cr6.compare<uint32_t>(r11.u32, 299, xer);
	// bgt cr6,0x825cd970
	if (cr6.gt) goto loc_825CD970;
	// lis r11,26214
	r11.s64 = 1717960704;
	// ori r11,r11,26215
	r11.u64 = r11.u64 | 26215;
	// mulhw r11,r31,r11
	r11.s64 = (int64_t(r31.s32) * int64_t(r11.s32)) >> 32;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x825cd950
	if (!cr6.lt) goto loc_825CD950;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
loc_825CD950:
	// addi r9,r30,80
	ctx.r9.s64 = r30.s64 + 80;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r30,20
	ctx.r8.s64 = r30.s64 + 20;
	// lfsx f0,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	f0.f64 = double(temp.f32);
	// lfsx f13,r10,r8
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 * ctx.f13.f64));
	// b 0x825cd9a0
	goto loc_825CD9A0;
loc_825CD970:
	// extsw r10,r31
	ctx.r10.s64 = r31.s32;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f1,32128(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + 32128);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,3008(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3008);
	f0.f64 = double(temp.f32);
	// fmuls f2,f13,f0
	ctx.f2.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x8239e6a0
	sub_8239E6A0(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
loc_825CD9A0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CD9B8"))) PPC_WEAK_FUNC(sub_825CD9B8);
PPC_FUNC_IMPL(__imp__sub_825CD9B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,176(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 176);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cdcf8
	if (!cr6.eq) goto loc_825CDCF8;
	// lwz r6,60(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// bgt cr6,0x825cda64
	if (cr6.gt) goto loc_825CDA64;
	// lwz r9,320(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r11,424(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 424);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cd9f8
	if (!cr6.eq) goto loc_825CD9F8;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825CD9F8:
	// lhz r8,34(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825cda2c
	if (!cr6.gt) goto loc_825CDA2C;
	// clrlwi r11,r8,16
	r11.u64 = ctx.r8.u32 & 0xFFFF;
	// addi r10,r9,40
	ctx.r10.s64 = ctx.r9.s64 + 40;
loc_825CDA0C:
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x825cda1c
	if (!cr6.eq) goto loc_825CDA1C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825CDA1C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 + 1776;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cda0c
	if (!cr6.eq) goto loc_825CDA0C;
loc_825CDA2C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825cda64
	if (cr6.eq) goto loc_825CDA64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825cda64
	if (!cr6.gt) goto loc_825CDA64;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// addi r10,r9,48
	ctx.r10.s64 = ctx.r9.s64 + 48;
loc_825CDA44:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825cda54
	if (cr6.eq) goto loc_825CDA54;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825CDA54:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 + 1776;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cda44
	if (!cr6.eq) goto loc_825CDA44;
loc_825CDA64:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// bgt cr6,0x825cdbac
	if (cr6.gt) goto loc_825CDBAC;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825cda84
	if (cr6.eq) goto loc_825CDA84;
	// lwz r9,464(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// b 0x825cdb78
	goto loc_825CDB78;
loc_825CDA84:
	// lwz r11,320(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r10,444(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 444);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,424(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lhz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// beq cr6,0x825cdac0
	if (cr6.eq) goto loc_825CDAC0;
	// lwz r9,456(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// sraw r11,r11,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// sraw r10,r10,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// b 0x825cdaec
	goto loc_825CDAEC;
loc_825CDAC0:
	// lwz r9,448(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 448);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825cdaec
	if (cr6.eq) goto loc_825CDAEC;
	// lwz r9,456(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// slw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// slw r9,r8,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
loc_825CDAEC:
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cdb04
	if (cr6.lt) goto loc_825CDB04;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825cdb24
	goto loc_825CDB24;
loc_825CDB04:
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// addze r11,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	r11.s64 = temp.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
loc_825CDB24:
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// bne cr6,0x825cdb54
	if (!cr6.eq) goto loc_825CDB54;
	// lwz r8,148(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x825cdb54
	if (!cr6.eq) goto loc_825CDB54;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
loc_825CDB54:
	// lwz r8,468(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 468);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// stw r5,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r5.u32);
	// srawi r11,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r11.s64 = ctx.r8.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
loc_825CDB78:
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cdce8
	if (cr6.eq) goto loc_825CDCE8;
	// li r11,0
	r11.s64 = 0;
loc_825CDB8C:
	// lwz r8,360(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lhz r8,34(r3)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x825cdb8c
	if (cr6.lt) goto loc_825CDB8C;
	// b 0x825cdce8
	goto loc_825CDCE8;
loc_825CDBAC:
	// lwz r11,372(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 372);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cdc20
	if (cr6.eq) goto loc_825CDC20;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cdce8
	if (cr6.eq) goto loc_825CDCE8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825CDBCC:
	// lwz r11,444(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cdbe8
	if (cr6.eq) goto loc_825CDBE8;
	// lwz r11,376(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 376);
	// lwz r8,456(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// srw r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r8.u8 & 0x3F));
	// b 0x825cdc00
	goto loc_825CDC00;
loc_825CDBE8:
	// lwz r11,448(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,376(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 376);
	// beq cr6,0x825cdc00
	if (cr6.eq) goto loc_825CDC00;
	// lwz r8,456(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// slw r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r8.u8 & 0x3F));
loc_825CDC00:
	// lwz r8,360(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stwx r11,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825cdbcc
	if (cr6.lt) goto loc_825CDBCC;
	// b 0x825cdce8
	goto loc_825CDCE8;
loc_825CDC20:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x825cdc38
	if (cr6.eq) goto loc_825CDC38;
	// lwz r11,468(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 468);
	// neg r8,r11
	ctx.r8.s64 = -r11.s64;
	// b 0x825cdca8
	goto loc_825CDCA8;
loc_825CDC38:
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cdca8
	if (cr6.eq) goto loc_825CDCA8;
	// lwz r11,320(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r7,444(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 444);
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// addi r10,r11,424
	ctx.r10.s64 = r11.s64 + 424;
loc_825CDC54:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// beq cr6,0x825cdc78
	if (cr6.eq) goto loc_825CDC78;
	// lwz r6,456(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// sraw r11,r11,r6
	temp.u32 = ctx.r6.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// b 0x825cdc8c
	goto loc_825CDC8C;
loc_825CDC78:
	// lwz r6,448(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 448);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x825cdc8c
	if (cr6.eq) goto loc_825CDC8C;
	// lwz r6,456(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 456);
	// slw r11,r11,r6
	r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r6.u8 & 0x3F));
loc_825CDC8C:
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x825cdc98
	if (!cr6.gt) goto loc_825CDC98;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_825CDC98:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 + 1776;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825cdc54
	if (!cr6.eq) goto loc_825CDC54;
loc_825CDCA8:
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cdce8
	if (cr6.eq) goto loc_825CDCE8;
	// li r11,0
	r11.s64 = 0;
loc_825CDCBC:
	// lwz r9,468(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 468);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r7,360(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 360);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stwx r9,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cdcbc
	if (cr6.lt) goto loc_825CDCBC;
loc_825CDCE8:
	// lwz r11,72(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825cdcf8
	if (!cr6.eq) goto loc_825CDCF8;
	// stw r5,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, ctx.r5.u32);
loc_825CDCF8:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CDD00"))) PPC_WEAK_FUNC(sub_825CDD00);
PPC_FUNC_IMPL(__imp__sub_825CDD00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcd0
	// addi r12,r1,-120
	r12.s64 = ctx.r1.s64 + -120;
	// bl 0x8239d5b4
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// li r30,1
	r30.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// lwz r5,60(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
	// lwz r24,552(r7)
	r24.u64 = PPC_LOAD_U32(ctx.r7.u32 + 552);
	// lwz r31,320(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 320);
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// ble cr6,0x825cdd60
	if (!cr6.gt) goto loc_825CDD60;
	// lwz r11,576(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 576);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cdd4c
	if (cr6.eq) goto loc_825CDD4C;
	// lwz r11,572(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 572);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cdd60
	if (!cr6.eq) goto loc_825CDD60;
loc_825CDD4C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r12,r1,-120
	r12.s64 = ctx.r1.s64 + -120;
	// bl 0x8239d600
	// b 0x8239bd20
	return;
loc_825CDD60:
	// lhz r11,580(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 580);
	// extsh r6,r11
	ctx.r6.s64 = r11.s16;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x825cddb0
	if (!cr6.gt) goto loc_825CDDB0;
	// lwz r8,584(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 584);
	// li r11,0
	r11.s64 = 0;
loc_825CDD78:
	// rlwinm r29,r11,1,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// lhzx r9,r29,r8
	ctx.r9.u64 = PPC_LOAD_U16(r29.u32 + ctx.r8.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// mulli r9,r9,1776
	ctx.r9.s64 = ctx.r9.s64 * 1776;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lwz r9,40(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// blt cr6,0x825cdd78
	if (cr6.lt) goto loc_825CDD78;
loc_825CDDB0:
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// bgt cr6,0x825cdf04
	if (cr6.gt) goto loc_825CDF04;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825cdf04
	if (!cr6.eq) goto loc_825CDF04;
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ce8e0
	if (cr6.eq) goto loc_825CE8E0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825ce8e0
	if (!cr6.eq) goto loc_825CE8E0;
	// lwz r10,320(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 320);
	// lhz r9,34(r7)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + 34);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,56(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// lwz r10,1832(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1832);
	// beq cr6,0x825cde14
	if (cr6.eq) goto loc_825CDE14;
	// li r9,0
	ctx.r9.s64 = 0;
loc_825CDDF0:
	// lwz r6,320(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 320);
	// mulli r8,r9,1776
	ctx.r8.s64 = ctx.r9.s64 * 1776;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// addi r5,r9,1
	ctx.r5.s64 = ctx.r9.s64 + 1;
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// stw r30,40(r8)
	PPC_STORE_U32(ctx.r8.u32 + 40, r30.u32);
	// lhz r8,34(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 34);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x825cddf0
	if (cr6.lt) goto loc_825CDDF0;
loc_825CDE14:
	// lhz r9,120(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 120);
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// blt cr6,0x825cdec4
	if (cr6.lt) goto loc_825CDEC4;
	// addi r9,r30,-4
	ctx.r9.s64 = r30.s64 + -4;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r30,r8,r30
	r30.s64 = r30.s64 - ctx.r8.s64;
loc_825CDE38:
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r7,r10,4
	ctx.r7.s64 = ctx.r10.s64 + 4;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// addi r5,r10,8
	ctx.r5.s64 = ctx.r10.s64 + 8;
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r11,12
	ctx.r4.s64 = r11.s64 + 12;
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,0(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r31,r10,12
	r31.s64 = ctx.r10.s64 + 12;
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,0(r6)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,0(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stfs f0,0(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// bne cr6,0x825cde38
	if (!cr6.eq) goto loc_825CDE38;
loc_825CDEC4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x825ce8e0
	if (!cr6.gt) goto loc_825CE8E0;
loc_825CDECC:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bgt cr6,0x825cdecc
	if (cr6.gt) goto loc_825CDECC;
	// addi r12,r1,-120
	r12.s64 = ctx.r1.s64 + -120;
	// bl 0x8239d600
	// b 0x8239bd20
	return;
loc_825CDF04:
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// blt cr6,0x825ce8e0
	if (cr6.lt) goto loc_825CE8E0;
	// lhz r23,730(r7)
	r23.u64 = PPC_LOAD_U16(ctx.r7.u32 + 730);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825ce8e0
	if (!cr6.eq) goto loc_825CE8E0;
	// lwz r11,572(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 572);
	// li r22,0
	r22.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ce8e0
	if (!cr6.gt) goto loc_825CE8E0;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f8,2480(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,-31700(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -31700);
	ctx.f7.f64 = double(temp.f32);
loc_825CDF38:
	// lwz r10,576(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 576);
	// mulli r11,r22,152
	r11.s64 = r22.s64 * 152;
	// add r25,r11,r10
	r25.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// lwz r30,4(r25)
	r30.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// bne cr6,0x825ce8d0
	if (!cr6.eq) goto loc_825CE8D0;
	// lwz r6,0(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmpwi cr6,r6,1
	cr6.compare<int32_t>(ctx.r6.s32, 1, xer);
	// beq cr6,0x825ce8d0
	if (cr6.eq) goto loc_825CE8D0;
	// lwz r11,12(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cdf78
	if (!cr6.eq) goto loc_825CDF78;
	// lwz r10,16(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 16);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x825ce8d0
	if (cr6.eq) goto loc_825CE8D0;
loc_825CDF78:
	// lhz r10,34(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 34);
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// bne cr6,0x825ce080
	if (!cr6.eq) goto loc_825CE080;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ce080
	if (!cr6.eq) goto loc_825CE080;
	// lwz r11,16(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 16);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ce080
	if (!cr6.eq) goto loc_825CE080;
	// lwz r11,320(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 320);
	// li r30,0
	r30.s64 = 0;
	// lwz r10,304(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r8,56(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// lwz r6,1832(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 1832);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// clrlwi r5,r23,16
	ctx.r5.u64 = r23.u32 & 0xFFFF;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r31,r25,24
	r31.s64 = r25.s64 + 24;
loc_825CDFC0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// lwz r11,308(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// lwzx r10,r11,r9
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// bne cr6,0x825ce01c
	if (!cr6.eq) goto loc_825CE01C;
loc_825CDFD4:
	// lwz r11,308(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x825cdfec
	if (!cr6.lt) goto loc_825CDFEC;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_825CDFEC:
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825ce064
	if (!cr6.lt) goto loc_825CE064;
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// fsubs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 - f0.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f12,0(r6)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// b 0x825cdfd4
	goto loc_825CDFD4;
loc_825CE01C:
	// lwz r11,308(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x825ce034
	if (!cr6.lt) goto loc_825CE034;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_825CE034:
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825ce064
	if (!cr6.lt) goto loc_825CE064;
	// lfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// fmuls f0,f0,f7
	f0.f64 = double(float(f0.f64 * ctx.f7.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fmuls f0,f0,f7
	f0.f64 = double(float(f0.f64 * ctx.f7.f64));
	// stfs f0,0(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// b 0x825ce01c
	goto loc_825CE01C;
loc_825CE064:
	// lwz r11,304(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825cdfc0
	if (cr6.lt) goto loc_825CDFC0;
	// b 0x825ce8d0
	goto loc_825CE8D0;
loc_825CE080:
	// lhz r10,580(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 580);
	// lwz r11,556(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 556);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,148(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// ble cr6,0x825ce0f0
	if (!cr6.gt) goto loc_825CE0F0;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r31,r11
	r31.u64 = r11.u64;
loc_825CE0A0:
	// lwz r9,584(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 584);
	// rlwinm r5,r8,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r5,r9
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r9.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r5,r5,r30
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + r30.u32);
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// bne cr6,0x825ce0d8
	if (!cr6.eq) goto loc_825CE0D8;
	// lwz r5,320(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 320);
	// mulli r9,r9,1776
	ctx.r9.s64 = ctx.r9.s64 * 1776;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r9,144(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 144);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
loc_825CE0D8:
	// addi r9,r8,1
	ctx.r9.s64 = ctx.r8.s64 + 1;
	// lhz r8,580(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 580);
	// extsh r5,r8
	ctx.r5.s64 = ctx.r8.s16;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// blt cr6,0x825ce0a0
	if (cr6.lt) goto loc_825CE0A0;
loc_825CE0F0:
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// bne cr6,0x825ce1f8
	if (!cr6.eq) goto loc_825CE1F8;
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r30,r25,24
	r30.s64 = r25.s64 + 24;
loc_825CE110:
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// bne cr6,0x825ce198
	if (!cr6.eq) goto loc_825CE198;
	// lwzx r5,r9,r6
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	// clrlwi r31,r23,16
	r31.u64 = r23.u32 & 0xFFFF;
loc_825CE128:
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpw cr6,r31,r9
	cr6.compare<int32_t>(r31.s32, ctx.r9.s32, xer);
	// bge cr6,0x825ce140
	if (!cr6.lt) goto loc_825CE140;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
loc_825CE140:
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// bge cr6,0x825ce1dc
	if (!cr6.lt) goto loc_825CE1DC;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lfs f10,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lfs f12,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// addi r28,r9,4
	r28.s64 = ctx.r9.s64 + 4;
	// lfs f9,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f0,f10
	ctx.f10.f64 = double(float(f0.f64 * ctx.f10.f64));
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// addi r28,r8,4
	r28.s64 = ctx.r8.s64 + 4;
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// fmadds f13,f9,f13,f10
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f10.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f0,f11,f0,f12
	f0.f64 = double(float(ctx.f11.f64 * f0.f64 + ctx.f12.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// b 0x825ce128
	goto loc_825CE128;
loc_825CE198:
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r31,4(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
loc_825CE1DC:
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r29,r9
	cr6.compare<int32_t>(r29.s32, ctx.r9.s32, xer);
	// blt cr6,0x825ce110
	if (cr6.lt) goto loc_825CE110;
	// b 0x825ce8d0
	goto loc_825CE8D0;
loc_825CE1F8:
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// bne cr6,0x825ce360
	if (!cr6.eq) goto loc_825CE360;
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r29,r25,24
	r29.s64 = r25.s64 + 24;
loc_825CE218:
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// bne cr6,0x825ce2dc
	if (!cr6.eq) goto loc_825CE2DC;
	// lwzx r31,r9,r5
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	// clrlwi r30,r23,16
	r30.u64 = r23.u32 & 0xFFFF;
loc_825CE230:
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpw cr6,r30,r9
	cr6.compare<int32_t>(r30.s32, ctx.r9.s32, xer);
	// bge cr6,0x825ce248
	if (!cr6.lt) goto loc_825CE248;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_825CE248:
	// cmpw cr6,r31,r9
	cr6.compare<int32_t>(r31.s32, ctx.r9.s32, xer);
	// bge cr6,0x825ce344
	if (!cr6.lt) goto loc_825CE344;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lfs f11,16(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lfs f3,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f10,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	ctx.f10.f64 = double(temp.f32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lfs f9,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// addi r26,r6,4
	r26.s64 = ctx.r6.s64 + 4;
	// lfs f2,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// addi r27,r9,4
	r27.s64 = ctx.r9.s64 + 4;
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lfs f12,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f3,f3,f12
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmuls f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f6,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// addi r27,r8,4
	r27.s64 = ctx.r8.s64 + 4;
	// lfs f1,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// stw r26,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r26.u32);
	// lfs f5,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	ctx.f4.f64 = double(temp.f32);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// fmadds f11,f9,f0,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * f0.f64 + ctx.f11.f64));
	// fmadds f9,f2,f13,f3
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fmadds f10,f6,f0,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * f0.f64 + ctx.f10.f64));
	// fmadds f13,f5,f12,f11
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fmadds f0,f0,f1,f9
	f0.f64 = double(float(f0.f64 * ctx.f1.f64 + ctx.f9.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f13,0(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmadds f12,f4,f12,f10
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f12,0(r6)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// b 0x825ce230
	goto loc_825CE230;
loc_825CE2DC:
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r30,4(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r30
	ctx.r9.s64 = r30.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r31,4(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_825CE344:
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// blt cr6,0x825ce218
	if (cr6.lt) goto loc_825CE218;
	// b 0x825ce8d0
	goto loc_825CE8D0;
loc_825CE360:
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// bne cr6,0x825ce538
	if (!cr6.eq) goto loc_825CE538;
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r28,r25,24
	r28.s64 = r25.s64 + 24;
loc_825CE380:
	// lwz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// bne cr6,0x825ce490
	if (!cr6.eq) goto loc_825CE490;
	// lwzx r30,r9,r8
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// clrlwi r29,r23,16
	r29.u64 = r23.u32 & 0xFFFF;
loc_825CE398:
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmpw cr6,r29,r8
	cr6.compare<int32_t>(r29.s32, ctx.r8.s32, xer);
	// bge cr6,0x825ce3b0
	if (!cr6.lt) goto loc_825CE3B0;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
loc_825CE3B0:
	// cmpw cr6,r30,r8
	cr6.compare<int32_t>(r30.s32, ctx.r8.s32, xer);
	// bge cr6,0x825ce51c
	if (!cr6.lt) goto loc_825CE51C;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lfs f10,20(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,36(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lfs f6,52(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lfs f5,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lfs f4,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	ctx.f4.f64 = double(temp.f32);
	// addi r26,r8,4
	r26.s64 = ctx.r8.s64 + 4;
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r25,r5,4
	r25.s64 = ctx.r5.s64 + 4;
	// fmuls f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmuls f9,f9,f13
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f3,48(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f6,f6,f13
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f27,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	f27.f64 = double(temp.f32);
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// lfs f2,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f2.f64 = double(temp.f32);
	// addi r26,r6,4
	r26.s64 = ctx.r6.s64 + 4;
	// lfs f1,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// addi r21,r31,4
	r21.s64 = r31.s64 + 4;
	// lfs f31,56(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	f31.f64 = double(temp.f32);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lfs f26,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	f26.f64 = double(temp.f32);
	// stw r25,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r25.u32);
	// lfs f11,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	f30.f64 = double(temp.f32);
	// stw r26,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r26.u32);
	// fmadds f10,f5,f0,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * f0.f64 + ctx.f10.f64));
	// lfs f29,44(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	f29.f64 = double(temp.f32);
	// fmadds f9,f4,f0,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * f0.f64 + ctx.f9.f64));
	// lfs f28,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	f28.f64 = double(temp.f32);
	// fmadds f6,f3,f0,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 * f0.f64 + ctx.f6.f64));
	// lfs f25,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	f25.f64 = double(temp.f32);
	// fmuls f5,f27,f12
	ctx.f5.f64 = double(float(f27.f64 * ctx.f12.f64));
	// lfs f24,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f24.f64 = double(temp.f32);
	// stw r21,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r21.u32);
	// fmadds f10,f2,f12,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f10.f64));
	// fmadds f9,f1,f12,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmadds f6,f31,f12,f6
	ctx.f6.f64 = double(float(f31.f64 * ctx.f12.f64 + ctx.f6.f64));
	// fmadds f5,f26,f13,f5
	ctx.f5.f64 = double(float(f26.f64 * ctx.f13.f64 + ctx.f5.f64));
	// fmadds f13,f30,f11,f10
	ctx.f13.f64 = double(float(f30.f64 * ctx.f11.f64 + ctx.f10.f64));
	// fmadds f12,f29,f11,f9
	ctx.f12.f64 = double(float(f29.f64 * ctx.f11.f64 + ctx.f9.f64));
	// fmadds f10,f28,f11,f6
	ctx.f10.f64 = double(float(f28.f64 * ctx.f11.f64 + ctx.f6.f64));
	// fmadds f11,f25,f11,f5
	ctx.f11.f64 = double(float(f25.f64 * ctx.f11.f64 + ctx.f5.f64));
	// fmadds f0,f24,f0,f11
	f0.f64 = double(float(f24.f64 * f0.f64 + ctx.f11.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f13,0(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f10,0(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// b 0x825ce398
	goto loc_825CE398;
loc_825CE490:
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r29,4(r8)
	r29.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r29
	ctx.r8.s64 = r29.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r31,4(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
loc_825CE51C:
	// lwz r8,304(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r27,r8
	cr6.compare<int32_t>(r27.s32, ctx.r8.s32, xer);
	// blt cr6,0x825ce380
	if (cr6.lt) goto loc_825CE380;
	// b 0x825ce8d0
	goto loc_825CE8D0;
loc_825CE538:
	// cmpwi cr6,r6,5
	cr6.compare<int32_t>(ctx.r6.s32, 5, xer);
	// li r26,0
	r26.s64 = 0;
	// bne cr6,0x825ce790
	if (!cr6.eq) goto loc_825CE790;
	// lwz r9,304(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r27,r25,24
	r27.s64 = r25.s64 + 24;
loc_825CE558:
	// lwz r8,0(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// bne cr6,0x825ce6c4
	if (!cr6.eq) goto loc_825CE6C4;
	// lwzx r29,r9,r8
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// clrlwi r28,r23,16
	r28.u64 = r23.u32 & 0xFFFF;
loc_825CE570:
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmpw cr6,r28,r8
	cr6.compare<int32_t>(r28.s32, ctx.r8.s32, xer);
	// bge cr6,0x825ce588
	if (!cr6.lt) goto loc_825CE588;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
loc_825CE588:
	// cmpw cr6,r29,r8
	cr6.compare<int32_t>(r29.s32, ctx.r8.s32, xer);
	// bge cr6,0x825ce774
	if (!cr6.lt) goto loc_825CE774;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lfs f9,24(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,44(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	ctx.f6.f64 = double(temp.f32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lfs f5,64(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	ctx.f5.f64 = double(temp.f32);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lfs f4,84(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 84);
	ctx.f4.f64 = double(temp.f32);
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lfs f3,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f3.f64 = double(temp.f32);
	// lwz r30,16(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lfs f13,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r25,r8,4
	r25.s64 = ctx.r8.s64 + 4;
	// fmuls f9,f9,f13
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmuls f6,f6,f13
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f2,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// lfs f1,60(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f4,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// lfs f31,80(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	f31.f64 = double(temp.f32);
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r21,r6,4
	r21.s64 = ctx.r6.s64 + 4;
	// lfs f26,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	f26.f64 = double(temp.f32);
	// addi r20,r5,4
	r20.s64 = ctx.r5.s64 + 4;
	// lfs f30,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	f30.f64 = double(temp.f32);
	// addi r19,r31,4
	r19.s64 = r31.s64 + 4;
	// lfs f29,48(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	f29.f64 = double(temp.f32);
	// addi r18,r30,4
	r18.s64 = r30.s64 + 4;
	// lfs f28,68(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 68);
	f28.f64 = double(temp.f32);
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// lfs f27,88(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	f27.f64 = double(temp.f32);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// fmadds f9,f3,f0,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * f0.f64 + ctx.f9.f64));
	// lfs f21,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	f21.f64 = double(temp.f32);
	// fmadds f6,f2,f0,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * f0.f64 + ctx.f6.f64));
	// lfs f11,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f5,f1,f0,f5
	ctx.f5.f64 = double(float(ctx.f1.f64 * f0.f64 + ctx.f5.f64));
	// lfs f25,32(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	f25.f64 = double(temp.f32);
	// fmadds f4,f31,f0,f4
	ctx.f4.f64 = double(float(f31.f64 * f0.f64 + ctx.f4.f64));
	// lfs f24,52(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 52);
	f24.f64 = double(temp.f32);
	// fmuls f3,f26,f12
	ctx.f3.f64 = double(float(f26.f64 * ctx.f12.f64));
	// lfs f23,72(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 72);
	f23.f64 = double(temp.f32);
	// lfs f22,92(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 92);
	f22.f64 = double(temp.f32);
	// stw r21,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r21.u32);
	// lfs f16,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	f16.f64 = double(temp.f32);
	// stw r20,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r20.u32);
	// lfs f10,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stw r19,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r19.u32);
	// lfs f20,36(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	f20.f64 = double(temp.f32);
	// stw r18,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r18.u32);
	// lfs f19,56(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	f19.f64 = double(temp.f32);
	// fmadds f9,f30,f12,f9
	ctx.f9.f64 = double(float(f30.f64 * ctx.f12.f64 + ctx.f9.f64));
	// lfs f18,76(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	f18.f64 = double(temp.f32);
	// fmadds f6,f29,f12,f6
	ctx.f6.f64 = double(float(f29.f64 * ctx.f12.f64 + ctx.f6.f64));
	// lfs f17,96(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	f17.f64 = double(temp.f32);
	// fmadds f5,f28,f12,f5
	ctx.f5.f64 = double(float(f28.f64 * ctx.f12.f64 + ctx.f5.f64));
	// lfs f15,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	f15.f64 = double(temp.f32);
	// fmadds f12,f27,f12,f4
	ctx.f12.f64 = double(float(f27.f64 * ctx.f12.f64 + ctx.f4.f64));
	// fmadds f13,f21,f13,f3
	ctx.f13.f64 = double(float(f21.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fmadds f9,f25,f11,f9
	ctx.f9.f64 = double(float(f25.f64 * ctx.f11.f64 + ctx.f9.f64));
	// fmadds f6,f24,f11,f6
	ctx.f6.f64 = double(float(f24.f64 * ctx.f11.f64 + ctx.f6.f64));
	// fmadds f5,f23,f11,f5
	ctx.f5.f64 = double(float(f23.f64 * ctx.f11.f64 + ctx.f5.f64));
	// fmadds f4,f22,f11,f12
	ctx.f4.f64 = double(float(f22.f64 * ctx.f11.f64 + ctx.f12.f64));
	// fmadds f3,f16,f11,f13
	ctx.f3.f64 = double(float(f16.f64 * ctx.f11.f64 + ctx.f13.f64));
	// fmadds f13,f20,f10,f9
	ctx.f13.f64 = double(float(f20.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f12,f19,f10,f6
	ctx.f12.f64 = double(float(f19.f64 * ctx.f10.f64 + ctx.f6.f64));
	// lfs f6,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f11,f18,f10,f5
	ctx.f11.f64 = double(float(f18.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fmadds f9,f17,f10,f4
	ctx.f9.f64 = double(float(f17.f64 * ctx.f10.f64 + ctx.f4.f64));
	// fmadds f10,f15,f10,f3
	ctx.f10.f64 = double(float(f15.f64 * ctx.f10.f64 + ctx.f3.f64));
	// fmadds f0,f6,f0,f10
	f0.f64 = double(float(ctx.f6.f64 * f0.f64 + ctx.f10.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f13,0(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f11,0(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// stfs f9,0(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// b 0x825ce570
	goto loc_825CE570;
loc_825CE6C4:
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r8.s64;
	// lwz r6,16(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r29,4(r8)
	r29.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r29
	ctx.r8.s64 = r29.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r30
	ctx.r8.s64 = r30.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r31,4(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r31
	ctx.r8.s64 = r31.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stw r8,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r8.u32);
	// lwz r8,308(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// subf r8,r8,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r8.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// stw r8,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r8.u32);
loc_825CE774:
	// lwz r8,304(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmpw cr6,r26,r8
	cr6.compare<int32_t>(r26.s32, ctx.r8.s32, xer);
	// blt cr6,0x825ce558
	if (cr6.lt) goto loc_825CE558;
	// b 0x825ce8d0
	goto loc_825CE8D0;
loc_825CE790:
	// lwz r10,304(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825ce8d0
	if (!cr6.gt) goto loc_825CE8D0;
	// li r30,0
	r30.s64 = 0;
	// addi r27,r25,24
	r27.s64 = r25.s64 + 24;
loc_825CE7A4:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825ce874
	if (!cr6.eq) goto loc_825CE874;
	// lwz r10,308(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// clrlwi r29,r23,16
	r29.u64 = r23.u32 & 0xFFFF;
	// lwzx r28,r30,r10
	r28.u64 = PPC_LOAD_U32(r30.u32 + ctx.r10.u32);
loc_825CE7BC:
	// lwz r10,308(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// bge cr6,0x825ce7d4
	if (!cr6.lt) goto loc_825CE7D4;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_825CE7D4:
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// bge cr6,0x825ce8b8
	if (!cr6.lt) goto loc_825CE8B8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x825ce86c
	if (!cr6.gt) goto loc_825CE86C;
	// li r31,0
	r31.s64 = 0;
loc_825CE7E8:
	// mullw r10,r31,r6
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lwz r5,148(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 148);
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// add r5,r8,r5
	ctx.r5.u64 = ctx.r8.u64 + ctx.r5.u64;
	// stfsx f8,r9,r24
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + r24.u32, temp.u32);
loc_825CE804:
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// lfsx f0,r9,r24
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + r24.u32);
	f0.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r21,r10,r11
	r21.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lfsx f13,r10,r5
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// lfs f12,0(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f12,f13,f0
	f0.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + f0.f64));
	// stfsx f0,r9,r24
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + r24.u32, temp.u32);
	// blt cr6,0x825ce804
	if (cr6.lt) goto loc_825CE804;
	// addi r10,r31,1
	ctx.r10.s64 = r31.s64 + 1;
	// extsh r31,r10
	r31.s64 = ctx.r10.s16;
	// cmpw cr6,r31,r6
	cr6.compare<int32_t>(r31.s32, ctx.r6.s32, xer);
	// blt cr6,0x825ce7e8
	if (cr6.lt) goto loc_825CE7E8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825CE844:
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// lfsx f0,r9,r24
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + r24.u32);
	f0.f64 = double(temp.f32);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// addi r5,r8,4
	ctx.r5.s64 = ctx.r8.s64 + 4;
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stwx r5,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r5.u32);
	// blt cr6,0x825ce844
	if (cr6.lt) goto loc_825CE844;
loc_825CE86C:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x825ce7bc
	goto loc_825CE7BC;
loc_825CE874:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x825ce8b8
	if (!cr6.gt) goto loc_825CE8B8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825CE880:
	// lwz r9,308(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 308);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// lwzx r5,r8,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// lwz r31,4(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r9,r9,r31
	ctx.r9.s64 = r31.s64 - ctx.r9.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// stwx r9,r8,r11
	PPC_STORE_U32(ctx.r8.u32 + r11.u32, ctx.r9.u32);
	// blt cr6,0x825ce880
	if (cr6.lt) goto loc_825CE880;
loc_825CE8B8:
	// lwz r10,304(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 304);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ce7a4
	if (cr6.lt) goto loc_825CE7A4;
loc_825CE8D0:
	// lwz r11,572(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 572);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// cmpw cr6,r22,r11
	cr6.compare<int32_t>(r22.s32, r11.s32, xer);
	// blt cr6,0x825cdf38
	if (cr6.lt) goto loc_825CDF38;
loc_825CE8E0:
	// addi r12,r1,-120
	r12.s64 = ctx.r1.s64 + -120;
	// bl 0x8239d600
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_825CE8EC"))) PPC_WEAK_FUNC(sub_825CE8EC);
PPC_FUNC_IMPL(__imp__sub_825CE8EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CE8F0"))) PPC_WEAK_FUNC(sub_825CE8F0);
PPC_FUNC_IMPL(__imp__sub_825CE8F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r26,r25
	r26.u64 = r25.u64;
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ce938
	if (cr6.eq) goto loc_825CE938;
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bgt cr6,0x825ce938
	if (cr6.gt) goto loc_825CE938;
	// lis r26,-32764
	r26.s64 = -2147221504;
	// ori r26,r26,2
	r26.u64 = r26.u64 | 2;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
loc_825CE938:
	// lwz r11,176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ce948
	if (!cr6.eq) goto loc_825CE948;
	// stw r25,380(r31)
	PPC_STORE_U32(r31.u32 + 380, r25.u32);
loc_825CE948:
	// lwz r11,52(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 52);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ceec8
	if (cr6.eq) goto loc_825CEEC8;
	// li r24,1
	r24.s64 = 1;
	// li r23,11
	r23.s64 = 11;
	// li r17,3
	r17.s64 = 3;
	// li r21,4
	r21.s64 = 4;
	// li r18,5
	r18.s64 = 5;
	// li r19,6
	r19.s64 = 6;
	// li r20,7
	r20.s64 = 7;
	// li r22,9
	r22.s64 = 9;
loc_825CE974:
	// lwz r11,52(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 52);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,10
	cr6.compare<uint32_t>(r11.u32, 10, xer);
	// bgt cr6,0x825ceebc
	if (cr6.gt) goto loc_825CEEBC;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,-5732
	r12.s64 = r12.s64 + -5732;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825CEB88;
	case 1:
		goto loc_825CE9C8;
	case 2:
		goto loc_825CEBDC;
	case 3:
		goto loc_825CEC08;
	case 4:
		goto loc_825CEC20;
	case 5:
		goto loc_825CEC58;
	case 6:
		goto loc_825CECA4;
	case 7:
		goto loc_825CECE0;
	case 8:
		goto loc_825CED30;
	case 9:
		goto loc_825CED64;
	case 10:
		goto loc_825CEDB4;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-5240(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5240);
	// lwz r18,-5688(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5688);
	// lwz r18,-5156(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5156);
	// lwz r18,-5112(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5112);
	// lwz r18,-5088(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5088);
	// lwz r18,-5032(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -5032);
	// lwz r18,-4956(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -4956);
	// lwz r18,-4896(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -4896);
	// lwz r18,-4816(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -4816);
	// lwz r18,-4764(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -4764);
	// lwz r18,-4684(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -4684);
loc_825CE9C8:
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ceb68
	if (cr6.eq) goto loc_825CEB68;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825ceb68
	if (cr6.gt) goto loc_825CEB68;
	// lwz r11,228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825cea04
	if (!cr6.gt) goto loc_825CEA04;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_825CE9F4:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825ce9f4
	if (cr6.gt) goto loc_825CE9F4;
loc_825CEA04:
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825cea20
	if (!cr6.gt) goto loc_825CEA20;
loc_825CEA10:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cea10
	if (cr6.gt) goto loc_825CEA10;
loc_825CEA20:
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r29,r27,224
	r29.s64 = r27.s64 + 224;
	// rlwinm r4,r30,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r9,256(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// slw r10,r24,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r24.u32 << (r11.u8 & 0x3F));
	// rotlwi r11,r9,1
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// divw r9,r9,r10
	ctx.r9.s32 = ctx.r9.s32 / ctx.r10.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// twllei r10,0
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// twlgei r11,-1
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// lwz r9,236(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// slw r7,r24,r8
	ctx.r7.u64 = ctx.r8.u8 & 0x20 ? 0 : (r24.u32 << (ctx.r8.u8 & 0x3F));
	// rotlwi r8,r11,1
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r6,r11,r7
	ctx.r6.s32 = r11.s32 / ctx.r7.s32;
	// addi r5,r8,-1
	ctx.r5.s64 = ctx.r8.s64 + -1;
	// twllei r7,0
	// andc r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 & ~ctx.r5.u64;
	// extsh r8,r6
	ctx.r8.s64 = ctx.r6.s16;
	// twlgei r7,-1
	// blt cr6,0x825ceefc
	if (cr6.lt) goto loc_825CEEFC;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bgt cr6,0x825ceefc
	if (cr6.gt) goto loc_825CEEFC;
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825ceefc
	if (cr6.lt) goto loc_825CEEFC;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bgt cr6,0x825ceefc
	if (cr6.gt) goto loc_825CEEFC;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ceb60
	if (cr6.eq) goto loc_825CEB60;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_825CEB04:
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mulli r11,r10,1776
	r11.s64 = ctx.r10.s64 * 1776;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r9,424(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// sth r30,-2(r9)
	PPC_STORE_U16(ctx.r9.u32 + -2, r30.u16);
	// lwz r9,424(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// sth r25,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r25.u16);
	// lwz r9,424(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
	// lwz r9,424(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// sth r25,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r25.u16);
	// lwz r11,424(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825ceb04
	if (cr6.lt) goto loc_825CEB04;
loc_825CEB60:
	// stw r23,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r23.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CEB68:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825ceb60
	if (!cr6.gt) goto loc_825CEB60;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r24,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r24.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc788
	sub_825CC788(ctx, base);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CEB88:
	// lwz r11,600(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 600);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cebd8
	if (cr6.eq) goto loc_825CEBD8;
	// addi r30,r31,608
	r30.s64 = r31.s64 + 608;
	// lwz r4,616(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 616);
	// addi r29,r27,224
	r29.s64 = r27.s64 + 224;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,704(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 704);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cebd8
	if (cr6.eq) goto loc_825CEBD8;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,616(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 616);
	// subf r4,r10,r11
	ctx.r4.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x825cbd50
	sub_825CBD50(ctx, base);
loc_825CEBD8:
	// stw r17,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r17.u32);
loc_825CEBDC:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825cec00
	if (!cr6.gt) goto loc_825CEC00;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc7e0
	sub_825CC7E0(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
loc_825CEC00:
	// stw r25,436(r27)
	PPC_STORE_U32(r27.u32 + 436, r25.u32);
	// stw r21,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r21.u32);
loc_825CEC08:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825d7850
	sub_825D7850(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// stw r18,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r18.u32);
loc_825CEC20:
	// lwz r11,624(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 624);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cec50
	if (cr6.eq) goto loc_825CEC50;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r11,201(r31)
	PPC_STORE_U8(r31.u32 + 201, r11.u8);
loc_825CEC50:
	// stw r19,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r19.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CEC58:
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825cec94
	if (!cr6.gt) goto loc_825CEC94;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r25,372(r31)
	PPC_STORE_U32(r31.u32 + 372, r25.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cec9c
	if (!cr6.eq) goto loc_825CEC9C;
loc_825CEC94:
	// stw r23,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r23.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CEC9C:
	// stw r20,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r20.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CECA4:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,372(r31)
	PPC_STORE_U32(r31.u32 + 372, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r11.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CECE0:
	// lwz r11,252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825ced04
	if (!cr6.gt) goto loc_825CED04;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_825CECF4:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r10,r11,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r4.u8 & 0x3F));
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bgt cr6,0x825cecf4
	if (cr6.gt) goto loc_825CECF4;
loc_825CED04:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
	// stw r22,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r22.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CED30:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// stw r11,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r11.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CED64:
	// lwz r11,252(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825ced88
	if (!cr6.gt) goto loc_825CED88;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_825CED78:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r10,r11,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r4.u8 & 0x3F));
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bgt cr6,0x825ced78
	if (cr6.gt) goto loc_825CED78;
loc_825CED88:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// addi r3,r27,224
	ctx.r3.s64 = r27.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x825ceee4
	if (cr6.lt) goto loc_825CEEE4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,380(r31)
	PPC_STORE_U32(r31.u32 + 380, r11.u32);
	// stw r23,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r23.u32);
	// b 0x825ceebc
	goto loc_825CEEBC;
loc_825CEDB4:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825ceeb8
	if (!cr6.gt) goto loc_825CEEB8;
	// lwz r11,160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825ceeb8
	if (cr6.eq) goto loc_825CEEB8;
	// lwz r11,164(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 164);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ceeb8
	if (!cr6.eq) goto loc_825CEEB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825c9df0
	sub_825C9DF0(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// ld r11,184(r27)
	r11.u64 = PPC_LOAD_U64(r27.u32 + 184);
	// extsw r28,r29
	r28.s64 = r29.s32;
	// cmpd cr6,r28,r11
	cr6.compare<int64_t>(r28.s64, r11.s64, xer);
	// bgt cr6,0x825ceeb0
	if (cr6.gt) goto loc_825CEEB0;
	// lwz r10,380(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825ceef0
	if (cr6.eq) goto loc_825CEEF0;
	// lwz r9,176(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// beq cr6,0x825ceef0
	if (cr6.eq) goto loc_825CEEF0;
	// lwz r11,444(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cee24
	if (cr6.eq) goto loc_825CEE24;
	// lwz r11,456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// srw r30,r10,r11
	r30.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// b 0x825cee40
	goto loc_825CEE40;
loc_825CEE24:
	// lwz r11,448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cee3c
	if (cr6.eq) goto loc_825CEE3C;
	// lwz r11,456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// slw r30,r10,r11
	r30.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// b 0x825cee40
	goto loc_825CEE40;
loc_825CEE3C:
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_825CEE40:
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825c6d58
	sub_825C6D58(ctx, base);
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// lwz r10,388(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 388);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825cee70
	if (!cr6.gt) goto loc_825CEE70;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// b 0x825cee74
	goto loc_825CEE74;
loc_825CEE70:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_825CEE74:
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825ceea4
	if (cr6.lt) goto loc_825CEEA4;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// bge cr6,0x825cee98
	if (!cr6.lt) goto loc_825CEE98;
	// ld r10,184(r27)
	ctx.r10.u64 = PPC_LOAD_U64(r27.u32 + 184);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// b 0x825ceea0
	goto loc_825CEEA0;
loc_825CEE98:
	// ld r11,184(r27)
	r11.u64 = PPC_LOAD_U64(r27.u32 + 184);
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
loc_825CEEA0:
	// std r11,184(r27)
	PPC_STORE_U64(r27.u32 + 184, r11.u64);
loc_825CEEA4:
	// ld r11,184(r27)
	r11.u64 = PPC_LOAD_U64(r27.u32 + 184);
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// bge cr6,0x825ceeb4
	if (!cr6.lt) goto loc_825CEEB4;
loc_825CEEB0:
	// std r25,184(r27)
	PPC_STORE_U64(r27.u32 + 184, r25.u64);
loc_825CEEB4:
	// stw r25,160(r27)
	PPC_STORE_U32(r27.u32 + 160, r25.u32);
loc_825CEEB8:
	// stw r25,52(r27)
	PPC_STORE_U32(r27.u32 + 52, r25.u32);
loc_825CEEBC:
	// lwz r11,52(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 52);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825ce974
	if (!cr6.eq) goto loc_825CE974;
loc_825CEEC8:
	// lwz r11,176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825ceee4
	if (!cr6.eq) goto loc_825CEEE4;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// lwz r10,380(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r11.u32);
loc_825CEEE4:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
loc_825CEEF0:
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// std r11,184(r27)
	PPC_STORE_U64(r27.u32 + 184, r11.u64);
	// b 0x825ceeb4
	goto loc_825CEEB4;
loc_825CEEFC:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_825CEF0C"))) PPC_WEAK_FUNC(sub_825CEF0C);
PPC_FUNC_IMPL(__imp__sub_825CEF0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CEF10"))) PPC_WEAK_FUNC(sub_825CEF10);
PPC_FUNC_IMPL(__imp__sub_825CEF10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r22,r24
	r22.u64 = r24.u64;
	// mr r25,r24
	r25.u64 = r24.u64;
	// lwz r31,0(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// lwz r20,256(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// beq cr6,0x825d07cc
	if (cr6.eq) goto loc_825D07CC;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r23,1
	r23.s64 = 1;
	// addi r19,r11,17256
	r19.s64 = r11.s64 + 17256;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r14,8
	r14.s64 = 8;
	// addi r18,r11,-23096
	r18.s64 = r11.s64 + -23096;
	// lis r11,32767
	r11.s64 = 2147418112;
	// li r17,36
	r17.s64 = 36;
	// ori r15,r11,65535
	r15.u64 = r11.u64 | 65535;
	// li r21,45
	r21.s64 = 45;
	// li r16,2
	r16.s64 = 2;
loc_825CEF6C:
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// li r29,12
	r29.s64 = 12;
	// li r28,14
	r28.s64 = 14;
	// li r27,4
	r27.s64 = 4;
	// cmplwi cr6,r11,52
	cr6.compare<uint32_t>(r11.u32, 52, xer);
	// bgt cr6,0x825d06d4
	if (cr6.gt) goto loc_825D06D4;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,-4196
	r12.s64 = r12.s64 + -4196;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825CF070;
	case 1:
		goto loc_825D06D4;
	case 2:
		goto loc_825D06D4;
	case 3:
		goto loc_825CFB28;
	case 4:
		goto loc_825D00EC;
	case 5:
		goto loc_825D0190;
	case 6:
		goto loc_825D01B8;
	case 7:
		goto loc_825D01B8;
	case 8:
		goto loc_825D01DC;
	case 9:
		goto loc_825D03AC;
	case 10:
		goto loc_825D06D4;
	case 11:
		goto loc_825CF3FC;
	case 12:
		goto loc_825CF544;
	case 13:
		goto loc_825CF4CC;
	case 14:
		goto loc_825CF5A0;
	case 15:
		goto loc_825CF620;
	case 16:
		goto loc_825D06D4;
	case 17:
		goto loc_825D06D4;
	case 18:
		goto loc_825CF8BC;
	case 19:
		goto loc_825CFB98;
	case 20:
		goto loc_825D06D4;
	case 21:
		goto loc_825D06D4;
	case 22:
		goto loc_825D06D4;
	case 23:
		goto loc_825D06D4;
	case 24:
		goto loc_825D06D4;
	case 25:
		goto loc_825D06D4;
	case 26:
		goto loc_825D06D4;
	case 27:
		goto loc_825D06D4;
	case 28:
		goto loc_825D06D4;
	case 29:
		goto loc_825CFC08;
	case 30:
		goto loc_825CFFD8;
	case 31:
		goto loc_825D0020;
	case 32:
		goto loc_825CF63C;
	case 33:
		goto loc_825CFC80;
	case 34:
		goto loc_825D06D4;
	case 35:
		goto loc_825D06D4;
	case 36:
		goto loc_825CF7E4;
	case 37:
		goto loc_825CF978;
	case 38:
		goto loc_825CF940;
	case 39:
		goto loc_825CF7BC;
	case 40:
		goto loc_825CF71C;
	case 41:
		goto loc_825CF764;
	case 42:
		goto loc_825CF790;
	case 43:
		goto loc_825D06D4;
	case 44:
		goto loc_825CFA14;
	case 45:
		goto loc_825CFACC;
	case 46:
		goto loc_825CFA70;
	case 47:
		goto loc_825D06D4;
	case 48:
		goto loc_825CF810;
	case 49:
		goto loc_825D06D4;
	case 50:
		goto loc_825D06D4;
	case 51:
		goto loc_825D06D4;
	case 52:
		goto loc_825CFE94;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-3984(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -3984);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-1240(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1240);
	// lwz r18,236(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 236);
	// lwz r18,400(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 400);
	// lwz r18,440(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 440);
	// lwz r18,440(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 440);
	// lwz r18,476(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 476);
	// lwz r18,940(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 940);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-3076(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -3076);
	// lwz r18,-2748(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2748);
	// lwz r18,-2868(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2868);
	// lwz r18,-2656(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2656);
	// lwz r18,-2528(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2528);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-1860(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1860);
	// lwz r18,-1128(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1128);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-1016(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1016);
	// lwz r18,-40(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -40);
	// lwz r18,32(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// lwz r18,-2500(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2500);
	// lwz r18,-896(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -896);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-2076(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2076);
	// lwz r18,-1672(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1672);
	// lwz r18,-1728(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1728);
	// lwz r18,-2116(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2116);
	// lwz r18,-2276(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2276);
	// lwz r18,-2204(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2204);
	// lwz r18,-2160(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2160);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-1516(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1516);
	// lwz r18,-1332(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1332);
	// lwz r18,-1424(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -1424);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-2032(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -2032);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,1748(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 1748);
	// lwz r18,-364(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + -364);
loc_825CF070:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825cf178
	if (cr6.gt) goto loc_825CF178;
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825cf11c
	if (cr6.eq) goto loc_825CF11C;
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 228);
	// mr r11,r24
	r11.u64 = r24.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825cf0ac
	if (!cr6.gt) goto loc_825CF0AC;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
loc_825CF09C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cf09c
	if (cr6.gt) goto loc_825CF09C;
loc_825CF0AC:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825cf0c8
	if (!cr6.gt) goto loc_825CF0C8;
loc_825CF0B8:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825cf0b8
	if (cr6.gt) goto loc_825CF0B8;
loc_825CF0C8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// lwz r9,236(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// slw r8,r23,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r20,r11,r8
	r20.s32 = r11.s32 / ctx.r8.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// twllei r8,0
	// andc r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r10.u64;
	// cmpw cr6,r20,r9
	cr6.compare<int32_t>(r20.s32, ctx.r9.s32, xer);
	// twlgei r10,-1
	// blt cr6,0x825d06ec
	if (cr6.lt) goto loc_825D06EC;
	// cmpw cr6,r20,r11
	cr6.compare<int32_t>(r20.s32, r11.s32, xer);
	// bgt cr6,0x825d06ec
	if (cr6.gt) goto loc_825D06EC;
loc_825CF11C:
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r11,580(r31)
	PPC_STORE_U16(r31.u32 + 580, r11.u16);
	// beq cr6,0x825cf15c
	if (cr6.eq) goto loc_825CF15C;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825CF134:
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// sthx r7,r11,r8
	PPC_STORE_U16(r11.u32 + ctx.r8.u32, ctx.r7.u16);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cf134
	if (cr6.lt) goto loc_825CF134;
loc_825CF15C:
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825b3eb8
	sub_825B3EB8(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// b 0x825cf328
	goto loc_825CF328;
loc_825CF178:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// lhz r4,34(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwz r3,320(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mullw r6,r4,r11
	ctx.r6.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825cf1f0
	if (!cr6.gt) goto loc_825CF1F0;
	// clrlwi r5,r4,16
	ctx.r5.u64 = ctx.r4.u32 & 0xFFFF;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// addi r11,r3,424
	r11.s64 = ctx.r3.s64 + 424;
loc_825CF1A8:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r28,r9
	r28.s64 = ctx.r9.s16;
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// ble cr6,0x825cf1dc
	if (!cr6.gt) goto loc_825CF1DC;
	// lhz r30,-310(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + -310);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// extsh r9,r30
	ctx.r9.s64 = r30.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r9,r10
	r30.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
loc_825CF1DC:
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// blt cr6,0x825cf1a8
	if (cr6.lt) goto loc_825CF1A8;
loc_825CF1F0:
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// sth r24,580(r31)
	PPC_STORE_U16(r31.u32 + 580, r24.u16);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825cf2f8
	if (!cr6.gt) goto loc_825CF2F8;
	// extsh r4,r8
	ctx.r4.s64 = ctx.r8.s16;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r11,r3,114
	r11.s64 = ctx.r3.s64 + 114;
loc_825CF20C:
	// lwz r10,310(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 310);
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// cmpw cr6,r4,r8
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r8.s32, xer);
	// bne cr6,0x825cf2dc
	if (!cr6.eq) goto loc_825CF2DC;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r3,r30
	ctx.r3.s64 = r30.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// cmpw cr6,r3,r8
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r8.s32, xer);
	// bne cr6,0x825cf2dc
	if (!cr6.eq) goto loc_825CF2DC;
	// lhz r8,580(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lwz r3,584(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r7,r8,r3
	PPC_STORE_U16(ctx.r8.u32 + ctx.r3.u32, ctx.r7.u16);
	// lhz r8,580(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// sth r8,580(r31)
	PPC_STORE_U16(r31.u32 + 580, ctx.r8.u16);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// sth r8,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r8.u16);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// sth r8,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r8.u16);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lhz r8,-2(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + -2);
	// sth r8,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r8.u16);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r3,r8
	ctx.r3.s64 = ctx.r8.s16;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// rlwinm r7,r3,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r7,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r10.u32);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// sth r8,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
loc_825CF2DC:
	// addi r10,r5,1
	ctx.r10.s64 = ctx.r5.s64 + 1;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r7,r10
	ctx.r7.s64 = ctx.r10.s16;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// blt cr6,0x825cf20c
	if (cr6.lt) goto loc_825CF20C;
loc_825CF2F8:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d07c4
	if (!cr6.gt) goto loc_825D07C4;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// cntlzw r11,r6
	r11.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,216(r26)
	PPC_STORE_U32(r26.u32 + 216, r11.u32);
loc_825CF328:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825c7880
	sub_825C7880(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825b5218
	sub_825B5218(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825cf364
	if (cr6.gt) goto loc_825CF364;
	// li r11,52
	r11.s64 = 52;
	// b 0x825d06d0
	goto loc_825D06D0;
loc_825CF364:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf3f4
	if (!cr6.gt) goto loc_825CF3F4;
	// mr r29,r24
	r29.u64 = r24.u64;
loc_825CF378:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r29,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,114(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 114);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cf3cc
	if (!cr6.eq) goto loc_825CF3CC;
	// li r5,112
	ctx.r5.s64 = 112;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r24,444(r30)
	PPC_STORE_U32(r30.u32 + 444, r24.u32);
	// stw r23,436(r30)
	PPC_STORE_U32(r30.u32 + 436, r23.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,112
	ctx.r5.s64 = 112;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r24,64(r30)
	PPC_STORE_U32(r30.u32 + 64, r24.u32);
loc_825CF3CC:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d6008
	sub_825D6008(ctx, base);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cf378
	if (cr6.lt) goto loc_825CF378;
loc_825CF3F4:
	// li r11,11
	r11.s64 = 11;
	// b 0x825d06d0
	goto loc_825D06D0;
loc_825CF3FC:
	// addi r30,r26,224
	r30.s64 = r26.s64 + 224;
	// li r4,22
	ctx.r4.s64 = 22;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r24,60(r26)
	PPC_STORE_U32(r26.u32 + 60, r24.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825cf44c
	if (!cr6.eq) goto loc_825CF44C;
	// stw r28,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r28.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF44C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825cf480
	if (cr6.eq) goto loc_825CF480;
	// stw r11,60(r26)
	PPC_STORE_U32(r26.u32 + 60, r11.u32);
	// stw r29,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r29.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF480:
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r29,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r29.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,60(r26)
	PPC_STORE_U32(r26.u32 + 60, r11.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF4CC:
	// lwz r11,60(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf53c
	if (!cr6.gt) goto loc_825CF53C;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825CF4DC:
	// lwz r30,60(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// ble cr6,0x825cf4ec
	if (!cr6.gt) goto loc_825CF4EC;
	// mr r30,r14
	r30.u64 = r14.u64;
loc_825CF4EC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// bge cr6,0x825cf524
	if (!cr6.lt) goto loc_825CF524;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subfic r10,r30,8
	xer.ca = r30.u32 <= 8;
	ctx.r10.s64 = 8 - r30.s64;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825CF524:
	// lwz r11,60(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// stw r11,60(r26)
	PPC_STORE_U32(r26.u32 + 60, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825cf4dc
	if (cr6.gt) goto loc_825CF4DC;
loc_825CF53C:
	// stw r28,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r28.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF544:
	// lwz r11,60(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf598
	if (!cr6.gt) goto loc_825CF598;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825CF554:
	// lwz r30,60(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// cmpwi cr6,r30,24
	cr6.compare<int32_t>(r30.s32, 24, xer);
	// ble cr6,0x825cf564
	if (!cr6.gt) goto loc_825CF564;
	// li r30,24
	r30.s64 = 24;
loc_825CF564:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,60(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// stw r11,60(r26)
	PPC_STORE_U32(r26.u32 + 60, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825cf554
	if (cr6.gt) goto loc_825CF554;
loc_825CF598:
	// stw r28,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r28.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF5A0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// stw r24,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r24.u32);
	// stw r11,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r11.u32);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf5f4
	if (!cr6.gt) goto loc_825CF5F4;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// mr r11,r24
	r11.u64 = r24.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
loc_825CF5E4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cf5e4
	if (cr6.lt) goto loc_825CF5E4;
loc_825CF5F4:
	// lwz r11,132(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825cf614
	if (!cr6.eq) goto loc_825CF614;
	// li r11,15
	r11.s64 = 15;
	// stw r24,88(r26)
	PPC_STORE_U32(r26.u32 + 88, r24.u32);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
	// stw r23,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r23.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF614:
	// stw r23,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r23.u32);
	// li r11,40
	r11.s64 = 40;
	// b 0x825d06d0
	goto loc_825D06D0;
loc_825CF620:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d7338
	sub_825D7338(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// li r11,32
	r11.s64 = 32;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF63C:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r28,r26,224
	r28.s64 = r26.s64 + 224;
	// mr r27,r23
	r27.u64 = r23.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf6f4
	if (!cr6.gt) goto loc_825CF6F4;
	// mr r29,r24
	r29.u64 = r24.u64;
loc_825CF674:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r29,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// and r27,r11,r27
	r27.u64 = r11.u64 & r27.u64;
	// bl 0x825b6ab8
	sub_825B6AB8(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cf674
	if (cr6.lt) goto loc_825CF674;
loc_825CF6F4:
	// lhz r11,110(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 110);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// mulli r11,r11,90
	r11.s64 = r11.s64 * 90;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// stw r15,136(r26)
	PPC_STORE_U32(r26.u32 + 136, r15.u32);
	// bne cr6,0x825d06fc
	if (!cr6.eq) goto loc_825D06FC;
	// li r11,30
	r11.s64 = 30;
	// b 0x825d06d0
	goto loc_825D06D0;
loc_825CF71C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cf75c
	if (!cr6.eq) goto loc_825CF75C;
	// li r11,41
	r11.s64 = 41;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
	// stw r23,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r23.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF75C:
	// stw r17,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r17.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CF764:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r11.u32);
	// li r11,42
	r11.s64 = 42;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF790:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
	// li r11,39
	r11.s64 = 39;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF7BC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
	// stw r17,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r17.u32);
loc_825CF7E4:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// li r11,48
	r11.s64 = 48;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF810:
	// addi r28,r26,224
	r28.s64 = r26.s64 + 224;
	// lhz r4,34(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825cf8b0
	if (!cr6.gt) goto loc_825CF8B0;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_825CF840:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r30,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,40(r29)
	PPC_STORE_U32(r29.u32 + 40, r11.u32);
	// bl 0x825b6ab8
	sub_825B6AB8(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cf840
	if (cr6.lt) goto loc_825CF840;
loc_825CF8B0:
	// li r11,18
	r11.s64 = 18;
	// stw r24,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r24.u32);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF8BC:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cf938
	if (!cr6.eq) goto loc_825CF938;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cf938
	if (!cr6.eq) goto loc_825CF938;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cf938
	if (cr6.eq) goto loc_825CF938;
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cf938
	if (cr6.eq) goto loc_825CF938;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cf938
	if (cr6.eq) goto loc_825CF938;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825d07c4
	if (cr6.eq) goto loc_825D07C4;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r11,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r11.u32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bne cr6,0x825d07c4
	if (!cr6.eq) goto loc_825D07C4;
loc_825CF938:
	// li r11,38
	r11.s64 = 38;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF940:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cf970
	if (!cr6.eq) goto loc_825CF970;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r11.u32);
loc_825CF970:
	// li r11,37
	r11.s64 = 37;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CF978:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cf9bc
	if (!cr6.eq) goto loc_825CF9BC;
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfa0c
	if (!cr6.eq) goto loc_825CFA0C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,168(r31)
	PPC_STORE_U16(r31.u32 + 168, r11.u16);
	// b 0x825cfa0c
	goto loc_825CFA0C;
loc_825CF9BC:
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfa0c
	if (!cr6.eq) goto loc_825CFA0C;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfa0c
	if (cr6.eq) goto loc_825CFA0C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,6
	ctx.r4.s64 = 6;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bge cr6,0x825d07c4
	if (!cr6.lt) goto loc_825D07C4;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// stw r11,632(r31)
	PPC_STORE_U32(r31.u32 + 632, r11.u32);
loc_825CFA0C:
	// li r11,44
	r11.s64 = 44;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CFA14:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfa68
	if (!cr6.eq) goto loc_825CFA68;
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfa68
	if (!cr6.eq) goto loc_825CFA68;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// cmpwi cr6,r10,12
	cr6.compare<int32_t>(ctx.r10.s32, 12, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// sth r11,170(r31)
	PPC_STORE_U16(r31.u32 + 170, r11.u16);
loc_825CFA68:
	// li r11,46
	r11.s64 = 46;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CFA70:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfac8
	if (!cr6.eq) goto loc_825CFAC8;
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfac8
	if (!cr6.eq) goto loc_825CFAC8;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// addi r11,r10,1
	r11.s64 = ctx.r10.s64 + 1;
	// sth r11,172(r31)
	PPC_STORE_U16(r31.u32 + 172, r11.u16);
loc_825CFAC8:
	// stw r21,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r21.u32);
loc_825CFACC:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfb0c
	if (cr6.eq) goto loc_825CFB0C;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfb0c
	if (!cr6.eq) goto loc_825CFB0C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,174(r31)
	PPC_STORE_U16(r31.u32 + 174, r11.u16);
loc_825CFB0C:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfb20
	if (!cr6.eq) goto loc_825CFB20;
	// li r11,3
	r11.s64 = 3;
	// b 0x825d06d0
	goto loc_825D06D0;
loc_825CFB20:
	// stw r14,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r14.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CFB28:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfb90
	if (!cr6.eq) goto loc_825CFB90;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfb90
	if (!cr6.eq) goto loc_825CFB90;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfb90
	if (cr6.eq) goto loc_825CFB90;
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfb90
	if (cr6.eq) goto loc_825CFB90;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfb90
	if (cr6.eq) goto loc_825CFB90;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825d07c4
	if (cr6.eq) goto loc_825D07C4;
	// stw r11,184(r31)
	PPC_STORE_U32(r31.u32 + 184, r11.u32);
loc_825CFB90:
	// li r11,19
	r11.s64 = 19;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CFB98:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfc00
	if (!cr6.eq) goto loc_825CFC00;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfc00
	if (!cr6.eq) goto loc_825CFC00;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc00
	if (cr6.eq) goto loc_825CFC00;
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc00
	if (cr6.eq) goto loc_825CFC00;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc00
	if (cr6.eq) goto loc_825CFC00;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825d07c4
	if (cr6.eq) goto loc_825D07C4;
	// stw r11,656(r31)
	PPC_STORE_U32(r31.u32 + 656, r11.u32);
loc_825CFC00:
	// li r11,29
	r11.s64 = 29;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CFC08:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfc70
	if (!cr6.eq) goto loc_825CFC70;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825cfc70
	if (!cr6.eq) goto loc_825CFC70;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc70
	if (cr6.eq) goto loc_825CFC70;
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc70
	if (cr6.eq) goto loc_825CFC70;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825cfc70
	if (cr6.eq) goto loc_825CFC70;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825d07c4
	if (cr6.eq) goto loc_825D07C4;
	// stw r11,716(r31)
	PPC_STORE_U32(r31.u32 + 716, r11.u32);
loc_825CFC70:
	// li r11,33
	r11.s64 = 33;
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
	// stw r24,44(r26)
	PPC_STORE_U32(r26.u32 + 44, r24.u32);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825CFC80:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d06cc
	if (!cr6.eq) goto loc_825D06CC;
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d06cc
	if (!cr6.eq) goto loc_825D06CC;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d06cc
	if (cr6.eq) goto loc_825D06CC;
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d06cc
	if (cr6.eq) goto loc_825D06CC;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d06cc
	if (cr6.eq) goto loc_825D06CC;
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825cfe8c
	if (!cr6.lt) goto loc_825CFE8C;
loc_825CFCD0:
	// lhz r10,150(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lwz r11,44(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 44);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x825cfd00
	if (cr6.lt) goto loc_825CFD00;
	// beq cr6,0x825cfd48
	if (cr6.eq) goto loc_825CFD48;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x825cfddc
	if (cr6.lt) goto loc_825CFDDC;
	// b 0x825cfe6c
	goto loc_825CFE6C;
loc_825CFD00:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// sth r11,182(r30)
	PPC_STORE_U16(r30.u32 + 182, r11.u16);
	// sth r24,184(r30)
	PPC_STORE_U16(r30.u32 + 184, r24.u16);
	// stw r23,44(r26)
	PPC_STORE_U32(r26.u32 + 44, r23.u32);
loc_825CFD48:
	// lhz r11,184(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// lhz r10,182(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825cfdd4
	if (!cr6.lt) goto loc_825CFDD4;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825CFD64:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,256
	cr6.compare<uint32_t>(r11.u32, 256, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// lhz r10,184(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 200, r11.u32);
	// lhz r11,184(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,184(r30)
	PPC_STORE_U16(r30.u32 + 184, r11.u16);
	// lhz r11,182(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 182);
	// lhz r10,184(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825cfd64
	if (cr6.lt) goto loc_825CFD64;
loc_825CFDD4:
	// sth r24,184(r30)
	PPC_STORE_U16(r30.u32 + 184, r24.u16);
	// stw r16,44(r26)
	PPC_STORE_U32(r26.u32 + 44, r16.u32);
loc_825CFDDC:
	// lhz r11,184(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// lhz r10,182(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825cfe64
	if (!cr6.lt) goto loc_825CFE64;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825CFDF8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// bgt cr6,0x825d07c4
	if (cr6.gt) goto loc_825D07C4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d07c4
	if (cr6.lt) goto loc_825D07C4;
	// lhz r11,184(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r10,220(r11)
	PPC_STORE_U32(r11.u32 + 220, ctx.r10.u32);
	// lhz r11,184(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,184(r30)
	PPC_STORE_U16(r30.u32 + 184, r11.u16);
	// lhz r11,182(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 182);
	// lhz r10,184(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825cfdf8
	if (cr6.lt) goto loc_825CFDF8;
loc_825CFE64:
	// sth r24,184(r30)
	PPC_STORE_U16(r30.u32 + 184, r24.u16);
	// stw r24,44(r26)
	PPC_STORE_U32(r26.u32 + 44, r24.u32);
loc_825CFE6C:
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825cfcd0
	if (cr6.lt) goto loc_825CFCD0;
loc_825CFE8C:
	// stw r23,20(r26)
	PPC_STORE_U32(r26.u32 + 20, r23.u32);
	// b 0x825d06cc
	goto loc_825D06CC;
loc_825CFE94:
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// addi r30,r26,224
	r30.s64 = r26.s64 + 224;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bne cr6,0x825cff04
	if (!cr6.eq) goto loc_825CFF04;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// stw r11,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, r11.u32);
	// lwz r11,320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r24,68(r11)
	PPC_STORE_U32(r11.u32 + 68, r24.u32);
	// cntlzw r11,r10
	r11.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// stw r24,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r24.u32);
	// rlwinm r30,r11,27,31,31
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x825cffc4
	goto loc_825CFFC4;
loc_825CFF04:
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,68(r10)
	PPC_STORE_U32(ctx.r10.u32 + 68, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// stw r11,1844(r10)
	PPC_STORE_U32(ctx.r10.u32 + 1844, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r10,1816(r11)
	PPC_STORE_U32(r11.u32 + 1816, ctx.r10.u32);
	// lwz r11,320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r4,r11,1776
	ctx.r4.s64 = r11.s64 + 1776;
	// lwz r10,1816(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 1816);
	// lwz r9,40(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lwz r8,68(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// and r30,r10,r9
	r30.u64 = ctx.r10.u64 & ctx.r9.u64;
	// beq cr6,0x825cffc0
	if (cr6.eq) goto loc_825CFFC0;
	// li r5,0
	ctx.r5.s64 = 0;
loc_825CFFC0:
	// bl 0x825d6008
	sub_825D6008(ctx, base);
loc_825CFFC4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r23,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r23.u32);
	// bne cr6,0x825d070c
	if (!cr6.eq) goto loc_825D070C;
	// stw r27,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r27.u32);
	// b 0x825d06d4
	goto loc_825D06D4;
loc_825CFFD8:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// stw r24,592(r31)
	PPC_STORE_U32(r31.u32 + 592, r24.u32);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// blt cr6,0x825d0014
	if (cr6.lt) goto loc_825D0014;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d0014
	if (cr6.eq) goto loc_825D0014;
	// stw r23,592(r31)
	PPC_STORE_U32(r31.u32 + 592, r23.u32);
loc_825D0014:
	// li r11,31
	r11.s64 = 31;
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825D0020:
	// lwz r11,592(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 592);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d00e8
	if (!cr6.eq) goto loc_825D00E8;
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d00e8
	if (!cr6.lt) goto loc_825D00E8;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825D0048:
	// lhz r10,150(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// mr r11,r24
	r11.u64 = r24.u64;
	// lwz r8,584(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r8
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825d00a0
	if (!cr6.gt) goto loc_825D00A0;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
loc_825D0090:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d0090
	if (cr6.gt) goto loc_825D0090;
loc_825D00A0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,484(r30)
	PPC_STORE_U32(r30.u32 + 484, r11.u32);
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d0048
	if (cr6.lt) goto loc_825D0048;
loc_825D00E8:
	// stw r27,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r27.u32);
loc_825D00EC:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d0104
	if (cr6.gt) goto loc_825D0104;
	// bl 0x825d5e40
	sub_825D5E40(ctx, base);
	// b 0x825d0108
	goto loc_825D0108;
loc_825D0104:
	// bl 0x825d7c50
	sub_825D7C50(ctx, base);
loc_825D0108:
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,296(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// bl 0x825cd780
	sub_825CD780(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// sth r11,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r11.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d0184
	if (!cr6.gt) goto loc_825D0184;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_825D013C:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r30,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// stb r24,180(r29)
	PPC_STORE_U8(r29.u32 + 180, r24.u8);
	// bl 0x825c9b70
	sub_825C9B70(ctx, base);
	// stfs f1,196(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r29.u32 + 196, temp.u32);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d013c
	if (cr6.lt) goto loc_825D013C;
loc_825D0184:
	// li r11,5
	r11.s64 = 5;
	// stw r24,132(r26)
	PPC_STORE_U32(r26.u32 + 132, r24.u32);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825D0190:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d7160
	sub_825D7160(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,400(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 400);
	// sth r11,152(r26)
	PPC_STORE_U16(r26.u32 + 152, r11.u16);
	// li r11,6
	r11.s64 = 6;
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825D01B8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d01d8
	if (!cr6.eq) goto loc_825D01D8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d6338
	sub_825D6338(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
loc_825D01D8:
	// stw r14,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r14.u32);
loc_825D01DC:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d0268
	if (cr6.gt) goto loc_825D0268;
	// addi r30,r26,224
	r30.s64 = r26.s64 + 224;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r22,r23
	r22.u64 = r23.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 216);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d0268
	if (cr6.eq) goto loc_825D0268;
	// lwz r11,320(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lwz r11,424(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x825d0268
	if (!cr6.gt) goto loc_825D0268;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r22,r11
	r22.u64 = r11.u64;
	// lhz r10,114(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 114);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d0268
	if (!cr6.eq) goto loc_825D0268;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d07c4
	if (!cr6.eq) goto loc_825D07C4;
loc_825D0268:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d039c
	if (!cr6.gt) goto loc_825D039C;
	// clrlwi r27,r22,24
	r27.u64 = r22.u32 & 0xFF;
	// mr r29,r24
	r29.u64 = r24.u64;
loc_825D0280:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r29,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,460(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// lwz r8,320(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r9,r11,1776
	ctx.r9.s64 = r11.s64 * 1776;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r9,r8
	r30.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x825d02cc
	if (cr6.eq) goto loc_825D02CC;
	// lwz r11,456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// lhz r10,118(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// sraw r9,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r9.s64 = ctx.r10.s32 >> temp.u32;
	// b 0x825d02f0
	goto loc_825D02F0;
loc_825D02CC:
	// lwz r11,448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lhz r11,118(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// beq cr6,0x825d02ec
	if (cr6.eq) goto loc_825D02EC;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// slw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// b 0x825d02f0
	goto loc_825D02F0;
loc_825D02EC:
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
loc_825D02F0:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x825d0308
	if (cr6.eq) goto loc_825D0308;
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// sraw r10,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	ctx.r10.s64 = r11.s32 >> temp.u32;
	// b 0x825d0328
	goto loc_825D0328;
loc_825D0308:
	// lwz r11,448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// beq cr6,0x825d0324
	if (cr6.eq) goto loc_825D0324;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// slw r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// b 0x825d0328
	goto loc_825D0328;
loc_825D0324:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_825D0328:
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r9,56(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// lwz r10,460(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d0354
	if (cr6.eq) goto loc_825D0354;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// sraw r11,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// b 0x825d0368
	goto loc_825D0368;
loc_825D0354:
	// lwz r10,448(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d0368
	if (cr6.eq) goto loc_825D0368;
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
loc_825D0368:
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r24,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r24.u32);
	// lwz r10,424(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 424);
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stb r27,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r27.u8);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d0280
	if (cr6.lt) goto loc_825D0280;
loc_825D039C:
	// li r11,9
	r11.s64 = 9;
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
	// sth r24,152(r26)
	PPC_STORE_U16(r26.u32 + 152, r24.u16);
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825D03AC:
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d06a4
	if (!cr6.eq) goto loc_825D06A4;
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d06cc
	if (!cr6.lt) goto loc_825D06CC;
loc_825D03D0:
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lwz r8,60(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// mulli r11,r4,1776
	r11.s64 = ctx.r4.s64 * 1776;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x825d0424
	if (!cr6.gt) goto loc_825D0424;
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// rlwinm r11,r4,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d0424
	if (!cr6.eq) goto loc_825D0424;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d7af0
	sub_825D7AF0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
loc_825D0424:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,424(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 424);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r22,0(r11)
	r22.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// bne cr6,0x825d0474
	if (!cr6.eq) goto loc_825D0474;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d0474
	if (cr6.gt) goto loc_825D0474;
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// bne cr6,0x825d0654
	if (!cr6.eq) goto loc_825D0654;
	// lwz r11,304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r24,64(r28)
	PPC_STORE_U32(r28.u32 + 64, r24.u32);
	// b 0x825d0654
	goto loc_825D0654;
loc_825D0474:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// mr r27,r18
	r27.u64 = r18.u64;
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825d048c
	if (!cr6.gt) goto loc_825D048C;
	// mr r27,r19
	r27.u64 = r19.u64;
loc_825D048C:
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// bne cr6,0x825d0608
	if (!cr6.eq) goto loc_825D0608;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825d04c8
	if (!cr6.gt) goto loc_825D04C8;
	// lwz r10,444(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 444);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825d04c8
	if (!cr6.eq) goto loc_825D04C8;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r5,304(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d83d0
	sub_825D83D0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// b 0x825d05e0
	goto loc_825D05E0;
loc_825D04C8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d0504
	if (!cr6.eq) goto loc_825D0504;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r26,224
	ctx.r3.s64 = r26.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,10
	r11.s64 = r11.s64 + 10;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lhz r11,152(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r26)
	PPC_STORE_U16(r26.u32 + 152, r11.u16);
loc_825D0504:
	// lhz r11,152(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 152);
	// lwz r10,304(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d05e0
	if (!cr6.lt) goto loc_825D05E0;
	// addi r29,r26,224
	r29.s64 = r26.s64 + 224;
loc_825D051C:
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// addi r9,r10,-60
	ctx.r9.s64 = ctx.r10.s64 + -60;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// ble cr6,0x825d0590
	if (!cr6.gt) goto loc_825D0590;
	// lhz r11,152(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 152);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d0590
	if (!cr6.eq) goto loc_825D0590;
	// lwz r11,436(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 436);
	// twllei r11,0
	// divw r11,r21,r11
	r11.s32 = r21.s32 / r11.s32;
	// b 0x825d05b4
	goto loc_825D05B4;
loc_825D0590:
	// lhz r11,152(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 152);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d05a8
	if (!cr6.eq) goto loc_825D05A8;
	// mr r11,r17
	r11.u64 = r17.u64;
	// b 0x825d05b4
	goto loc_825D05B4;
loc_825D05A8:
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
loc_825D05B4:
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r30
	PPC_STORE_U32(ctx.r10.u32 + r30.u32, r11.u32);
	// lhz r11,152(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r26)
	PPC_STORE_U16(r26.u32 + 152, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lwz r10,304(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d051c
	if (cr6.lt) goto loc_825D051C;
loc_825D05E0:
	// lhz r11,114(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 114);
	// lwz r10,424(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 424);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r11.u32);
	// sth r24,152(r26)
	PPC_STORE_U16(r26.u32 + 152, r24.u16);
	// b 0x825d0610
	goto loc_825D0610;
loc_825D0608:
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// beq cr6,0x825d0654
	if (cr6.eq) goto loc_825D0654;
loc_825D0610:
	// lwz r11,304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x825d064c
	if (!cr6.gt) goto loc_825D064C;
	// rotlwi r8,r11,0
	ctx.r8.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r11,r23
	r11.u64 = r23.u64;
loc_825D0628:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x825d063c
	if (!cr6.gt) goto loc_825D063C;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_825D063C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x825d0628
	if (cr6.lt) goto loc_825D0628;
loc_825D064C:
	// stw r9,64(r28)
	PPC_STORE_U32(r28.u32 + 64, ctx.r9.u32);
	// stw r23,444(r28)
	PPC_STORE_U32(r28.u32 + 444, r23.u32);
loc_825D0654:
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825d067c
	if (!cr6.gt) goto loc_825D067C;
	// cmpwi cr6,r22,1
	cr6.compare<int32_t>(r22.s32, 1, xer);
	// bne cr6,0x825d067c
	if (!cr6.eq) goto loc_825D067C;
	// lhz r11,118(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,428(r28)
	PPC_STORE_U32(r28.u32 + 428, r11.u32);
	// lwz r11,304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// stw r11,432(r28)
	PPC_STORE_U32(r28.u32 + 432, r11.u32);
loc_825D067C:
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d03d0
	if (cr6.lt) goto loc_825D03D0;
	// b 0x825d06cc
	goto loc_825D06CC;
loc_825D06A4:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d6170
	sub_825D6170(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d06cc
	if (!cr6.eq) goto loc_825D06CC;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d66d8
	sub_825D66D8(ctx, base);
loc_825D06CC:
	// li r11,10
	r11.s64 = 10;
loc_825D06D0:
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
loc_825D06D4:
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x825cef6c
	if (!cr6.eq) goto loc_825CEF6C;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D06EC:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D06FC:
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d07a8
	if (!cr6.eq) goto loc_825D07A8;
	// b 0x825d079c
	goto loc_825D079C;
loc_825D070C:
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d07b0
	if (!cr6.eq) goto loc_825D07B0;
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d0798
	if (!cr6.gt) goto loc_825D0798;
loc_825D072C:
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r10,114(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 114);
	// lwz r11,424(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// ble cr6,0x825d0770
	if (!cr6.gt) goto loc_825D0770;
	// stb r24,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r24.u8);
	// b 0x825d0774
	goto loc_825D0774;
loc_825D0770:
	// stb r23,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r23.u8);
loc_825D0774:
	// lhz r11,150(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d072c
	if (cr6.lt) goto loc_825D072C;
loc_825D0798:
	// sth r24,150(r26)
	PPC_STORE_U16(r26.u32 + 150, r24.u16);
loc_825D079C:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d6170
	sub_825D6170(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
loc_825D07A8:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d07cc
	if (cr6.lt) goto loc_825D07CC;
loc_825D07B0:
	// li r11,10
	r11.s64 = 10;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,40(r26)
	PPC_STORE_U32(r26.u32 + 40, r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D07C4:
	// lis r25,-32764
	r25.s64 = -2147221504;
	// ori r25,r25,2
	r25.u64 = r25.u64 | 2;
loc_825D07CC:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825D07D8"))) PPC_WEAK_FUNC(sub_825D07D8);
PPC_FUNC_IMPL(__imp__sub_825D07D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// addi r12,r1,-56
	r12.s64 = ctx.r1.s64 + -56;
	// bl 0x8239d5e4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lhz r11,580(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 580);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bne cr6,0x825d0d6c
	if (!cr6.eq) goto loc_825D0D6C;
	// lwz r11,584(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 584);
	// lwz r8,60(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 60);
	// lwz r10,320(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r11,2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// mulli r11,r8,1776
	r11.s64 = ctx.r8.s64 * 1776;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// mulli r11,r7,1776
	r11.s64 = ctx.r7.s64 * 1776;
	// lhz r30,122(r29)
	r30.u64 = PPC_LOAD_U16(r29.u32 + 122);
	// add r26,r11,r10
	r26.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x825d084c
	if (!cr6.gt) goto loc_825D084C;
	// lhz r11,122(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 122);
	// extsh r10,r30
	ctx.r10.s64 = r30.s16;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x825d0d6c
	if (!cr6.eq) goto loc_825D0D6C;
loc_825D084C:
	// lhz r31,124(r29)
	r31.u64 = PPC_LOAD_U16(r29.u32 + 124);
	// extsh r28,r30
	r28.s64 = r30.s16;
	// addi r9,r1,86
	ctx.r9.s64 = ctx.r1.s64 + 86;
	// lfs f29,72(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 72);
	f29.f64 = double(temp.f32);
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// lfs f31,76(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 76);
	f31.f64 = double(temp.f32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lfs f28,80(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 80);
	f28.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f27,84(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 84);
	f27.f64 = double(temp.f32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lfs f30,88(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 88);
	f30.f64 = double(temp.f32);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x825cd548
	sub_825CD548(ctx, base);
	// addi r8,r1,82
	ctx.r8.s64 = ctx.r1.s64 + 82;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cd4a8
	sub_825CD4A8(ctx, base);
	// extsh r7,r31
	ctx.r7.s64 = r31.s16;
	// lwz r4,56(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 56);
	// srawi r11,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r11.s64 = ctx.r7.s32 >> 1;
	// addze r8,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r8.s64 = temp.s64;
	// lwz r11,56(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 56);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825d0918
	if (!cr6.gt) goto loc_825D0918;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r8,-1
	ctx.r6.s64 = ctx.r8.s64 + -1;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r5,r6,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// subf r6,r11,r4
	ctx.r6.s64 = ctx.r4.s64 - r11.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_825D08E0:
	// lfsx f0,r6,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r6,r10
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, temp.u32);
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stfsx f12,r6,r9
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r9.u32, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x825d08e0
	if (!cr6.eq) goto loc_825D08E0;
loc_825D0918:
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// ble cr6,0x825d0978
	if (!cr6.gt) goto loc_825D0978;
	// addi r10,r7,-2
	ctx.r10.s64 = ctx.r7.s64 + -2;
	// addi r6,r8,-2
	ctx.r6.s64 = ctx.r8.s64 + -2;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r6,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// subf r6,r11,r4
	ctx.r6.s64 = ctx.r4.s64 - r11.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_825D0940:
	// lfsx f0,r10,r6
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	f0.f64 = double(temp.f32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lfsx f12,r9,r6
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r10,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, temp.u32);
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stfsx f12,r9,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x825d0940
	if (!cr6.eq) goto loc_825D0940;
loc_825D0978:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// ble cr6,0x825d09d8
	if (!cr6.gt) goto loc_825D09D8;
	// addi r10,r7,-3
	ctx.r10.s64 = ctx.r7.s64 + -3;
	// addi r6,r8,-3
	ctx.r6.s64 = ctx.r8.s64 + -3;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r6,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// subf r6,r11,r4
	ctx.r6.s64 = ctx.r4.s64 - r11.s64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
loc_825D09A0:
	// lfsx f0,r10,r6
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	f0.f64 = double(temp.f32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lfsx f12,r9,r6
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r10,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, temp.u32);
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stfsx f12,r9,r6
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x825d09a0
	if (!cr6.eq) goto loc_825D09A0;
loc_825D09D8:
	// cmpwi cr6,r8,3
	cr6.compare<int32_t>(ctx.r8.s32, 3, xer);
	// ble cr6,0x825d0a38
	if (!cr6.gt) goto loc_825D0A38;
	// addi r10,r7,-4
	ctx.r10.s64 = ctx.r7.s64 + -4;
	// addi r6,r8,-4
	ctx.r6.s64 = ctx.r8.s64 + -4;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,30,2,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_825D0A00:
	// lfsx f0,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lfsx f12,r9,r11
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfsx f12,r10,r11
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, temp.u32);
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stfsx f12,r9,r11
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x825d0a00
	if (!cr6.eq) goto loc_825D0A00;
loc_825D0A38:
	// srawi r9,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r9.s64 = r28.s32 >> 1;
	// lwz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 56);
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,56(r26)
	ctx.r6.u64 = PPC_LOAD_U32(r26.u32 + 56);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// add r5,r6,r11
	ctx.r5.u64 = ctx.r6.u64 + r11.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// cmpw cr6,r7,r28
	cr6.compare<int32_t>(ctx.r7.s32, r28.s32, xer);
	// addi r11,r9,-4
	r11.s64 = ctx.r9.s64 + -4;
	// subf r9,r8,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r8,r8,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r8.s64;
	// addi r10,r5,-4
	ctx.r10.s64 = ctx.r5.s64 + -4;
	// ble cr6,0x825d0a98
	if (!cr6.gt) goto loc_825D0A98;
	// lhz r6,82(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// lhz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// subf r7,r7,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r7.s64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// b 0x825d0ac4
	goto loc_825D0AC4;
loc_825D0A98:
	// lhz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// lhz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lhz r5,82(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r7,r28,r7
	ctx.r7.s64 = ctx.r7.s64 - r28.s64;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r6,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r6.s64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
loc_825D0AC4:
	// srawi r6,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 1;
	// li r26,0
	r26.s64 = 0;
	// addze r27,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	r27.s64 = temp.s64;
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// blt cr6,0x825d0cc8
	if (cr6.lt) goto loc_825D0CC8;
	// addi r7,r27,-4
	ctx.r7.s64 = r27.s64 + -4;
	// rlwinm r7,r7,30,2,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r26,r7,2,0,29
	r26.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
loc_825D0AE8:
	// lfs f12,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f30,f31,f28
	ctx.f13.f64 = double(float(f30.f64 * f31.f64 + f28.f64));
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmr f8,f12
	ctx.f8.f64 = ctx.f12.f64;
	// fmuls f9,f11,f31
	ctx.f9.f64 = double(float(ctx.f11.f64 * f31.f64));
	// addi r5,r9,8
	ctx.r5.s64 = ctx.r9.s64 + 8;
	// fmuls f5,f12,f31
	ctx.f5.f64 = double(float(ctx.f12.f64 * f31.f64));
	// lfs f12,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmr f4,f11
	ctx.f4.f64 = ctx.f11.f64;
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fneg f10,f29
	ctx.f10.u64 = f29.u64 ^ 0x8000000000000000;
	// addi r6,r11,-8
	ctx.r6.s64 = r11.s64 + -8;
	// fmr f1,f11
	ctx.f1.f64 = ctx.f11.f64;
	// addi r4,r8,8
	ctx.r4.s64 = ctx.r8.s64 + 8;
	// fmr f6,f31
	ctx.f6.f64 = f31.f64;
	// addi r3,r10,-8
	ctx.r3.s64 = ctx.r10.s64 + -8;
	// fnmsubs f0,f30,f29,f27
	f0.f64 = double(float(-(f30.f64 * f29.f64 - f27.f64)));
	// addi r31,r9,12
	r31.s64 = ctx.r9.s64 + 12;
	// fmr f7,f29
	ctx.f7.f64 = f29.f64;
	// addi r30,r11,-12
	r30.s64 = r11.s64 + -12;
	// fmr f3,f13
	ctx.f3.f64 = ctx.f13.f64;
	// addi r29,r8,12
	r29.s64 = ctx.r8.s64 + 12;
	// fmr f2,f13
	ctx.f2.f64 = ctx.f13.f64;
	// addi r28,r10,-12
	r28.s64 = ctx.r10.s64 + -12;
	// fmadds f9,f8,f29,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * f29.f64 + ctx.f9.f64));
	// stfs f9,0(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fneg f8,f13
	ctx.f8.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// fmr f9,f13
	ctx.f9.f64 = ctx.f13.f64;
	// fmadds f13,f10,f4,f5
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f5.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmuls f4,f12,f31
	ctx.f4.f64 = double(float(ctx.f12.f64 * f31.f64));
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * f31.f64));
	// lfs f11,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fmr f31,f12
	f31.f64 = ctx.f12.f64;
	// lfs f12,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmr f28,f11
	f28.f64 = ctx.f11.f64;
	// fmr f27,f12
	f27.f64 = ctx.f12.f64;
	// fmadds f13,f0,f30,f7
	ctx.f13.f64 = double(float(f0.f64 * f30.f64 + ctx.f7.f64));
	// fmr f7,f0
	ctx.f7.f64 = f0.f64;
	// fmadds f10,f10,f1,f4
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f4.f64));
	// stfs f10,0(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmr f4,f8
	ctx.f4.f64 = ctx.f8.f64;
	// fmadds f10,f31,f29,f5
	ctx.f10.f64 = double(float(f31.f64 * f29.f64 + ctx.f5.f64));
	// stfs f10,0(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmuls f29,f0,f12
	f29.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f0,f11
	f31.f64 = double(float(f0.f64 * ctx.f11.f64));
	// lfs f11,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fmr f5,f9
	ctx.f5.f64 = ctx.f9.f64;
	// fmr f1,f8
	ctx.f1.f64 = ctx.f8.f64;
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fmr f8,f13
	ctx.f8.f64 = ctx.f13.f64;
	// fmadds f4,f4,f28,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * f28.f64 + f29.f64));
	// stfs f4,4(r9)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fmadds f4,f3,f27,f31
	ctx.f4.f64 = double(float(ctx.f3.f64 * f27.f64 + f31.f64));
	// stfs f4,-4(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// fmuls f3,f0,f12
	ctx.f3.f64 = double(float(f0.f64 * ctx.f12.f64));
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// fmr f31,f11
	f31.f64 = ctx.f11.f64;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// fmuls f4,f0,f11
	ctx.f4.f64 = double(float(f0.f64 * ctx.f11.f64));
	// lfs f11,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f0,f30,f5,f6
	f0.f64 = double(float(-(f30.f64 * ctx.f5.f64 - ctx.f6.f64)));
	// fmr f6,f12
	ctx.f6.f64 = ctx.f12.f64;
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f5,f1,f31,f3
	ctx.f5.f64 = double(float(ctx.f1.f64 * f31.f64 + ctx.f3.f64));
	// stfs f5,4(r8)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// fmr f1,f13
	ctx.f1.f64 = ctx.f13.f64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// fmuls f5,f0,f12
	ctx.f5.f64 = double(float(f0.f64 * ctx.f12.f64));
	// fmadds f6,f2,f6,f4
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f4.f64));
	// stfs f6,-4(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// fmr f2,f13
	ctx.f2.f64 = ctx.f13.f64;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// fmr f6,f0
	ctx.f6.f64 = f0.f64;
	// fmuls f4,f0,f11
	ctx.f4.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmr f3,f8
	ctx.f3.f64 = ctx.f8.f64;
	// fmr f31,f10
	f31.f64 = ctx.f10.f64;
	// fmadds f13,f10,f11,f5
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + ctx.f5.f64));
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fmadds f12,f2,f12,f4
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f4.f64));
	// stfs f12,0(r6)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lfs f12,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f0,f30,f9
	ctx.f13.f64 = double(float(f0.f64 * f30.f64 + ctx.f9.f64));
	// lfs f11,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f9,f0,f12
	ctx.f9.f64 = double(float(f0.f64 * ctx.f12.f64));
	// fmr f4,f11
	ctx.f4.f64 = ctx.f11.f64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// fmuls f5,f0,f11
	ctx.f5.f64 = double(float(f0.f64 * ctx.f11.f64));
	// lfs f11,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmr f2,f12
	ctx.f2.f64 = ctx.f12.f64;
	// lfs f12,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f0,f30,f3,f7
	f0.f64 = double(float(-(f30.f64 * ctx.f3.f64 - ctx.f7.f64)));
	// fmr f7,f12
	ctx.f7.f64 = ctx.f12.f64;
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fmr f28,f13
	f28.f64 = ctx.f13.f64;
	// fmadds f9,f31,f4,f9
	ctx.f9.f64 = double(float(f31.f64 * ctx.f4.f64 + ctx.f9.f64));
	// stfs f9,0(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fnmsubs f31,f30,f13,f6
	f31.f64 = double(float(-(f30.f64 * ctx.f13.f64 - ctx.f6.f64)));
	// fmadds f9,f1,f2,f5
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f2.f64 + ctx.f5.f64));
	// stfs f9,0(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// fmuls f5,f0,f12
	ctx.f5.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,0(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmuls f4,f0,f11
	ctx.f4.f64 = double(float(f0.f64 * ctx.f11.f64));
	// lfs f11,0(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmr f27,f0
	f27.f64 = f0.f64;
	// fmadds f29,f0,f30,f8
	f29.f64 = double(float(f0.f64 * f30.f64 + ctx.f8.f64));
	// fmadds f9,f10,f9,f5
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f5.f64));
	// stfs f9,0(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// fmadds f9,f13,f7,f4
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + ctx.f4.f64));
	// stfs f9,0(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// fmuls f9,f0,f12
	ctx.f9.f64 = double(float(f0.f64 * ctx.f12.f64));
	// fmuls f0,f0,f11
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmadds f11,f10,f11,f9
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + ctx.f9.f64));
	// stfs f11,0(r29)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// fmadds f0,f13,f12,f0
	f0.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + f0.f64));
	// stfs f0,0(r28)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// bne cr6,0x825d0ae8
	if (!cr6.eq) goto loc_825D0AE8;
loc_825D0CC8:
	// cmpw cr6,r26,r27
	cr6.compare<int32_t>(r26.s32, r27.s32, xer);
	// bge cr6,0x825d10e4
	if (!cr6.lt) goto loc_825D10E4;
	// subf r7,r26,r27
	ctx.r7.s64 = r27.s64 - r26.s64;
loc_825D0CD4:
	// lfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmadds f11,f30,f31,f28
	ctx.f11.f64 = double(float(f30.f64 * f31.f64 + f28.f64));
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmr f5,f0
	ctx.f5.f64 = f0.f64;
	// fmuls f7,f12,f31
	ctx.f7.f64 = double(float(ctx.f12.f64 * f31.f64));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// fmuls f8,f0,f31
	ctx.f8.f64 = double(float(f0.f64 * f31.f64));
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// fneg f13,f29
	ctx.f13.u64 = f29.u64 ^ 0x8000000000000000;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// fmr f6,f12
	ctx.f6.f64 = ctx.f12.f64;
	// lfs f12,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f30,f29,f27
	ctx.f10.f64 = double(float(-(f30.f64 * f29.f64 - f27.f64)));
	// fmr f9,f29
	ctx.f9.f64 = f29.f64;
	// fmr f28,f29
	f28.f64 = f29.f64;
	// fmr f27,f31
	f27.f64 = f31.f64;
	// fmadds f7,f5,f29,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * f29.f64 + ctx.f7.f64));
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fmr f29,f11
	f29.f64 = ctx.f11.f64;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// fmadds f11,f13,f6,f8
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f8.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(f0.f64 * f31.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fmuls f8,f12,f31
	ctx.f8.f64 = double(float(ctx.f12.f64 * f31.f64));
	// fmr f31,f10
	f31.f64 = ctx.f10.f64;
	// fmadds f13,f13,f12,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f11.f64));
	// stfs f13,0(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmadds f0,f0,f9,f8
	f0.f64 = double(float(f0.f64 * ctx.f9.f64 + ctx.f8.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// bne cr6,0x825d0cd4
	if (!cr6.eq) goto loc_825D0CD4;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// addi r12,r1,-56
	r12.s64 = ctx.r1.s64 + -56;
	// bl 0x8239d630
	// b 0x8239bd40
	return;
loc_825D0D6C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825d10e4
	if (!cr6.gt) goto loc_825D10E4;
	// li r26,0
	r26.s64 = 0;
loc_825D0D78:
	// lwz r11,584(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 584);
	// rlwinm r7,r26,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// addi r9,r1,86
	ctx.r9.s64 = ctx.r1.s64 + 86;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lhzx r11,r7,r11
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lhz r30,122(r31)
	r30.u64 = PPC_LOAD_U16(r31.u32 + 122);
	// lfs f30,72(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 72);
	f30.f64 = double(temp.f32);
	// lhz r29,124(r31)
	r29.u64 = PPC_LOAD_U16(r31.u32 + 124);
	// lfs f29,76(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 76);
	f29.f64 = double(temp.f32);
	// extsh r28,r30
	r28.s64 = r30.s16;
	// lfs f28,80(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 80);
	f28.f64 = double(temp.f32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lfs f27,84(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 84);
	f27.f64 = double(temp.f32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lfs f31,88(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 88);
	f31.f64 = double(temp.f32);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// bl 0x825cd548
	sub_825CD548(ctx, base);
	// addi r8,r1,82
	ctx.r8.s64 = ctx.r1.s64 + 82;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cd4a8
	sub_825CD4A8(ctx, base);
	// extsh r7,r29
	ctx.r7.s64 = r29.s16;
	// lwz r5,56(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// li r8,0
	ctx.r8.s64 = 0;
	// srawi r11,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r11.s64 = ctx.r7.s32 >> 1;
	// addze r6,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r6.s64 = temp.s64;
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// blt cr6,0x825d0e8c
	if (cr6.lt) goto loc_825D0E8C;
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// addi r10,r7,-2
	ctx.r10.s64 = ctx.r7.s64 + -2;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825D0E2C:
	// lfs f0,-8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	f0.f64 = double(temp.f32);
	// addi r4,r10,-4
	ctx.r4.s64 = ctx.r10.s64 + -4;
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// stfs f13,-8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// addi r30,r10,-8
	r30.s64 = ctx.r10.s64 + -8;
	// stfs f0,4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	f0.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stfs f0,0(r4)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f13,0(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f0,0(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// bne cr6,0x825d0e2c
	if (!cr6.eq) goto loc_825D0E2C;
loc_825D0E8C:
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// bge cr6,0x825d0ed4
	if (!cr6.lt) goto loc_825D0ED4;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
loc_825D0EB0:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// bne cr6,0x825d0eb0
	if (!cr6.eq) goto loc_825D0EB0;
loc_825D0ED4:
	// srawi r9,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r9.s64 = r28.s32 >> 1;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r7,r28
	cr6.compare<int32_t>(ctx.r7.s32, r28.s32, xer);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// ble cr6,0x825d0f20
	if (!cr6.gt) goto loc_825D0F20;
	// lhz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// b 0x825d0f40
	goto loc_825D0F40;
loc_825D0F20:
	// lhz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
	// lhz r7,82(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// subf r9,r28,r9
	ctx.r9.s64 = ctx.r9.s64 - r28.s64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_825D0F40:
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// addze r7,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r7.s64 = temp.s64;
	// cmpwi cr6,r7,4
	cr6.compare<int32_t>(ctx.r7.s32, 4, xer);
	// blt cr6,0x825d1070
	if (cr6.lt) goto loc_825D1070;
	// addi r9,r7,-4
	ctx.r9.s64 = ctx.r7.s64 + -4;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825D0F64:
	// lfs f12,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f31,f29,f28
	ctx.f13.f64 = double(float(f31.f64 * f29.f64 + f28.f64));
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f29
	ctx.f5.f64 = double(float(ctx.f12.f64 * f29.f64));
	// fmr f3,f11
	ctx.f3.f64 = ctx.f11.f64;
	// addi r8,r10,-12
	ctx.r8.s64 = ctx.r10.s64 + -12;
	// fneg f7,f30
	ctx.f7.u64 = f30.u64 ^ 0x8000000000000000;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fmuls f4,f11,f29
	ctx.f4.f64 = double(float(ctx.f11.f64 * f29.f64));
	// lfs f11,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmr f2,f12
	ctx.f2.f64 = ctx.f12.f64;
	// lfs f12,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// fmr f6,f30
	ctx.f6.f64 = f30.f64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// fnmsubs f0,f31,f30,f27
	f0.f64 = double(float(-(f31.f64 * f30.f64 - f27.f64)));
	// fmr f9,f29
	ctx.f9.f64 = f29.f64;
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fneg f1,f13
	ctx.f1.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fmr f29,f13
	f29.f64 = ctx.f13.f64;
	// fmadds f13,f7,f3,f5
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f3.f64 + ctx.f5.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fmr f5,f12
	ctx.f5.f64 = ctx.f12.f64;
	// fmadds f13,f2,f6,f4
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f4.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f13,f0,f31,f30
	ctx.f13.f64 = double(float(f0.f64 * f31.f64 + f30.f64));
	// fmuls f6,f0,f12
	ctx.f6.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f0
	ctx.f8.f64 = f0.f64;
	// fmuls f0,f0,f11
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmr f4,f11
	ctx.f4.f64 = ctx.f11.f64;
	// lfs f11,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f7,f10
	ctx.f7.f64 = ctx.f10.f64;
	// fmr f3,f11
	ctx.f3.f64 = ctx.f11.f64;
	// fmadds f0,f1,f5,f0
	f0.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + f0.f64));
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// fmadds f0,f29,f4,f6
	f0.f64 = double(float(f29.f64 * ctx.f4.f64 + ctx.f6.f64));
	// stfs f0,-4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// fnmsubs f0,f31,f10,f9
	f0.f64 = double(float(-(f31.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f4,f12
	ctx.f4.f64 = ctx.f12.f64;
	// fneg f6,f13
	ctx.f6.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fmr f5,f13
	ctx.f5.f64 = ctx.f13.f64;
	// fmuls f2,f0,f11
	ctx.f2.f64 = double(float(f0.f64 * ctx.f11.f64));
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f0,f12
	ctx.f1.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f0,f31,f7
	ctx.f13.f64 = double(float(f0.f64 * f31.f64 + ctx.f7.f64));
	// fmr f9,f0
	ctx.f9.f64 = f0.f64;
	// fnmsubs f0,f31,f10,f8
	f0.f64 = double(float(-(f31.f64 * ctx.f10.f64 - ctx.f8.f64)));
	// fmadds f8,f6,f4,f2
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f4.f64 + ctx.f2.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// fmadds f8,f5,f3,f1
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f3.f64 + ctx.f1.f64));
	// stfs f8,-8(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// fneg f8,f13
	ctx.f8.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// fnmsubs f29,f31,f13,f9
	f29.f64 = double(float(-(f31.f64 * ctx.f13.f64 - ctx.f9.f64)));
	// fmuls f7,f0,f11
	ctx.f7.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmr f27,f0
	f27.f64 = f0.f64;
	// fmadds f30,f0,f31,f10
	f30.f64 = double(float(f0.f64 * f31.f64 + ctx.f10.f64));
	// fmr f28,f13
	f28.f64 = ctx.f13.f64;
	// fmadds f8,f8,f12,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f8,12(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// fmadds f0,f13,f11,f12
	f0.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// bne cr6,0x825d0f64
	if (!cr6.eq) goto loc_825D0F64;
loc_825D1070:
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// bge cr6,0x825d10cc
	if (!cr6.lt) goto loc_825D10CC;
	// subf r9,r6,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r6.s64;
loc_825D107C:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// fneg f10,f30
	ctx.f10.u64 = f30.u64 ^ 0x8000000000000000;
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f0,f29
	ctx.f9.f64 = double(float(f0.f64 * f29.f64));
	// fmuls f8,f13,f29
	ctx.f8.f64 = double(float(ctx.f13.f64 * f29.f64));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fmadds f12,f31,f29,f28
	ctx.f12.f64 = double(float(f31.f64 * f29.f64 + f28.f64));
	// fnmsubs f11,f31,f30,f27
	ctx.f11.f64 = double(float(-(f31.f64 * f30.f64 - f27.f64)));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// fmr f28,f30
	f28.f64 = f30.f64;
	// fmr f27,f29
	f27.f64 = f29.f64;
	// fmadds f13,f10,f13,f9
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f9.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fmadds f0,f0,f30,f8
	f0.f64 = double(float(f0.f64 * f30.f64 + ctx.f8.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmr f30,f12
	f30.f64 = ctx.f12.f64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// fmr f29,f11
	f29.f64 = ctx.f11.f64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d107c
	if (!cr6.eq) goto loc_825D107C;
loc_825D10CC:
	// lhz r10,580(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 580);
	// addi r11,r26,1
	r11.s64 = r26.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r26,r11
	r26.s64 = r11.s16;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d0d78
	if (cr6.lt) goto loc_825D0D78;
loc_825D10E4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// addi r12,r1,-56
	r12.s64 = ctx.r1.s64 + -56;
	// bl 0x8239d630
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D10F8"))) PPC_WEAK_FUNC(sub_825D10F8);
PPC_FUNC_IMPL(__imp__sub_825D10F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r25,0(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,60(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 60);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bge cr6,0x825d1130
	if (!cr6.lt) goto loc_825D1130;
loc_825D1120:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_825D1130:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// li r23,1
	r23.s64 = 1;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x825d2244
	if (cr6.eq) goto loc_825D2244;
	// li r22,2
	r22.s64 = 2;
	// li r21,3
	r21.s64 = 3;
	// li r16,22
	r16.s64 = 22;
	// li r17,33
	r17.s64 = 33;
	// li r14,35
	r14.s64 = 35;
	// li r19,50
	r19.s64 = 50;
	// li r18,49
	r18.s64 = 49;
	// li r15,47
	r15.s64 = 47;
	// li r20,10
	r20.s64 = 10;
loc_825D1164:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmplwi cr6,r11,51
	cr6.compare<uint32_t>(r11.u32, 51, xer);
	// bgt cr6,0x825d2238
	if (cr6.gt) goto loc_825D2238;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,4488
	r12.s64 = r12.s64 + 4488;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D1258;
	case 1:
		goto loc_825D1E6C;
	case 2:
		goto loc_825D141C;
	case 3:
		goto loc_825D14C8;
	case 4:
		goto loc_825D2238;
	case 5:
		goto loc_825D2238;
	case 6:
		goto loc_825D2238;
	case 7:
		goto loc_825D2238;
	case 8:
		goto loc_825D2238;
	case 9:
		goto loc_825D2238;
	case 10:
		goto loc_825D2238;
	case 11:
		goto loc_825D2238;
	case 12:
		goto loc_825D2238;
	case 13:
		goto loc_825D2238;
	case 14:
		goto loc_825D2238;
	case 15:
		goto loc_825D2238;
	case 16:
		goto loc_825D2238;
	case 17:
		goto loc_825D14A4;
	case 18:
		goto loc_825D1470;
	case 19:
		goto loc_825D14F0;
	case 20:
		goto loc_825D151C;
	case 21:
		goto loc_825D1568;
	case 22:
		goto loc_825D15B4;
	case 23:
		goto loc_825D1650;
	case 24:
		goto loc_825D169C;
	case 25:
		goto loc_825D16EC;
	case 26:
		goto loc_825D1724;
	case 27:
		goto loc_825D17C0;
	case 28:
		goto loc_825D1884;
	case 29:
		goto loc_825D1980;
	case 30:
		goto loc_825D2238;
	case 31:
		goto loc_825D2238;
	case 32:
		goto loc_825D1EFC;
	case 33:
		goto loc_825D19AC;
	case 34:
		goto loc_825D1DF8;
	case 35:
		goto loc_825D1E1C;
	case 36:
		goto loc_825D2238;
	case 37:
		goto loc_825D1FD4;
	case 38:
		goto loc_825D1FA0;
	case 39:
		goto loc_825D2238;
	case 40:
		goto loc_825D2238;
	case 41:
		goto loc_825D2238;
	case 42:
		goto loc_825D2238;
	case 43:
		goto loc_825D2238;
	case 44:
		goto loc_825D2018;
	case 45:
		goto loc_825D2238;
	case 46:
		goto loc_825D2068;
	case 47:
		goto loc_825D20C0;
	case 48:
		goto loc_825D2238;
	case 49:
		goto loc_825D1F88;
	case 50:
		goto loc_825D21D0;
	case 51:
		goto loc_825D2208;
	default:
		__builtin_unreachable();
	}
	// lwz r18,4696(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 4696);
	// lwz r18,7788(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 7788);
	// lwz r18,5148(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5148);
	// lwz r18,5320(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5320);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,5284(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5284);
	// lwz r18,5232(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5232);
	// lwz r18,5360(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5360);
	// lwz r18,5404(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5404);
	// lwz r18,5480(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5480);
	// lwz r18,5556(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5556);
	// lwz r18,5712(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5712);
	// lwz r18,5788(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5788);
	// lwz r18,5868(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5868);
	// lwz r18,5924(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 5924);
	// lwz r18,6080(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6080);
	// lwz r18,6276(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6276);
	// lwz r18,6528(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6528);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,7932(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 7932);
	// lwz r18,6572(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6572);
	// lwz r18,7672(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 7672);
	// lwz r18,7708(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 7708);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8148(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8148);
	// lwz r18,8096(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8096);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8216(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8216);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8296(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8296);
	// lwz r18,8384(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8384);
	// lwz r18,8760(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8760);
	// lwz r18,8072(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8072);
	// lwz r18,8656(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8656);
	// lwz r18,8712(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 8712);
loc_825D1258:
	// lwz r11,256(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 256);
	// lhz r4,34(r25)
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwz r3,320(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// mullw r7,r11,r4
	ctx.r7.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825d12d0
	if (!cr6.gt) goto loc_825D12D0;
	// clrlwi r5,r4,16
	ctx.r5.u64 = ctx.r4.u32 & 0xFFFF;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// addi r11,r3,424
	r11.s64 = ctx.r3.s64 + 424;
loc_825D1288:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r28,r9
	r28.s64 = ctx.r9.s16;
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// ble cr6,0x825d12bc
	if (!cr6.gt) goto loc_825D12BC;
	// lhz r31,-310(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -310);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// extsh r9,r31
	ctx.r9.s64 = r31.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r31,r9,r10
	r31.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
loc_825D12BC:
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x825d1288
	if (cr6.lt) goto loc_825D1288;
loc_825D12D0:
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// sth r24,580(r25)
	PPC_STORE_U16(r25.u32 + 580, r24.u16);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825d13c4
	if (!cr6.gt) goto loc_825D13C4;
	// extsh r5,r8
	ctx.r5.s64 = ctx.r8.s16;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// addi r11,r3,114
	r11.s64 = ctx.r3.s64 + 114;
loc_825D12EC:
	// lwz r10,310(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 310);
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// bne cr6,0x825d13a8
	if (!cr6.eq) goto loc_825D13A8;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r4,r31
	ctx.r4.s64 = r31.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpw cr6,r4,r9
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r9.s32, xer);
	// bne cr6,0x825d13a8
	if (!cr6.eq) goto loc_825D13A8;
	// lhz r9,580(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// lwz r4,584(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 584);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r6,r9,r4
	PPC_STORE_U16(ctx.r9.u32 + ctx.r4.u32, ctx.r6.u16);
	// lhz r9,580(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r9,580(r25)
	PPC_STORE_U16(r25.u32 + 580, ctx.r9.u16);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// sth r9,12(r11)
	PPC_STORE_U16(r11.u32 + 12, ctx.r9.u16);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// sth r9,10(r11)
	PPC_STORE_U16(r11.u32 + 10, ctx.r9.u16);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lhz r9,-2(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + -2);
	// sth r9,8(r11)
	PPC_STORE_U16(r11.u32 + 8, ctx.r9.u16);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
loc_825D13A8:
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// lhz r9,34(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// addi r11,r11,1776
	r11.s64 = r11.s64 + 1776;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d12ec
	if (cr6.lt) goto loc_825D12EC;
loc_825D13C4:
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// lhz r10,34(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d1120
	if (!cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// cntlzw r11,r7
	r11.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r11,216(r30)
	PPC_STORE_U32(r30.u32 + 216, r11.u32);
	// bl 0x825c7880
	sub_825C7880(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825b5218
	sub_825B5218(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// stw r22,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r22.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D141C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r9,34(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stw r10,192(r25)
	PPC_STORE_U32(r25.u32 + 192, ctx.r10.u32);
	// bne cr6,0x825d1120
	if (!cr6.eq) goto loc_825D1120;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1468
	if (!cr6.eq) goto loc_825D1468;
	// li r11,18
	r11.s64 = 18;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D1468:
	// stw r23,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r23.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D1470:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,188(r25)
	PPC_STORE_U32(r25.u32 + 188, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d1120
	if (cr6.eq) goto loc_825D1120;
	// li r11,17
	r11.s64 = 17;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D14A4:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,180(r25)
	PPC_STORE_U32(r25.u32 + 180, r11.u32);
	// stw r21,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r21.u32);
loc_825D14C8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,184(r25)
	PPC_STORE_U32(r25.u32 + 184, r11.u32);
	// li r11,19
	r11.s64 = 19;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D14F0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,656(r25)
	PPC_STORE_U32(r25.u32 + 656, r11.u32);
	// li r11,20
	r11.s64 = 20;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D151C:
	// lwz r11,180(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 180);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1560
	if (!cr6.eq) goto loc_825D1560;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,648(r25)
	PPC_STORE_U32(r25.u32 + 648, r11.u32);
loc_825D1560:
	// li r11,21
	r11.s64 = 21;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1568:
	// lwz r11,180(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 180);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d15ac
	if (!cr6.eq) goto loc_825D15AC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// cmpwi cr6,r10,12
	cr6.compare<int32_t>(ctx.r10.s32, 12, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// sth r11,582(r25)
	PPC_STORE_U16(r25.u32 + 582, r11.u16);
loc_825D15AC:
	// stw r24,652(r25)
	PPC_STORE_U32(r25.u32 + 652, r24.u32);
	// stw r16,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r16.u32);
loc_825D15B4:
	// lwz r11,180(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 180);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1648
	if (!cr6.eq) goto loc_825D1648;
	// lwz r11,652(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 652);
	// lwz r10,648(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 648);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d1648
	if (!cr6.lt) goto loc_825D1648;
loc_825D15D0:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d162c
	if (!cr6.eq) goto loc_825D162C;
	// lhz r11,582(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 582);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addic. r11,r11,1
	xer.ca = r11.u32 > 4294967294;
	r11.s64 = r11.s64 + 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825d1120
	if (!cr0.gt) goto loc_825D1120;
	// lhz r10,582(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 582);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// slw r10,r23,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r10.u8 & 0x3F));
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// lwz r10,652(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 652);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r10,r10,158
	ctx.r10.s64 = ctx.r10.s64 + 158;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r25
	PPC_STORE_U32(ctx.r10.u32 + r25.u32, r11.u32);
loc_825D162C:
	// lwz r11,652(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 652);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,652(r25)
	PPC_STORE_U32(r25.u32 + 652, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,648(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 648);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d15d0
	if (cr6.lt) goto loc_825D15D0;
loc_825D1648:
	// li r11,23
	r11.s64 = 23;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1650:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1694
	if (!cr6.eq) goto loc_825D1694;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// stw r11,664(r25)
	PPC_STORE_U32(r25.u32 + 664, r11.u32);
loc_825D1694:
	// li r11,24
	r11.s64 = 24;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D169C:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d16e4
	if (!cr6.eq) goto loc_825D16E4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// addi r4,r25,664
	ctx.r4.s64 = r25.s64 + 664;
	// stw r11,672(r25)
	PPC_STORE_U32(r25.u32 + 672, r11.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825da8c8
	sub_825DA8C8(ctx, base);
loc_825D16E4:
	// li r11,25
	r11.s64 = 25;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D16EC:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1718
	if (!cr6.eq) goto loc_825D1718;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,660(r25)
	PPC_STORE_U32(r25.u32 + 660, r11.u32);
loc_825D1718:
	// li r11,26
	r11.s64 = 26;
	// sth r24,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r24.u16);
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1724:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d17b4
	if (!cr6.eq) goto loc_825D17B4;
	// lwz r11,660(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 660);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d17b4
	if (!cr6.eq) goto loc_825D17B4;
	// lwz r10,672(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 672);
	// mr r11,r24
	r11.u64 = r24.u64;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// ble cr6,0x825d1768
	if (!cr6.gt) goto loc_825D1768;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825D1758:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r8,r10,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bgt cr6,0x825d1758
	if (cr6.gt) goto loc_825D1758;
loc_825D1768:
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825d177c
	if (!cr6.lt) goto loc_825D177C;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
loc_825D177C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r10,672(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 672);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// addi r9,r11,2
	ctx.r9.s64 = r11.s64 + 2;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,680(r25)
	PPC_STORE_U32(r25.u32 + 680, r11.u32);
loc_825D17B4:
	// li r11,27
	r11.s64 = 27;
	// sth r24,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r24.u16);
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D17C0:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1874
	if (!cr6.eq) goto loc_825D1874;
	// lwz r11,660(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 660);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1874
	if (!cr6.eq) goto loc_825D1874;
	// lhz r11,34(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// lwz r9,664(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 664);
	// lhz r8,146(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// lwz r7,680(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 680);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lwz r10,672(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 672);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// subfic r31,r7,32
	xer.ca = ctx.r7.u32 <= 32;
	r31.s64 = 32 - ctx.r7.s64;
	// subfic r29,r10,30
	xer.ca = ctx.r10.u32 <= 30;
	r29.s64 = 30 - ctx.r10.s64;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x825d1874
	if (!cr6.lt) goto loc_825D1874;
	// addi r28,r30,224
	r28.s64 = r30.s64 + 224;
loc_825D180C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,680(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 680);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r11,r11,r31
	r11.u64 = r31.u8 & 0x20 ? 0 : (r11.u32 << (r31.u8 & 0x3F));
	// sraw r11,r11,r29
	temp.u32 = r29.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r9,696(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 696);
	// sthx r11,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r11.u16);
	// lhz r11,146(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r11.u16);
	// lhz r11,34(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// lwz r10,664(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 664);
	// lhz r9,146(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825d180c
	if (cr6.lt) goto loc_825D180C;
loc_825D1874:
	// li r11,28
	r11.s64 = 28;
	// sth r24,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r24.u16);
	// sth r24,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r24.u16);
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1884:
	// lwz r11,656(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1978
	if (!cr6.eq) goto loc_825D1978;
	// lwz r11,660(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 660);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1978
	if (!cr6.eq) goto loc_825D1978;
	// lhz r10,150(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lwz r9,680(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 680);
	// lwz r11,672(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 672);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r8,34(r25)
	ctx.r8.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// subfic r31,r9,32
	xer.ca = ctx.r9.u32 <= 32;
	r31.s64 = 32 - ctx.r9.s64;
	// subfic r29,r11,30
	xer.ca = r11.u32 <= 30;
	r29.s64 = 30 - r11.s64;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x825d1978
	if (!cr6.lt) goto loc_825D1978;
loc_825D18C0:
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d1954
	if (!cr6.lt) goto loc_825D1954;
	// addi r28,r30,224
	r28.s64 = r30.s64 + 224;
loc_825D18DC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,680(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 680);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,150(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lhz r9,146(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// slw r11,r11,r31
	r11.u64 = r31.u8 & 0x20 ? 0 : (r11.u32 << (r31.u8 & 0x3F));
	// sraw r11,r11,r29
	temp.u32 = r29.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lhz r9,34(r25)
	ctx.r9.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// lwz r7,704(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 704);
	// mullw r11,r9,r8
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r6,r11,r7
	PPC_STORE_U16(r11.u32 + ctx.r7.u32, ctx.r6.u16);
	// lhz r11,146(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r11.u16);
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d18dc
	if (cr6.lt) goto loc_825D18DC;
loc_825D1954:
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// sth r24,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r24.u16);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,34(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d18c0
	if (cr6.lt) goto loc_825D18C0;
loc_825D1978:
	// li r11,29
	r11.s64 = 29;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1980:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,716(r25)
	PPC_STORE_U32(r25.u32 + 716, r11.u32);
	// sth r24,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r24.u16);
	// stw r17,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r17.u32);
	// stw r24,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r24.u32);
loc_825D19AC:
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// lhz r10,150(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d1df0
	if (!cr6.lt) goto loc_825D1DF0;
loc_825D19C4:
	// lhz r10,150(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lwz r8,584(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 584);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// lwz r9,320(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// lhzx r10,r10,r8
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r31,r10,r9
	r31.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bgt cr6,0x825d1dcc
	if (cr6.gt) goto loc_825D1DCC;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,6668
	r12.s64 = r12.s64 + 6668;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D1A1C;
	case 1:
		goto loc_825D1A58;
	case 2:
		goto loc_825D1AE8;
	case 3:
		goto loc_825D1B74;
	default:
		__builtin_unreachable();
	}
	// lwz r18,6684(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6684);
	// lwz r18,6744(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6744);
	// lwz r18,6888(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 6888);
	// lwz r18,7028(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 7028);
loc_825D1A1C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// sth r11,182(r31)
	PPC_STORE_U16(r31.u32 + 182, r11.u16);
	// sth r24,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r24.u16);
	// stw r23,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r23.u32);
loc_825D1A58:
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d1ae0
	if (!cr6.lt) goto loc_825D1AE0;
	// addi r29,r30,224
	r29.s64 = r30.s64 + 224;
loc_825D1A74:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,256
	cr6.compare<uint32_t>(r11.u32, 256, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// stw r11,200(r10)
	PPC_STORE_U32(ctx.r10.u32 + 200, r11.u32);
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r11.u16);
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d1a74
	if (cr6.lt) goto loc_825D1A74;
loc_825D1AE0:
	// sth r24,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r24.u16);
	// stw r22,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r22.u32);
loc_825D1AE8:
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d1b68
	if (!cr6.lt) goto loc_825D1B68;
	// addi r29,r30,224
	r29.s64 = r30.s64 + 224;
loc_825D1B04:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r10,12
	cr6.compare<int32_t>(ctx.r10.s32, 12, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// stw r10,220(r11)
	PPC_STORE_U32(r11.u32 + 220, ctx.r10.u32);
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r11.u16);
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d1b04
	if (cr6.lt) goto loc_825D1B04;
loc_825D1B68:
	// sth r24,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r24.u16);
	// stw r21,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r21.u32);
	// stw r24,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r24.u32);
loc_825D1B74:
	// lwz r11,716(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 716);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1dc4
	if (!cr6.eq) goto loc_825D1DC4;
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d1dc4
	if (!cr6.lt) goto loc_825D1DC4;
loc_825D1B98:
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d1bb4
	if (cr6.lt) goto loc_825D1BB4;
	// beq cr6,0x825d1c44
	if (cr6.eq) goto loc_825D1C44;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x825d1ce0
	if (cr6.lt) goto loc_825D1CE0;
	// b 0x825d1d9c
	goto loc_825D1D9C;
loc_825D1BB4:
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// mr r11,r24
	r11.u64 = r24.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r9,200(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// ble cr6,0x825d1bf8
	if (!cr6.gt) goto loc_825D1BF8;
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,200(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 200);
loc_825D1BE8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r8,r10,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bgt cr6,0x825d1be8
	if (cr6.gt) goto loc_825D1BE8;
loc_825D1BF8:
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825d1c0c
	if (!cr6.lt) goto loc_825D1C0C;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
loc_825D1C0C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// stw r10,212(r11)
	PPC_STORE_U32(r11.u32 + 212, ctx.r10.u32);
	// stw r23,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r23.u32);
loc_825D1C44:
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// mr r11,r24
	r11.u64 = r24.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,220(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 220);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// ble cr6,0x825d1c90
	if (!cr6.gt) goto loc_825D1C90;
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,56
	ctx.r10.s64 = ctx.r10.s64 * 56;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,220(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 220);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825D1C80:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r8,r10,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bgt cr6,0x825d1c80
	if (cr6.gt) goto loc_825D1C80;
loc_825D1C90:
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825d1ca4
	if (!cr6.lt) goto loc_825D1CA4;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
loc_825D1CA4:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// stw r10,216(r11)
	PPC_STORE_U32(r11.u32 + 216, ctx.r10.u32);
	// sth r24,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r24.u16);
	// stw r22,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r22.u32);
loc_825D1CE0:
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r27,216(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 216);
	// lwz r9,220(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 220);
	// lwz r11,212(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// subfic r29,r27,32
	xer.ca = r27.u32 <= 32;
	r29.s64 = 32 - r27.s64;
	// subfic r28,r9,30
	xer.ca = ctx.r9.u32 <= 30;
	r28.s64 = 30 - ctx.r9.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d1d98
	if (!cr6.lt) goto loc_825D1D98;
	// addi r26,r30,224
	r26.s64 = r30.s64 + 224;
loc_825D1D18:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r11,r11,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r11.u32 << (r29.u8 & 0x3F));
	// sraw r11,r11,r28
	temp.u32 = r28.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lhz r9,184(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r11,252(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 252);
	// sthx r8,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r8.u16);
	// lhz r11,146(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,146(r30)
	PPC_STORE_U16(r30.u32 + 146, r11.u16);
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// lhz r10,146(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 146);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r11,r11,56
	r11.s64 = r11.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r11,212(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 212);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d1d18
	if (cr6.lt) goto loc_825D1D18;
loc_825D1D98:
	// stw r24,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r24.u32);
loc_825D1D9C:
	// stw r24,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r24.u32);
	// lhz r11,184(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r11.u16);
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// lhz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 184);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d1b98
	if (cr6.lt) goto loc_825D1B98;
loc_825D1DC4:
	// sth r24,184(r31)
	PPC_STORE_U16(r31.u32 + 184, r24.u16);
	// stw r24,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r24.u32);
loc_825D1DCC:
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d19c4
	if (cr6.lt) goto loc_825D19C4;
loc_825D1DF0:
	// li r11,34
	r11.s64 = 34;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1DF8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// sth r11,728(r25)
	PPC_STORE_U16(r25.u32 + 728, r11.u16);
	// stw r14,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r14.u32);
loc_825D1E1C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// cmplwi cr6,r11,128
	cr6.compare<uint32_t>(r11.u32, 128, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// lwz r10,192(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// sth r11,208(r25)
	PPC_STORE_U16(r25.u32 + 208, r11.u16);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825d1e68
	if (!cr6.eq) goto loc_825D1E68;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r4,320(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// bl 0x825ddb40
	sub_825DDB40(ctx, base);
loc_825D1E68:
	// stw r23,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r23.u32);
loc_825D1E6C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lhz r10,580(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// stw r11,204(r25)
	PPC_STORE_U32(r25.u32 + 204, r11.u32);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d1edc
	if (!cr6.gt) goto loc_825D1EDC;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D1EA0:
	// lwz r8,584(r25)
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + 584);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r9,320(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// lhzx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r8.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r23,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, r23.u32);
	// lhz r10,580(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d1ea0
	if (cr6.lt) goto loc_825D1EA0;
loc_825D1EDC:
	// lwz r11,204(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1ef0
	if (!cr6.eq) goto loc_825D1EF0;
	// stw r19,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r19.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D1EF0:
	// li r11,32
	r11.s64 = 32;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D1EFC:
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// addi r28,r30,224
	r28.s64 = r30.s64 + 224;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d1f80
	if (!cr6.gt) goto loc_825D1F80;
	// mr r31,r24
	r31.u64 = r24.u64;
loc_825D1F2C:
	// lwz r11,584(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 584);
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// extsh r31,r11
	r31.s64 = r11.s16;
	// stw r10,40(r29)
	PPC_STORE_U32(r29.u32 + 40, ctx.r10.u32);
	// lhz r10,580(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d1f2c
	if (cr6.lt) goto loc_825D1F2C;
loc_825D1F80:
	// sth r24,760(r25)
	PPC_STORE_U16(r25.u32 + 760, r24.u16);
	// stw r18,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r18.u32);
loc_825D1F88:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d21c8
	if (!cr6.eq) goto loc_825D21C8;
	// li r11,38
	r11.s64 = 38;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D1FA0:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d1fcc
	if (!cr6.eq) goto loc_825D1FCC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,164(r25)
	PPC_STORE_U32(r25.u32 + 164, r11.u32);
loc_825D1FCC:
	// li r11,37
	r11.s64 = 37;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D1FD4:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2010
	if (!cr6.eq) goto loc_825D2010;
	// lwz r11,164(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2010
	if (!cr6.eq) goto loc_825D2010;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,168(r25)
	PPC_STORE_U16(r25.u32 + 168, r11.u16);
loc_825D2010:
	// li r11,44
	r11.s64 = 44;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D2018:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2060
	if (!cr6.eq) goto loc_825D2060;
	// lwz r11,164(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2060
	if (!cr6.eq) goto loc_825D2060;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// sth r11,170(r25)
	PPC_STORE_U16(r25.u32 + 170, r11.u16);
loc_825D2060:
	// li r11,46
	r11.s64 = 46;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_825D2068:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d20b4
	if (!cr6.eq) goto loc_825D20B4;
	// lwz r11,164(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d20b4
	if (!cr6.eq) goto loc_825D20B4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d1120
	if (cr6.lt) goto loc_825D1120;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bgt cr6,0x825d1120
	if (cr6.gt) goto loc_825D1120;
	// sth r11,172(r25)
	PPC_STORE_U16(r25.u32 + 172, r11.u16);
loc_825D20B4:
	// sth r24,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r24.u16);
	// sth r24,148(r30)
	PPC_STORE_U16(r30.u32 + 148, r24.u16);
	// stw r15,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r15.u32);
loc_825D20C0:
	// lwz r11,120(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d21c8
	if (!cr6.eq) goto loc_825D21C8;
	// lwz r11,164(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d21c8
	if (!cr6.eq) goto loc_825D21C8;
	// lhz r11,580(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// lhz r10,150(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d21c8
	if (!cr6.lt) goto loc_825D21C8;
loc_825D20F0:
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// lwz r9,584(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 320);
	// lhz r8,168(r25)
	ctx.r8.u64 = PPC_LOAD_U16(r25.u32 + 168);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r7,148(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 148);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r28,r11,1456
	r28.s64 = r11.s64 + 1456;
	// bge cr6,0x825d21a0
	if (!cr6.lt) goto loc_825D21A0;
	// addi r29,r30,224
	r29.s64 = r30.s64 + 224;
loc_825D2134:
	// lhz r11,172(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 172);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r10,170(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 170);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r31,r11
	r31.s64 = r11.s16;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subfic r11,r31,32
	xer.ca = r31.u32 <= 32;
	r11.s64 = 32 - r31.s64;
	// lhz r10,148(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 148);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// slw r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// sraw r11,r9,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	r11.s64 = ctx.r9.s32 >> temp.u32;
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, r11.u32);
	// lhz r11,148(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,148(r30)
	PPC_STORE_U16(r30.u32 + 148, r11.u16);
	// lhz r11,168(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 168);
	// lhz r10,148(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 148);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d2134
	if (cr6.lt) goto loc_825D2134;
loc_825D21A0:
	// lhz r11,150(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 150);
	// sth r24,148(r30)
	PPC_STORE_U16(r30.u32 + 148, r24.u16);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r30)
	PPC_STORE_U16(r30.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d20f0
	if (cr6.lt) goto loc_825D20F0;
loc_825D21C8:
	// stw r19,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r19.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D21D0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d21fc
	if (!cr6.eq) goto loc_825D21FC;
	// stb r24,200(r25)
	PPC_STORE_U8(r25.u32 + 200, r24.u8);
	// b 0x825d2234
	goto loc_825D2234;
loc_825D21FC:
	// li r11,51
	r11.s64 = 51;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// b 0x825d2238
	goto loc_825D2238;
loc_825D2208:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d2254
	if (cr6.lt) goto loc_825D2254;
	// lhz r10,110(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x825d1120
	if (!cr6.lt) goto loc_825D1120;
	// stb r11,200(r25)
	PPC_STORE_U8(r25.u32 + 200, r11.u8);
loc_825D2234:
	// stw r20,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r20.u32);
loc_825D2238:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x825d1164
	if (!cr6.eq) goto loc_825D1164;
loc_825D2244:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2254
	if (!cr6.eq) goto loc_825D2254;
	// stw r23,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r23.u32);
loc_825D2254:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825D225C"))) PPC_WEAK_FUNC(sub_825D225C);
PPC_FUNC_IMPL(__imp__sub_825D225C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D2260"))) PPC_WEAK_FUNC(sub_825D2260);
PPC_FUNC_IMPL(__imp__sub_825D2260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r8,424(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 424);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// lwz r10,56(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// extsh r11,r8
	r11.s64 = ctx.r8.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// lhz r9,118(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 118);
	// lwz r11,72(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 72);
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d22d0
	if (cr6.eq) goto loc_825D22D0;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x825d22d8
	if (cr6.eq) goto loc_825D22D8;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D22D0:
	// li r11,10
	r11.s64 = 10;
	// stw r11,72(r28)
	PPC_STORE_U32(r28.u32 + 72, r11.u32);
loc_825D22D8:
	// lbz r11,200(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 200);
	// lhz r10,110(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 110);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d22fc
	if (cr6.lt) goto loc_825D22FC;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D22FC:
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x825d2394
	if (!cr6.lt) goto loc_825D2394;
	// addi r26,r28,224
	r26.s64 = r28.s64 + 224;
	// li r27,1
	r27.s64 = 1;
loc_825D2314:
	// lbz r11,200(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 200);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r10,110(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 110);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// subf r4,r11,r10
	ctx.r4.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d239c
	if (cr6.lt) goto loc_825D239C;
	// lbz r11,200(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 200);
	// lhz r10,110(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 110);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r10,r27,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r27.u32 << (r11.u8 & 0x3F));
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & r11.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825d236c
	if (cr6.eq) goto loc_825D236C;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// orc r11,r11,r10
	r11.u64 = r11.u64 | ~ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825D236C:
	// lhz r10,202(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r11.u32);
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r31)
	PPC_STORE_U16(r31.u32 + 202, r11.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825d2314
	if (cr6.lt) goto loc_825D2314;
loc_825D2394:
	// li r11,11
	r11.s64 = 11;
	// stw r11,72(r28)
	PPC_STORE_U32(r28.u32 + 72, r11.u32);
loc_825D239C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D23A4"))) PPC_WEAK_FUNC(sub_825D23A4);
PPC_FUNC_IMPL(__imp__sub_825D23A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D23A8"))) PPC_WEAK_FUNC(sub_825D23A8);
PPC_FUNC_IMPL(__imp__sub_825D23A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lwz r11,456(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 456);
	// lwz r10,452(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 452);
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// blt cr6,0x825d2444
	if (cr6.lt) goto loc_825D2444;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d240c
	if (cr6.gt) goto loc_825D240C;
	// lhz r9,118(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// add r28,r9,r11
	r28.u64 = ctx.r9.u64 + r11.u64;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d23fc
	if (cr6.gt) goto loc_825D23FC;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
loc_825D23FC:
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// b 0x825d2450
	goto loc_825D2450;
loc_825D240C:
	// lhz r10,118(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// mr r27,r11
	r27.u64 = r11.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// add r29,r9,r11
	r29.u64 = ctx.r9.u64 + r11.u64;
	// subf r11,r10,r29
	r11.s64 = r29.s64 - ctx.r10.s64;
	// stw r11,452(r4)
	PPC_STORE_U32(ctx.r4.u32 + 452, r11.u32);
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// ble cr6,0x825d2474
	if (!cr6.gt) goto loc_825D2474;
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// b 0x825d2474
	goto loc_825D2474;
loc_825D2444:
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_825D2450:
	// stw r11,452(r4)
	PPC_STORE_U32(ctx.r4.u32 + 452, r11.u32);
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// ble cr6,0x825d246c
	if (!cr6.gt) goto loc_825D246C;
	// lhz r11,118(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 118);
	// extsh r28,r11
	r28.s64 = r11.s16;
loc_825D246C:
	// mr r29,r28
	r29.u64 = r28.u64;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_825D2474:
	// lwz r11,452(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 452);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x825d2488
	if (!cr6.lt) goto loc_825D2488;
	// li r11,0
	r11.s64 = 0;
	// stw r11,452(r4)
	PPC_STORE_U32(ctx.r4.u32 + 452, r11.u32);
loc_825D2488:
	// lhz r11,182(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r24,r11,-1
	r24.s64 = r11.s64 + -1;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// blt cr6,0x825d2594
	if (cr6.lt) goto loc_825D2594;
	// mulli r11,r24,56
	r11.s64 = r24.s64 * 56;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r31,r11,200
	r31.s64 = r11.s64 + 200;
loc_825D24A8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825d24ec
	if (!cr6.gt) goto loc_825D24EC;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825dbb50
	sub_825DBB50(ctx, base);
	// lwz r11,212(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 212);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825dbb50
	sub_825DBB50(ctx, base);
loc_825D24EC:
	// cmpw cr6,r27,r28
	cr6.compare<int32_t>(r27.s32, r28.s32, xer);
	// ble cr6,0x825d2514
	if (!cr6.gt) goto loc_825D2514;
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,212(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 212);
	// subf r6,r28,r27
	ctx.r6.s64 = r27.s64 - r28.s64;
	// add r5,r11,r25
	ctx.r5.u64 = r11.u64 + r25.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825D2514:
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// ble cr6,0x825d255c
	if (!cr6.gt) goto loc_825D255C;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825dbb50
	sub_825DBB50(ctx, base);
	// rlwinm r11,r27,2,0,29
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,212(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 212);
	// subf r6,r27,r29
	ctx.r6.s64 = r29.s64 - r27.s64;
	// add r5,r11,r25
	ctx.r5.u64 = r11.u64 + r25.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825dbb50
	sub_825DBB50(ctx, base);
loc_825D255C:
	// cmpw cr6,r23,r29
	cr6.compare<int32_t>(r23.s32, r29.s32, xer);
	// ble cr6,0x825d2584
	if (!cr6.gt) goto loc_825D2584;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,212(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 212);
	// subf r6,r29,r23
	ctx.r6.s64 = r23.s64 - r29.s64;
	// add r5,r11,r25
	ctx.r5.u64 = r11.u64 + r25.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825D2584:
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// addi r31,r31,-56
	r31.s64 = r31.s64 + -56;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bge cr6,0x825d24a8
	if (!cr6.lt) goto loc_825D24A8;
loc_825D2594:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825D259C"))) PPC_WEAK_FUNC(sub_825D259C);
PPC_FUNC_IMPL(__imp__sub_825D259C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D25A0"))) PPC_WEAK_FUNC(sub_825D25A0);
PPC_FUNC_IMPL(__imp__sub_825D25A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// li r22,1
	r22.s64 = 1;
	// li r24,0
	r24.s64 = 0;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r10,424(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 424);
	// mr r30,r24
	r30.u64 = r24.u64;
	// lhz r23,728(r25)
	r23.u64 = PPC_LOAD_U16(r25.u32 + 728);
	// mr r27,r24
	r27.u64 = r24.u64;
	// lhz r8,118(r26)
	ctx.r8.u64 = PPC_LOAD_U16(r26.u32 + 118);
	// mr r28,r24
	r28.u64 = r24.u64;
	// extsh r6,r23
	ctx.r6.s64 = r23.s16;
	// lwz r9,256(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 256);
	// addi r11,r23,1
	r11.s64 = r23.s64 + 1;
	// lwz r7,56(r26)
	ctx.r7.u64 = PPC_LOAD_U32(r26.u32 + 56);
	// lwz r5,12(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// extsh r21,r8
	r21.s64 = ctx.r8.s16;
	// extsh r19,r11
	r19.s64 = r11.s16;
	// lhz r11,0(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// add r20,r8,r7
	r20.u64 = ctx.r8.u64 + ctx.r7.u64;
	// slw r18,r22,r6
	r18.u64 = ctx.r6.u8 & 0x20 ? 0 : (r22.u32 << (ctx.r6.u8 & 0x3F));
	// bge cr6,0x825d2b28
	if (!cr6.lt) goto loc_825D2B28;
	// rotlwi r11,r5,0
	r11.u64 = __builtin_rotateleft32(ctx.r5.u32, 0);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r21
	r11.u64 = r11.u64 + r21.u64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bgt cr6,0x825d2b28
	if (cr6.gt) goto loc_825D2B28;
	// lhz r11,182(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bgt cr6,0x825d2b28
	if (cr6.gt) goto loc_825D2B28;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x825d2b28
	if (cr6.lt) goto loc_825D2B28;
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmplwi cr6,r11,10
	cr6.compare<uint32_t>(r11.u32, 10, xer);
	// bgt cr6,0x825d2aec
	if (cr6.gt) goto loc_825D2AEC;
	// li r16,2
	r16.s64 = 2;
	// li r17,3
	r17.s64 = 3;
	// li r29,9
	r29.s64 = 9;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,9840
	r12.s64 = r12.s64 + 9840;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D269C;
	case 1:
		goto loc_825D2AEC;
	case 2:
		goto loc_825D26A0;
	case 3:
		goto loc_825D26E0;
	case 4:
		goto loc_825D2AEC;
	case 5:
		goto loc_825D2AEC;
	case 6:
		goto loc_825D2AEC;
	case 7:
		goto loc_825D2AEC;
	case 8:
		goto loc_825D2AEC;
	case 9:
		goto loc_825D272C;
	case 10:
		goto loc_825D2770;
	default:
		__builtin_unreachable();
	}
	// lwz r18,9884(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 9884);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,9888(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 9888);
	// lwz r18,9952(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 9952);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,10988(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10988);
	// lwz r18,10028(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10028);
	// lwz r18,10096(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10096);
loc_825D269C:
	// stw r16,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r16.u32);
loc_825D26A0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d26d4
	if (!cr6.eq) goto loc_825D26D4;
	// stw r17,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r17.u32);
	// stw r24,456(r26)
	PPC_STORE_U32(r26.u32 + 456, r24.u32);
	// b 0x825d26e0
	goto loc_825D26E0;
loc_825D26D4:
	// li r11,-1
	r11.s64 = -1;
	// stw r29,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r29.u32);
	// stw r11,456(r26)
	PPC_STORE_U32(r26.u32 + 456, r11.u32);
loc_825D26E0:
	// lwz r11,456(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 456);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x825d2728
	if (cr6.eq) goto loc_825D2728;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// cmplwi cr6,r21,1
	cr6.compare<uint32_t>(r21.u32, 1, xer);
	// ble cr6,0x825d2708
	if (!cr6.gt) goto loc_825D2708;
loc_825D26F8:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r11,r21,r4
	r11.u64 = ctx.r4.u8 & 0x20 ? 0 : (r21.u32 >> (ctx.r4.u8 & 0x3F));
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825d26f8
	if (cr6.gt) goto loc_825D26F8;
loc_825D2708:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,456(r26)
	PPC_STORE_U32(r26.u32 + 456, r11.u32);
loc_825D2728:
	// stw r29,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r29.u32);
loc_825D272C:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2760
	if (!cr6.eq) goto loc_825D2760;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,110(r25)
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r11,188(r26)
	PPC_STORE_U32(r26.u32 + 188, r11.u32);
loc_825D2760:
	// li r11,10
	r11.s64 = 10;
	// stw r24,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r24.u32);
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// sth r24,202(r25)
	PPC_STORE_U16(r25.u32 + 202, r24.u16);
loc_825D2770:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2884
	if (!cr6.eq) goto loc_825D2884;
	// lhz r11,202(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d2884
	if (!cr6.eq) goto loc_825D2884;
	// lwz r11,184(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 184);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2828
	if (!cr6.eq) goto loc_825D2828;
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d27a8
	if (cr6.lt) goto loc_825D27A8;
	// beq cr6,0x825d27d0
	if (cr6.eq) goto loc_825D27D0;
	// b 0x825d2878
	goto loc_825D2878;
loc_825D27A8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r22,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r22.u32);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
loc_825D27D0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,110(r25)
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lhz r11,110(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// lwz r9,84(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// slw r10,r22,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r22.u32 << (r11.u8 & 0x3F));
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & r11.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x825d2820
	if (cr6.eq) goto loc_825D2820;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// orc r11,r11,r10
	r11.u64 = r11.u64 | ~ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825D2820:
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// b 0x825d2878
	goto loc_825D2878;
loc_825D2828:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d2878
	if (!cr6.eq) goto loc_825D2878;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,110(r25)
	ctx.r4.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lhz r11,110(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 110);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r22,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r22.u32 << (r11.u8 & 0x3F));
	// and r9,r11,r10
	ctx.r9.u64 = r11.u64 & ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d2874
	if (cr6.eq) goto loc_825D2874;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// orc r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | ~r11.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_825D2874:
	// stw r10,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r10.u32);
loc_825D2878:
	// lhz r11,202(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r25)
	PPC_STORE_U16(r25.u32 + 202, r11.u16);
loc_825D2884:
	// lhz r11,202(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x825d2ae4
	if (!cr6.lt) goto loc_825D2AE4;
loc_825D2894:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x825d2a78
	if (cr6.gt) goto loc_825D2A78;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,10424
	r12.s64 = r12.s64 + 10424;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D28C8;
	case 1:
		goto loc_825D28F4;
	case 2:
		goto loc_825D2928;
	case 3:
		goto loc_825D29B8;
	default:
		__builtin_unreachable();
	}
	// lwz r18,10440(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10440);
	// lwz r18,10484(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10484);
	// lwz r18,10536(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10536);
	// lwz r18,10680(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 10680);
loc_825D28C8:
	// addi r29,r31,200
	r29.s64 = r31.s64 + 200;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x825cc490
	sub_825CC490(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// blt cr6,0x825d29b4
	if (cr6.lt) goto loc_825D29B4;
	// stw r22,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r22.u32);
loc_825D28F4:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x825d2b28
	if (cr6.gt) goto loc_825D2B28;
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stw r16,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r16.u32);
loc_825D2928:
	// lwz r4,204(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmplwi cr6,r4,24
	cr6.compare<uint32_t>(ctx.r4.u32, 24, xer);
	// bgt cr6,0x825d293c
	if (cr6.gt) goto loc_825D293C;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// b 0x825d2990
	goto loc_825D2990;
loc_825D293C:
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r4,r11,-24
	ctx.r4.s64 = r11.s64 + -24;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r4,24
	ctx.r4.s64 = 24;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r11,r11,24,0,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF000000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
loc_825D2990:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
loc_825D29B4:
	// stw r17,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r17.u32);
loc_825D29B8:
	// lwz r11,188(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 188);
	// extsh r10,r19
	ctx.r10.s64 = r19.s16;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r24.u32);
	// mr r28,r24
	r28.u64 = r24.u64;
	// add r11,r11,r18
	r11.u64 = r11.u64 + r18.u64;
	// srw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825d29f0
	if (!cr6.gt) goto loc_825D29F0;
loc_825D29D8:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// slw r10,r22,r28
	ctx.r10.u64 = r28.u8 & 0x20 ? 0 : (r22.u32 << (r28.u8 & 0x3F));
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x825d29d8
	if (cr6.lt) goto loc_825D29D8;
	// cmplwi cr6,r28,24
	cr6.compare<uint32_t>(r28.u32, 24, xer);
	// bgt cr6,0x825d2a14
	if (cr6.gt) goto loc_825D2A14;
loc_825D29F0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x825d2a78
	goto loc_825D2A78;
loc_825D2A14:
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r28,-24
	ctx.r4.s64 = r28.s64 + -24;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,24
	ctx.r4.s64 = 24;
	// rlwinm r29,r11,24,0,7
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFF000000;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d2b30
	if (cr6.lt) goto loc_825D2B30;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r27,r11,r29
	r27.u64 = r11.u64 + r29.u64;
loc_825D2A78:
	// lwz r10,188(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 188);
	// extsh r9,r23
	ctx.r9.s64 = r23.s16;
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// srw r9,r10,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r9.u8 & 0x3F));
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,188(r26)
	PPC_STORE_U32(r26.u32 + 188, ctx.r10.u32);
	// beq cr6,0x825d2ab0
	if (cr6.eq) goto loc_825D2AB0;
	// subfic r11,r11,-1
	xer.ca = r11.u32 <= 4294967295;
	r11.s64 = -1 - r11.s64;
loc_825D2AB0:
	// lhz r10,202(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r20
	PPC_STORE_U32(ctx.r10.u32 + r20.u32, r11.u32);
	// stw r24,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r24.u32);
	// stw r24,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r24.u32);
	// stw r24,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r24.u32);
	// lhz r11,202(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r25)
	PPC_STORE_U16(r25.u32 + 202, r11.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// blt cr6,0x825d2894
	if (cr6.lt) goto loc_825D2894;
loc_825D2AE4:
	// li r11,11
	r11.s64 = 11;
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
loc_825D2AEC:
	// lwz r11,192(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2b04
	if (!cr6.eq) goto loc_825D2B04;
	// lhz r11,118(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,452(r26)
	PPC_STORE_U32(r26.u32 + 452, r11.u32);
loc_825D2B04:
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d23a8
	sub_825D23A8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
loc_825D2B28:
	// lis r30,-32764
	r30.s64 = -2147221504;
	// ori r30,r30,2
	r30.u64 = r30.u64 | 2;
loc_825D2B30:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_825D2B3C"))) PPC_WEAK_FUNC(sub_825D2B3C);
PPC_FUNC_IMPL(__imp__sub_825D2B3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D2B40"))) PPC_WEAK_FUNC(sub_825D2B40);
PPC_FUNC_IMPL(__imp__sub_825D2B40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r23,r24
	r23.u64 = r24.u64;
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// lwz r31,0(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x825d32c8
	if (cr6.eq) goto loc_825D32C8;
	// li r20,3
	r20.s64 = 3;
	// li r18,6
	r18.s64 = 6;
	// li r21,7
	r21.s64 = 7;
	// li r22,1
	r22.s64 = 1;
	// li r19,8
	r19.s64 = 8;
loc_825D2B7C:
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bgt cr6,0x825d32a8
	if (cr6.gt) goto loc_825D32A8;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,11168
	r12.s64 = r12.s64 + 11168;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D2BC0;
	case 1:
		goto loc_825D32A8;
	case 2:
		goto loc_825D32A8;
	case 3:
		goto loc_825D2BFC;
	case 4:
		goto loc_825D32A8;
	case 5:
		goto loc_825D32A8;
	case 6:
		goto loc_825D2D90;
	case 7:
		goto loc_825D3120;
	default:
		__builtin_unreachable();
	}
	// lwz r18,11200(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 11200);
	// lwz r18,12968(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 12968);
	// lwz r18,12968(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 12968);
	// lwz r18,11260(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 11260);
	// lwz r18,12968(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 12968);
	// lwz r18,12968(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 12968);
	// lwz r18,11664(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 11664);
	// lwz r18,12576(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 12576);
loc_825D2BC0:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825d10f8
	sub_825D10F8(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// blt cr6,0x825d32c8
	if (cr6.lt) goto loc_825D32C8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// stw r20,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r20.u32);
	// sth r24,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r24.u16);
	// stw r24,72(r27)
	PPC_STORE_U32(r27.u32 + 72, r24.u32);
	// sth r24,148(r27)
	PPC_STORE_U16(r27.u32 + 148, r24.u16);
	// sth r24,202(r11)
	PPC_STORE_U16(r11.u32 + 202, r24.u16);
	// stw r24,76(r27)
	PPC_STORE_U32(r27.u32 + 76, r24.u32);
	// stw r24,200(r27)
	PPC_STORE_U32(r27.u32 + 200, r24.u32);
	// stw r24,208(r27)
	PPC_STORE_U32(r27.u32 + 208, r24.u32);
	// b 0x825d32a8
	goto loc_825D32A8;
loc_825D2BFC:
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d2cdc
	if (!cr6.eq) goto loc_825D2CDC;
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d32c0
	if (cr6.eq) goto loc_825D32C0;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,150(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d2cd0
	if (!cr6.lt) goto loc_825D2CD0;
loc_825D2C2C:
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d2c78
	if (cr6.eq) goto loc_825D2C78;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d25a0
	sub_825D25A0(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// blt cr6,0x825d32c8
	if (cr6.lt) goto loc_825D32C8;
loc_825D2C78:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lhz r10,118(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r9,202(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 202);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d32c0
	if (cr6.gt) goto loc_825D32C0;
	// stw r24,72(r27)
	PPC_STORE_U32(r27.u32 + 72, r24.u32);
	// sth r24,148(r27)
	PPC_STORE_U16(r27.u32 + 148, r24.u16);
	// sth r24,202(r11)
	PPC_STORE_U16(r11.u32 + 202, r24.u16);
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// stw r24,76(r27)
	PPC_STORE_U32(r27.u32 + 76, r24.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r24,200(r27)
	PPC_STORE_U32(r27.u32 + 200, r24.u32);
	// stw r24,208(r27)
	PPC_STORE_U32(r27.u32 + 208, r24.u32);
	// sth r11,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r11.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,150(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d2c2c
	if (cr6.lt) goto loc_825D2C2C;
loc_825D2CD0:
	// stw r18,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r18.u32);
	// sth r24,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r24.u16);
	// b 0x825d32a8
	goto loc_825D32A8;
loc_825D2CDC:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2d88
	if (!cr6.eq) goto loc_825D2D88;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,150(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d2d84
	if (!cr6.lt) goto loc_825D2D84;
loc_825D2CFC:
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x825d2260
	sub_825D2260(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// blt cr6,0x825d32c8
	if (cr6.lt) goto loc_825D32C8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lhz r10,118(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r9,202(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 202);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d32c0
	if (cr6.gt) goto loc_825D32C0;
	// stw r24,72(r27)
	PPC_STORE_U32(r27.u32 + 72, r24.u32);
	// sth r24,202(r11)
	PPC_STORE_U16(r11.u32 + 202, r24.u16);
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r11.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,150(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d2cfc
	if (cr6.lt) goto loc_825D2CFC;
loc_825D2D84:
	// stw r21,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r21.u32);
loc_825D2D88:
	// sth r24,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r24.u16);
	// b 0x825d32a8
	goto loc_825D32A8;
loc_825D2D90:
	// sth r24,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r24.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d2e2c
	if (!cr6.gt) goto loc_825D2E2C;
loc_825D2DA4:
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d2e08
	if (!cr6.eq) goto loc_825D2E08;
	// lwz r11,424(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 424);
	// li r4,0
	ctx.r4.s64 = 0;
	// lhz r9,118(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r24,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r24.u32);
loc_825D2E08:
	// lhz r11,150(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r27)
	PPC_STORE_U16(r27.u32 + 150, r11.u16);
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,150(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x825d2da4
	if (cr6.lt) goto loc_825D2DA4;
loc_825D2E2C:
	// lwz r11,656(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 656);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d2f94
	if (!cr6.eq) goto loc_825D2F94;
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lwz r30,364(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lwz r28,368(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 368);
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r25,r11
	r25.s64 = r11.s16;
	// bne cr6,0x825d32c0
	if (!cr6.eq) goto loc_825D32C0;
	// mr r29,r24
	r29.u64 = r24.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825d2f94
	if (!cr6.gt) goto loc_825D2F94;
	// addi r26,r31,664
	r26.s64 = r31.s64 + 664;
loc_825D2E84:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d2ef0
	if (!cr6.gt) goto loc_825D2EF0;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
loc_825D2E90:
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mulli r11,r9,1776
	r11.s64 = ctx.r9.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,424(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lwz r7,56(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lhz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r7
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	// stwx r8,r10,r30
	PPC_STORE_U32(ctx.r10.u32 + r30.u32, ctx.r8.u32);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d2ed8
	if (!cr6.eq) goto loc_825D2ED8;
	// stwx r24,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, r24.u32);
	// b 0x825d2edc
	goto loc_825D2EDC;
loc_825D2ED8:
	// stwx r22,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, r22.u32);
loc_825D2EDC:
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d2e90
	if (cr6.lt) goto loc_825D2E90;
loc_825D2EF0:
	// lwz r11,500(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 500);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,504(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 504);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d2f88
	if (!cr6.gt) goto loc_825D2F88;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D2F3C:
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mulli r10,r11,1776
	ctx.r10.s64 = r11.s64 * 1776;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// lwzx r9,r8,r30
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r30.u32);
	// lwz r8,424(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 424);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r7,56(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lhz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r10,r7
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, ctx.r9.u32);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d2f3c
	if (cr6.lt) goto loc_825D2F3C;
loc_825D2F88:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// cmpw cr6,r29,r25
	cr6.compare<int32_t>(r29.s32, r25.s32, xer);
	// blt cr6,0x825d2e84
	if (cr6.lt) goto loc_825D2E84;
loc_825D2F94:
	// lwz r11,184(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 184);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3058
	if (!cr6.eq) goto loc_825D3058;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x825d3058
	if (!cr6.eq) goto loc_825D3058;
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r11,2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mulli r11,r9,1776
	r11.s64 = ctx.r9.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mulli r9,r8,1776
	ctx.r9.s64 = ctx.r8.s64 * 1776;
	// lwz r7,424(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lhz r9,118(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lwz r7,424(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 424);
	// lwz r10,56(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r7,12(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x825d3058
	if (cr6.eq) goto loc_825D3058;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_825D3024:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwzx r7,r9,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stwx r8,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r8.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwzx r7,r9,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d3024
	if (!cr6.eq) goto loc_825D3024;
loc_825D3058:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x825d32c0
	if (!cr6.eq) goto loc_825D32C0;
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3080
	if (!cr6.eq) goto loc_825D3080;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cadc0
	sub_825CADC0(ctx, base);
loc_825D3080:
	// lhz r11,208(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 208);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x825d3094
	if (cr6.eq) goto loc_825D3094;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cad20
	sub_825CAD20(ctx, base);
loc_825D3094:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3118
	if (!cr6.eq) goto loc_825D3118;
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3118
	if (!cr6.eq) goto loc_825D3118;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d3118
	if (!cr6.gt) goto loc_825D3118;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_825D30C0:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r8,r30,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lhz r9,168(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 168);
	// lhzx r11,r8,r11
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r7,r11,1456
	ctx.r7.s64 = r11.s64 + 1456;
	// addi r5,r11,1616
	ctx.r5.s64 = r11.s64 + 1616;
	// lwz r6,56(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// lhz r10,118(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// bl 0x825cb110
	sub_825CB110(ctx, base);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d30c0
	if (cr6.lt) goto loc_825D30C0;
loc_825D3118:
	// stw r21,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r21.u32);
	// b 0x825d32a8
	goto loc_825D32A8;
loc_825D3120:
	// lbz r11,200(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 200);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d31c4
	if (cr6.eq) goto loc_825D31C4;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d31c4
	if (!cr6.gt) goto loc_825D31C4;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
loc_825D3140:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,320(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r7,424(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 424);
	// lhz r10,118(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// ble cr6,0x825d31ac
	if (!cr6.gt) goto loc_825D31AC;
loc_825D3188:
	// lbz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 200);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d3188
	if (!cr6.eq) goto loc_825D3188;
loc_825D31AC:
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// addi r11,r8,1
	r11.s64 = ctx.r8.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d3140
	if (cr6.lt) goto loc_825D3140;
loc_825D31C4:
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d3234
	if (!cr6.gt) goto loc_825D3234;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D31D8:
	// lwz r10,584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,320(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r10.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,424(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 424);
	// lhz r10,114(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 114);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,12(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lhzx r9,r8,r9
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r9.u32);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d31d8
	if (cr6.lt) goto loc_825D31D8;
loc_825D3234:
	// lwz r11,176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d32a4
	if (!cr6.eq) goto loc_825D32A4;
	// lhz r11,580(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d32a4
	if (!cr6.gt) goto loc_825D32A4;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D3254:
	// lwz r10,584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,320(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 320);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r8,356(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 356);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r10.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mulli r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 * 1776;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lwzx r7,r9,r8
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// lhz r10,118(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 118);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
	// lhz r10,580(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d3254
	if (cr6.lt) goto loc_825D3254;
loc_825D32A4:
	// stw r19,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r19.u32);
loc_825D32A8:
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bne cr6,0x825d2b7c
	if (!cr6.eq) goto loc_825D2B7C;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd20
	return;
loc_825D32C0:
	// lis r23,-32764
	r23.s64 = -2147221504;
	// ori r23,r23,2
	r23.u64 = r23.u64 | 2;
loc_825D32C8:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_825D32D4"))) PPC_WEAK_FUNC(sub_825D32D4);
PPC_FUNC_IMPL(__imp__sub_825D32D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D32D8"))) PPC_WEAK_FUNC(sub_825D32D8);
PPC_FUNC_IMPL(__imp__sub_825D32D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r11,204(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 204);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d33dc
	if (!cr6.eq) goto loc_825D33DC;
	// lwz r11,460(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 460);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d3318
	if (cr6.eq) goto loc_825D3318;
	// lwz r28,328(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 328);
	// b 0x825d331c
	goto loc_825D331C;
loc_825D3318:
	// lwz r28,56(r22)
	r28.u64 = PPC_LOAD_U32(r22.u32 + 56);
loc_825D331C:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// lwz r10,320(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// bge cr6,0x825d3764
	if (!cr6.lt) goto loc_825D3764;
	// addi r31,r31,224
	r31.s64 = r31.s64 + 224;
	// li r23,1
	r23.s64 = 1;
loc_825D3354:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,110(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 110);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lhz r11,110(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 110);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & r11.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d3394
	if (cr6.eq) goto loc_825D3394;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// orc r11,r11,r10
	r11.u64 = r11.u64 | ~ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825D3394:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r10,202(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfsx f0,r10,r28
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, temp.u32);
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r30)
	PPC_STORE_U16(r30.u32 + 202, r11.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// blt cr6,0x825d3354
	if (cr6.lt) goto loc_825D3354;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_825D33DC:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// lwz r10,320(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r25,r11
	r25.s64 = r11.s16;
	// lbz r11,145(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 145);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bge cr6,0x825d3764
	if (!cr6.lt) goto loc_825D3764;
	// addi r24,r22,1456
	r24.s64 = r22.s64 + 1456;
	// li r23,1
	r23.s64 = 1;
	// li r18,7
	r18.s64 = 7;
	// li r20,10
	r20.s64 = 10;
	// li r19,3
	r19.s64 = 3;
	// li r21,6
	r21.s64 = 6;
loc_825D3424:
	// lwz r11,460(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 460);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r11,145(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 145);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// mullw r11,r11,r25
	r11.s64 = int64_t(r11.s32) * int64_t(r25.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// beq cr6,0x825d3448
	if (cr6.eq) goto loc_825D3448;
	// lwz r10,328(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 328);
	// b 0x825d344c
	goto loc_825D344C;
loc_825D3448:
	// lwz r10,56(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 56);
loc_825D344C:
	// add r26,r11,r10
	r26.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// beq cr6,0x825d3470
	if (cr6.eq) goto loc_825D3470;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x825d3514
	if (cr6.eq) goto loc_825D3514;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x825d354c
	if (cr6.eq) goto loc_825D354C;
	// b 0x825d36c8
	goto loc_825D36C8;
loc_825D3470:
	// lwz r11,120(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3510
	if (!cr6.eq) goto loc_825D3510;
	// lwz r11,164(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3510
	if (!cr6.eq) goto loc_825D3510;
	// lhz r11,168(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 168);
	// lhz r10,148(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 148);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825d3510
	if (!cr6.lt) goto loc_825D3510;
	// addi r28,r31,224
	r28.s64 = r31.s64 + 224;
loc_825D34A4:
	// lhz r11,172(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 172);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r10,170(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 170);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r29,r11
	r29.s64 = r11.s16;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subfic r11,r29,32
	xer.ca = r29.u32 <= 32;
	r11.s64 = 32 - r29.s64;
	// lhz r10,148(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 148);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// slw r9,r9,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// sraw r11,r9,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	r11.s64 = ctx.r9.s32 >> temp.u32;
	// stwx r11,r10,r24
	PPC_STORE_U32(ctx.r10.u32 + r24.u32, r11.u32);
	// lhz r11,148(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 148);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,148(r31)
	PPC_STORE_U16(r31.u32 + 148, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,168(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 168);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d34a4
	if (cr6.lt) goto loc_825D34A4;
loc_825D3510:
	// stw r18,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r18.u32);
loc_825D3514:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lhz r10,110(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 110);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d375c
	if (cr6.gt) goto loc_825D375C;
	// sth r11,196(r31)
	PPC_STORE_U16(r31.u32 + 196, r11.u16);
	// stw r20,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r20.u32);
	// stw r27,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r27.u32);
	// sth r27,202(r30)
	PPC_STORE_U16(r30.u32 + 202, r27.u16);
loc_825D354C:
	// lwz r11,192(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d35f8
	if (!cr6.eq) goto loc_825D35F8;
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d35f8
	if (!cr6.eq) goto loc_825D35F8;
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x825d3578
	if (cr6.lt) goto loc_825D3578;
	// beq cr6,0x825d359c
	if (cr6.eq) goto loc_825D359C;
	// b 0x825d35ec
	goto loc_825D35EC;
loc_825D3578:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r23,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r23.u32);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
loc_825D359C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,110(r30)
	ctx.r4.u64 = PPC_LOAD_U16(r30.u32 + 110);
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lhz r11,110(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 110);
	// lwz r9,84(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// slw r10,r23,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r23.u32 << (r11.u8 & 0x3F));
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & r11.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x825d35e8
	if (cr6.eq) goto loc_825D35E8;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// orc r11,r11,r10
	r11.u64 = r11.u64 | ~ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825D35E8:
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_825D35EC:
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r30)
	PPC_STORE_U16(r30.u32 + 202, r11.u16);
loc_825D35F8:
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// bge cr6,0x825d36c8
	if (!cr6.lt) goto loc_825D36C8;
loc_825D3608:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d3620
	if (cr6.eq) goto loc_825D3620;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825d365c
	if (cr6.eq) goto loc_825D365C;
	// b 0x825d3680
	goto loc_825D3680;
loc_825D3620:
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
loc_825D3624:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3658
	if (!cr6.eq) goto loc_825D3658;
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
	// b 0x825d3624
	goto loc_825D3624;
loc_825D3658:
	// stw r19,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r19.u32);
loc_825D365C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lhz r4,196(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 196);
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3764
	if (cr6.lt) goto loc_825D3764;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r11.u32);
loc_825D3680:
	// lwz r8,200(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// lhz r11,196(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 196);
	// lhz r10,202(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// slw r11,r8,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwx r11,r9,r26
	PPC_STORE_U32(ctx.r9.u32 + r26.u32, r11.u32);
	// stw r27,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r27.u32);
	// stw r27,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r27.u32);
	// stw r27,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r27.u32);
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r30)
	PPC_STORE_U16(r30.u32 + 202, r11.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// blt cr6,0x825d3608
	if (cr6.lt) goto loc_825D3608;
loc_825D36C8:
	// lwz r11,192(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// bge cr6,0x825d3724
	if (!cr6.lt) goto loc_825D3724;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r11,r25
	ctx.r9.s64 = r25.s64 - r11.s64;
	// add r11,r10,r26
	r11.u64 = ctx.r10.u64 + r26.u64;
loc_825D36EC:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825d370c
	if (cr6.eq) goto loc_825D370C;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// b 0x825d3710
	goto loc_825D3710;
loc_825D370C:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
loc_825D3710:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d36ec
	if (!cr6.eq) goto loc_825D36EC;
loc_825D3724:
	// stw r21,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r21.u32);
	// sth r27,148(r31)
	PPC_STORE_U16(r31.u32 + 148, r27.u16);
	// sth r27,202(r30)
	PPC_STORE_U16(r30.u32 + 202, r27.u16);
	// lbz r11,145(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 145);
	// stw r27,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r27.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r27,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r27.u32);
	// stw r27,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r27.u32);
	// stb r11,145(r31)
	PPC_STORE_U8(r31.u32 + 145, r11.u8);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x825d3424
	if (cr6.lt) goto loc_825D3424;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_825D375C:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_825D3764:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_825D376C"))) PPC_WEAK_FUNC(sub_825D376C);
PPC_FUNC_IMPL(__imp__sub_825D376C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D3770"))) PPC_WEAK_FUNC(sub_825D3770);
PPC_FUNC_IMPL(__imp__sub_825D3770) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// rlwinm r24,r26,2,0,29
	r24.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r29,0(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lhz r11,182(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// blt cr6,0x825d37f0
	if (cr6.lt) goto loc_825D37F0;
	// mulli r11,r31,56
	r11.s64 = r31.s64 * 56;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// addi r28,r11,200
	r28.s64 = r11.s64 + 200;
loc_825D37C4:
	// lwz r11,212(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 212);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r28,r28,-56
	r28.s64 = r28.s64 + -56;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x825d37c4
	if (!cr6.lt) goto loc_825D37C4;
loc_825D37F0:
	// lwz r11,120(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3840
	if (!cr6.eq) goto loc_825D3840;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lhz r31,168(r29)
	r31.u64 = PPC_LOAD_U16(r29.u32 + 168);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lhz r5,174(r29)
	ctx.r5.u64 = PPC_LOAD_U16(r29.u32 + 174);
	// bl 0x825dbc10
	sub_825DBC10(ctx, base);
	// lwz r11,164(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d38b8
	if (!cr6.eq) goto loc_825D38B8;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// addi r7,r25,1456
	ctx.r7.s64 = r25.s64 + 1456;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// addi r5,r25,1616
	ctx.r5.s64 = r25.s64 + 1616;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cb110
	sub_825CB110(ctx, base);
	// b 0x825d38b8
	goto loc_825D38B8;
loc_825D3840:
	// lwz r11,192(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 192);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d386c
	if (cr6.eq) goto loc_825D386C;
	// lwz r11,632(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// lwz r9,468(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 468);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_825D386C:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// ble cr6,0x825d38ac
	if (!cr6.gt) goto loc_825D38AC;
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// addi r10,r26,-1
	ctx.r10.s64 = r26.s64 + -1;
loc_825D387C:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,632(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// srawi r9,r9,6
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 6;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d387c
	if (!cr6.eq) goto loc_825D387C;
loc_825D38AC:
	// add r11,r24,r30
	r11.u64 = r24.u64 + r30.u64;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r11,468(r25)
	PPC_STORE_U32(r25.u32 + 468, r11.u32);
loc_825D38B8:
	// srawi r10,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r10.s64 = r26.s32 >> 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d38f0
	if (!cr6.gt) goto loc_825D38F0;
	// add r11,r24,r30
	r11.u64 = r24.u64 + r30.u64;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
loc_825D38CC:
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// stw r9,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r9.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne cr6,0x825d38cc
	if (!cr6.eq) goto loc_825D38CC;
loc_825D38F0:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825D38FC"))) PPC_WEAK_FUNC(sub_825D38FC);
PPC_FUNC_IMPL(__imp__sub_825D38FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D3900"))) PPC_WEAK_FUNC(sub_825D3900);
PPC_FUNC_IMPL(__imp__sub_825D3900) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lhz r5,174(r31)
	ctx.r5.u64 = PPC_LOAD_U16(r31.u32 + 174);
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// bl 0x825dbc10
	sub_825DBC10(ctx, base);
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d395c
	if (!cr6.eq) goto loc_825D395C;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// lhz r9,168(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 168);
	// addi r7,r30,1456
	ctx.r7.s64 = r30.s64 + 1456;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r5,r30,1616
	ctx.r5.s64 = r30.s64 + 1616;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cb110
	sub_825CB110(ctx, base);
loc_825D395C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825D3968"))) PPC_WEAK_FUNC(sub_825D3968);
PPC_FUNC_IMPL(__imp__sub_825D3968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lhz r5,174(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 174);
	// bl 0x825dbc10
	sub_825DBC10(ctx, base);
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// ble cr6,0x825d39d4
	if (!cr6.gt) goto loc_825D39D4;
	// li r11,1
	r11.s64 = 1;
loc_825D399C:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,632(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// lwz r9,-4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// srawi r9,r9,6
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 6;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// blt cr6,0x825d399c
	if (cr6.lt) goto loc_825D399C;
loc_825D39D4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825D39E0"))) PPC_WEAK_FUNC(sub_825D39E0);
PPC_FUNC_IMPL(__imp__sub_825D39E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r11,584(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 584);
	// lwz r10,320(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// lwz r9,460(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 460);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// beq cr6,0x825d3a30
	if (cr6.eq) goto loc_825D3A30;
	// lwz r31,328(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 328);
	// b 0x825d3a34
	goto loc_825D3A34;
loc_825D3A30:
	// lwz r31,56(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 56);
loc_825D3A34:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d3a44
	if (cr6.eq) goto loc_825D3A44;
	// lwz r29,328(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 328);
	// b 0x825d3a48
	goto loc_825D3A48;
loc_825D3A44:
	// lwz r29,144(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 144);
loc_825D3A48:
	// lwz r11,204(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 204);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d3bd8
	if (!cr6.eq) goto loc_825D3BD8;
	// lwz r11,140(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 140);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3aac
	if (!cr6.eq) goto loc_825D3AAC;
	// lwz r11,148(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 148);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d3aac
	if (cr6.eq) goto loc_825D3AAC;
	// lwz r11,156(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d3aac
	if (cr6.eq) goto loc_825D3AAC;
	// lwz r11,20(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d3a94
	if (!cr6.eq) goto loc_825D3A94;
loc_825D3A84:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd48
	return;
loc_825D3A94:
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// bl 0x825d3770
	sub_825D3770(ctx, base);
	// b 0x825d3ae0
	goto loc_825D3AE0;
loc_825D3AAC:
	// lwz r11,192(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 192);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d3a84
	if (cr6.eq) goto loc_825D3A84;
	// lwz r11,120(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 120);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bne cr6,0x825d3adc
	if (!cr6.eq) goto loc_825D3ADC;
	// bl 0x825d3900
	sub_825D3900(ctx, base);
	// b 0x825d3ae0
	goto loc_825D3AE0;
loc_825D3ADC:
	// bl 0x825d3968
	sub_825D3968(ctx, base);
loc_825D3AE0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d3cb0
	if (cr6.lt) goto loc_825D3CB0;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// blt cr6,0x825d3b94
	if (cr6.lt) goto loc_825D3B94;
	// addi r11,r30,-4
	r11.s64 = r30.s64 + -4;
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// rlwinm r9,r11,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r29,4
	r11.s64 = r29.s64 + 4;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r7,r29,r31
	ctx.r7.s64 = r31.s64 - r29.s64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825D3B10:
	// lwz r6,-12(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lwzx r6,r7,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r6.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r6,-4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r6.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r6.u64);
	// lfd f0,104(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x825d3b10
	if (!cr6.eq) goto loc_825D3B10;
loc_825D3B94:
	// cmpw cr6,r8,r30
	cr6.compare<int32_t>(ctx.r8.s32, r30.s32, xer);
	// bge cr6,0x825d3bd8
	if (!cr6.lt) goto loc_825D3BD8;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r29,r31
	ctx.r9.s64 = r31.s64 - r29.s64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// subf r10,r8,r30
	ctx.r10.s64 = r30.s64 - ctx.r8.s64;
loc_825D3BAC:
	// lwzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d3bac
	if (!cr6.eq) goto loc_825D3BAC;
loc_825D3BD8:
	// lwz r11,448(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 448);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d3cb0
	if (!cr6.eq) goto loc_825D3CB0;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r10,1
	r11.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// lfs f0,-31696(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -31696);
	f0.f64 = double(temp.f32);
	// blt cr6,0x825d3c74
	if (cr6.lt) goto loc_825D3C74;
	// rlwinm r8,r11,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r10,-2
	ctx.r9.s64 = ctx.r10.s64 + -2;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
loc_825D3C20:
	// lfs f13,8(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,-4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-16(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -16, temp.u32);
	// stfs f13,-12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -12, temp.u32);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// bne cr6,0x825d3c20
	if (!cr6.eq) goto loc_825D3C20;
loc_825D3C74:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d3cb0
	if (cr6.lt) goto loc_825D3CB0;
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
loc_825D3C8C:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// bge cr6,0x825d3c8c
	if (!cr6.lt) goto loc_825D3C8C;
loc_825D3CB0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825D3CB8"))) PPC_WEAK_FUNC(sub_825D3CB8);
PPC_FUNC_IMPL(__imp__sub_825D3CB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,64(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 64);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// cmpwi cr6,r11,72
	cr6.compare<int32_t>(r11.s32, 72, xer);
	// blt cr6,0x825d3cd8
	if (cr6.lt) goto loc_825D3CD8;
	// li r11,71
	r11.s64 = 71;
loc_825D3CD8:
	// srawi r8,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r8.s64 = r11.s32 >> 2;
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	f0.f64 = double(temp.f32);
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r10,r10,-4648
	ctx.r10.s64 = ctx.r10.s64 + -4648;
	// lfsx f13,r11,r10
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// slw r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f12,-16(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fdivs f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D3D18"))) PPC_WEAK_FUNC(sub_825D3D18);
PPC_FUNC_IMPL(__imp__sub_825D3D18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// and r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 & r11.u64;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// and r9,r8,r11
	ctx.r9.u64 = ctx.r8.u64 & r11.u64;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// srw r11,r7,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 >> (ctx.r10.u8 & 0x3F));
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// beq cr6,0x825d3d64
	if (cr6.eq) goto loc_825D3D64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwzx r7,r10,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r10,r8,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// mullw r11,r7,r9
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// blr 
	return;
loc_825D3D64:
	// li r7,1
	ctx.r7.s64 = 1;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// slw r8,r7,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// twllei r8,0
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r11,r11,r8
	r11.s32 = r11.s32 / ctx.r8.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// andc r11,r8,r9
	r11.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// twlgei r11,-1
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D3DA0"))) PPC_WEAK_FUNC(sub_825D3DA0);
PPC_FUNC_IMPL(__imp__sub_825D3DA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// std r4,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r4.u64);
	// li r31,0
	r31.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// beq cr6,0x825d3e4c
	if (cr6.eq) goto loc_825D3E4C;
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// beq cr6,0x825d3e04
	if (cr6.eq) goto loc_825D3E04;
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// beq cr6,0x825d3de8
	if (cr6.eq) goto loc_825D3DE8;
	// lis r31,-32761
	r31.s64 = -2147024896;
	// ori r31,r31,87
	r31.u64 = r31.u64 | 87;
	// b 0x825d3e50
	goto loc_825D3E50;
loc_825D3DE8:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,-27588
	ctx.r3.s64 = r11.s64 + -27588;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r4,r11,8,0,23
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// b 0x825d3e50
	goto loc_825D3E50;
loc_825D3E04:
	// subfic r11,r6,24
	xer.ca = ctx.r6.u32 <= 24;
	r11.s64 = 24 - ctx.r6.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d3e2c
	if (cr6.lt) goto loc_825D3E2C;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r3,r10,-27588
	ctx.r3.s64 = ctx.r10.s64 + -27588;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// slw r4,r10,r11
	ctx.r4.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// b 0x825d3e50
	goto loc_825D3E50;
loc_825D3E2C:
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// neg r10,r11
	ctx.r10.s64 = -r11.s64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,-27588
	ctx.r3.s64 = r11.s64 + -27588;
	// sraw r4,r9,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r4.s64 = ctx.r9.s32 >> temp.u32;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// b 0x825d3e50
	goto loc_825D3E50;
loc_825D3E4C:
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
loc_825D3E50:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x825d3e60
	if (cr6.eq) goto loc_825D3E60;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_825D3E60:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D3E78"))) PPC_WEAK_FUNC(sub_825D3E78);
PPC_FUNC_IMPL(__imp__sub_825D3E78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r5,120
	ctx.r5.s64 = 120;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r9,-96
	ctx.r9.s64 = -6291456;
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r8,200
	ctx.r8.s64 = 200;
	// stw r9,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r9.u32);
	// li r9,500
	ctx.r9.s64 = 500;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// stw r9,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r9.u32);
	// li r9,2
	ctx.r9.s64 = 2;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// stw r10,104(r31)
	PPC_STORE_U32(r31.u32 + 104, ctx.r10.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// stw r11,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r11.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// stw r10,60(r31)
	PPC_STORE_U32(r31.u32 + 60, ctx.r10.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// stw r8,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r8.u32);
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// stw r9,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r9.u32);
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D3F2C"))) PPC_WEAK_FUNC(sub_825D3F2C);
PPC_FUNC_IMPL(__imp__sub_825D3F2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D3F30"))) PPC_WEAK_FUNC(sub_825D3F30);
PPC_FUNC_IMPL(__imp__sub_825D3F30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,6
	ctx.r9.s64 = 6;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_825D3F40:
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bdnz 0x825d3f40
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_825D3F40;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// std r10,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, ctx.r10.u64);
	// std r10,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.r10.u64);
	// stw r10,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r10.u32);
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// stw r10,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r10.u32);
	// stw r10,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r10.u32);
	// stw r10,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r10.u32);
	// stw r10,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D3F78"))) PPC_WEAK_FUNC(sub_825D3F78);
PPC_FUNC_IMPL(__imp__sub_825D3F78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r5,304
	ctx.r5.s64 = 304;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,120
	ctx.r5.s64 = 120;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stw r11,120(r31)
	PPC_STORE_U32(r31.u32 + 120, r11.u32);
	// lfs f0,2480(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	f0.f64 = double(temp.f32);
	// stw r11,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r11.u32);
	// stfs f0,128(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 128, temp.u32);
	// stw r11,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r11.u32);
	// stw r11,136(r31)
	PPC_STORE_U32(r31.u32 + 136, r11.u32);
	// stw r11,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r11.u32);
	// stw r11,144(r31)
	PPC_STORE_U32(r31.u32 + 144, r11.u32);
	// stw r11,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r11.u32);
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
	// stw r11,160(r31)
	PPC_STORE_U32(r31.u32 + 160, r11.u32);
	// stw r11,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r11.u32);
	// stw r11,168(r31)
	PPC_STORE_U32(r31.u32 + 168, r11.u32);
	// stw r11,172(r31)
	PPC_STORE_U32(r31.u32 + 172, r11.u32);
	// stw r11,176(r31)
	PPC_STORE_U32(r31.u32 + 176, r11.u32);
	// stw r11,180(r31)
	PPC_STORE_U32(r31.u32 + 180, r11.u32);
	// stw r11,184(r31)
	PPC_STORE_U32(r31.u32 + 184, r11.u32);
	// stw r11,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r11.u32);
	// stw r11,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r11.u32);
	// stw r11,196(r31)
	PPC_STORE_U32(r31.u32 + 196, r11.u32);
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stw r11,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r11.u32);
	// stw r11,212(r31)
	PPC_STORE_U32(r31.u32 + 212, r11.u32);
	// stw r11,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r11.u32);
	// stw r11,220(r31)
	PPC_STORE_U32(r31.u32 + 220, r11.u32);
	// stw r11,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r11.u32);
	// stw r11,228(r31)
	PPC_STORE_U32(r31.u32 + 228, r11.u32);
	// stw r11,236(r31)
	PPC_STORE_U32(r31.u32 + 236, r11.u32);
	// stw r11,232(r31)
	PPC_STORE_U32(r31.u32 + 232, r11.u32);
	// stw r11,240(r31)
	PPC_STORE_U32(r31.u32 + 240, r11.u32);
	// stw r11,244(r31)
	PPC_STORE_U32(r31.u32 + 244, r11.u32);
	// stw r11,248(r31)
	PPC_STORE_U32(r31.u32 + 248, r11.u32);
	// stw r11,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r11.u32);
	// stw r11,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r11.u32);
	// stw r11,260(r31)
	PPC_STORE_U32(r31.u32 + 260, r11.u32);
	// stw r11,264(r31)
	PPC_STORE_U32(r31.u32 + 264, r11.u32);
	// stw r11,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r11.u32);
	// stw r11,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r11.u32);
	// stw r11,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r11.u32);
	// stw r11,280(r31)
	PPC_STORE_U32(r31.u32 + 280, r11.u32);
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// stw r11,288(r31)
	PPC_STORE_U32(r31.u32 + 288, r11.u32);
	// stw r11,292(r31)
	PPC_STORE_U32(r31.u32 + 292, r11.u32);
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D407C"))) PPC_WEAK_FUNC(sub_825D407C);
PPC_FUNC_IMPL(__imp__sub_825D407C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D4080"))) PPC_WEAK_FUNC(sub_825D4080);
PPC_FUNC_IMPL(__imp__sub_825D4080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x825d41b8
	if (cr6.eq) goto loc_825D41B8;
	// lwz r3,192(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d40a8
	if (cr6.eq) goto loc_825D40A8;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D40A8:
	// lwz r3,196(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 196);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d40b8
	if (cr6.eq) goto loc_825D40B8;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D40B8:
	// lwz r3,296(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d40c8
	if (cr6.eq) goto loc_825D40C8;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D40C8:
	// lwz r11,256(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 256);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d4130
	if (!cr6.gt) goto loc_825D4130;
	// li r31,0
	r31.s64 = 0;
loc_825D40DC:
	// lwz r11,268(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 268);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d40fc
	if (cr6.eq) goto loc_825D40FC;
	// lwzx r10,r31,r11
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d40fc
	if (cr6.eq) goto loc_825D40FC;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D40FC:
	// lwz r11,272(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 272);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d411c
	if (cr6.eq) goto loc_825D411C;
	// lwzx r10,r31,r11
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d411c
	if (cr6.eq) goto loc_825D411C;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D411C:
	// lwz r11,256(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 256);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x825d40dc
	if (cr6.lt) goto loc_825D40DC;
loc_825D4130:
	// lwz r3,268(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 268);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4140
	if (cr6.eq) goto loc_825D4140;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4140:
	// lwz r3,272(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 272);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4150
	if (cr6.eq) goto loc_825D4150;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4150:
	// lwz r3,260(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 260);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4160
	if (cr6.eq) goto loc_825D4160;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4160:
	// lwz r3,264(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 264);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4170
	if (cr6.eq) goto loc_825D4170;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4170:
	// lwz r3,280(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 280);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4180
	if (cr6.eq) goto loc_825D4180;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4180:
	// lwz r3,48(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d4190
	if (cr6.eq) goto loc_825D4190;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D4190:
	// lwz r3,288(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 288);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d41a0
	if (cr6.eq) goto loc_825D41A0;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D41A0:
	// lwz r3,292(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 292);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d41b0
	if (cr6.eq) goto loc_825D41B0;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D41B0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825d3f78
	sub_825D3F78(ctx, base);
loc_825D41B8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825D41C0"))) PPC_WEAK_FUNC(sub_825D41C0);
PPC_FUNC_IMPL(__imp__sub_825D41C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	// lwz r9,52(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x825d4240
	if (!cr6.gt) goto loc_825D4240;
	// lis r6,-32244
	ctx.r6.s64 = -2113142784;
	// lfd f0,-23112(r6)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r6.u32 + -23112);
loc_825D41E8:
	// lfd f13,0(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// fmul f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lwz r6,-16(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpw cr6,r4,r6
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r6.s32, xer);
	// blt cr6,0x825d4228
	if (cr6.lt) goto loc_825D4228;
	// lfd f13,24(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// addi r6,r1,-12
	ctx.r6.s64 = ctx.r1.s64 + -12;
	// fmul f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// lwz r6,-12(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// cmpw cr6,r4,r6
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r6.s32, xer);
	// ble cr6,0x825d4248
	if (!cr6.gt) goto loc_825D4248;
loc_825D4228:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x825d41e8
	if (cr6.lt) goto loc_825D41E8;
loc_825D4240:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825D4248:
	// lfd f13,0(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 * f0.f64;
	// addi r10,r1,-12
	ctx.r10.s64 = ctx.r1.s64 + -12;
	// lfd f12,0(r8)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// fmul f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 * f0.f64;
	// lfd f11,0(r9)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// lfd f0,-23120(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -23120);
	// fmul f0,f11,f0
	f0.f64 = ctx.f11.f64 * f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// addi r10,r1,-12
	ctx.r10.s64 = ctx.r1.s64 + -12;
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r10,-12(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// addi r9,r1,-12
	ctx.r9.s64 = ctx.r1.s64 + -12;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// mulld r11,r11,r10
	r11.s64 = r11.s64 * ctx.r10.s64;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r10,-12(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D42B8"))) PPC_WEAK_FUNC(sub_825D42B8);
PPC_FUNC_IMPL(__imp__sub_825D42B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// lwz r11,36(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4378
	if (cr6.eq) goto loc_825D4378;
	// lwz r10,8(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4360
	if (cr6.eq) goto loc_825D4360;
	// lwz r11,48(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d4300
	if (!cr6.eq) goto loc_825D4300;
	// lwz r11,208(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// bgt cr6,0x825d4398
	if (cr6.gt) goto loc_825D4398;
	// lwz r11,200(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 200);
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D4300:
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x825d41c0
	sub_825D41C0(ctx, base);
	// lwz r11,208(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bgt cr6,0x825d4324
	if (cr6.gt) goto loc_825D4324;
	// lwz r11,200(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 200);
	// subf r10,r4,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r4.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D4324:
	// lwz r11,208(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// lwz r10,204(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 204);
	// subf r8,r11,r3
	ctx.r8.s64 = ctx.r3.s64 - r11.s64;
	// lwz r9,200(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 200);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// mulld r10,r8,r10
	ctx.r10.s64 = ctx.r8.s64 * ctx.r10.s64;
	// sradi r10,r10,20
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 20;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// subf r10,r4,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r4.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D4360:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4378
	if (cr6.eq) goto loc_825D4378;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x825d41c0
	sub_825D41C0(ctx, base);
	// subf r11,r4,r3
	r11.s64 = ctx.r3.s64 - ctx.r4.s64;
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D4378:
	// lwz r11,8(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d43cc
	if (cr6.eq) goto loc_825D43CC;
	// lwz r11,208(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// bgt cr6,0x825d4398
	if (cr6.gt) goto loc_825D4398;
	// lwz r11,200(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 200);
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D4398:
	// lwz r11,208(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// lwz r10,204(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 204);
	// subf r8,r11,r4
	ctx.r8.s64 = ctx.r4.s64 - r11.s64;
	// lwz r9,200(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 200);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// mulld r10,r8,r10
	ctx.r10.s64 = ctx.r8.s64 * ctx.r10.s64;
	// sradi r10,r10,20
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 20;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x825d43d0
	goto loc_825D43D0;
loc_825D43CC:
	// li r11,0
	r11.s64 = 0;
loc_825D43D0:
	// lis r10,-1024
	ctx.r10.s64 = -67108864;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d43e8
	if (!cr6.lt) goto loc_825D43E8;
	// lis r4,-1024
	ctx.r4.s64 = -67108864;
	// b 0x825d43fc
	goto loc_825D43FC;
loc_825D43E8:
	// lis r10,1023
	ctx.r10.s64 = 67043328;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825d43fc
	if (!cr6.gt) goto loc_825D43FC;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
loc_825D43FC:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r3,r11,-23452
	ctx.r3.s64 = r11.s64 + -23452;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D4418"))) PPC_WEAK_FUNC(sub_825D4418);
PPC_FUNC_IMPL(__imp__sub_825D4418) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r26,r25
	r26.u64 = r25.u64;
	// mr r23,r25
	r23.u64 = r25.u64;
	// lwz r11,108(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 108);
	// mr r31,r25
	r31.u64 = r25.u64;
	// mr r24,r25
	r24.u64 = r25.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4618
	if (cr6.eq) goto loc_825D4618;
	// li r3,4100
	ctx.r3.s64 = 4100;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x825d446c
	if (!cr6.eq) goto loc_825D446C;
loc_825D4460:
	// lis r25,-32761
	r25.s64 = -2147024896;
	// ori r25,r25,14
	r25.u64 = r25.u64 | 14;
	// b 0x825d4618
	goto loc_825D4618;
loc_825D446C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// mr r23,r26
	r23.u64 = r26.u64;
	// mr r31,r25
	r31.u64 = r25.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// addi r28,r11,-27588
	r28.s64 = r11.s64 + -27588;
	// lis r27,128
	r27.s64 = 8388608;
loc_825D4484:
	// rlwinm r4,r31,13,0,18
	ctx.r4.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 13) & 0xFFFFE000;
	// cmpw cr6,r4,r27
	cr6.compare<int32_t>(ctx.r4.s32, r27.s32, xer);
	// bne cr6,0x825d4498
	if (!cr6.eq) goto loc_825D4498;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x825d44a4
	goto loc_825D44A4;
loc_825D4498:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_825D44A4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825d42b8
	sub_825D42B8(ctx, base);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpwi cr6,r31,1024
	cr6.compare<int32_t>(r31.s32, 1024, xer);
	// ble cr6,0x825d4484
	if (!cr6.gt) goto loc_825D4484;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lis r10,320
	ctx.r10.s64 = 20971520;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d44d8
	if (cr6.lt) goto loc_825D44D8;
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_825D44D8:
	// li r3,4100
	ctx.r3.s64 = 4100;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825d4460
	if (cr6.eq) goto loc_825D4460;
	// mr r24,r31
	r24.u64 = r31.u64;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// li r6,1024
	ctx.r6.s64 = 1024;
loc_825D44FC:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// subf. r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt 0x825d4510
	if (cr0.gt) goto loc_825D4510;
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
loc_825D4510:
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// ble cr6,0x825d451c
	if (!cr6.gt) goto loc_825D451C;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
loc_825D451C:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825d44fc
	if (!cr6.eq) goto loc_825D44FC;
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// bgt cr6,0x825d4538
	if (cr6.gt) goto loc_825D4538;
	// li r7,2
	ctx.r7.s64 = 2;
loc_825D4538:
	// addi r10,r7,-1
	ctx.r10.s64 = ctx.r7.s64 + -1;
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825d4558
	if (!cr6.gt) goto loc_825D4558;
loc_825D4548:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d4548
	if (cr6.gt) goto loc_825D4548;
loc_825D4558:
	// lwz r10,296(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// subfic r11,r11,29
	xer.ca = r11.u32 <= 29;
	r11.s64 = 29 - r11.s64;
	// rlwinm r8,r22,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, r11.u32);
	// lwz r11,296(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// lwzx r11,r8,r11
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d4584
	if (!cr6.gt) goto loc_825D4584;
	// lwz r11,296(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// lwzx r9,r8,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// b 0x825d4588
	goto loc_825D4588;
loc_825D4584:
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
loc_825D4588:
	// lwz r7,296(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// mr r11,r26
	r11.u64 = r26.u64;
	// subf r10,r26,r31
	ctx.r10.s64 = r31.s64 - r26.s64;
	// li r6,1024
	ctx.r6.s64 = 1024;
	// stwx r9,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r9.u32);
loc_825D459C:
	// lwz r9,296(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// lwzx r9,r8,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// slw r9,r7,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// srawi r9,r9,13
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1FFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 13;
	// stwx r9,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r5,296(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// lwzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// rlwinm r3,r9,13,0,18
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 13) & 0xFFFFE000;
	// lwzx r5,r8,r5
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r5.u32);
	// subf r9,r9,r3
	ctx.r9.s64 = ctx.r3.s64 - ctx.r9.s64;
	// sraw r9,r9,r5
	temp.u32 = ctx.r5.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r9.s64 = ctx.r9.s32 >> temp.u32;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// ble cr6,0x825d45f8
	if (!cr6.gt) goto loc_825D45F8;
	// cmpw cr6,r9,r4
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r4.s32, xer);
	// ble cr6,0x825d4604
	if (!cr6.gt) goto loc_825D4604;
	// b 0x825d4600
	goto loc_825D4600;
loc_825D45F8:
	// cmpw cr6,r9,r4
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r4.s32, xer);
	// bge cr6,0x825d4604
	if (!cr6.lt) goto loc_825D4604;
loc_825D4600:
	// stwx r25,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r25.u32);
loc_825D4604:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825d459c
	if (!cr6.eq) goto loc_825D459C;
	// stw r25,4096(r31)
	PPC_STORE_U32(r31.u32 + 4096, r25.u32);
loc_825D4618:
	// lwz r10,268(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 268);
	// rlwinm r11,r22,2,0,29
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stwx r26,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r26.u32);
	// lwz r10,260(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 260);
	// stwx r23,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r23.u32);
	// lwz r10,272(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 272);
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// lwz r10,264(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 264);
	// stwx r24,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r24.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825D4648"))) PPC_WEAK_FUNC(sub_825D4648);
PPC_FUNC_IMPL(__imp__sub_825D4648) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r6,-32126
	ctx.r6.s64 = -2105409536;
	// lwz r11,-25104(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + -25104);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d46a4
	if (!cr6.eq) goto loc_825D46A4;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r11,13824
	ctx.r7.s64 = r11.s64 + 13824;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_825D4668:
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// addi r5,r7,1024
	ctx.r5.s64 = ctx.r7.s64 + 1024;
	// extsb r4,r9
	ctx.r4.s64 = ctx.r9.s8;
	// clrlwi r8,r9,27
	ctx.r8.u64 = ctx.r9.u32 & 0x1F;
	// srawi r9,r4,5
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1F) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 5;
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// addi r9,r9,15
	ctx.r9.s64 = ctx.r9.s64 + 15;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// slw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// blt cr6,0x825d4668
	if (cr6.lt) goto loc_825D4668;
	// li r11,1
	r11.s64 = 1;
	// stw r11,-25104(r6)
	PPC_STORE_U32(ctx.r6.u32 + -25104, r11.u32);
loc_825D46A4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D46AC"))) PPC_WEAK_FUNC(sub_825D46AC);
PPC_FUNC_IMPL(__imp__sub_825D46AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D46B0"))) PPC_WEAK_FUNC(sub_825D46B0);
PPC_FUNC_IMPL(__imp__sub_825D46B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, f31.u64);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r18,0
	r18.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r16,r18
	r16.u64 = r18.u64;
	// bl 0x825d4080
	sub_825D4080(ctx, base);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825d4928
	if (cr6.eq) goto loc_825D4928;
	// lwz r11,24(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 24);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825d4708
	if (!cr6.eq) goto loc_825D4708;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4928
	if (cr6.eq) goto loc_825D4928;
	// lwz r17,28(r25)
	r17.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// lwz r24,40(r25)
	r24.u64 = PPC_LOAD_U32(r25.u32 + 40);
	// lwz r10,32(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 32);
	// b 0x825d471c
	goto loc_825D471C;
loc_825D4708:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d4928
	if (cr6.eq) goto loc_825D4928;
	// lwz r17,4(r30)
	r17.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lhz r24,18(r30)
	r24.u64 = PPC_LOAD_U16(r30.u32 + 18);
	// lhz r10,2(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 2);
loc_825D471C:
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x825d4928
	if (!cr6.gt) goto loc_825D4928;
	// subfic r11,r24,24
	xer.ca = r24.u32 <= 24;
	r11.s64 = 24 - r24.s64;
	// li r19,1
	r19.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r11.u32);
	// blt cr6,0x825d4754
	if (cr6.lt) goto loc_825D4754;
	// slw r11,r19,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r19.u32 << (r11.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// b 0x825d477c
	goto loc_825D477C;
loc_825D4754:
	// neg r11,r11
	r11.s64 = -r11.s64;
	// slw r11,r19,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r19.u32 << (r11.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// fdivs f0,f0,f13
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
loc_825D477C:
	// stfs f0,128(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 128, temp.u32);
	// lwz r11,44(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 44);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// stw r10,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r10.u32);
	// stw r11,284(r31)
	PPC_STORE_U32(r31.u32 + 284, r11.u32);
	// bne cr6,0x825d47a0
	if (!cr6.eq) goto loc_825D47A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d3e78
	sub_825D3E78(ctx, base);
	// b 0x825d49d8
	goto loc_825D49D8;
loc_825D47A0:
	// lwz r11,60(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 60);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// lwz r11,64(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 64);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// lwz r11,80(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// lwz r11,92(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// li r5,120
	ctx.r5.s64 = 120;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// stw r18,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r18.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d4808
	if (!cr6.eq) goto loc_825D4808;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4968
	if (cr6.eq) goto loc_825D4968;
loc_825D4808:
	// lwz r10,48(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d4928
	if (cr6.eq) goto loc_825D4928;
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// blt cr6,0x825d4928
	if (cr6.lt) goto loc_825D4928;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lfd f13,0(r10)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// lfd f0,-23104(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -23104);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x825d4928
	if (!cr6.eq) goto loc_825D4928;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lfd f0,-16(r10)
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -16);
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lfd f31,-31368(r10)
	f31.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31368);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bne cr6,0x825d4928
	if (!cr6.eq) goto loc_825D4928;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// bne cr6,0x825d4884
	if (!cr6.eq) goto loc_825D4884;
loc_825D486C:
	// lis r16,-32761
	r16.s64 = -2147024896;
	// ori r16,r16,14
	r16.u64 = r16.u64 | 14;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8239bd18
	return;
loc_825D4884:
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d4968
	if (!cr6.gt) goto loc_825D4968;
	// mr r11,r18
	r11.u64 = r18.u64;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_825D48A0:
	// lwz r9,48(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// lwz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lfdx f0,r11,r9
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + ctx.r9.u32);
	// stfdx f0,r8,r10
	PPC_STORE_U64(ctx.r8.u32 + ctx.r10.u32, f0.u64);
	// lwz r8,48(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lfd f0,8(r8)
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 8);
	// stfd f0,8(r9)
	PPC_STORE_U64(ctx.r9.u32 + 8, f0.u64);
	// lwz r9,52(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// beq cr6,0x825d4940
	if (cr6.eq) goto loc_825D4940;
	// lwz r7,48(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// lwz r9,48(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// add r5,r7,r10
	ctx.r5.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// add r4,r8,r9
	ctx.r4.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lfdx f0,r8,r9
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r9.u32);
	// lfd f12,8(r7)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
	// lfd f13,8(r4)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r4.u32 + 8);
	// lfd f11,0(r7)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// fsub f13,f13,f12
	ctx.f13.f64 = ctx.f13.f64 - ctx.f12.f64;
	// fsub f0,f0,f11
	f0.f64 = f0.f64 - ctx.f11.f64;
	// fdiv f0,f13,f0
	f0.f64 = ctx.f13.f64 / f0.f64;
	// stfd f0,16(r5)
	PPC_STORE_U64(ctx.r5.u32 + 16, f0.u64);
	// lwz r9,48(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// lfdx f0,r8,r9
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r9.u32);
	// lfdx f13,r11,r9
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + ctx.r9.u32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x825d494c
	if (!cr6.lt) goto loc_825D494C;
loc_825D4928:
	// lis r16,-32761
	r16.s64 = -2147024896;
	// ori r16,r16,87
	r16.u64 = r16.u64 | 87;
loc_825D4930:
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8239bd18
	return;
loc_825D4940:
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stfd f31,16(r9)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r9.u32 + 16, f31.u64);
loc_825D494C:
	// lwz r9,52(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d48a0
	if (cr6.lt) goto loc_825D48A0;
loc_825D4968:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4988
	if (cr6.eq) goto loc_825D4988;
	// li r11,100
	r11.s64 = 100;
	// stw r19,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r19.u32);
	// li r10,500
	ctx.r10.s64 = 500;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
	// stw r10,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r10.u32);
loc_825D4988:
	// lwz r11,36(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d49b0
	if (!cr6.eq) goto loc_825D49B0;
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d49b0
	if (!cr6.eq) goto loc_825D49B0;
	// lwz r11,44(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 44);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// beq cr6,0x825d49b4
	if (cr6.eq) goto loc_825D49B4;
loc_825D49B0:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_825D49B4:
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d49c8
	if (cr6.eq) goto loc_825D49C8;
	// stw r19,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r19.u32);
loc_825D49C8:
	// lwz r11,104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d49d8
	if (!cr6.eq) goto loc_825D49D8;
	// stw r18,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r18.u32);
loc_825D49D8:
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4a00
	if (cr6.eq) goto loc_825D4A00;
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d4a00
	if (cr6.eq) goto loc_825D4A00;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x825d4a00
	if (cr6.eq) goto loc_825D4A00;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825d4928
	if (!cr6.eq) goto loc_825D4928;
loc_825D4A00:
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4a18
	if (cr6.eq) goto loc_825D4A18;
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d4928
	if (!cr6.eq) goto loc_825D4928;
loc_825D4A18:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,288(r31)
	PPC_STORE_U32(r31.u32 + 288, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,292(r31)
	PPC_STORE_U32(r31.u32 + 292, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lis r10,4194
	ctx.r10.s64 = 274857984;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// ori r20,r10,19923
	r20.u64 = ctx.r10.u64 | 19923;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// addi r29,r10,-23156
	r29.s64 = ctx.r10.s64 + -23156;
	// beq cr6,0x825d4a90
	if (cr6.eq) goto loc_825D4A90;
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// mulhw r11,r11,r20
	r11.s64 = (int64_t(r11.s32) * int64_t(r20.s32)) >> 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// lis r30,16384
	r30.s64 = 1073741824;
	// subf r11,r3,r30
	r11.s64 = r30.s64 - ctx.r3.s64;
	// stw r11,176(r31)
	PPC_STORE_U32(r31.u32 + 176, r11.u32);
	// b 0x825d4a98
	goto loc_825D4A98;
loc_825D4A90:
	// lis r30,16384
	r30.s64 = 1073741824;
	// stw r30,176(r31)
	PPC_STORE_U32(r31.u32 + 176, r30.u32);
loc_825D4A98:
	// lwz r27,176(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// subf r10,r27,r30
	ctx.r10.s64 = r30.s64 - r27.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,184(r31)
	PPC_STORE_U32(r31.u32 + 184, ctx.r10.u32);
	// beq cr6,0x825d4ad8
	if (cr6.eq) goto loc_825D4AD8;
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// mulhw r11,r11,r20
	r11.s64 = (int64_t(r11.s32) * int64_t(r20.s32)) >> 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// subf r11,r3,r30
	r11.s64 = r30.s64 - ctx.r3.s64;
	// stw r11,180(r31)
	PPC_STORE_U32(r31.u32 + 180, r11.u32);
	// b 0x825d4adc
	goto loc_825D4ADC;
loc_825D4AD8:
	// stw r30,180(r31)
	PPC_STORE_U32(r31.u32 + 180, r30.u32);
loc_825D4ADC:
	// lwz r28,180(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// subf r10,r28,r30
	ctx.r10.s64 = r30.s64 - r28.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,188(r31)
	PPC_STORE_U32(r31.u32 + 188, ctx.r10.u32);
	// beq cr6,0x825d4b1c
	if (cr6.eq) goto loc_825D4B1C;
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// mulhw r11,r11,r20
	r11.s64 = (int64_t(r11.s32) * int64_t(r20.s32)) >> 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// subf r11,r3,r30
	r11.s64 = r30.s64 - ctx.r3.s64;
	// stw r11,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r11.u32);
	// b 0x825d4b20
	goto loc_825D4B20;
loc_825D4B1C:
	// stw r30,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r30.u32);
loc_825D4B20:
	// lwz r10,148(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// subf r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,156(r31)
	PPC_STORE_U32(r31.u32 + 156, ctx.r10.u32);
	// beq cr6,0x825d4b60
	if (cr6.eq) goto loc_825D4B60;
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// mulhw r11,r11,r20
	r11.s64 = (int64_t(r11.s32) * int64_t(r20.s32)) >> 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// subf r11,r3,r30
	r11.s64 = r30.s64 - ctx.r3.s64;
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// b 0x825d4b64
	goto loc_825D4B64;
loc_825D4B60:
	// stw r30,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r30.u32);
loc_825D4B64:
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// divw r10,r30,r28
	ctx.r10.s32 = r30.s32 / r28.s32;
	// divw r9,r30,r27
	ctx.r9.s32 = r30.s32 / r27.s32;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// twllei r28,0
	// twllei r27,0
	// stw r10,168(r31)
	PPC_STORE_U32(r31.u32 + 168, ctx.r10.u32);
	// stw r9,164(r31)
	PPC_STORE_U32(r31.u32 + 164, ctx.r9.u32);
	// stw r11,160(r31)
	PPC_STORE_U32(r31.u32 + 160, r11.u32);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825d4ba4
	if (!cr6.eq) goto loc_825D4BA4;
	// lwz r11,16(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d4ba4
	if (cr6.gt) goto loc_825D4BA4;
	// stw r18,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r18.u32);
loc_825D4BA4:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825d4bc4
	if (!cr6.eq) goto loc_825D4BC4;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d4bc4
	if (cr6.gt) goto loc_825D4BC4;
	// stw r19,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r19.u32);
	// stw r18,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r18.u32);
loc_825D4BC4:
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825d4bec
	if (!cr6.eq) goto loc_825D4BEC;
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d4bec
	if (cr6.gt) goto loc_825D4BEC;
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// stw r11,4(r26)
	PPC_STORE_U32(r26.u32 + 4, r11.u32);
	// ld r11,16(r25)
	r11.u64 = PPC_LOAD_U64(r25.u32 + 16);
	// std r11,16(r26)
	PPC_STORE_U64(r26.u32 + 16, r11.u64);
loc_825D4BEC:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825d4c14
	if (!cr6.eq) goto loc_825D4C14;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d4c14
	if (cr6.gt) goto loc_825D4C14;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// ld r11,8(r25)
	r11.u64 = PPC_LOAD_U64(r25.u32 + 8);
	// std r11,24(r26)
	PPC_STORE_U64(r26.u32 + 24, r11.u64);
loc_825D4C14:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4cc8
	if (cr6.eq) goto loc_825D4CC8;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r5,4(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// ld r4,16(r25)
	ctx.r4.u64 = PPC_LOAD_U64(r25.u32 + 16);
	// bl 0x825d3da0
	sub_825D3DA0(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r5,0(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// ld r4,8(r25)
	ctx.r4.u64 = PPC_LOAD_U64(r25.u32 + 8);
	// bl 0x825d3da0
	sub_825D3DA0(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r5,4(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// ld r4,16(r26)
	ctx.r4.u64 = PPC_LOAD_U64(r26.u32 + 16);
	// bl 0x825d3da0
	sub_825D3DA0(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// lwz r5,0(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// ld r4,24(r26)
	ctx.r4.u64 = PPC_LOAD_U64(r26.u32 + 24);
	// bl 0x825d3da0
	sub_825D3DA0(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
	// lwz r22,84(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpw cr6,r22,r21
	cr6.compare<int32_t>(r22.s32, r21.s32, xer);
	// blt cr6,0x825d4cc0
	if (cr6.lt) goto loc_825D4CC0;
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x825d4cd8
	if (!cr6.lt) goto loc_825D4CD8;
loc_825D4CC0:
	// stw r18,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r18.u32);
	// b 0x825d4cd8
	goto loc_825D4CD8;
loc_825D4CC8:
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r22,84(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_825D4CD8:
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4cfc
	if (cr6.eq) goto loc_825D4CFC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4d00
	if (cr6.eq) goto loc_825D4D00;
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4d00
	if (cr6.eq) goto loc_825D4D00;
loc_825D4CFC:
	// stw r19,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r19.u32);
loc_825D4D00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4dc8
	if (cr6.eq) goto loc_825D4DC8;
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d4dc8
	if (cr6.eq) goto loc_825D4DC8;
	// cmpw cr6,r21,r11
	cr6.compare<int32_t>(r21.s32, r11.s32, xer);
	// bne cr6,0x825d4d38
	if (!cr6.eq) goto loc_825D4D38;
	// cmpw cr6,r22,r8
	cr6.compare<int32_t>(r22.s32, ctx.r8.s32, xer);
	// bne cr6,0x825d4d38
	if (!cr6.eq) goto loc_825D4D38;
	// addis r8,r21,192
	ctx.r8.s64 = r21.s64 + 12582912;
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble 0x825d4d38
	if (!cr0.gt) goto loc_825D4D38;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
loc_825D4D38:
	// subf r10,r8,r11
	ctx.r10.s64 = r11.s64 - ctx.r8.s64;
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// subf r10,r21,r10
	ctx.r10.s64 = ctx.r10.s64 - r21.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x825d4d60
	if (cr6.gt) goto loc_825D4D60;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_825D4D60:
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// stw r18,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r18.u32);
	// add. r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bgt 0x825d4d74
	if (cr0.gt) goto loc_825D4D74;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
loc_825D4D74:
	// add. r11,r10,r8
	r11.u64 = ctx.r10.u64 + ctx.r8.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r18,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r18.u32);
	// bgt 0x825d4d84
	if (cr0.gt) goto loc_825D4D84;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_825D4D84:
	// subf r11,r9,r7
	r11.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r11,r21,r11
	r11.s64 = r11.s64 - r21.s64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d4d9c
	if (cr6.gt) goto loc_825D4D9C;
	// mr r11,r18
	r11.u64 = r18.u64;
loc_825D4D9C:
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stw r18,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r18.u32);
	// add. r10,r11,r7
	ctx.r10.u64 = r11.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt 0x825d4db0
	if (cr0.gt) goto loc_825D4DB0;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_825D4DB0:
	// add. r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r18,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r18.u32);
	// bgt 0x825d4dc0
	if (cr0.gt) goto loc_825D4DC0;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
loc_825D4DC0:
	// li r11,2
	r11.s64 = 2;
	// stw r11,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r11.u32);
loc_825D4DC8:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d4e70
	if (!cr6.gt) goto loc_825D4E70;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,192(r31)
	PPC_STORE_U32(r31.u32 + 192, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,196(r31)
	PPC_STORE_U32(r31.u32 + 196, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,296(r31)
	PPC_STORE_U32(r31.u32 + 296, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,280(r31)
	PPC_STORE_U32(r31.u32 + 280, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825D4E70:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4f40
	if (cr6.eq) goto loc_825D4F40;
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4ea0
	if (cr6.eq) goto loc_825D4EA0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4f40
	if (cr6.eq) goto loc_825D4F40;
	// lwz r11,104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4f40
	if (cr6.eq) goto loc_825D4F40;
loc_825D4EA0:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,260(r31)
	PPC_STORE_U32(r31.u32 + 260, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,268(r31)
	PPC_STORE_U32(r31.u32 + 268, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,264(r31)
	PPC_STORE_U32(r31.u32 + 264, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,272(r31)
	PPC_STORE_U32(r31.u32 + 272, ctx.r3.u32);
	// beq cr6,0x825d486c
	if (cr6.eq) goto loc_825D486C;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825D4F40:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x825d4f6c
	if (!cr6.gt) goto loc_825D4F6C;
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d4f6c
	if (!cr6.eq) goto loc_825D4F6C;
	// lis r16,-32764
	r16.s64 = -2147221504;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8239bd18
	return;
loc_825D4F6C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lis r10,1023
	ctx.r10.s64 = 67043328;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// mr r26,r18
	r26.u64 = r18.u64;
	// mr r27,r18
	r27.u64 = r18.u64;
	// lis r25,-1024
	r25.s64 = -67108864;
	// ori r23,r10,65535
	r23.u64 = ctx.r10.u64 | 65535;
	// addi r24,r11,-23452
	r24.s64 = r11.s64 + -23452;
	// ble cr6,0x825d50dc
	if (!cr6.gt) goto loc_825D50DC;
	// mr r29,r18
	r29.u64 = r18.u64;
loc_825D4F94:
	// lwz r11,196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// stw r18,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r18.u32);
	// stw r18,208(r31)
	PPC_STORE_U32(r31.u32 + 208, r18.u32);
	// stw r18,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r18.u32);
	// stwx r18,r29,r11
	PPC_STORE_U32(r29.u32 + r11.u32, r18.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5094
	if (cr6.eq) goto loc_825D5094;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lwzx r10,r29,r9
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + ctx.r9.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d4fc8
	if (cr6.lt) goto loc_825D4FC8;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_825D4FC8:
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// stwx r10,r29,r9
	PPC_STORE_U32(r29.u32 + ctx.r9.u32, ctx.r10.u32);
	// lwzx r30,r29,r11
	r30.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d4fe0
	if (cr6.lt) goto loc_825D4FE0;
	// mr r30,r18
	r30.u64 = r18.u64;
loc_825D4FE0:
	// subf r4,r21,r10
	ctx.r4.s64 = ctx.r10.s64 - r21.s64;
	// stwx r30,r29,r11
	PPC_STORE_U32(r29.u32 + r11.u32, r30.u32);
	// subf r28,r22,r30
	r28.s64 = r30.s64 - r22.s64;
	// cmpw cr6,r4,r25
	cr6.compare<int32_t>(ctx.r4.s32, r25.s32, xer);
	// stw r4,200(r31)
	PPC_STORE_U32(r31.u32 + 200, ctx.r4.u32);
	// bge cr6,0x825d5000
	if (!cr6.lt) goto loc_825D5000;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x825d500c
	goto loc_825D500C;
loc_825D5000:
	// cmpw cr6,r4,r23
	cr6.compare<int32_t>(ctx.r4.s32, r23.s32, xer);
	// ble cr6,0x825d500c
	if (!cr6.gt) goto loc_825D500C;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
loc_825D500C:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// lwz r11,192(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// stwx r3,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, ctx.r3.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 200);
	// cmpwi cr6,r10,-10485
	cr6.compare<int32_t>(ctx.r10.s32, -10485, xer);
	// ble cr6,0x825d5038
	if (!cr6.gt) goto loc_825D5038;
	// cmpwi cr6,r10,10485
	cr6.compare<int32_t>(ctx.r10.s32, 10485, xer);
	// bge cr6,0x825d5038
	if (!cr6.lt) goto loc_825D5038;
	// cmpwi cr6,r28,-10485
	cr6.compare<int32_t>(r28.s32, -10485, xer);
	// bgt cr6,0x825d503c
	if (cr6.gt) goto loc_825D503C;
loc_825D5038:
	// mr r26,r19
	r26.u64 = r19.u64;
loc_825D503C:
	// add r11,r10,r22
	r11.u64 = ctx.r10.u64 + r22.u64;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825d5094
	if (cr6.lt) goto loc_825D5094;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// subf r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// srawi. r10,r9,10
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r9,208(r31)
	PPC_STORE_U32(r31.u32 + 208, ctx.r9.u32);
	// bne 0x825d5068
	if (!cr0.eq) goto loc_825D5068;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x825d5084
	goto loc_825D5084;
loc_825D5068:
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// twllei r10,0
	// andc r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// rlwinm r11,r11,10,0,21
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0xFFFFFC00;
	// twlgei r10,-1
loc_825D5084:
	// lwz r10,196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// lwz r9,104(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// stw r11,204(r31)
	PPC_STORE_U32(r31.u32 + 204, r11.u32);
	// stwx r9,r29,r10
	PPC_STORE_U32(r29.u32 + ctx.r10.u32, ctx.r9.u32);
loc_825D5094:
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d50b0
	if (cr6.eq) goto loc_825D50B0;
	// lwz r11,196(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d50c8
	if (cr6.eq) goto loc_825D50C8;
loc_825D50B0:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d4418
	sub_825D4418(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
loc_825D50C8:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825d4f94
	if (cr6.lt) goto loc_825D4F94;
loc_825D50DC:
	// lis r10,127
	ctx.r10.s64 = 8323072;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r28,r18
	r28.u64 = r18.u64;
	// stw r26,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r26.u32);
	// ori r29,r10,65534
	r29.u64 = ctx.r10.u64 | 65534;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r29,276(r31)
	PPC_STORE_U32(r31.u32 + 276, r29.u32);
	// ble cr6,0x825d5168
	if (!cr6.gt) goto loc_825D5168;
	// mr r30,r18
	r30.u64 = r18.u64;
loc_825D5100:
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// stwx r29,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, r29.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5154
	if (cr6.eq) goto loc_825D5154;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// lwzx r4,r30,r11
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// cmpw cr6,r4,r25
	cr6.compare<int32_t>(ctx.r4.s32, r25.s32, xer);
	// bge cr6,0x825d512c
	if (!cr6.lt) goto loc_825D512C;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x825d5138
	goto loc_825D5138;
loc_825D512C:
	// cmpw cr6,r4,r23
	cr6.compare<int32_t>(ctx.r4.s32, r23.s32, xer);
	// ble cr6,0x825d5138
	if (!cr6.gt) goto loc_825D5138;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
loc_825D5138:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825d3d18
	sub_825D3D18(ctx, base);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// mulld r10,r10,r29
	ctx.r10.s64 = ctx.r10.s64 * r29.s64;
	// sradi r10,r10,20
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 20;
	// stwx r10,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, ctx.r10.u32);
loc_825D5154:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x825d5100
	if (cr6.lt) goto loc_825D5100;
loc_825D5168:
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lis r30,16
	r30.s64 = 1048576;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d51d4
	if (cr6.eq) goto loc_825D51D4;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5198
	if (cr6.eq) goto loc_825D5198;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d4648
	sub_825D4648(ctx, base);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// blt cr6,0x825d4930
	if (cr6.lt) goto loc_825D4930;
loc_825D5198:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d51d4
	if (!cr6.eq) goto loc_825D51D4;
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d51d4
	if (!cr6.gt) goto loc_825D51D4;
	// mr r11,r18
	r11.u64 = r18.u64;
loc_825D51B8:
	// lwz r9,192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r30,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r30.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,256(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d51b8
	if (cr6.lt) goto loc_825D51B8;
loc_825D51D4:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d51f8
	if (cr6.eq) goto loc_825D51F8;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d51f0
	if (cr6.eq) goto loc_825D51F0;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
loc_825D51F0:
	// stw r18,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r18.u32);
	// stw r18,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r18.u32);
loc_825D51F8:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// stw r18,144(r31)
	PPC_STORE_U32(r31.u32 + 144, r18.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r30,172(r31)
	PPC_STORE_U32(r31.u32 + 172, r30.u32);
	// stw r18,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r18.u32);
	// beq cr6,0x825d5230
	if (cr6.eq) goto loc_825D5230;
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// mulhw r11,r11,r20
	r11.s64 = (int64_t(r11.s32) * int64_t(r20.s32)) >> 32;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,212(r31)
	PPC_STORE_U32(r31.u32 + 212, r11.u32);
	// b 0x825d5234
	goto loc_825D5234;
loc_825D5230:
	// stw r18,212(r31)
	PPC_STORE_U32(r31.u32 + 212, r18.u32);
loc_825D5234:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d4930
	if (cr6.eq) goto loc_825D4930;
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r11,212(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 212);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// beq cr6,0x825d5268
	if (cr6.eq) goto loc_825D5268;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stw r9,228(r31)
	PPC_STORE_U32(r31.u32 + 228, ctx.r9.u32);
loc_825D5268:
	// stw r11,224(r31)
	PPC_STORE_U32(r31.u32 + 224, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r18,220(r31)
	PPC_STORE_U32(r31.u32 + 220, r18.u32);
	// stw r18,236(r31)
	PPC_STORE_U32(r31.u32 + 236, r18.u32);
	// stw r18,232(r31)
	PPC_STORE_U32(r31.u32 + 232, r18.u32);
	// beq cr6,0x825d4930
	if (cr6.eq) goto loc_825D4930;
	// stw r30,240(r31)
	PPC_STORE_U32(r31.u32 + 240, r30.u32);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// stw r30,244(r31)
	PPC_STORE_U32(r31.u32 + 244, r30.u32);
	// stw r18,248(r31)
	PPC_STORE_U32(r31.u32 + 248, r18.u32);
	// stw r18,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r18.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_825D52A0"))) PPC_WEAK_FUNC(sub_825D52A0);
PPC_FUNC_IMPL(__imp__sub_825D52A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r10,172(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 172);
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d52d4
	if (cr6.gt) goto loc_825D52D4;
	// lwz r9,164(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 164);
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bgelr cr6
	if (!cr6.lt) return;
	// lwz r9,176(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 176);
	// b 0x825d52e4
	goto loc_825D52E4;
loc_825D52D4:
	// lwz r9,168(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 168);
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bgelr cr6
	if (!cr6.lt) return;
	// lwz r9,180(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 180);
loc_825D52E4:
	// extsw r4,r10
	ctx.r4.s64 = ctx.r10.s32;
	// mullw r10,r9,r7
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// lis r8,16384
	ctx.r8.s64 = 1073741824;
	// extsw r9,r3
	ctx.r9.s64 = ctx.r3.s32;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// mulld r9,r10,r9
	ctx.r9.s64 = ctx.r10.s64 * ctx.r9.s64;
	// mulld r10,r8,r4
	ctx.r10.s64 = ctx.r8.s64 * ctx.r4.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// stw r10,172(r11)
	PPC_STORE_U32(r11.u32 + 172, ctx.r10.u32);
	// extsw r10,r5
	ctx.r10.s64 = ctx.r5.s32;
	// lwz r9,172(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 172);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// mulld r9,r9,r10
	ctx.r9.s64 = ctx.r9.s64 * ctx.r10.s64;
	// sradi r9,r9,20
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s64 >> 20;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// blt cr6,0x825d5358
	if (cr6.lt) goto loc_825D5358;
	// rldicr r9,r6,36,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u64, 36) & 0xFFFFFFF000000000;
	// tdllei r10,0
	// divd r8,r9,r10
	ctx.r8.s64 = ctx.r9.s64 / ctx.r10.s64;
	// rotldi r9,r9,1
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 1);
	// sradi r8,r8,16
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s64 >> 16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// andc r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// tdlgei r10,-1
	// stw r8,172(r11)
	PPC_STORE_U32(r11.u32 + 172, ctx.r8.u32);
loc_825D5358:
	// lwz r3,172(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 172);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D5360"))) PPC_WEAK_FUNC(sub_825D5360);
PPC_FUNC_IMPL(__imp__sub_825D5360) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stfd f29,-136(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -136, f29.u64);
	// stfd f30,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, f30.u64);
	// stfd f31,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, f31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// li r19,0
	r19.s64 = 0;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// lwz r4,284(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// blt cr6,0x825d58c0
	if (cr6.lt) goto loc_825D58C0;
	// beq cr6,0x825d53c0
	if (cr6.eq) goto loc_825D53C0;
	// cmplwi cr6,r6,3
	cr6.compare<uint32_t>(ctx.r6.u32, 3, xer);
	// blt cr6,0x825d53b8
	if (cr6.lt) goto loc_825D53B8;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lfd f29,-136(r1)
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f30,-128(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f31,-120(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// b 0x8239bd24
	return;
loc_825D53B8:
	// li r20,1
	r20.s64 = 1;
	// b 0x825d53c4
	goto loc_825D53C4;
loc_825D53C0:
	// li r20,0
	r20.s64 = 0;
loc_825D53C4:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d53dc
	if (!cr6.eq) goto loc_825D53DC;
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d58c0
	if (cr6.eq) goto loc_825D58C0;
loc_825D53DC:
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d53f0
	if (!cr6.eq) goto loc_825D53F0;
	// lis r23,16
	r23.s64 = 1048576;
	// b 0x825d5400
	goto loc_825D5400;
loc_825D53F0:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// rlwinm r10,r5,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FC;
	// addi r11,r11,13824
	r11.s64 = r11.s64 + 13824;
	// lwzx r23,r10,r11
	r23.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
loc_825D5400:
	// lwz r24,120(r27)
	r24.u64 = PPC_LOAD_U32(r27.u32 + 120);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x825d5440
	if (!cr6.gt) goto loc_825D5440;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D5418:
	// lwz r8,320(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 320);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r6,392(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 392);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 + 1776;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r8,60(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 60);
	// stwx r8,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825d5418
	if (!cr6.eq) goto loc_825D5418;
loc_825D5440:
	// srawi r11,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	r11.s64 = ctx.r7.s32 >> 8;
	// addze r10,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r10.s64 = temp.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bge cr6,0x825d5458
	if (!cr6.lt) goto loc_825D5458;
	// li r10,2
	ctx.r10.s64 = 2;
	// b 0x825d5464
	goto loc_825D5464;
loc_825D5458:
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// ble cr6,0x825d5464
	if (!cr6.gt) goto loc_825D5464;
	// li r10,16
	ctx.r10.s64 = 16;
loc_825D5464:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d58c0
	if (cr6.lt) goto loc_825D58C0;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_825D5478:
	// rotlwi r6,r11,1
	ctx.r6.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r5,r11,r10
	ctx.r5.s32 = r11.s32 / ctx.r10.s32;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// andc r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 & ~ctx.r6.u64;
	// twllei r10,0
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// twlgei r6,-1
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d5478
	if (!cr6.eq) goto loc_825D5478;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d58c0
	if (!cr6.gt) goto loc_825D58C0;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lis r8,127
	ctx.r8.s64 = 8323072;
	// addi r26,r1,148
	r26.s64 = ctx.r1.s64 + 148;
	// lfs f31,5736(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5736);
	f31.f64 = double(temp.f32);
	// ori r21,r8,65535
	r21.u64 = ctx.r8.u64 | 65535;
	// lfs f30,2480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	f30.f64 = double(temp.f32);
	// lfs f29,-15172(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -15172);
	f29.f64 = double(temp.f32);
loc_825D54D8:
	// lwz r29,-4(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + -4);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r28,0(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x825d5558
	if (!cr6.gt) goto loc_825D5558;
	// lwz r7,392(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + 392);
	// rlwinm r6,r29,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
loc_825D54F8:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// bge cr6,0x825d553c
	if (!cr6.lt) goto loc_825D553C;
	// subf r8,r29,r28
	ctx.r8.s64 = r28.s64 - r29.s64;
loc_825D5510:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d5520
	if (cr6.gt) goto loc_825D5520;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_825D5520:
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x825d552c
	if (!cr6.gt) goto loc_825D552C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_825D552C:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d5510
	if (!cr6.eq) goto loc_825D5510;
loc_825D553C:
	// cmpw cr6,r9,r5
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r5.s32, xer);
	// ble cr6,0x825d5548
	if (!cr6.gt) goto loc_825D5548;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
loc_825D5548:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x825d54f8
	if (!cr6.eq) goto loc_825D54F8;
loc_825D5558:
	// lwz r11,124(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 124);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d556c
	if (!cr6.gt) goto loc_825D556C;
	// slw r5,r5,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r5.u32 << (r11.u8 & 0x3F));
	// b 0x825d5578
	goto loc_825D5578;
loc_825D556C:
	// bge cr6,0x825d5578
	if (!cr6.lt) goto loc_825D5578;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sraw r5,r5,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r5.s64 = ctx.r5.s32 >> temp.u32;
loc_825D5578:
	// cmpw cr6,r5,r21
	cr6.compare<int32_t>(ctx.r5.s32, r21.s32, xer);
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// blt cr6,0x825d5588
	if (cr6.lt) goto loc_825D5588;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
loc_825D5588:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5638
	if (cr6.eq) goto loc_825D5638;
	// lwz r10,196(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 196);
	// rlwinm r11,r20,2,0,29
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d5610
	if (cr6.eq) goto loc_825D5610;
	// lwz r6,260(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 260);
	// extsw r10,r23
	ctx.r10.s64 = r23.s32;
	// lwz r4,280(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r8,264(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 264);
	// lwz r7,296(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 296);
	// lwzx r3,r6,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// lwzx r6,r4,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + r11.u32);
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// lwzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// mulld r11,r9,r10
	r11.s64 = ctx.r9.s64 * ctx.r10.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// clrlwi r4,r11,19
	ctx.r4.u64 = r11.u32 & 0x1FFF;
	// srawi r11,r11,13
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFF) != 0);
	r11.s64 = r11.s32 >> 13;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// lwzx r9,r3,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	// mullw r11,r8,r4
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// sraw r11,r11,r7
	temp.u32 = ctx.r7.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// mulld r11,r11,r10
	r11.s64 = r11.s64 * ctx.r10.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r4,r11
	ctx.r4.s64 = r11.s32;
	// b 0x825d5644
	goto loc_825D5644;
loc_825D5610:
	// lwz r10,192(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 192);
	// extsw r9,r23
	ctx.r9.s64 = r23.s32;
	// lwz r8,280(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r6,r8,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// mulld r11,r11,r9
	r11.s64 = r11.s64 * ctx.r9.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r4,r11
	ctx.r4.s64 = r11.s32;
	// b 0x825d5644
	goto loc_825D5644;
loc_825D5638:
	// lis r6,127
	ctx.r6.s64 = 8323072;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// ori r6,r6,65534
	ctx.r6.u64 = ctx.r6.u64 | 65534;
loc_825D5644:
	// subf r30,r29,r28
	r30.s64 = r28.s64 - r29.s64;
	// lwz r31,172(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 172);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// bl 0x825d52a0
	sub_825D52A0(ctx, base);
	// srawi r11,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r11.s64 = r30.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5680
	if (cr6.eq) goto loc_825D5680;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825d5680
	if (!cr6.gt) goto loc_825D5680;
loc_825D5670:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d5670
	if (cr6.gt) goto loc_825D5670;
loc_825D5680:
	// cmpw cr6,r3,r31
	cr6.compare<int32_t>(ctx.r3.s32, r31.s32, xer);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// bgt cr6,0x825d5690
	if (cr6.gt) goto loc_825D5690;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_825D5690:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d569c
	if (cr6.gt) goto loc_825D569C;
	// li r11,2
	r11.s64 = 2;
loc_825D569C:
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825d56bc
	if (!cr6.gt) goto loc_825D56BC;
loc_825D56AC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d56ac
	if (cr6.gt) goto loc_825D56AC;
loc_825D56BC:
	// extsw r11,r3
	r11.s64 = ctx.r3.s32;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f13,f0,f29
	ctx.f13.f64 = double(float(f0.f64 * f29.f64));
	// ble cr6,0x825d58ac
	if (!cr6.gt) goto loc_825D58AC;
	// rlwinm r6,r29,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
loc_825D56E8:
	// lwz r11,392(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 392);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// lwzx r11,r11,r7
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// blt cr6,0x825d583c
	if (cr6.lt) goto loc_825D583C;
	// subf r10,r29,r28
	ctx.r10.s64 = r28.s64 - r29.s64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r9,r29
	ctx.r8.u64 = ctx.r9.u64 + r29.u64;
loc_825D5718:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x825d5748
	if (!cr6.lt) goto loc_825D5748;
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// b 0x825d574c
	goto loc_825D574C;
loc_825D5748:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f31.f64));
loc_825D574C:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfd f0,104(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x825d578c
	if (!cr6.lt) goto loc_825D578C;
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// b 0x825d5790
	goto loc_825D5790;
loc_825D578C:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f31.f64));
loc_825D5790:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// std r9,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r9.u64);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfd f0,112(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x825d57d0
	if (!cr6.lt) goto loc_825D57D0;
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// b 0x825d57d4
	goto loc_825D57D4;
loc_825D57D0:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f31.f64));
loc_825D57D4:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r9,r11,12
	ctx.r9.s64 = r11.s64 + 12;
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// std r4,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r4.u64);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfd f0,120(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x825d5818
	if (!cr6.lt) goto loc_825D5818;
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// b 0x825d581c
	goto loc_825D581C;
loc_825D5818:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f31.f64));
loc_825D581C:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r4
	PPC_STORE_U32(ctx.r4.u32, f0.u32);
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r4,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
	// bne cr6,0x825d5718
	if (!cr6.eq) goto loc_825D5718;
loc_825D583C:
	// cmpw cr6,r8,r28
	cr6.compare<int32_t>(ctx.r8.s32, r28.s32, xer);
	// bge cr6,0x825d589c
	if (!cr6.lt) goto loc_825D589C;
	// subf r10,r8,r28
	ctx.r10.s64 = r28.s64 - ctx.r8.s64;
loc_825D5848:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x825d5878
	if (!cr6.lt) goto loc_825D5878;
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// b 0x825d587c
	goto loc_825D587C;
loc_825D5878:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f31.f64));
loc_825D587C:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d5848
	if (!cr6.eq) goto loc_825D5848;
loc_825D589C:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825d56e8
	if (!cr6.eq) goto loc_825D56E8;
loc_825D58AC:
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// stw r3,172(r27)
	PPC_STORE_U32(r27.u32 + 172, ctx.r3.u32);
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x825d54d8
	if (!cr6.eq) goto loc_825D54D8;
loc_825D58C0:
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lfd f29,-136(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f30,-128(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f31,-120(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825D58D8"))) PPC_WEAK_FUNC(sub_825D58D8);
PPC_FUNC_IMPL(__imp__sub_825D58D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stfd f29,-136(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -136, f29.u64);
	// stfd f30,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, f30.u64);
	// stfd f31,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, f31.u64);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// li r19,0
	r19.s64 = 0;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// lwz r4,284(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// blt cr6,0x825d5e28
	if (cr6.lt) goto loc_825D5E28;
	// beq cr6,0x825d5938
	if (cr6.eq) goto loc_825D5938;
	// cmplwi cr6,r6,3
	cr6.compare<uint32_t>(ctx.r6.u32, 3, xer);
	// blt cr6,0x825d5930
	if (cr6.lt) goto loc_825D5930;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// lfd f29,-136(r1)
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f30,-128(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f31,-120(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// b 0x8239bd24
	return;
loc_825D5930:
	// li r20,1
	r20.s64 = 1;
	// b 0x825d593c
	goto loc_825D593C;
loc_825D5938:
	// li r20,0
	r20.s64 = 0;
loc_825D593C:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d5954
	if (!cr6.eq) goto loc_825D5954;
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5e28
	if (cr6.eq) goto loc_825D5E28;
loc_825D5954:
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d5968
	if (!cr6.eq) goto loc_825D5968;
	// lis r24,16
	r24.s64 = 1048576;
	// b 0x825d5978
	goto loc_825D5978;
loc_825D5968:
	// lis r11,-32126
	r11.s64 = -2105409536;
	// rlwinm r10,r5,2,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0x3FC;
	// addi r11,r11,13824
	r11.s64 = r11.s64 + 13824;
	// lwzx r24,r10,r11
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
loc_825D5978:
	// lwz r25,120(r27)
	r25.u64 = PPC_LOAD_U32(r27.u32 + 120);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825d59b8
	if (!cr6.gt) goto loc_825D59B8;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_825D5990:
	// lwz r8,320(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 320);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r6,388(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 388);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r10,r10,1776
	ctx.r10.s64 = ctx.r10.s64 + 1776;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r8,60(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 60);
	// stwx r8,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825d5990
	if (!cr6.eq) goto loc_825D5990;
loc_825D59B8:
	// srawi r11,r7,8
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFF) != 0);
	r11.s64 = ctx.r7.s32 >> 8;
	// addze r10,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r10.s64 = temp.s64;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bge cr6,0x825d59d0
	if (!cr6.lt) goto loc_825D59D0;
	// li r10,2
	ctx.r10.s64 = 2;
	// b 0x825d59dc
	goto loc_825D59DC;
loc_825D59D0:
	// cmpwi cr6,r10,16
	cr6.compare<int32_t>(ctx.r10.s32, 16, xer);
	// ble cr6,0x825d59dc
	if (!cr6.gt) goto loc_825D59DC;
	// li r10,16
	ctx.r10.s64 = 16;
loc_825D59DC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825d5e28
	if (cr6.lt) goto loc_825D5E28;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_825D59F0:
	// rotlwi r6,r11,1
	ctx.r6.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r5,r11,r10
	ctx.r5.s32 = r11.s32 / ctx.r10.s32;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// andc r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 & ~ctx.r6.u64;
	// twllei r10,0
	// stw r5,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r5.u32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// twlgei r6,-1
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d59f0
	if (!cr6.eq) goto loc_825D59F0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d5e28
	if (!cr6.gt) goto loc_825D5E28;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r8,127
	ctx.r8.s64 = 8323072;
	// addi r26,r1,116
	r26.s64 = ctx.r1.s64 + 116;
	// lfs f29,-15172(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -15172);
	f29.f64 = double(temp.f32);
	// ori r21,r8,65535
	r21.u64 = ctx.r8.u64 | 65535;
	// lfs f30,5736(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5736);
	f30.f64 = double(temp.f32);
	// lfs f31,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f31.f64 = double(temp.f32);
loc_825D5A50:
	// lwz r30,-4(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + -4);
	// fmr f12,f31
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = f31.f64;
	// lwz r28,0(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825d5b78
	if (!cr6.gt) goto loc_825D5B78;
	// lwz r8,388(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 388);
	// rlwinm r7,r30,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r30,r28
	ctx.r6.s64 = r28.s64 - r30.s64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
loc_825D5A74:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// fmr f13,f31
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = f31.f64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// blt cr6,0x825d5b24
	if (cr6.lt) goto loc_825D5B24;
	// subf r10,r30,r28
	ctx.r10.s64 = r28.s64 - r30.s64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
loc_825D5AA4:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bgt cr6,0x825d5ab4
	if (cr6.gt) goto loc_825D5AB4;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_825D5AB4:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x825d5ac0
	if (!cr6.gt) goto loc_825D5AC0;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
loc_825D5AC0:
	// lfs f0,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bgt cr6,0x825d5ad0
	if (cr6.gt) goto loc_825D5AD0;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_825D5AD0:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x825d5adc
	if (!cr6.gt) goto loc_825D5ADC;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
loc_825D5ADC:
	// lfs f0,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bgt cr6,0x825d5aec
	if (cr6.gt) goto loc_825D5AEC;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_825D5AEC:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x825d5af8
	if (!cr6.gt) goto loc_825D5AF8;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
loc_825D5AF8:
	// lfs f0,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bgt cr6,0x825d5b08
	if (cr6.gt) goto loc_825D5B08;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_825D5B08:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x825d5b14
	if (!cr6.gt) goto loc_825D5B14;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
loc_825D5B14:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d5aa4
	if (!cr6.eq) goto loc_825D5AA4;
loc_825D5B24:
	// cmpw cr6,r9,r28
	cr6.compare<int32_t>(ctx.r9.s32, r28.s32, xer);
	// bge cr6,0x825d5b5c
	if (!cr6.lt) goto loc_825D5B5C;
	// subf r10,r9,r28
	ctx.r10.s64 = r28.s64 - ctx.r9.s64;
loc_825D5B30:
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bgt cr6,0x825d5b40
	if (cr6.gt) goto loc_825D5B40;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_825D5B40:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x825d5b4c
	if (!cr6.gt) goto loc_825D5B4C;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
loc_825D5B4C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d5b30
	if (!cr6.eq) goto loc_825D5B30;
loc_825D5B5C:
	// fcmpu cr6,f13,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// ble cr6,0x825d5b68
	if (!cr6.gt) goto loc_825D5B68;
	// fmr f12,f13
	ctx.f12.f64 = ctx.f13.f64;
loc_825D5B68:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825d5a74
	if (!cr6.eq) goto loc_825D5A74;
loc_825D5B78:
	// lfs f0,128(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 128);
	f0.f64 = double(temp.f32);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// fmuls f0,f0,f12
	f0.f64 = double(float(f0.f64 * ctx.f12.f64));
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bge cr6,0x825d5b94
	if (!cr6.lt) goto loc_825D5B94;
	// fsubs f13,f0,f30
	ctx.f13.f64 = double(float(f0.f64 - f30.f64));
	// b 0x825d5b98
	goto loc_825D5B98;
loc_825D5B94:
	// fadds f13,f0,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64 + f30.f64));
loc_825D5B98:
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x825d5bd8
	if (!cr6.lt) goto loc_825D5BD8;
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// bge cr6,0x825d5bc8
	if (!cr6.lt) goto loc_825D5BC8;
	// fsubs f13,f0,f30
	ctx.f13.f64 = double(float(f0.f64 - f30.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// b 0x825d5bdc
	goto loc_825D5BDC;
loc_825D5BC8:
	// fadds f13,f0,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64 + f30.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// b 0x825d5bdc
	goto loc_825D5BDC;
loc_825D5BD8:
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
loc_825D5BDC:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5c90
	if (cr6.eq) goto loc_825D5C90;
	// lwz r10,196(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 196);
	// rlwinm r11,r20,2,0,29
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d5c68
	if (cr6.eq) goto loc_825D5C68;
	// lwz r6,260(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 260);
	// extsw r10,r24
	ctx.r10.s64 = r24.s32;
	// lwz r5,280(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// lwz r8,264(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 264);
	// lwz r7,296(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 296);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwzx r4,r6,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwzx r6,r5,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + r11.u32);
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// lwzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// mulld r11,r9,r10
	r11.s64 = ctx.r9.s64 * ctx.r10.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// clrlwi r5,r11,19
	ctx.r5.u64 = r11.u32 & 0x1FFF;
	// srawi r11,r11,13
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFF) != 0);
	r11.s64 = r11.s32 >> 13;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// lwzx r9,r4,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + r11.u32);
	// mullw r11,r8,r5
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// sraw r11,r11,r7
	temp.u32 = ctx.r7.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// mulld r11,r11,r10
	r11.s64 = r11.s64 * ctx.r10.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r4,r11
	ctx.r4.s64 = r11.s32;
	// b 0x825d5c9c
	goto loc_825D5C9C;
loc_825D5C68:
	// lwz r10,192(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 192);
	// extsw r9,r24
	ctx.r9.s64 = r24.s32;
	// lwz r8,280(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 280);
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r6,r8,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// mulld r11,r11,r9
	r11.s64 = r11.s64 * ctx.r9.s64;
	// sradi r11,r11,20
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFF) != 0);
	r11.s64 = r11.s64 >> 20;
	// extsw r4,r11
	ctx.r4.s64 = r11.s32;
	// b 0x825d5c9c
	goto loc_825D5C9C;
loc_825D5C90:
	// lis r6,127
	ctx.r6.s64 = 8323072;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// ori r6,r6,65534
	ctx.r6.u64 = ctx.r6.u64 | 65534;
loc_825D5C9C:
	// lwz r31,172(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 172);
	// fcmpu cr6,f0,f31
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f31.f64);
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// bge cr6,0x825d5cb4
	if (!cr6.lt) goto loc_825D5CB4;
	// fsubs f0,f0,f30
	f0.f64 = double(float(f0.f64 - f30.f64));
	// b 0x825d5cb8
	goto loc_825D5CB8;
loc_825D5CB4:
	// fadds f0,f0,f30
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + f30.f64));
loc_825D5CB8:
	// subf r29,r30,r28
	r29.s64 = r28.s64 - r30.s64;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x825d52a0
	sub_825D52A0(ctx, base);
	// srawi r11,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	r11.s64 = r29.s32 >> 2;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5cfc
	if (cr6.eq) goto loc_825D5CFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825d5cfc
	if (!cr6.gt) goto loc_825D5CFC;
loc_825D5CEC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d5cec
	if (cr6.gt) goto loc_825D5CEC;
loc_825D5CFC:
	// cmpw cr6,r3,r31
	cr6.compare<int32_t>(ctx.r3.s32, r31.s32, xer);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// bgt cr6,0x825d5d0c
	if (cr6.gt) goto loc_825D5D0C;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_825D5D0C:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d5d18
	if (cr6.gt) goto loc_825D5D18;
	// li r11,2
	r11.s64 = 2;
loc_825D5D18:
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825d5d38
	if (!cr6.gt) goto loc_825D5D38;
loc_825D5D28:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d5d28
	if (cr6.gt) goto loc_825D5D28;
loc_825D5D38:
	// extsw r11,r3
	r11.s64 = ctx.r3.s32;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f29
	f0.f64 = double(float(f0.f64 * f29.f64));
	// ble cr6,0x825d5e14
	if (!cr6.gt) goto loc_825D5E14;
	// lwz r5,388(r22)
	ctx.r5.u64 = PPC_LOAD_U32(r22.u32 + 388);
	// rlwinm r4,r30,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_825D5D64:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// blt cr6,0x825d5ddc
	if (cr6.lt) goto loc_825D5DDC;
	// subf r10,r30,r28
	ctx.r10.s64 = r28.s64 - r30.s64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r9,r30
	ctx.r6.u64 = ctx.r9.u64 + r30.u64;
loc_825D5D90:
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// addi r7,r11,12
	ctx.r7.s64 = r11.s64 + 12;
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lfs f12,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f11,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * f0.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f12,0(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// bne cr6,0x825d5d90
	if (!cr6.eq) goto loc_825D5D90;
loc_825D5DDC:
	// cmpw cr6,r6,r28
	cr6.compare<int32_t>(ctx.r6.s32, r28.s32, xer);
	// bge cr6,0x825d5e04
	if (!cr6.lt) goto loc_825D5E04;
	// subf r10,r6,r28
	ctx.r10.s64 = r28.s64 - ctx.r6.s64;
loc_825D5DE8:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d5de8
	if (!cr6.eq) goto loc_825D5DE8;
loc_825D5E04:
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825d5d64
	if (!cr6.eq) goto loc_825D5D64;
loc_825D5E14:
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// stw r3,172(r27)
	PPC_STORE_U32(r27.u32 + 172, ctx.r3.u32);
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x825d5a50
	if (!cr6.eq) goto loc_825D5A50;
loc_825D5E28:
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// lfd f29,-136(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f30,-128(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f31,-120(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825D5E40"))) PPC_WEAK_FUNC(sub_825D5E40);
PPC_FUNC_IMPL(__imp__sub_825D5E40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r30,r3,224
	r30.s64 = ctx.r3.s64 + 224;
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,21
	ctx.r4.s64 = 21;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d5ec4
	if (cr6.lt) goto loc_825D5EC4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d5ec4
	if (cr6.lt) goto loc_825D5EC4;
loc_825D5E88:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// cmpwi cr6,r10,127
	cr6.compare<int32_t>(ctx.r10.s32, 127, xer);
	// bne cr6,0x825d5ebc
	if (!cr6.eq) goto loc_825D5EBC;
	// addi r11,r11,127
	r11.s64 = r11.s64 + 127;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825d5e88
	if (!cr6.lt) goto loc_825D5E88;
	// b 0x825d5ec4
	goto loc_825D5EC4;
loc_825D5EBC:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r11.u32);
loc_825D5EC4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D5EDC"))) PPC_WEAK_FUNC(sub_825D5EDC);
PPC_FUNC_IMPL(__imp__sub_825D5EDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D5EE0"))) PPC_WEAK_FUNC(sub_825D5EE0);
PPC_FUNC_IMPL(__imp__sub_825D5EE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// li r27,0
	r27.s64 = 0;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r28,0(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r11,r27
	r11.u64 = r27.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// ble cr6,0x825d5f28
	if (!cr6.gt) goto loc_825D5F28;
loc_825D5F18:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r9,r10,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d5f18
	if (cr6.gt) goto loc_825D5F18;
loc_825D5F28:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,312(r29)
	PPC_STORE_U16(r29.u32 + 312, r11.u16);
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x825d5fbc
	if (!cr6.lt) goto loc_825D5FBC;
loc_825D5F40:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8fd8
	sub_825D8FD8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d5ffc
	if (cr6.lt) goto loc_825D5FFC;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// xor r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// lhz r10,202(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// bge cr6,0x825d5fd4
	if (!cr6.lt) goto loc_825D5FD4;
	// lhz r10,202(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sth r11,202(r31)
	PPC_STORE_U16(r31.u32 + 202, r11.u16);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + r28.u32, r11.u32);
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,202(r31)
	PPC_STORE_U16(r31.u32 + 202, r11.u16);
	// stw r27,56(r29)
	PPC_STORE_U32(r29.u32 + 56, r27.u32);
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825d5f40
	if (cr6.lt) goto loc_825D5F40;
loc_825D5FBC:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d5fe4
	if (cr6.eq) goto loc_825D5FE4;
	// sth r30,490(r26)
	PPC_STORE_U16(r26.u32 + 490, r30.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D5FD4:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D5FE4:
	// lhz r11,202(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 202);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// sth r11,490(r26)
	PPC_STORE_U16(r26.u32 + 490, r11.u16);
loc_825D5FFC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D6004"))) PPC_WEAK_FUNC(sub_825D6004);
PPC_FUNC_IMPL(__imp__sub_825D6004) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D6008"))) PPC_WEAK_FUNC(sub_825D6008);
PPC_FUNC_IMPL(__imp__sub_825D6008) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,288(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 288);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x825d6084
	if (!cr6.eq) goto loc_825D6084;
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d6054
	if (!cr6.eq) goto loc_825D6054;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-15048
	r11.s64 = r11.s64 + -15048;
	// addi r10,r10,-2104
	ctx.r10.s64 = ctx.r10.s64 + -2104;
	// addi r9,r9,-1232
	ctx.r9.s64 = ctx.r9.s64 + -1232;
	// li r8,40
	ctx.r8.s64 = 40;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
loc_825D6054:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-22488
	r11.s64 = r11.s64 + -22488;
	// addi r10,r10,-4008
	ctx.r10.s64 = ctx.r10.s64 + -4008;
	// addi r9,r9,-3056
	ctx.r9.s64 = ctx.r9.s64 + -3056;
	// li r8,70
	ctx.r8.s64 = 70;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
loc_825D6084:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d60f8
	if (!cr6.eq) goto loc_825D60F8;
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d60c8
	if (!cr6.eq) goto loc_825D60C8;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-6856
	r11.s64 = r11.s64 + -6856;
	// addi r10,r10,11928
	ctx.r10.s64 = ctx.r10.s64 + 11928;
	// addi r9,r9,13040
	ctx.r9.s64 = ctx.r9.s64 + 13040;
	// li r8,40
	ctx.r8.s64 = 40;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
loc_825D60C8:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-9344
	r11.s64 = r11.s64 + -9344;
	// addi r10,r10,9272
	ctx.r10.s64 = ctx.r10.s64 + 9272;
	// addi r9,r9,10600
	ctx.r9.s64 = ctx.r9.s64 + 10600;
	// li r8,60
	ctx.r8.s64 = 60;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
loc_825D60F8:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d613c
	if (!cr6.eq) goto loc_825D613C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-13352
	r11.s64 = r11.s64 + -13352;
	// addi r10,r10,4984
	ctx.r10.s64 = ctx.r10.s64 + 4984;
	// addi r9,r9,7128
	ctx.r9.s64 = ctx.r9.s64 + 7128;
	// li r8,180
	ctx.r8.s64 = 180;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
loc_825D613C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,-20744
	r11.s64 = r11.s64 + -20744;
	// addi r10,r10,-360
	ctx.r10.s64 = ctx.r10.s64 + -360;
	// addi r9,r9,2312
	ctx.r9.s64 = ctx.r9.s64 + 2312;
	// li r8,340
	ctx.r8.s64 = 340;
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// sth r8,314(r3)
	PPC_STORE_U16(ctx.r3.u32 + 314, ctx.r8.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D616C"))) PPC_WEAK_FUNC(sub_825D616C);
PPC_FUNC_IMPL(__imp__sub_825D616C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D6170"))) PPC_WEAK_FUNC(sub_825D6170);
PPC_FUNC_IMPL(__imp__sub_825D6170) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stfd f31,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r10,580(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d6324
	if (!cr6.lt) goto loc_825D6324;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f31,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f31.f64 = double(temp.f32);
loc_825D61B0:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// lwz r9,584(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,424(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 424);
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// beq cr6,0x825d62a4
	if (cr6.eq) goto loc_825D62A4;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d62e0
	if (!cr6.eq) goto loc_825D62E0;
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r29,148(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bge cr6,0x825d6270
	if (!cr6.lt) goto loc_825D6270;
	// addi r28,r31,224
	r28.s64 = r31.s64 + 224;
loc_825D620C:
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d6230
	if (cr6.eq) goto loc_825D6230;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x825d6230
	if (cr6.eq) goto loc_825D6230;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// li r4,4
	ctx.r4.s64 = 4;
	// bne cr6,0x825d6234
	if (!cr6.eq) goto loc_825D6234;
loc_825D6230:
	// li r4,3
	ctx.r4.s64 = 3;
loc_825D6234:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6328
	if (cr6.lt) goto loc_825D6328;
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stbx r10,r11,r29
	PPC_STORE_U8(r11.u32 + r29.u32, ctx.r10.u8);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// blt cr6,0x825d620c
	if (cr6.lt) goto loc_825D620C;
loc_825D6270:
	// li r6,10
	ctx.r6.s64 = 10;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825dee00
	sub_825DEE00(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825df278
	sub_825DF278(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x825d6324
	if (cr6.lt) goto loc_825D6324;
	// b 0x825d62fc
	goto loc_825D62FC;
loc_825D62A4:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d62e0
	if (!cr6.eq) goto loc_825D62E0;
	// lhz r11,118(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 118);
	// stfs f31,156(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r30.u32 + 156, temp.u32);
	// lwz r10,52(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 52);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d62fc
	if (!cr6.gt) goto loc_825D62FC;
loc_825D62C4:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stfs f31,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d62c4
	if (cr6.gt) goto loc_825D62C4;
	// b 0x825d62fc
	goto loc_825D62FC;
loc_825D62E0:
	// lhz r11,114(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 114);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d62fc
	if (!cr6.gt) goto loc_825D62FC;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825dc590
	sub_825DC590(ctx, base);
loc_825D62FC:
	// sth r26,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r26.u16);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d61b0
	if (cr6.lt) goto loc_825D61B0;
loc_825D6324:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
loc_825D6328:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f31,-64(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D6334"))) PPC_WEAK_FUNC(sub_825D6334);
PPC_FUNC_IMPL(__imp__sub_825D6334) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D6338"))) PPC_WEAK_FUNC(sub_825D6338);
PPC_FUNC_IMPL(__imp__sub_825D6338) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// beq cr6,0x825d6370
	if (cr6.eq) goto loc_825D6370;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x825d64f8
	if (cr6.eq) goto loc_825D64F8;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_825D6370:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// lhz r10,34(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d64e8
	if (!cr6.lt) goto loc_825D64E8;
	// addi r25,r31,224
	r25.s64 = r31.s64 + 224;
loc_825D6388:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r9,304(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 304);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r8,400(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 400);
	// lwz r10,320(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// subf r4,r8,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6638
	if (cr6.lt) goto loc_825D6638;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d64bc
	if (cr6.eq) goto loc_825D64BC;
	// lwz r26,12(r28)
	r26.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// stb r24,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r24.u8);
	// lwz r11,404(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 404);
	// lwz r10,264(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 264);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r11.u32);
loc_825D63DC:
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r9,308(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 308);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// lwz r11,404(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 404);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r11
	r29.u64 = r11.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825d6408
	if (cr6.gt) goto loc_825D6408;
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
loc_825D6408:
	// lwz r11,268(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 268);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// bge cr6,0x825d64a0
	if (!cr6.lt) goto loc_825D64A0;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d6428
	if (cr6.lt) goto loc_825D6428;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
loc_825D6428:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6638
	if (cr6.lt) goto loc_825D6638;
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stbx r10,r11,r26
	PPC_STORE_U8(r11.u32 + r26.u32, ctx.r10.u8);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lbzx r11,r11,r26
	r11.u64 = PPC_LOAD_U8(r11.u32 + r26.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d6480
	if (cr6.eq) goto loc_825D6480;
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r11,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r11.u8);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
	// b 0x825d63dc
	goto loc_825D63DC;
loc_825D6480:
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 36);
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stw r11,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r11.u32);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
	// b 0x825d63dc
	goto loc_825D63DC;
loc_825D64A0:
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r10,304(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 304);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d64c0
	if (!cr6.lt) goto loc_825D64C0;
	// stbx r24,r11,r26
	PPC_STORE_U8(r11.u32 + r26.u32, r24.u8);
	// b 0x825d64c0
	goto loc_825D64C0;
loc_825D64BC:
	// stw r24,36(r28)
	PPC_STORE_U32(r28.u32 + 36, r24.u32);
loc_825D64C0:
	// lwz r11,400(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 400);
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,34(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d6388
	if (cr6.lt) goto loc_825D6388;
loc_825D64E8:
	// li r11,7
	r11.s64 = 7;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// sth r24,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r24.u16);
	// sth r24,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r24.u16);
loc_825D64F8:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// lhz r10,34(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d6638
	if (!cr6.lt) goto loc_825D6638;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r26,r11,-22640
	r26.s64 = r11.s64 + -22640;
loc_825D6514:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// lwz r10,320(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d6614
	if (cr6.eq) goto loc_825D6614;
	// lwz r29,12(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r30,20(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d6614
	if (cr6.eq) goto loc_825D6614;
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d6584
	if (!cr6.eq) goto loc_825D6584;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,7
	ctx.r4.s64 = 7;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6638
	if (cr6.lt) goto loc_825D6638;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,-19
	r11.s64 = r11.s64 + -19;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
loc_825D6584:
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d6614
	if (!cr6.lt) goto loc_825D6614;
	// addi r28,r31,224
	r28.s64 = r31.s64 + 224;
loc_825D659C:
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6638
	if (cr6.lt) goto loc_825D6638;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6638
	if (cr6.lt) goto loc_825D6638;
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,-18
	ctx.r10.s64 = ctx.r10.s64 + -18;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lhz r11,152(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 152);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d659c
	if (cr6.lt) goto loc_825D659C;
loc_825D6614:
	// sth r24,152(r31)
	PPC_STORE_U16(r31.u32 + 152, r24.u16);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,34(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 34);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d6514
	if (cr6.lt) goto loc_825D6514;
loc_825D6638:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825D6640"))) PPC_WEAK_FUNC(sub_825D6640);
PPC_FUNC_IMPL(__imp__sub_825D6640) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// lfs f0,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// blt cr6,0x825d6698
	if (cr6.lt) goto loc_825D6698;
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
loc_825D6668:
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// fmadds f0,f13,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + f0.f64));
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lfs f10,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// fmadds f0,f12,f12,f0
	f0.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + f0.f64));
	// fmadds f0,f11,f11,f0
	f0.f64 = double(float(ctx.f11.f64 * ctx.f11.f64 + f0.f64));
	// fmadds f0,f10,f10,f0
	f0.f64 = double(float(ctx.f10.f64 * ctx.f10.f64 + f0.f64));
	// bne cr6,0x825d6668
	if (!cr6.eq) goto loc_825D6668;
loc_825D6698:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d66b8
	if (!cr6.gt) goto loc_825D66B8;
loc_825D66A0:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// fmadds f0,f13,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + f0.f64));
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x825d66a0
	if (cr6.gt) goto loc_825D66A0;
loc_825D66B8:
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f13,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 / ctx.f13.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D66D4"))) PPC_WEAK_FUNC(sub_825D66D4);
PPC_FUNC_IMPL(__imp__sub_825D66D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D66D8"))) PPC_WEAK_FUNC(sub_825D66D8);
PPC_FUNC_IMPL(__imp__sub_825D66D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, f31.u64);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// li r22,0
	r22.s64 = 0;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d683c
	if (cr6.eq) goto loc_825D683C;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r23,0
	r23.s64 = 0;
	// lfs f31,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f31.f64 = double(temp.f32);
loc_825D670C:
	// lwz r10,320(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d6824
	if (cr6.eq) goto loc_825D6824;
	// lwz r10,320(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r9,400(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 400);
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + r23.u64;
	// lwz r8,308(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 308);
	// lwz r27,404(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 404);
	// lwz r29,268(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 268);
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r8,r11
	r30.u64 = ctx.r8.u64 + r11.u64;
	// lwz r24,12(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r25,16(r10)
	r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lwz r28,148(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 148);
	// add r26,r9,r24
	r26.u64 = ctx.r9.u64 + r24.u64;
loc_825D6754:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// ble cr6,0x825d6764
	if (!cr6.gt) goto loc_825D6764;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_825D6764:
	// cmpw cr6,r10,r29
	cr6.compare<int32_t>(ctx.r10.s32, r29.s32, xer);
	// bge cr6,0x825d67bc
	if (!cr6.lt) goto loc_825D67BC;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmpw cr6,r29,r9
	cr6.compare<int32_t>(r29.s32, ctx.r9.s32, xer);
	// bge cr6,0x825d677c
	if (!cr6.lt) goto loc_825D677C;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_825D677C:
	// lbz r11,0(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d67b0
	if (!cr6.eq) goto loc_825D67B0;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// clrlwi r31,r7,24
	r31.u64 = ctx.r7.u32 & 0xFF;
	// bl 0x825d6640
	sub_825D6640(ctx, base);
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r31,1
	ctx.r9.s64 = r31.s64 + 1;
	// clrlwi r7,r9,24
	ctx.r7.u64 = ctx.r9.u32 & 0xFF;
	// stfsx f1,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, temp.u32);
loc_825D67B0:
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// b 0x825d6754
	goto loc_825D6754;
loc_825D67BC:
	// clrlwi r8,r7,24
	ctx.r8.u64 = ctx.r7.u32 & 0xFF;
	// addi r10,r8,-1
	ctx.r10.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d680c
	if (!cr6.gt) goto loc_825D680C;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// subf r9,r25,r6
	ctx.r9.s64 = ctx.r6.s64 - r25.s64;
	// lfs f0,-4(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -4);
	f0.f64 = double(temp.f32);
	// fdivs f0,f31,f0
	f0.f64 = double(float(f31.f64 / f0.f64));
loc_825D67EC:
	// lfsx f13,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// fsqrts f13,f13
	ctx.f13.f64 = double(float(sqrt(ctx.f13.f64)));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825d67ec
	if (!cr6.eq) goto loc_825D67EC;
loc_825D680C:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x825d6820
	if (cr6.eq) goto loc_825D6820;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// stfs f31,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
loc_825D6820:
	// stb r7,0(r24)
	PPC_STORE_U8(r24.u32 + 0, ctx.r7.u8);
loc_825D6824:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r23,r23,1776
	r23.s64 = r23.s64 + 1776;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// cmpw cr6,r22,r10
	cr6.compare<int32_t>(r22.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d670c
	if (cr6.lt) goto loc_825D670C;
loc_825D683C:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// lfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_825D6848"))) PPC_WEAK_FUNC(sub_825D6848);
PPC_FUNC_IMPL(__imp__sub_825D6848) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r30,0(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d68a4
	if (cr6.eq) goto loc_825D68A4;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,36(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// bl 0x825d5ee0
	sub_825D5EE0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d68a8
	if (cr6.lt) goto loc_825D68A8;
	// lhz r11,490(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 490);
	// lhz r10,730(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 730);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x825d689c
	if (!cr6.gt) goto loc_825D689C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_825D689C:
	// sth r11,730(r30)
	PPC_STORE_U16(r30.u32 + 730, r11.u16);
	// b 0x825d68a8
	goto loc_825D68A8;
loc_825D68A4:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
loc_825D68A8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D68C0"))) PPC_WEAK_FUNC(sub_825D68C0);
PPC_FUNC_IMPL(__imp__sub_825D68C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r4,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r4.u32);
	// li r26,0
	r26.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r10,580(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d69c4
	if (!cr6.lt) goto loc_825D69C4;
loc_825D68F8:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// mr r27,r26
	r27.u64 = r26.u64;
	// lwz r9,584(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 584);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r10,320(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 320);
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d6968
	if (cr6.eq) goto loc_825D6968;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,36(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d5ee0
	sub_825D5EE0(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x825d69c0
	if (cr6.lt) goto loc_825D69C0;
	// lhz r11,490(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 490);
	// lhz r10,730(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 730);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x825d6964
	if (!cr6.gt) goto loc_825D6964;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_825D6964:
	// sth r11,730(r28)
	PPC_STORE_U16(r28.u32 + 730, r11.u16);
loc_825D6968:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x825d69c0
	if (cr6.lt) goto loc_825D69C0;
	// lwz r11,60(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 60);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d6990
	if (!cr6.eq) goto loc_825D6990;
	// addi r11,r31,224
	r11.s64 = r31.s64 + 224;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r10,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r10.u32);
loc_825D6990:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// sth r26,202(r29)
	PPC_STORE_U16(r29.u32 + 202, r26.u16);
	// bl 0x825b3720
	sub_825B3720(ctx, base);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d68f8
	if (cr6.lt) goto loc_825D68F8;
loc_825D69C0:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
loc_825D69C4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D69CC"))) PPC_WEAK_FUNC(sub_825D69CC);
PPC_FUNC_IMPL(__imp__sub_825D69CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D69D0"))) PPC_WEAK_FUNC(sub_825D69D0);
PPC_FUNC_IMPL(__imp__sub_825D69D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,116(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 116);
	// lwz r9,120(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 120);
	// mulli r10,r10,152
	ctx.r10.s64 = ctx.r10.s64 * 152;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r31,r10,r9
	r31.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r26,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r26.u32);
	// lwz r10,96(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bne cr6,0x825d6a40
	if (!cr6.eq) goto loc_825D6A40;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r8,148(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// li r9,2
	ctx.r9.s64 = 2;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r9.u32);
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6A40:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// bne cr6,0x825d6b28
	if (!cr6.eq) goto loc_825D6B28;
	// addi r30,r11,224
	r30.s64 = r11.s64 + 224;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6d60
	if (cr6.lt) goto loc_825D6D60;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d6ab8
	if (!cr6.eq) goto loc_825D6AB8;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r10,1
	ctx.r10.s64 = 1;
	// lfs f0,16088(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16088);
	f0.f64 = double(temp.f32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r10.u32);
	// lfs f13,16084(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16084);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6AB8:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6d60
	if (cr6.lt) goto loc_825D6D60;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d6d60
	if (!cr6.eq) goto loc_825D6D60;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,2
	ctx.r9.s64 = 2;
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r9.u32);
	// lfs f13,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lwz r11,148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6B28:
	// addi r29,r11,224
	r29.s64 = r11.s64 + 224;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6d60
	if (cr6.lt) goto loc_825D6D60;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d6c1c
	if (!cr6.eq) goto loc_825D6C1C;
	// li r11,1
	r11.s64 = 1;
	// li r10,2
	ctx.r10.s64 = 2;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r10,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r10.u32);
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// blt cr6,0x825d6be4
	if (cr6.lt) goto loc_825D6BE4;
	// addi r11,r30,-4
	r11.s64 = r30.s64 + -4;
	// addi r8,r30,1
	ctx.r8.s64 = r30.s64 + 1;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r5,r30,1
	ctx.r5.s64 = r30.s64 + 1;
	// addi r6,r30,1
	ctx.r6.s64 = r30.s64 + 1;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// rlwinm r11,r8,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r6,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r30,1
	ctx.r7.s64 = r30.s64 + 1;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_825D6BA8:
	// lwz r4,148(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stfsx f0,r9,r4
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r4.u32, temp.u32);
	// lwz r4,148(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// stfsx f0,r8,r4
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r4.u32, temp.u32);
	// lwz r4,148(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// stfsx f0,r7,r4
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + ctx.r4.u32, temp.u32);
	// lwz r4,148(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// stfsx f0,r6,r4
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + ctx.r4.u32, temp.u32);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// bne cr6,0x825d6ba8
	if (!cr6.eq) goto loc_825D6BA8;
loc_825D6BE4:
	// cmpw cr6,r5,r30
	cr6.compare<int32_t>(ctx.r5.s32, r30.s32, xer);
	// bge cr6,0x825d6d60
	if (!cr6.lt) goto loc_825D6D60;
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r5,r30
	r11.s64 = r30.s64 - ctx.r5.s64;
	// mullw r10,r9,r5
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
loc_825D6BFC:
	// lwz r8,148(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stfsx f0,r10,r8
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bne cr6,0x825d6bfc
	if (!cr6.eq) goto loc_825D6BFC;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6C1C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6d60
	if (cr6.lt) goto loc_825D6D60;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d6d58
	if (!cr6.eq) goto loc_825D6D58;
	// li r11,1
	r11.s64 = 1;
	// li r10,3
	ctx.r10.s64 = 3;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r10.u32);
	// ble cr6,0x825d6d60
	if (!cr6.gt) goto loc_825D6D60;
	// rlwinm r5,r30,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// mr r27,r30
	r27.u64 = r30.u64;
loc_825D6C70:
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// blt cr6,0x825d6cf0
	if (cr6.lt) goto loc_825D6CF0;
	// addi r11,r30,-4
	r11.s64 = r30.s64 + -4;
	// lwz r8,548(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 548);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwzx r11,r8,r5
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r5.u32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r6
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_825D6CA0:
	// lwz r7,148(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// lfs f0,-8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	f0.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stfsx f0,r10,r7
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, temp.u32);
	// lwz r7,148(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// lfs f0,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	f0.f64 = double(temp.f32);
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stfs f0,4(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// lwz r7,148(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stfs f0,8(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// lwz r7,148(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stfs f0,12(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// bne cr6,0x825d6ca0
	if (!cr6.eq) goto loc_825D6CA0;
loc_825D6CF0:
	// cmpw cr6,r8,r30
	cr6.compare<int32_t>(ctx.r8.s32, r30.s32, xer);
	// bge cr6,0x825d6d38
	if (!cr6.lt) goto loc_825D6D38;
	// lwz r11,548(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 548);
	// add r10,r4,r8
	ctx.r10.u64 = ctx.r4.u64 + ctx.r8.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r5
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	// subf r11,r8,r30
	r11.s64 = r30.s64 - ctx.r8.s64;
	// lwzx r10,r10,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
loc_825D6D18:
	// lwz r8,148(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stfsx f0,r8,r9
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825d6d18
	if (!cr6.eq) goto loc_825D6D18;
loc_825D6D38:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// add r29,r5,r29
	r29.u64 = ctx.r5.u64 + r29.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x825d6c70
	if (!cr6.eq) goto loc_825D6C70;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6D58:
	// stw r26,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r26.u32);
	// stw r26,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r26.u32);
loc_825D6D60:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D6D68"))) PPC_WEAK_FUNC(sub_825D6D68);
PPC_FUNC_IMPL(__imp__sub_825D6D68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// li r26,24
	r26.s64 = 24;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// lwz r9,304(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// bgt cr6,0x825d6da0
	if (cr6.gt) goto loc_825D6DA0;
	// mr r26,r11
	r26.u64 = r11.u64;
loc_825D6DA0:
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// ble cr6,0x825d6f8c
	if (!cr6.gt) goto loc_825D6F8C;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825d6dd4
	if (!cr6.eq) goto loc_825D6DD4;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x825d6ddc
	if (!cr6.eq) goto loc_825D6DDC;
loc_825D6DD4:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d6f8c
	if (!cr6.eq) goto loc_825D6F8C;
loc_825D6DDC:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x825d6e74
	if (cr6.eq) goto loc_825D6E74;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x825d6fc8
	if (!cr6.eq) goto loc_825D6FC8;
	// subf r11,r26,r9
	r11.s64 = ctx.r9.s64 - r26.s64;
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6fd0
	if (cr6.lt) goto loc_825D6FD0;
	// lwz r11,304(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// addi r30,r26,-1
	r30.s64 = r26.s64 + -1;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bge cr6,0x825d6fc8
	if (!cr6.lt) goto loc_825D6FC8;
loc_825D6E1C:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6fd0
	if (cr6.lt) goto loc_825D6FD0;
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,38
	r11.s64 = r11.s64 * 38;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,304(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825d6e1c
	if (cr6.lt) goto loc_825D6E1C;
	// li r11,7
	r11.s64 = 7;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6E74:
	// addi r27,r31,224
	r27.s64 = r31.s64 + 224;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6fd0
	if (cr6.lt) goto loc_825D6FD0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6fd0
	if (cr6.lt) goto loc_825D6FD0;
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d6f18
	if (!cr6.eq) goto loc_825D6F18;
	// lwz r10,304(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825d6fc8
	if (!cr6.gt) goto loc_825D6FC8;
	// li r9,1
	ctx.r9.s64 = 1;
loc_825D6EDC:
	// lwz r10,116(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r8,120(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r10,r10,38
	ctx.r10.s64 = ctx.r10.s64 * 38;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// lwz r10,304(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d6edc
	if (cr6.lt) goto loc_825D6EDC;
	// li r11,7
	r11.s64 = 7;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6F18:
	// addi r29,r26,-1
	r29.s64 = r26.s64 + -1;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x825d6f68
	if (!cr6.gt) goto loc_825D6F68;
loc_825D6F24:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d6fd0
	if (cr6.lt) goto loc_825D6FD0;
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,38
	r11.s64 = r11.s64 * 38;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// cmpw cr6,r30,r29
	cr6.compare<int32_t>(r30.s32, r29.s32, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// blt cr6,0x825d6f24
	if (cr6.lt) goto loc_825D6F24;
loc_825D6F68:
	// lwz r11,304(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subf r11,r26,r11
	r11.s64 = r11.s64 - r26.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825D6F8C:
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825d6fc8
	if (!cr6.gt) goto loc_825D6FC8;
	// li r9,1
	ctx.r9.s64 = 1;
loc_825D6F9C:
	// lwz r10,116(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r8,120(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r10,r10,38
	ctx.r10.s64 = ctx.r10.s64 * 38;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,6
	ctx.r10.s64 = ctx.r10.s64 + 6;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// lwz r10,304(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 304);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d6f9c
	if (cr6.lt) goto loc_825D6F9C;
loc_825D6FC8:
	// li r11,7
	r11.s64 = 7;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
loc_825D6FD0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D6FD8"))) PPC_WEAK_FUNC(sub_825D6FD8);
PPC_FUNC_IMPL(__imp__sub_825D6FD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r22,r28
	r22.u64 = r28.u64;
	// lwz r30,0(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// lwz r11,116(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 116);
	// lhz r10,34(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d7018
	if (cr6.lt) goto loc_825D7018;
loc_825D7008:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
loc_825D7018:
	// lwz r10,120(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,92(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 92);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d70b0
	if (cr6.gt) goto loc_825D70B0;
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d70a4
	if (cr6.eq) goto loc_825D70A4;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r25,1
	r25.s64 = 1;
loc_825D7064:
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// xori r7,r7,1
	ctx.r7.u64 = ctx.r7.u64 ^ 1;
	// stwx r7,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r25,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r25.u32);
	// lhz r9,34(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d7064
	if (cr6.lt) goto loc_825D7064;
loc_825D70A4:
	// lwz r11,92(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 92);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x825d7148
	goto loc_825D7148;
loc_825D70B0:
	// lhz r11,580(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 580);
	// mr r23,r28
	r23.u64 = r28.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d7148
	if (!cr6.gt) goto loc_825D7148;
	// mr r24,r28
	r24.u64 = r28.u64;
	// li r25,1
	r25.s64 = 1;
loc_825D70CC:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// lwz r26,8(r29)
	r26.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lhzx r11,r24,r11
	r11.u64 = PPC_LOAD_U16(r24.u32 + r11.u32);
	// extsh r28,r11
	r28.s64 = r11.s16;
	// rlwinm r27,r28,3,0,28
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r27,r26
	r11.u64 = PPC_LOAD_U32(r27.u32 + r26.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d7130
	if (!cr6.eq) goto loc_825D7130;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r29,224
	ctx.r3.s64 = r29.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x825d7154
	if (cr6.lt) goto loc_825D7154;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d7130
	if (!cr6.eq) goto loc_825D7130;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r25,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r25.u32);
	// stwx r25,r27,r26
	PPC_STORE_U32(r27.u32 + r26.u32, r25.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_825D7130:
	// lhz r11,580(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 580);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r24,r24,2
	r24.s64 = r24.s64 + 2;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r23,r11
	cr6.compare<int32_t>(r23.s32, r11.s32, xer);
	// blt cr6,0x825d70cc
	if (cr6.lt) goto loc_825D70CC;
loc_825D7148:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// blt cr6,0x825d7008
	if (cr6.lt) goto loc_825D7008;
loc_825D7154:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825D7160"))) PPC_WEAK_FUNC(sub_825D7160);
PPC_FUNC_IMPL(__imp__sub_825D7160) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// lwz r11,60(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d719c
	if (cr6.gt) goto loc_825D719C;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r26,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r26.u16);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
loc_825D719C:
	// lhz r11,580(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 580);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d71e4
	if (!cr6.eq) goto loc_825D71E4;
	// lwz r11,584(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 584);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,320(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// li r3,0
	ctx.r3.s64 = 0;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r26,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r26.u16);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// stb r26,180(r31)
	PPC_STORE_U8(r31.u32 + 180, r26.u8);
	// bl 0x825c9b70
	sub_825C9B70(ctx, base);
	// stfs f1,196(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r31.u32 + 196, temp.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
loc_825D71E4:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// cmplwi cr6,r11,65535
	cr6.compare<uint32_t>(r11.u32, 65535, xer);
	// bne cr6,0x825d721c
	if (!cr6.eq) goto loc_825D721C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,3
	ctx.r4.s64 = 3;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d732c
	if (cr6.lt) goto loc_825D732C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// sth r26,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r26.u16);
	// stw r11,132(r31)
	PPC_STORE_U32(r31.u32 + 132, r11.u32);
loc_825D721C:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// lhz r10,580(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d732c
	if (!cr6.lt) goto loc_825D732C;
	// addi r27,r31,224
	r27.s64 = r31.s64 + 224;
	// li r25,1
	r25.s64 = 1;
loc_825D723C:
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r9,584(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 584);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// lwz r11,132(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// lwz r10,320(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 320);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// lhzx r11,r8,r9
	r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d732c
	if (cr6.lt) goto loc_825D732C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d732c
	if (cr6.lt) goto loc_825D732C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d72b4
	if (!cr6.eq) goto loc_825D72B4;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r26,180(r29)
	PPC_STORE_U8(r29.u32 + 180, r26.u8);
	// b 0x825d72fc
	goto loc_825D72FC;
loc_825D72B4:
	// lwz r4,132(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 132);
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825d72cc
	if (!cr6.eq) goto loc_825D72CC;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r25,180(r29)
	PPC_STORE_U8(r29.u32 + 180, r25.u8);
	// b 0x825d72fc
	goto loc_825D72FC;
loc_825D72CC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d732c
	if (cr6.lt) goto loc_825D732C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stb r11,180(r29)
	PPC_STORE_U8(r29.u32 + 180, r11.u8);
loc_825D72FC:
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825c9b70
	sub_825C9B70(ctx, base);
	// stfs f1,196(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r29.u32 + 196, temp.u32);
	// lhz r11,150(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 150);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,150(r31)
	PPC_STORE_U16(r31.u32 + 150, r11.u16);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lhz r10,580(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d723c
	if (cr6.lt) goto loc_825D723C;
loc_825D732C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825D7338"))) PPC_WEAK_FUNC(sub_825D7338);
PPC_FUNC_IMPL(__imp__sub_825D7338) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// mr r25,r29
	r25.u64 = r29.u64;
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// lhz r11,34(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d7408
	if (!cr6.eq) goto loc_825D7408;
	// lwz r11,744(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 744);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d7408
	if (cr6.eq) goto loc_825D7408;
	// li r24,1
	r24.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r24,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r24.u32);
	// stw r24,572(r26)
	PPC_STORE_U32(r26.u32 + 572, r24.u32);
	// lwz r30,120(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// stw r24,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r24.u32);
	// lhz r11,34(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r5,112
	ctx.r5.s64 = 112;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r30,24
	ctx.r3.s64 = r30.s64 + 24;
	// stw r24,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r24.u32);
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// stw r24,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r24.u32);
	// stw r24,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r24.u32);
	// stw r29,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r29.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,148(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r10,2
	ctx.r10.s64 = 2;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r9,148(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// li r8,9
	ctx.r8.s64 = 9;
	// stw r24,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r24.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,16(r30)
	PPC_STORE_U32(r30.u32 + 16, ctx.r10.u32);
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stw r8,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r8.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
loc_825D7408:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// beq cr6,0x825d7814
	if (cr6.eq) goto loc_825D7814;
	// li r24,1
	r24.s64 = 1;
	// li r22,2
	r22.s64 = 2;
	// li r23,3
	r23.s64 = 3;
	// li r19,4
	r19.s64 = 4;
	// li r20,5
	r20.s64 = 5;
	// li r21,8
	r21.s64 = 8;
loc_825D742C:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bgt cr6,0x825d7808
	if (cr6.gt) goto loc_825D7808;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,29776
	r12.s64 = r12.s64 + 29776;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D7474;
	case 1:
		goto loc_825D7558;
	case 2:
		goto loc_825D7590;
	case 3:
		goto loc_825D7634;
	case 4:
		goto loc_825D76BC;
	case 5:
		goto loc_825D7740;
	case 6:
		goto loc_825D7740;
	case 7:
		goto loc_825D7808;
	case 8:
		goto loc_825D778C;
	default:
		__builtin_unreachable();
	}
	// lwz r18,29812(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 29812);
	// lwz r18,30040(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30040);
	// lwz r18,30096(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30096);
	// lwz r18,30260(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30260);
	// lwz r18,30396(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30396);
	// lwz r18,30528(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30528);
	// lwz r18,30528(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30528);
	// lwz r18,30728(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30728);
	// lwz r18,30604(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30604);
loc_825D7474:
	// stw r29,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r29.u32);
	// mr r11,r29
	r11.u64 = r29.u64;
	// lhz r10,580(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 580);
	// stw r29,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r29.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r29,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r29.u32);
	// stw r29,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r29.u32);
	// stw r29,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r29.u32);
	// stw r29,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r29.u32);
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// lhz r10,34(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d74c8
	if (cr6.eq) goto loc_825D74C8;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_825D74AC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r29,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r29.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lhz r9,34(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d74ac
	if (cr6.lt) goto loc_825D74AC;
loc_825D74C8:
	// lhz r11,580(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 580);
	// mr r27,r29
	r27.u64 = r29.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825d7550
	if (!cr6.gt) goto loc_825D7550;
	// mr r28,r29
	r28.u64 = r29.u64;
loc_825D74E0:
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// li r4,0
	ctx.r4.s64 = 0;
	// add r30,r11,r28
	r30.u64 = r11.u64 + r28.u64;
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// lhz r11,34(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// rotlwi r5,r11,2
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 2);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,112
	ctx.r5.s64 = 112;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// addi r3,r30,24
	ctx.r3.s64 = r30.s64 + 24;
	// stw r29,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r29.u32);
	// stw r29,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r29.u32);
	// stw r29,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r29.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,148(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 148);
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,580(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 580);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// addi r28,r28,152
	r28.s64 = r28.s64 + 152;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825d74e0
	if (cr6.lt) goto loc_825D74E0;
loc_825D7550:
	// stw r24,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r24.u32);
	// b 0x825d7808
	goto loc_825D7808;
loc_825D7558:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// b 0x825d7804
	goto loc_825D7804;
loc_825D7590:
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d75bc
	if (!cr6.eq) goto loc_825D75BC;
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d7820
	if (cr6.eq) goto loc_825D7820;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d75bc
	if (!cr6.eq) goto loc_825D75BC;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// beq cr6,0x825d783c
	if (cr6.eq) goto loc_825D783C;
loc_825D75BC:
	// addi r4,r11,3
	ctx.r4.s64 = r11.s64 + 3;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d6fd8
	sub_825D6FD8(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d69d0
	sub_825D69D0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// stw r29,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r29.u32);
	// stw r29,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r29.u32);
	// stw r23,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r23.u32);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r11.u32);
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// stw r11,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r11.u32);
	// b 0x825d7808
	goto loc_825D7808;
loc_825D7634:
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d76b4
	if (!cr6.eq) goto loc_825D76B4;
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d76b4
	if (!cr6.lt) goto loc_825D76B4;
	// addi r28,r31,224
	r28.s64 = r31.s64 + 224;
loc_825D7664:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,136(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 136);
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d7664
	if (cr6.lt) goto loc_825D7664;
loc_825D76B4:
	// stw r19,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r19.u32);
	// b 0x825d7808
	goto loc_825D7808;
loc_825D76BC:
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r11,r11,152
	r11.s64 = r11.s64 * 152;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d7738
	if (!cr6.eq) goto loc_825D7738;
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825d7738
	if (!cr6.lt) goto loc_825D7738;
	// addi r28,r31,224
	r28.s64 = r31.s64 + 224;
loc_825D76EC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,140(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 140);
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stbx r9,r11,r10
	PPC_STORE_U8(r11.u32 + ctx.r10.u32, ctx.r9.u8);
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825d76ec
	if (cr6.lt) goto loc_825D76EC;
loc_825D7738:
	// stw r20,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r20.u32);
	// b 0x825d7808
	goto loc_825D7808;
loc_825D7740:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d6d68
	sub_825D6D68(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x825d7808
	if (!cr6.eq) goto loc_825D7808;
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,120(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// mulli r8,r11,152
	ctx.r8.s64 = r11.s64 * 152;
	// lwz r9,92(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// stw r21,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r21.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r11.u32);
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// b 0x825d7808
	goto loc_825D7808;
loc_825D778C:
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d7804
	if (cr6.eq) goto loc_825D7804;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x825d7814
	if (cr6.lt) goto loc_825D7814;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d7804
	if (!cr6.eq) goto loc_825D7804;
	// stw r29,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r29.u32);
	// mr r11,r29
	r11.u64 = r29.u64;
	// lhz r10,580(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 580);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// stw r10,92(r31)
	PPC_STORE_U32(r31.u32 + 92, ctx.r10.u32);
	// lhz r10,34(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d7804
	if (cr6.eq) goto loc_825D7804;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_825D77E8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r29,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, r29.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lhz r9,34(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 34);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825d77e8
	if (cr6.lt) goto loc_825D77E8;
loc_825D7804:
	// stw r22,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r22.u32);
loc_825D7808:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x825d742c
	if (!cr6.eq) goto loc_825D742C;
loc_825D7814:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
loc_825D7820:
	// li r11,9
	r11.s64 = 9;
	// lwz r10,116(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// stw r10,572(r26)
	PPC_STORE_U32(r26.u32 + 572, ctx.r10.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
loc_825D783C:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825D784C"))) PPC_WEAK_FUNC(sub_825D784C);
PPC_FUNC_IMPL(__imp__sub_825D784C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D7850"))) PPC_WEAK_FUNC(sub_825D7850);
PPC_FUNC_IMPL(__imp__sub_825D7850) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -120, f29.u64);
	// stfd f30,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, f30.u64);
	// stfd f31,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, f31.u64);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// mr r28,r27
	r28.u64 = r27.u64;
	// lwz r24,0(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 436);
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// lhz r26,34(r24)
	r26.u64 = PPC_LOAD_U16(r24.u32 + 34);
	// beq cr6,0x825d7ad8
	if (cr6.eq) goto loc_825D7AD8;
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r21,1
	r21.s64 = 1;
	// li r25,4
	r25.s64 = 4;
	// lfs f29,-27636(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27636);
	f29.f64 = double(temp.f32);
	// li r22,3
	r22.s64 = 3;
	// lfs f30,5736(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5736);
	f30.f64 = double(temp.f32);
	// li r23,-16
	r23.s64 = -16;
	// lfs f31,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f31.f64 = double(temp.f32);
loc_825D78B4:
	// lwz r11,436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 436);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x825d7acc
	if (cr6.gt) goto loc_825D7ACC;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,30936
	r12.s64 = r12.s64 + 30936;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D78E8;
	case 1:
		goto loc_825D7960;
	case 2:
		goto loc_825D79AC;
	case 3:
		goto loc_825D7A40;
	default:
		__builtin_unreachable();
	}
	// lwz r18,30952(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 30952);
	// lwz r18,31072(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 31072);
	// lwz r18,31148(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 31148);
	// lwz r18,31296(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 31296);
loc_825D78E8:
	// lwz r11,440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 440);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r11,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r11.u32);
	// beq cr6,0x825d7914
	if (cr6.eq) goto loc_825D7914;
	// lwz r4,448(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x825d7914
	if (cr6.eq) goto loc_825D7914;
	// mullw r11,r26,r26
	r11.s64 = int64_t(r26.s32) * int64_t(r26.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825D7914:
	// lwz r3,448(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// stw r27,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r27.u32);
	// stw r27,444(r31)
	PPC_STORE_U32(r31.u32 + 444, r27.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825d7938
	if (cr6.eq) goto loc_825D7938;
	// mullw r11,r26,r26
	r11.s64 = int64_t(r26.s32) * int64_t(r26.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825D7938:
	// lwz r11,60(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// ble cr6,0x825d7ac8
	if (!cr6.gt) goto loc_825D7AC8;
	// cmpwi cr6,r26,2
	cr6.compare<int32_t>(r26.s32, 2, xer);
	// blt cr6,0x825d7ac8
	if (cr6.lt) goto loc_825D7AC8;
	// lwz r11,176(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 176);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d7ac8
	if (cr6.eq) goto loc_825D7AC8;
	// stw r21,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r21.u32);
	// b 0x825d7acc
	goto loc_825D7ACC;
loc_825D7960:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x825d7ad8
	if (cr6.lt) goto loc_825D7AD8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// stw r11,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r11.u32);
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r10,436(r31)
	PPC_STORE_U32(r31.u32 + 436, ctx.r10.u32);
	// b 0x825d7acc
	goto loc_825D7ACC;
loc_825D79AC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x825d7ad8
	if (cr6.lt) goto loc_825D7AD8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,444(r31)
	PPC_STORE_U32(r31.u32 + 444, r11.u32);
	// beq cr6,0x825d79f4
	if (cr6.eq) goto loc_825D79F4;
	// stw r27,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r27.u32);
	// stw r22,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r22.u32);
	// b 0x825d7acc
	goto loc_825D7ACC;
loc_825D79F4:
	// lhz r11,34(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 34);
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bne cr6,0x825d7a38
	if (!cr6.eq) goto loc_825D7A38;
	// lwz r11,104(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 104);
	// cmplwi cr6,r11,63
	cr6.compare<uint32_t>(r11.u32, 63, xer);
	// bne cr6,0x825d7a38
	if (!cr6.eq) goto loc_825D7A38;
	// lwz r11,448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d7ac8
	if (cr6.eq) goto loc_825D7AC8;
	// stfs f31,140(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 140, temp.u32);
	// stfs f31,112(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 112, temp.u32);
	// stfs f31,84(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 84, temp.u32);
	// stfs f31,28(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stfs f31,0(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f30,52(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 52, temp.u32);
	// stfs f30,48(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 48, temp.u32);
	// b 0x825d7ac8
	goto loc_825D7AC8;
loc_825D7A38:
	// stw r27,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r27.u32);
	// b 0x825d7ac8
	goto loc_825D7AC8;
loc_825D7A40:
	// lwz r11,452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 452);
	// mullw r30,r26,r26
	r30.s64 = int64_t(r26.s32) * int64_t(r26.s32);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x825d7ac8
	if (!cr6.lt) goto loc_825D7AC8;
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
loc_825D7A54:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x825d7ad8
	if (cr6.lt) goto loc_825D7AD8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d7a88
	if (cr6.eq) goto loc_825D7A88;
	// or r11,r11,r23
	r11.u64 = r11.u64 | r23.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
loc_825D7A88:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lwz r10,452(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 452);
	// lwz r9,448(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f29
	f0.f64 = double(float(f0.f64 * f29.f64));
	// stfsx f0,r10,r9
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, temp.u32);
	// lwz r11,452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 452);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825d7a54
	if (cr6.lt) goto loc_825D7A54;
loc_825D7AC8:
	// stw r25,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r25.u32);
loc_825D7ACC:
	// lwz r11,436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 436);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825d78b4
	if (!cr6.eq) goto loc_825D78B4;
loc_825D7AD8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f30,-112(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_825D7AF0"))) PPC_WEAK_FUNC(sub_825D7AF0);
PPC_FUNC_IMPL(__imp__sub_825D7AF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
	// mulli r28,r4,1776
	r28.s64 = ctx.r4.s64 * 1776;
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,320(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// add r25,r11,r28
	r25.u64 = r11.u64 + r28.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d7c44
	if (cr6.lt) goto loc_825D7C44;
	// lhz r11,114(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 114);
	// li r23,1
	r23.s64 = 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x825d7b70
	if (cr6.gt) goto loc_825D7B70;
	// lwz r11,132(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 132);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825d7b70
	if (cr6.eq) goto loc_825D7B70;
	// lwz r11,424(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 424);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stb r23,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r23.u8);
	// b 0x825d7ba0
	goto loc_825D7BA0;
loc_825D7B70:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d7c44
	if (cr6.lt) goto loc_825D7C44;
	// lwz r10,424(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 424);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
loc_825D7BA0:
	// lwz r11,424(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 424);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d7bf4
	if (!cr6.eq) goto loc_825D7BF4;
	// lwz r11,444(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d7bf4
	if (!cr6.eq) goto loc_825D7BF4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x825d7c44
	if (cr6.lt) goto loc_825D7C44;
	// lwz r11,320(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 320);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,436(r11)
	PPC_STORE_U32(r11.u32 + 436, ctx.r10.u32);
loc_825D7BF4:
	// lwz r11,424(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 424);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d7c40
	if (!cr6.eq) goto loc_825D7C40;
	// lwz r11,444(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 444);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d7c40
	if (!cr6.eq) goto loc_825D7C40;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// sth r26,32(r11)
	PPC_STORE_U16(r11.u32 + 32, r26.u16);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r26,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r26.u32);
	// lwz r10,320(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// lwz r9,304(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 304);
	// add r11,r10,r28
	r11.u64 = ctx.r10.u64 + r28.u64;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_825D7C40:
	// stw r23,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r23.u32);
loc_825D7C44:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825D7C50"))) PPC_WEAK_FUNC(sub_825D7C50);
PPC_FUNC_IMPL(__imp__sub_825D7C50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,32767
	r11.s64 = 2147418112;
	// li r3,0
	ctx.r3.s64 = 0;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x825d7cec
	if (!cr6.eq) goto loc_825D7CEC;
	// li r11,0
	r11.s64 = 0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,6
	ctx.r4.s64 = 6;
	// addi r3,r31,224
	ctx.r3.s64 = r31.s64 + 224;
	// stw r11,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r11.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7d80
	if (cr6.lt) goto loc_825D7D80;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// stw r11,136(r31)
	PPC_STORE_U32(r31.u32 + 136, r11.u32);
	// bne cr6,0x825d7cc8
	if (!cr6.eq) goto loc_825D7CC8;
	// li r11,-64
	r11.s64 = -64;
	// or r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | r11.u64;
loc_825D7CC8:
	// lwz r11,296(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// cmpwi cr6,r10,-32
	cr6.compare<int32_t>(ctx.r10.s32, -32, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,296(r30)
	PPC_STORE_U32(r30.u32 + 296, r11.u32);
	// ble cr6,0x825d7ce4
	if (!cr6.gt) goto loc_825D7CE4;
	// cmpwi cr6,r10,31
	cr6.compare<int32_t>(ctx.r10.s32, 31, xer);
	// blt cr6,0x825d7cec
	if (cr6.lt) goto loc_825D7CEC;
loc_825D7CE4:
	// li r11,1
	r11.s64 = 1;
	// stw r11,140(r31)
	PPC_STORE_U32(r31.u32 + 140, r11.u32);
loc_825D7CEC:
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d7d80
	if (cr6.eq) goto loc_825D7D80;
	// addi r29,r31,224
	r29.s64 = r31.s64 + 224;
loc_825D7CFC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7d80
	if (cr6.lt) goto loc_825D7D80;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,31
	cr6.compare<int32_t>(r11.s32, 31, xer);
	// bne cr6,0x825d7d54
	if (!cr6.eq) goto loc_825D7D54;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,296(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// stw r11,296(r30)
	PPC_STORE_U32(r30.u32 + 296, r11.u32);
	// blt cr6,0x825d7d70
	if (cr6.lt) goto loc_825D7D70;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d7cfc
	if (!cr6.eq) goto loc_825D7CFC;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_825D7D54:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,296(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 296);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,296(r30)
	PPC_STORE_U32(r30.u32 + 296, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_825D7D70:
	// li r11,62
	r11.s64 = 62;
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// stw r11,296(r30)
	PPC_STORE_U32(r30.u32 + 296, r11.u32);
loc_825D7D80:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825D7D88"))) PPC_WEAK_FUNC(sub_825D7D88);
PPC_FUNC_IMPL(__imp__sub_825D7D88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// li r25,1
	r25.s64 = 1;
	// li r26,2
	r26.s64 = 2;
	// li r27,3
	r27.s64 = 3;
	// li r28,4
	r28.s64 = 4;
loc_825D7DB4:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bgt cr6,0x825d7db4
	if (cr6.gt) goto loc_825D7DB4;
	// lis r12,-32163
	r12.s64 = -2107834368;
	// addi r12,r12,32216
	r12.s64 = r12.s64 + 32216;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D7DF0;
	case 1:
		goto loc_825D7E94;
	case 2:
		goto loc_825D7EB0;
	case 3:
		goto loc_825D7ECC;
	case 4:
		goto loc_825D7EE8;
	case 5:
		goto loc_825D7F08;
	default:
		__builtin_unreachable();
	}
	// lwz r18,32240(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32240);
	// lwz r18,32404(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32404);
	// lwz r18,32432(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32432);
	// lwz r18,32460(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32460);
	// lwz r18,32488(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32488);
	// lwz r18,32520(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + 32520);
loc_825D7DF0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d8d00
	sub_825D8D00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwinm r11,r11,3,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0x7;
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x825d7e3c
	if (!cr6.eq) goto loc_825D7E3C;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// stw r25,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r25.u32);
	// b 0x825d7db4
	goto loc_825D7DB4;
loc_825D7E3C:
	// rlwinm r10,r11,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x6;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x825d7e60
	if (!cr6.eq) goto loc_825D7E60;
	// li r4,2
	ctx.r4.s64 = 2;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// stw r26,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r26.u32);
	// b 0x825d7db4
	goto loc_825D7DB4;
loc_825D7E60:
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// li r4,3
	ctx.r4.s64 = 3;
	// bne cr6,0x825d7e80
	if (!cr6.eq) goto loc_825D7E80;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// stw r27,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r27.u32);
	// b 0x825d7db4
	goto loc_825D7DB4;
loc_825D7E80:
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// stw r28,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r28.u32);
	// b 0x825d7db4
	goto loc_825D7DB4;
loc_825D7E94:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// b 0x825d7f34
	goto loc_825D7F34;
loc_825D7EB0:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// b 0x825d7f34
	goto loc_825D7F34;
loc_825D7ECC:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// b 0x825d7f34
	goto loc_825D7F34;
loc_825D7EE8:
	// addi r5,r30,52
	ctx.r5.s64 = r30.s64 + 52;
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// li r11,5
	r11.s64 = 5;
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
loc_825D7F08:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d7f48
	if (cr6.lt) goto loc_825D7F48;
	// lwz r11,52(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 52);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_825D7F34:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
loc_825D7F48:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825D7F50"))) PPC_WEAK_FUNC(sub_825D7F50);
PPC_FUNC_IMPL(__imp__sub_825D7F50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r4,r30,3
	ctx.r4.s64 = r30.s64 + 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d8e78
	sub_825D8E78(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825d8d00
	sub_825D8D00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d7fc8
	if (!cr6.eq) goto loc_825D7FC8;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_825D7FC8:
	// rlwinm r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d8014
	if (!cr6.eq) goto loc_825D8014;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_825D8014:
	// rlwinm r11,r11,0,2,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000000;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d8060
	if (!cr6.eq) goto loc_825D8060;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8068
	if (cr6.lt) goto loc_825D8068;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
loc_825D8060:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_825D8068:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825D8070"))) PPC_WEAK_FUNC(sub_825D8070);
PPC_FUNC_IMPL(__imp__sub_825D8070) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r28,r26,224
	r28.s64 = r26.s64 + 224;
	// addi r31,r26,516
	r31.s64 = r26.s64 + 516;
	// li r25,17
	r25.s64 = 17;
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// li r27,0
	r27.s64 = 0;
loc_825D8098:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,-15
	r11.s64 = r11.s64 + -15;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x825d8098
	if (cr6.gt) goto loc_825D8098;
	// lis r12,-32162
	r12.s64 = -2107768832;
	// addi r12,r12,-32576
	r12.s64 = r12.s64 + -32576;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D80D0;
	case 1:
		goto loc_825D81EC;
	case 2:
		goto loc_825D81BC;
	case 3:
		goto loc_825D8228;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-32560(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -32560);
	// lwz r18,-32276(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -32276);
	// lwz r18,-32324(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -32324);
	// lwz r18,-32216(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -32216);
loc_825D80D0:
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// lwz r3,24(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d811c
	if (!cr6.eq) goto loc_825D811C;
	// stw r25,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r25.u32);
	// stw r27,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r27.u32);
	// b 0x825d8098
	goto loc_825D8098;
loc_825D811C:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d8150
	if (!cr6.eq) goto loc_825D8150;
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// stw r27,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r27.u32);
	// lwz r10,36(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r27,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r27.u32);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
loc_825D8150:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r27,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r27.u32);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// lwz r10,32(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r11,r11,1,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
loc_825D81BC:
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x825d7d88
	sub_825D7D88(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_825D81EC:
	// lhz r11,312(r26)
	r11.u64 = PPC_LOAD_U16(r26.u32 + 312);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x825d7f50
	sub_825D7F50(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// li r9,18
	ctx.r9.s64 = 18;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// stw r27,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r27.u32);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
loc_825D8228:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8254
	if (cr6.lt) goto loc_825D8254;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r10,15
	ctx.r10.s64 = 15;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_825D8254:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825D825C"))) PPC_WEAK_FUNC(sub_825D825C);
PPC_FUNC_IMPL(__imp__sub_825D825C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8260"))) PPC_WEAK_FUNC(sub_825D8260);
PPC_FUNC_IMPL(__imp__sub_825D8260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r11,124(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 124);
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// beq cr6,0x825d82a0
	if (cr6.eq) goto loc_825D82A0;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x825d82e8
	if (cr6.eq) goto loc_825D82E8;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
loc_825D82A0:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r29,r30,224
	r29.s64 = r30.s64 + 224;
	// addi r3,r11,17728
	ctx.r3.s64 = r11.s64 + 17728;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d83c8
	if (cr6.lt) goto loc_825D83C8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d83c8
	if (cr6.lt) goto loc_825D83C8;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d8340
	if (!cr6.eq) goto loc_825D8340;
loc_825D82E8:
	// li r11,3
	r11.s64 = 3;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,14
	ctx.r4.s64 = 14;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// stw r11,124(r30)
	PPC_STORE_U32(r30.u32 + 124, r11.u32);
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d83c8
	if (cr6.lt) goto loc_825D83C8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// sth r11,30(r31)
	PPC_STORE_U16(r31.u32 + 30, r11.u16);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,0,26,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3E;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// sth r11,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r11.u16);
	// stw r28,124(r30)
	PPC_STORE_U32(r30.u32 + 124, r28.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
loc_825D8340:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825d836c
	if (!cr6.eq) goto loc_825D836C;
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// lwz r10,304(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 304);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// sth r28,30(r31)
	PPC_STORE_U16(r31.u32 + 30, r28.u16);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// sth r11,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r11.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
loc_825D836C:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d83c8
	if (cr6.lt) goto loc_825D83C8;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r10,r11,-2
	ctx.r10.s64 = r11.s64 + -2;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,19088
	r11.s64 = r11.s64 + 19088;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r10,r10,19328
	ctx.r10.s64 = ctx.r10.s64 + 19328;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// sth r11,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r11.u16);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// sth r11,30(r31)
	PPC_STORE_U16(r31.u32 + 30, r11.u16);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r11,r11,1,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
loc_825D83C8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825D83D0"))) PPC_WEAK_FUNC(sub_825D83D0);
PPC_FUNC_IMPL(__imp__sub_825D83D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r27,0
	r27.s64 = 0;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r29,4(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x825d849c
	if (!cr6.lt) goto loc_825D849C;
loc_825D8408:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825d8260
	sub_825D8260(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d849c
	if (cr6.lt) goto loc_825D849C;
	// lhz r9,30(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// lhz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 28);
	// xor r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 ^ r11.u64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// sth r11,30(r31)
	PPC_STORE_U16(r31.u32 + 30, r11.u16);
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x825d849c
	if (!cr6.lt) goto loc_825D849C;
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sth r11,32(r31)
	PPC_STORE_U16(r31.u32 + 32, r11.u16);
	// lhz r8,30(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// lwzx r11,r10,r29
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwx r11,r9,r29
	PPC_STORE_U32(ctx.r9.u32 + r29.u32, r11.u32);
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sth r11,32(r31)
	PPC_STORE_U16(r31.u32 + 32, r11.u16);
	// stw r27,124(r28)
	PPC_STORE_U32(r28.u32 + 124, r27.u32);
	// lhz r11,32(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x825d8408
	if (cr6.lt) goto loc_825D8408;
loc_825D849C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825D84A4"))) PPC_WEAK_FUNC(sub_825D84A4);
PPC_FUNC_IMPL(__imp__sub_825D84A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D84A8"))) PPC_WEAK_FUNC(sub_825D84A8);
PPC_FUNC_IMPL(__imp__sub_825D84A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r14,r4
	r14.u64 = ctx.r4.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// li r17,0
	r17.s64 = 0;
	// addi r29,r25,224
	r29.s64 = r25.s64 + 224;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r24,36(r14)
	r24.u64 = PPC_LOAD_U32(r14.u32 + 36);
	// addi r31,r25,516
	r31.s64 = r25.s64 + 516;
	// lwz r30,0(r25)
	r30.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lis r26,-32244
	r26.s64 = -2113142784;
	// srawi r11,r24,8
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xFF) != 0);
	r11.s64 = r24.s32 >> 8;
	// lis r18,-32244
	r18.s64 = -2113142784;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// lis r19,-32244
	r19.s64 = -2113142784;
	// li r15,2
	r15.s64 = 2;
	// li r23,5
	r23.s64 = 5;
	// lis r16,-32768
	r16.s64 = -2147483648;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r22,r11,21104
	r22.s64 = r11.s64 + 21104;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r27,r11,25848
	r27.s64 = r11.s64 + 25848;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r21,r11,20560
	r21.s64 = r11.s64 + 20560;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r28,r11,25592
	r28.s64 = r11.s64 + 25592;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r20,r11,20080
	r20.s64 = r11.s64 + 20080;
loc_825D8520:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// bgt cr6,0x825d8520
	if (cr6.gt) goto loc_825D8520;
	// lis r12,-32162
	r12.s64 = -2107768832;
	// addi r12,r12,-31416
	r12.s64 = r12.s64 + -31416;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D8584;
	case 1:
		goto loc_825D8660;
	case 2:
		goto loc_825D8700;
	case 3:
		goto loc_825D878C;
	case 4:
		goto loc_825D87EC;
	case 5:
		goto loc_825D8520;
	case 6:
		goto loc_825D8520;
	case 7:
		goto loc_825D8520;
	case 8:
		goto loc_825D8520;
	case 9:
		goto loc_825D8520;
	case 10:
		goto loc_825D8520;
	case 11:
		goto loc_825D8520;
	case 12:
		goto loc_825D8520;
	case 13:
		goto loc_825D88B4;
	case 14:
		goto loc_825D8948;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-31356(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31356);
	// lwz r18,-31136(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31136);
	// lwz r18,-30976(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -30976);
	// lwz r18,-30836(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -30836);
	// lwz r18,-30740(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -30740);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-31456(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -31456);
	// lwz r18,-30540(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -30540);
	// lwz r18,-30392(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -30392);
loc_825D8584:
	// stw r17,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r17.u32);
	// lwz r11,592(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 592);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d85a4
	if (cr6.eq) goto loc_825D85A4;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r10,484(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + 484);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// beq cr6,0x825d8940
	if (cr6.eq) goto loc_825D8940;
loc_825D85A4:
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r24
	cr6.compare<int32_t>(r11.s32, r24.s32, xer);
	// bge cr6,0x825d8968
	if (!cr6.lt) goto loc_825D8968;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,27208(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + 27208);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x825d8618
	if (!cr6.eq) goto loc_825D8618;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r15,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r15.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D8618:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// lbzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// clrlwi r10,r10,28
	ctx.r10.u64 = ctx.r10.u32 & 0xF;
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// lhzx r10,r11,r28
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// rlwinm r10,r10,28,28,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xF;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D8660:
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,27216(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 27216);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x825d86b0
	if (!cr6.eq) goto loc_825D86B0;
loc_825D86A4:
	// li r11,3
	r11.s64 = 3;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D86B0:
	// lbzx r11,r11,r27
	r11.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// rlwinm r10,r11,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r10,r10,28
	ctx.r10.u64 = ctx.r10.u32 & 0xF;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bne cr6,0x825d8520
	if (!cr6.eq) goto loc_825D8520;
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D8700:
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x825d9730
	sub_825D9730(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,27224(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 27224);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x825d8750
	if (!cr6.eq) goto loc_825D8750;
	// li r11,4
	r11.s64 = 4;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D8750:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bne cr6,0x825d877c
	if (!cr6.eq) goto loc_825D877C;
	// stw r15,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r15.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D877C:
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825d8520
	if (!cr6.eq) goto loc_825D8520;
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D878C:
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// lwz r4,27224(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 27224);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x825d7d88
	sub_825D7D88(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bne cr6,0x825d87dc
	if (!cr6.eq) goto loc_825D87DC;
	// stw r15,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r15.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D87DC:
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x825d86a4
	if (!cr6.eq) goto loc_825D86A4;
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D87EC:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d8d00
	sub_825D8D00(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// mr r11,r16
	r11.u64 = r16.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// beq cr6,0x825d882c
	if (cr6.eq) goto loc_825D882C;
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// lis r11,16384
	r11.s64 = 1073741824;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
loc_825D882C:
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d8850
	if (cr6.eq) goto loc_825D8850;
	// subfic r9,r4,31
	xer.ca = ctx.r4.u32 <= 31;
	ctx.r9.s64 = 31 - ctx.r4.s64;
	// and r8,r11,r10
	ctx.r8.u64 = r11.u64 & ctx.r10.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r9.u8 & 0x3F));
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
loc_825D8850:
	// lwz r9,28(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d8874
	if (cr6.eq) goto loc_825D8874;
	// subfic r9,r4,31
	xer.ca = ctx.r4.u32 <= 31;
	ctx.r9.s64 = 31 - ctx.r4.s64;
	// and r8,r11,r10
	ctx.r8.u64 = r11.u64 & ctx.r10.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r9.u8 & 0x3F));
	// stw r9,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r9.u32);
loc_825D8874:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825d8894
	if (cr6.eq) goto loc_825D8894;
	// subfic r9,r4,31
	xer.ca = ctx.r4.u32 <= 31;
	ctx.r9.s64 = 31 - ctx.r4.s64;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// srw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r9.u8 & 0x3F));
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_825D8894:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8960
	if (cr6.lt) goto loc_825D8960;
	// li r11,14
	r11.s64 = 14;
	// stw r17,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r17.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D88B4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r11,5
	ctx.r10.s64 = r11.s64 + 5;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d8984
	if (!cr6.eq) goto loc_825D8984;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// lwz r10,592(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 592);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825d8900
	if (!cr6.eq) goto loc_825D8900;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825d8900
	if (!cr6.gt) goto loc_825D8900;
	// li r11,1
	r11.s64 = 1;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
loc_825D8900:
	// lhz r11,202(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 202);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r24
	cr6.compare<int32_t>(r11.s32, r24.s32, xer);
	// bge cr6,0x825d8968
	if (!cr6.lt) goto loc_825D8968;
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// bne cr6,0x825d8520
	if (!cr6.eq) goto loc_825D8520;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,28,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x825d8520
	goto loc_825D8520;
loc_825D8940:
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_825D8948:
	// lis r11,-32162
	r11.s64 = -2107768832;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// addi r11,r11,-32656
	r11.s64 = r11.s64 + -32656;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,484(r30)
	PPC_STORE_U32(r30.u32 + 484, r11.u32);
	// bl 0x825d8070
	sub_825D8070(ctx, base);
loc_825D8960:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D8968:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// stw r17,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r17.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r17,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r17.u32);
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D8984:
	// lwz r11,592(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 592);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d89a8
	if (!cr6.eq) goto loc_825D89A8;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825d89a8
	if (!cr6.gt) goto loc_825D89A8;
	// li r11,1
	r11.s64 = 1;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
loc_825D89A8:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r17,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r17.u32);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,9
	r11.s64 = r11.s64 + 9;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	r11.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bne cr6,0x825d8960
	if (!cr6.eq) goto loc_825D8960;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d8a14
	if (cr6.eq) goto loc_825D8A14;
	// li r11,15
	r11.s64 = 15;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_825D8A14:
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825D8A24"))) PPC_WEAK_FUNC(sub_825D8A24);
PPC_FUNC_IMPL(__imp__sub_825D8A24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8A28"))) PPC_WEAK_FUNC(sub_825D8A28);
PPC_FUNC_IMPL(__imp__sub_825D8A28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r29,r30,516
	r29.s64 = r30.s64 + 516;
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,816(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 816);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d8ac0
	if (!cr6.eq) goto loc_825D8AC0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r30,224
	ctx.r3.s64 = r30.s64 + 224;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d8b24
	if (cr6.lt) goto loc_825D8B24;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825d8a94
	if (!cr6.eq) goto loc_825D8A94;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,21512
	r11.s64 = r11.s64 + 21512;
	// addi r10,r10,23528
	ctx.r10.s64 = ctx.r10.s64 + 23528;
	// addi r9,r9,24016
	ctx.r9.s64 = ctx.r9.s64 + 24016;
	// li r8,52
	ctx.r8.s64 = 52;
	// b 0x825d8ab0
	goto loc_825D8AB0;
loc_825D8A94:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r11,r11,22464
	r11.s64 = r11.s64 + 22464;
	// addi r10,r10,24504
	ctx.r10.s64 = ctx.r10.s64 + 24504;
	// addi r9,r9,25048
	ctx.r9.s64 = ctx.r9.s64 + 25048;
	// li r8,28
	ctx.r8.s64 = 28;
loc_825D8AB0:
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r9,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r9.u32);
	// sth r8,314(r30)
	PPC_STORE_U16(r30.u32 + 314, ctx.r8.u16);
loc_825D8AC0:
	// li r10,1
	ctx.r10.s64 = 1;
	// li r11,0
	r11.s64 = 0;
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// addi r10,r29,100
	ctx.r10.s64 = r29.s64 + 100;
	// stw r11,16(r29)
	PPC_STORE_U32(r29.u32 + 16, r11.u32);
	// stw r11,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r11.u32);
	// stw r11,56(r29)
	PPC_STORE_U32(r29.u32 + 56, r11.u32);
	// stw r11,60(r29)
	PPC_STORE_U32(r29.u32 + 60, r11.u32);
	// stw r11,64(r29)
	PPC_STORE_U32(r29.u32 + 64, r11.u32);
	// stw r11,132(r29)
	PPC_STORE_U32(r29.u32 + 132, r11.u32);
loc_825D8AEC:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r11,-16(r10)
	PPC_STORE_U32(ctx.r10.u32 + -16, r11.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d8aec
	if (!cr6.eq) goto loc_825D8AEC;
	// lis r11,-32162
	r11.s64 = -2107768832;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r11,r11,-31576
	r11.s64 = r11.s64 + -31576;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,484(r28)
	PPC_STORE_U32(r28.u32 + 484, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_825D8B24:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825D8B2C"))) PPC_WEAK_FUNC(sub_825D8B2C);
PPC_FUNC_IMPL(__imp__sub_825D8B2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8B30"))) PPC_WEAK_FUNC(sub_825D8B30);
PPC_FUNC_IMPL(__imp__sub_825D8B30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,588(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 588);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r10,r10,-30168
	ctx.r10.s64 = ctx.r10.s64 + -30168;
	// stw r9,516(r3)
	PPC_STORE_U32(ctx.r3.u32 + 516, ctx.r9.u32);
	// stw r10,484(r11)
	PPC_STORE_U32(r11.u32 + 484, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D8B58"))) PPC_WEAK_FUNC(sub_825D8B58);
PPC_FUNC_IMPL(__imp__sub_825D8B58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x825d8b68
	if (!cr6.eq) goto loc_825D8B68;
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// blt cr6,0x825d8cf4
	if (cr6.lt) goto loc_825D8CF4;
loc_825D8B68:
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// bgt cr6,0x825d8cf4
	if (cr6.gt) goto loc_825D8CF4;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x825d8cf4
	if (!cr6.gt) goto loc_825D8CF4;
	// cmpwi cr6,r3,8000
	cr6.compare<int32_t>(ctx.r3.s32, 8000, xer);
	// bgt cr6,0x825d8b88
	if (cr6.gt) goto loc_825D8B88;
	// li r11,512
	r11.s64 = 512;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8B88:
	// cmpwi cr6,r3,11025
	cr6.compare<int32_t>(ctx.r3.s32, 11025, xer);
	// bgt cr6,0x825d8b98
	if (cr6.gt) goto loc_825D8B98;
	// li r11,512
	r11.s64 = 512;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8B98:
	// cmpwi cr6,r3,16000
	cr6.compare<int32_t>(ctx.r3.s32, 16000, xer);
	// bgt cr6,0x825d8ba8
	if (cr6.gt) goto loc_825D8BA8;
	// li r11,512
	r11.s64 = 512;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8BA8:
	// cmpwi cr6,r3,22050
	cr6.compare<int32_t>(ctx.r3.s32, 22050, xer);
	// bgt cr6,0x825d8bb8
	if (cr6.gt) goto loc_825D8BB8;
	// li r11,1024
	r11.s64 = 1024;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8BB8:
	// cmpwi cr6,r3,32000
	cr6.compare<int32_t>(ctx.r3.s32, 32000, xer);
	// bgt cr6,0x825d8bd0
	if (cr6.gt) goto loc_825D8BD0;
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// bne cr6,0x825d8bf8
	if (!cr6.eq) goto loc_825D8BF8;
	// li r11,1024
	r11.s64 = 1024;
	// b 0x825d8c6c
	goto loc_825D8C6C;
loc_825D8BD0:
	// lis r11,0
	r11.s64 = 0;
	// ori r11,r11,44100
	r11.u64 = r11.u64 | 44100;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bgt cr6,0x825d8be8
	if (cr6.gt) goto loc_825D8BE8;
	// li r11,2048
	r11.s64 = 2048;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8BE8:
	// lis r11,0
	r11.s64 = 0;
	// ori r11,r11,48000
	r11.u64 = r11.u64 | 48000;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bgt cr6,0x825d8c00
	if (cr6.gt) goto loc_825D8C00;
loc_825D8BF8:
	// li r11,2048
	r11.s64 = 2048;
	// b 0x825d8c18
	goto loc_825D8C18;
loc_825D8C00:
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,30464
	r11.u64 = r11.u64 | 30464;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// li r11,4096
	r11.s64 = 4096;
	// ble cr6,0x825d8c18
	if (!cr6.gt) goto loc_825D8C18;
	// li r11,8192
	r11.s64 = 8192;
loc_825D8C18:
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// bne cr6,0x825d8c68
	if (!cr6.eq) goto loc_825D8C68;
	// rlwinm r10,r6,0,29,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x6;
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// bne cr6,0x825d8c38
	if (!cr6.eq) goto loc_825D8C38;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
loc_825D8C38:
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x825d8c50
	if (!cr6.eq) goto loc_825D8C50;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
loc_825D8C50:
	// cmplwi cr6,r10,6
	cr6.compare<uint32_t>(ctx.r10.u32, 6, xer);
	// bne cr6,0x825d8cec
	if (!cr6.eq) goto loc_825D8CEC;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
loc_825D8C68:
	// bge cr6,0x825d8cec
	if (!cr6.lt) goto loc_825D8CEC;
loc_825D8C6C:
	// srawi r10,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 1;
	// mullw r8,r11,r4
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// twllei r3,0
	// add r9,r10,r8
	ctx.r9.u64 = ctx.r10.u64 + ctx.r8.u64;
	// divwu r9,r9,r3
	ctx.r9.u32 = ctx.r9.u32 / ctx.r3.u32;
	// addi r9,r9,7
	ctx.r9.s64 = ctx.r9.s64 + 7;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d8cb8
	if (!cr6.eq) goto loc_825D8CB8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d8cc8
	if (!cr6.eq) goto loc_825D8CC8;
	// mullw r9,r11,r3
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r3.s32);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// twllei r3,0
	// divwu r9,r9,r3
	ctx.r9.u32 = ctx.r9.u32 / ctx.r3.u32;
	// addi r9,r9,7
	ctx.r9.s64 = ctx.r9.s64 + 7;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
loc_825D8CB8:
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825d8cec
	if (cr6.gt) goto loc_825D8CEC;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d8cec
	if (!cr6.eq) goto loc_825D8CEC;
loc_825D8CC8:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// twllei r3,0
	// mullw r9,r11,r4
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// divwu r9,r9,r3
	ctx.r9.u32 = ctx.r9.u32 / ctx.r3.u32;
	// addi r9,r9,7
	ctx.r9.s64 = ctx.r9.s64 + 7;
	// rlwinm r9,r9,0,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF8;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825d8cc8
	if (cr6.eq) goto loc_825D8CC8;
loc_825D8CEC:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
loc_825D8CF4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D8CFC"))) PPC_WEAK_FUNC(sub_825D8CFC);
PPC_FUNC_IMPL(__imp__sub_825D8CFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8D00"))) PPC_WEAK_FUNC(sub_825D8D00);
PPC_FUNC_IMPL(__imp__sub_825D8D00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// li r23,0
	r23.s64 = 0;
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r28,36(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r27,32(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplw cr6,r30,r25
	cr6.compare<uint32_t>(r30.u32, r25.u32, xer);
	// lwz r29,28(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r26,48(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r24,44(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bge cr6,0x825d8e48
	if (!cr6.lt) goto loc_825D8E48;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825d8d78
	if (cr6.eq) goto loc_825D8D78;
	// subfic r11,r30,32
	xer.ca = r30.u32 <= 32;
	r11.s64 = 32 - r30.s64;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// blt cr6,0x825d8d54
	if (cr6.lt) goto loc_825D8D54;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_825D8D54:
	// subf r26,r11,r26
	r26.s64 = r26.s64 - r11.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r9,r28,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 << (r11.u8 & 0x3F));
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// slw r11,r10,r26
	r11.u64 = r26.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r26.u8 & 0x3F));
	// srw r10,r24,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (r24.u32 >> (r26.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// or r28,r9,r10
	r28.u64 = ctx.r9.u64 | ctx.r10.u64;
	// and r24,r11,r24
	r24.u64 = r11.u64 & r24.u64;
loc_825D8D78:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x825d8dc0
	if (!cr6.eq) goto loc_825D8DC0;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d8dfc
	if (cr6.gt) goto loc_825D8DFC;
loc_825D8D94:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825d8dfc
	if (cr6.eq) goto loc_825D8DFC;
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwinm r10,r28,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 8) & 0xFFFFFF00;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// or r28,r10,r11
	r28.u64 = ctx.r10.u64 | r11.u64;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d8d94
	if (!cr6.gt) goto loc_825D8D94;
	// b 0x825d8dfc
	goto loc_825D8DFC;
loc_825D8DC0:
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d8dfc
	if (cr6.gt) goto loc_825D8DFC;
loc_825D8DC8:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825d8dfc
	if (cr6.eq) goto loc_825D8DFC;
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwimi r3,r28,8,0,23
	ctx.r3.u64 = (__builtin_rotateleft32(r28.u32, 8) & 0xFFFFFF00) | (ctx.r3.u64 & 0xFFFFFFFF000000FF);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d8dc8
	if (!cr6.gt) goto loc_825D8DC8;
loc_825D8DFC:
	// stw r28,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r28.u32);
	// cmplw cr6,r30,r25
	cr6.compare<uint32_t>(r30.u32, r25.u32, xer);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// stw r29,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r29.u32);
	// stw r26,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r26.u32);
	// stw r24,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r24.u32);
	// bge cr6,0x825d8e48
	if (!cr6.lt) goto loc_825D8E48;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// blt cr6,0x825d8e68
	if (cr6.lt) goto loc_825D8E68;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// bge cr6,0x825d8e48
	if (!cr6.lt) goto loc_825D8E48;
	// mr r25,r11
	r25.u64 = r11.u64;
loc_825D8E48:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// subfic r10,r25,32
	xer.ca = r25.u32 <= 32;
	ctx.r10.s64 = 32 - r25.s64;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// slw r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
loc_825D8E68:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825D8E74"))) PPC_WEAK_FUNC(sub_825D8E74);
PPC_FUNC_IMPL(__imp__sub_825D8E74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8E78"))) PPC_WEAK_FUNC(sub_825D8E78);
PPC_FUNC_IMPL(__imp__sub_825D8E78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r31,32(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// lwz r25,48(r30)
	r25.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r28,40(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// lwz r26,36(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lwz r29,28(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// ble cr6,0x825d8fd0
	if (!cr6.gt) goto loc_825D8FD0;
	// lis r11,-32209
	r11.s64 = -2110849024;
	// lwz r10,84(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x825d8f30
	if (!cr6.eq) goto loc_825D8F30;
	// cmplwi cr6,r28,24
	cr6.compare<uint32_t>(r28.u32, 24, xer);
	// bgt cr6,0x825d8efc
	if (cr6.gt) goto loc_825D8EFC;
loc_825D8ED4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825d8efc
	if (cr6.eq) goto loc_825D8EFC;
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// rlwinm r10,r26,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 8) & 0xFFFFFF00;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// or r26,r10,r11
	r26.u64 = ctx.r10.u64 | r11.u64;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmplwi cr6,r28,24
	cr6.compare<uint32_t>(r28.u32, 24, xer);
	// ble cr6,0x825d8ed4
	if (!cr6.gt) goto loc_825D8ED4;
loc_825D8EFC:
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825d8fa8
	if (cr6.eq) goto loc_825D8FA8;
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
loc_825D8F10:
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// rlwinm r10,r27,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 8) & 0xFFFFFF00;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// or r27,r10,r11
	r27.u64 = ctx.r10.u64 | r11.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825d8f10
	if (!cr6.eq) goto loc_825D8F10;
	// b 0x825d8fa8
	goto loc_825D8FA8;
loc_825D8F30:
	// cmplwi cr6,r28,24
	cr6.compare<uint32_t>(r28.u32, 24, xer);
	// bgt cr6,0x825d8f6c
	if (cr6.gt) goto loc_825D8F6C;
loc_825D8F38:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825d8f6c
	if (cr6.eq) goto loc_825D8F6C;
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lwz r11,84(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// rlwimi r3,r26,8,0,23
	ctx.r3.u64 = (__builtin_rotateleft32(r26.u32, 8) & 0xFFFFFF00) | (ctx.r3.u64 & 0xFFFFFFFF000000FF);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,24
	cr6.compare<uint32_t>(r28.u32, 24, xer);
	// ble cr6,0x825d8f38
	if (!cr6.gt) goto loc_825D8F38;
loc_825D8F6C:
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x825d8fa8
	if (cr6.eq) goto loc_825D8FA8;
	// rlwinm r11,r31,3,0,28
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
loc_825D8F80:
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lwz r11,84(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 84);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// rlwimi r3,r27,8,0,23
	ctx.r3.u64 = (__builtin_rotateleft32(r27.u32, 8) & 0xFFFFFF00) | (ctx.r3.u64 & 0xFFFFFFFF000000FF);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x825d8f80
	if (!cr6.eq) goto loc_825D8F80;
loc_825D8FA8:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// stw r26,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r26.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r28,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r28.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r31,32(r30)
	PPC_STORE_U32(r30.u32 + 32, r31.u32);
	// stw r29,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r29.u32);
	// stw r25,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r25.u32);
	// stw r27,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r27.u32);
	// bl 0x825cc190
	sub_825CC190(ctx, base);
loc_825D8FD0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825D8FD8"))) PPC_WEAK_FUNC(sub_825D8FD8);
PPC_FUNC_IMPL(__imp__sub_825D8FD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// mr r19,r4
	r19.u64 = ctx.r4.u64;
	// addi r31,r21,224
	r31.s64 = r21.s64 + 224;
	// li r17,0
	r17.s64 = 0;
	// li r28,23
	r28.s64 = 23;
	// lwz r11,56(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 56);
	// mr r20,r17
	r20.u64 = r17.u64;
	// lwz r23,24(r19)
	r23.u64 = PPC_LOAD_U32(r19.u32 + 24);
	// lwz r25,0(r21)
	r25.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r26,32(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r27,28(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r24,48(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r22,44(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bgt cr6,0x825d9720
	if (cr6.gt) goto loc_825D9720;
	// li r18,1
	r18.s64 = 1;
	// lis r12,-32162
	r12.s64 = -2107768832;
	// addi r12,r12,-28604
	r12.s64 = r12.s64 + -28604;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_825D90B0;
	case 1:
		goto loc_825D9720;
	case 2:
		goto loc_825D9720;
	case 3:
		goto loc_825D9720;
	case 4:
		goto loc_825D9060;
	case 5:
		goto loc_825D9464;
	case 6:
		goto loc_825D964C;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-28496(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -28496);
	// lwz r18,-26848(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -26848);
	// lwz r18,-26848(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -26848);
	// lwz r18,-26848(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -26848);
	// lwz r18,-28576(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -28576);
	// lwz r18,-27548(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -27548);
	// lwz r18,-27060(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -27060);
loc_825D9060:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// clrlwi r5,r11,16
	ctx.r5.u64 = r11.u32 & 0xFFFF;
	// bl 0x825d6008
	sub_825D6008(ctx, base);
	// stw r17,56(r21)
	PPC_STORE_U32(r21.u32 + 56, r17.u32);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r28,23
	r28.s64 = 23;
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r26,32(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r27,28(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r24,48(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r22,44(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 44);
loc_825D90B0:
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// bge cr6,0x825d91d4
	if (!cr6.lt) goto loc_825D91D4;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825d90f0
	if (cr6.eq) goto loc_825D90F0;
	// subfic r11,r30,32
	xer.ca = r30.u32 <= 32;
	r11.s64 = 32 - r30.s64;
	// cmplw cr6,r11,r24
	cr6.compare<uint32_t>(r11.u32, r24.u32, xer);
	// blt cr6,0x825d90d0
	if (cr6.lt) goto loc_825D90D0;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_825D90D0:
	// subf r24,r11,r24
	r24.s64 = r24.s64 - r11.s64;
	// slw r10,r29,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// slw r11,r18,r24
	r11.u64 = r24.u8 & 0x20 ? 0 : (r18.u32 << (r24.u8 & 0x3F));
	// srw r9,r22,r24
	ctx.r9.u64 = r24.u8 & 0x20 ? 0 : (r22.u32 >> (r24.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// or r29,r10,r9
	r29.u64 = ctx.r10.u64 | ctx.r9.u64;
	// and r22,r11,r22
	r22.u64 = r11.u64 & r22.u64;
loc_825D90F0:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x825d9138
	if (!cr6.eq) goto loc_825D9138;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d9174
	if (cr6.gt) goto loc_825D9174;
loc_825D910C:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825d9174
	if (cr6.eq) goto loc_825D9174;
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwinm r10,r29,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFFFFFF00;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// or r29,r10,r11
	r29.u64 = ctx.r10.u64 | r11.u64;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d910c
	if (!cr6.gt) goto loc_825D910C;
	// b 0x825d9174
	goto loc_825D9174;
loc_825D9138:
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d9174
	if (cr6.gt) goto loc_825D9174;
loc_825D9140:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825d9174
	if (cr6.eq) goto loc_825D9174;
	// lbz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwimi r3,r29,8,0,23
	ctx.r3.u64 = (__builtin_rotateleft32(r29.u32, 8) & 0xFFFFFF00) | (ctx.r3.u64 & 0xFFFFFFFF000000FF);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d9140
	if (!cr6.gt) goto loc_825D9140;
loc_825D9174:
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// bge cr6,0x825d91d4
	if (!cr6.lt) goto loc_825D91D4;
	// li r5,23
	ctx.r5.s64 = 23;
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r26,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r26.u32);
	// stw r27,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r27.u32);
	// stw r24,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r24.u32);
	// stw r22,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r22.u32);
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r26,32(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// lwz r27,28(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r24,48(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r22,44(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bge cr6,0x825d91d4
	if (!cr6.lt) goto loc_825D91D4;
	// mr r28,r30
	r28.u64 = r30.u64;
loc_825D91D4:
	// subf r11,r28,r30
	r11.s64 = r30.s64 - r28.s64;
	// subfic r10,r28,32
	xer.ca = r28.u32 <= 32;
	ctx.r10.s64 = 32 - r28.s64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// slw r7,r11,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r11,r7,3,29,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x6;
	// add r10,r11,r23
	ctx.r10.u64 = r11.u64 + r23.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r9,r11,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r9,2,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x1;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r9,r11,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d93d8
	if (!cr6.eq) goto loc_825D93D8;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_825D93D8:
	// rlwinm r4,r11,22,27,31
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 22) & 0x1F;
	// clrlwi r28,r11,22
	r28.u64 = r11.u32 & 0x3FF;
	// cmplwi cr6,r28,1020
	cr6.compare<uint32_t>(r28.u32, 1020, xer);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// blt cr6,0x825d93fc
	if (cr6.lt) goto loc_825D93FC;
	// clrlwi r11,r28,30
	r11.u64 = r28.u32 & 0x3;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r28,r11,r10
	r28.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
loc_825D93FC:
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// cmplw cr6,r30,r4
	cr6.compare<uint32_t>(r30.u32, ctx.r4.u32, xer);
	// slw r29,r7,r4
	r29.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r4.u8 & 0x3F));
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// stw r26,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r26.u32);
	// stw r27,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r27.u32);
	// stw r24,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r24.u32);
	// stw r22,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r22.u32);
	// bge cr6,0x825d943c
	if (!cr6.lt) goto loc_825D943C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// b 0x825d9440
	goto loc_825D9440;
loc_825D943C:
	// subf r11,r4,r30
	r11.s64 = r30.s64 - ctx.r4.s64;
loc_825D9440:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bne cr6,0x825d94a4
	if (!cr6.eq) goto loc_825D94A4;
	// lwz r11,60(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,5
	r11.s64 = 5;
	// stw r11,56(r21)
	PPC_STORE_U32(r21.u32 + 56, r11.u32);
	// ble cr6,0x825d9464
	if (!cr6.gt) goto loc_825D9464;
	// stw r17,24(r25)
	PPC_STORE_U32(r25.u32 + 24, r17.u32);
loc_825D9464:
	// lwz r11,60(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d9540
	if (cr6.gt) goto loc_825D9540;
	// lwz r11,52(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 52);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r4,r11,16
	ctx.r4.u64 = r11.u32 & 0xFFFF;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r10,6
	ctx.r10.s64 = 6;
	// stw r11,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r11.u32);
	// stw r10,56(r21)
	PPC_STORE_U32(r21.u32 + 56, ctx.r10.u32);
	// b 0x825d964c
	goto loc_825D964C;
loc_825D94A4:
	// cmplwi cr6,r28,1
	cr6.compare<uint32_t>(r28.u32, 1, xer);
	// bne cr6,0x825d94d8
	if (!cr6.eq) goto loc_825D94D8;
	// lhz r11,202(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 202);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// stw r17,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r17.u32);
	// lwz r10,36(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + 36);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,16(r25)
	PPC_STORE_U32(r25.u32 + 16, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
loc_825D94D8:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x825d9500
	if (!cr6.lt) goto loc_825D9500;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// b 0x825d9504
	goto loc_825D9504;
loc_825D9500:
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
loc_825D9504:
	// addi r11,r28,-2
	r11.s64 = r28.s64 + -2;
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// lwz r9,28(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + 28);
	// rlwinm r10,r29,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0x1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lhzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// stw r9,16(r25)
	PPC_STORE_U32(r25.u32 + 16, ctx.r9.u32);
	// lwz r9,32(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + 32);
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// stw r10,24(r25)
	PPC_STORE_U32(r25.u32 + 24, ctx.r10.u32);
	// stw r11,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
loc_825D9540:
	// lwz r11,24(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 24);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d95fc
	if (!cr6.eq) goto loc_825D95FC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r30,r17
	r30.u64 = r17.u64;
	// bl 0x825d8d00
	sub_825D8D00(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// li r11,4
	r11.s64 = 4;
	// stw r17,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r17.u32);
	// stw r11,24(r25)
	PPC_STORE_U32(r25.u32 + 24, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d95e4
	if (cr6.eq) goto loc_825D95E4;
	// li r11,16
	r11.s64 = 16;
	// li r10,8
	ctx.r10.s64 = 8;
	// mr r30,r18
	r30.u64 = r18.u64;
	// stw r11,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r11.u32);
	// stw r10,24(r25)
	PPC_STORE_U32(r25.u32 + 24, ctx.r10.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d95e4
	if (cr6.eq) goto loc_825D95E4;
	// lis r8,-32768
	ctx.r8.s64 = -2147483648;
loc_825D95B0:
	// lwz r11,24(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 24);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r9,20(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 20);
	// slw r10,r18,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (r18.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srw r9,r8,r30
	ctx.r9.u64 = r30.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r30.u8 & 0x3F));
	// stw r11,24(r25)
	PPC_STORE_U32(r25.u32 + 24, r11.u32);
	// stw r10,20(r25)
	PPC_STORE_U32(r25.u32 + 20, ctx.r10.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d95b0
	if (!cr6.eq) goto loc_825D95B0;
loc_825D95E4:
	// addi r4,r30,1
	ctx.r4.s64 = r30.s64 + 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825cc610
	sub_825CC610(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
loc_825D95FC:
	// lwz r11,24(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 24);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r9,6
	ctx.r9.s64 = 6;
	// lwz r10,20(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 20);
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,24(r25)
	PPC_STORE_U32(r25.u32 + 24, r11.u32);
	// stw r9,56(r21)
	PPC_STORE_U32(r21.u32 + 56, ctx.r9.u32);
loc_825D964C:
	// lwz r11,60(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 60);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bgt cr6,0x825d96d4
	if (cr6.gt) goto loc_825D96D4;
	// lwz r11,248(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 248);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r4,r11,16
	ctx.r4.u64 = r11.u32 & 0xFFFF;
	// bl 0x825cc320
	sub_825CC320(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r9,-1
	ctx.r9.s64 = -1;
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// lwz r11,248(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 248);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r8,24(r25)
	PPC_STORE_U32(r25.u32 + 24, ctx.r8.u32);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// extsh r11,r11
	r11.s64 = r11.s16;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
	// stw r11,16(r25)
	PPC_STORE_U32(r25.u32 + 16, r11.u32);
	// stw r17,56(r21)
	PPC_STORE_U32(r21.u32 + 56, r17.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
loc_825D96D4:
	// lhz r11,312(r21)
	r11.u64 = PPC_LOAD_U16(r21.u32 + 312);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// bl 0x825d7f50
	sub_825D7F50(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// blt cr6,0x825d9720
	if (cr6.lt) goto loc_825D9720;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,16(r25)
	PPC_STORE_U32(r25.u32 + 16, r11.u32);
	// bne cr6,0x825d971c
	if (!cr6.eq) goto loc_825D971C;
	// lhz r11,314(r21)
	r11.u64 = PPC_LOAD_U16(r21.u32 + 314);
	// lwz r10,20(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 20);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,20(r25)
	PPC_STORE_U32(r25.u32 + 20, r11.u32);
loc_825D971C:
	// stw r17,56(r21)
	PPC_STORE_U32(r21.u32 + 56, r17.u32);
loc_825D9720:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_825D972C"))) PPC_WEAK_FUNC(sub_825D972C);
PPC_FUNC_IMPL(__imp__sub_825D972C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D9730"))) PPC_WEAK_FUNC(sub_825D9730);
PPC_FUNC_IMPL(__imp__sub_825D9730) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r19,r7
	r19.u64 = ctx.r7.u64;
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// li r24,23
	r24.s64 = 23;
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r22,0
	r22.s64 = 0;
	// lwz r27,32(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// lwz r28,28(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r26,48(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r25,44(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bge cr6,0x825d9898
	if (!cr6.lt) goto loc_825D9898;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x825d97b4
	if (cr6.eq) goto loc_825D97B4;
	// subfic r11,r30,32
	xer.ca = r30.u32 <= 32;
	r11.s64 = 32 - r30.s64;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// blt cr6,0x825d9790
	if (cr6.lt) goto loc_825D9790;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_825D9790:
	// subf r26,r11,r26
	r26.s64 = r26.s64 - r11.s64;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r9,r29,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// slw r11,r10,r26
	r11.u64 = r26.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r26.u8 & 0x3F));
	// srw r10,r25,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (r25.u32 >> (r26.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// or r29,r9,r10
	r29.u64 = ctx.r9.u64 | ctx.r10.u64;
	// and r25,r11,r25
	r25.u64 = r11.u64 & r25.u64;
loc_825D97B4:
	// lis r11,-32209
	r11.s64 = -2110849024;
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// addi r11,r11,21608
	r11.s64 = r11.s64 + 21608;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x825d97fc
	if (!cr6.eq) goto loc_825D97FC;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d9838
	if (cr6.gt) goto loc_825D9838;
loc_825D97D0:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825d9838
	if (cr6.eq) goto loc_825D9838;
	// lbz r11,0(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwinm r10,r29,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 8) & 0xFFFFFF00;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// or r29,r10,r11
	r29.u64 = ctx.r10.u64 | r11.u64;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d97d0
	if (!cr6.gt) goto loc_825D97D0;
	// b 0x825d9838
	goto loc_825D9838;
loc_825D97FC:
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// bgt cr6,0x825d9838
	if (cr6.gt) goto loc_825D9838;
loc_825D9804:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x825d9838
	if (cr6.eq) goto loc_825D9838;
	// lbz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwimi r3,r29,8,0,23
	ctx.r3.u64 = (__builtin_rotateleft32(r29.u32, 8) & 0xFFFFFF00) | (ctx.r3.u64 & 0xFFFFFFFF000000FF);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,24
	cr6.compare<uint32_t>(r30.u32, 24, xer);
	// ble cr6,0x825d9804
	if (!cr6.gt) goto loc_825D9804;
loc_825D9838:
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// bge cr6,0x825d9898
	if (!cr6.lt) goto loc_825D9898;
	// li r5,23
	ctx.r5.s64 = 23;
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// stw r28,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r28.u32);
	// stw r26,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r26.u32);
	// stw r25,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r25.u32);
	// bl 0x825cc190
	sub_825CC190(ctx, base);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x825d9afc
	if (cr6.lt) goto loc_825D9AFC;
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r27,32(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r30,23
	cr6.compare<uint32_t>(r30.u32, 23, xer);
	// lwz r28,28(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// lwz r26,48(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r25,44(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bge cr6,0x825d9898
	if (!cr6.lt) goto loc_825D9898;
	// mr r24,r30
	r24.u64 = r30.u64;
loc_825D9898:
	// subf r11,r24,r30
	r11.s64 = r30.s64 - r24.s64;
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// subfic r10,r24,32
	xer.ca = r24.u32 <= 32;
	ctx.r10.s64 = 32 - r24.s64;
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// stw r28,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r28.u32);
	// stw r26,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r26.u32);
	// stw r25,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r25.u32);
	// srw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 >> (r11.u8 & 0x3F));
	// slw r7,r11,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// rlwinm r11,r7,3,29,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x6;
	// add r10,r11,r23
	ctx.r10.u64 = r11.u64 + r23.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r9,r11,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,2,30,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x3;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r9,1,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r8,r11,0,0,16
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r9,2,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x1;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r9,r11,0,0,16
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825d9ab4
	if (!cr6.eq) goto loc_825D9AB4;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_825D9AB4:
	// rlwinm r9,r11,22,27,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 22) & 0x1F;
	// clrlwi r11,r11,22
	r11.u64 = r11.u32 & 0x3FF;
	// cmplwi cr6,r11,1020
	cr6.compare<uint32_t>(r11.u32, 1020, xer);
	// stw r9,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r9.u32);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// blt cr6,0x825d9ae0
	if (cr6.lt) goto loc_825D9AE0;
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r10
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
loc_825D9AE0:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// beq cr6,0x825d9b00
	if (cr6.eq) goto loc_825D9B00;
	// slw r11,r7,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// stw r11,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
loc_825D9AFC:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
loc_825D9B00:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825D9B08"))) PPC_WEAK_FUNC(sub_825D9B08);
PPC_FUNC_IMPL(__imp__sub_825D9B08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r9,r6,-1
	ctx.r9.s64 = ctx.r6.s64 + -1;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r11,-25100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -25100);
	// li r7,16
	ctx.r7.s64 = 16;
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,28(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_825D9B3C"))) PPC_WEAK_FUNC(sub_825D9B3C);
PPC_FUNC_IMPL(__imp__sub_825D9B3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D9B40"))) PPC_WEAK_FUNC(sub_825D9B40);
PPC_FUNC_IMPL(__imp__sub_825D9B40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r12{};
	// cmplwi cr6,r3,11
	cr6.compare<uint32_t>(ctx.r3.u32, 11, xer);
	// bgt cr6,0x825d9c08
	if (cr6.gt) {
		sub_825D9C08(ctx, base);
		return;
	}
	// lis r12,-32162
	r12.s64 = -2107768832;
	// addi r12,r12,-25760
	r12.s64 = r12.s64 + -25760;
	// rlwinm r0,r3,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r3.u64) {
	case 0:
		// ERROR: 0x825D9B90
		return;
	case 1:
		// ERROR: 0x825D9BA0
		return;
	case 2:
		// ERROR: 0x825D9BA8
		return;
	case 3:
		// ERROR: 0x825D9BB4
		return;
	case 4:
		// ERROR: 0x825D9BCC
		return;
	case 5:
		// ERROR: 0x825D9BC0
		return;
	case 6:
		// ERROR: 0x825D9BD8
		return;
	case 7:
		// ERROR: 0x825D9BE4
		return;
	case 8:
		// ERROR: 0x825D9BF0
		return;
	case 9:
		// ERROR: 0x825D9B98
		return;
	case 10:
		// ERROR: 0x825D9B90
		return;
	case 11:
		// ERROR: 0x825D9BFC
		return;
	default:
		__builtin_unreachable();
	}
}

__attribute__((alias("__imp__sub_825D9B60"))) PPC_WEAK_FUNC(sub_825D9B60);
PPC_FUNC_IMPL(__imp__sub_825D9B60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r18{};
	PPCRegister r29{};
	// lwz r18,-25712(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25712);
	// lwz r18,-25696(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25696);
	// lwz r18,-25688(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25688);
	// lwz r18,-25676(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25676);
	// lwz r18,-25652(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25652);
	// lwz r18,-25664(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25664);
	// lwz r18,-25640(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25640);
	// lwz r18,-25628(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25628);
	// lwz r18,-25616(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25616);
	// lwz r18,-25704(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25704);
	// lwz r18,-25712(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25712);
	// lwz r18,-25604(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + -25604);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9B98"))) PPC_WEAK_FUNC(sub_825D9B98);
PPC_FUNC_IMPL(__imp__sub_825D9B98) {
	PPC_FUNC_PROLOGUE();
	// lis r3,96
	ctx.r3.s64 = 6291456;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BA0"))) PPC_WEAK_FUNC(sub_825D9BA0);
PPC_FUNC_IMPL(__imp__sub_825D9BA0) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BA8"))) PPC_WEAK_FUNC(sub_825D9BA8);
PPC_FUNC_IMPL(__imp__sub_825D9BA8) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,3
	ctx.r3.u64 = ctx.r3.u64 | 3;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BB4"))) PPC_WEAK_FUNC(sub_825D9BB4);
PPC_FUNC_IMPL(__imp__sub_825D9BB4) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,4
	ctx.r3.u64 = ctx.r3.u64 | 4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BC0"))) PPC_WEAK_FUNC(sub_825D9BC0);
PPC_FUNC_IMPL(__imp__sub_825D9BC0) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,183
	ctx.r3.u64 = ctx.r3.u64 | 183;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BCC"))) PPC_WEAK_FUNC(sub_825D9BCC);
PPC_FUNC_IMPL(__imp__sub_825D9BCC) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,5
	ctx.r3.u64 = ctx.r3.u64 | 5;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BD8"))) PPC_WEAK_FUNC(sub_825D9BD8);
PPC_FUNC_IMPL(__imp__sub_825D9BD8) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,6
	ctx.r3.u64 = ctx.r3.u64 | 6;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BE4"))) PPC_WEAK_FUNC(sub_825D9BE4);
PPC_FUNC_IMPL(__imp__sub_825D9BE4) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BF0"))) PPC_WEAK_FUNC(sub_825D9BF0);
PPC_FUNC_IMPL(__imp__sub_825D9BF0) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,7
	ctx.r3.u64 = ctx.r3.u64 | 7;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9BFC"))) PPC_WEAK_FUNC(sub_825D9BFC);
PPC_FUNC_IMPL(__imp__sub_825D9BFC) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32672
	ctx.r3.s64 = -2141192192;
	// ori r3,r3,8
	ctx.r3.u64 = ctx.r3.u64 | 8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9C08"))) PPC_WEAK_FUNC(sub_825D9C08);
PPC_FUNC_IMPL(__imp__sub_825D9C08) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9C14"))) PPC_WEAK_FUNC(sub_825D9C14);
PPC_FUNC_IMPL(__imp__sub_825D9C14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D9C18"))) PPC_WEAK_FUNC(sub_825D9C18);
PPC_FUNC_IMPL(__imp__sub_825D9C18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r26,0
	r26.s64 = 0;
	// lwz r11,32(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// mr r20,r4
	r20.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// std r26,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r26.u64);
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// stw r26,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r26.u32);
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// beq cr6,0x825d9ea8
	if (cr6.eq) goto loc_825D9EA8;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825d9eb4
	if (cr6.eq) goto loc_825D9EB4;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825d9ea8
	if (cr6.eq) goto loc_825D9EA8;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x825d9ea8
	if (cr6.eq) goto loc_825D9EA8;
	// li r21,1
	r21.s64 = 1;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// stw r21,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r21.u32);
	// lwz r31,32(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r22,4(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d9cb8
	if (cr6.eq) goto loc_825D9CB8;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// li r3,10
	ctx.r3.s64 = 10;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// lhz r11,88(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 88);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// stw r26,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r26.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
loc_825D9CB8:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d9d3c
	if (!cr6.eq) goto loc_825D9D3C;
	// addi r30,r31,12
	r30.s64 = r31.s64 + 12;
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r4,r31,20
	ctx.r4.s64 = r31.s64 + 20;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mtctr r23
	ctr.u64 = r23.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x825d9d10
	if (!cr6.lt) goto loc_825D9D10;
loc_825D9CF8:
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// li r3,10
	ctx.r3.s64 = 10;
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// stw r21,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r21.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
loc_825D9D10:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d9d24
	if (cr6.eq) goto loc_825D9D24;
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,72(r31)
	PPC_STORE_U64(r31.u32 + 72, r11.u64);
loc_825D9D24:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d9d34
	if (cr6.eq) goto loc_825D9D34;
	// stw r21,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r21.u32);
loc_825D9D34:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
loc_825D9D3C:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x825d9d70
	if (!cr6.eq) goto loc_825D9D70;
	// lwz r30,16(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// blt cr6,0x825d9d70
	if (cr6.lt) goto loc_825D9D70;
	// addi r29,r31,20
	r29.s64 = r31.s64 + 20;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// clrlwi r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825d9d70
	if (!cr6.eq) goto loc_825D9D70;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r30,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r30.u32);
	// b 0x825d9e5c
	goto loc_825D9E5C;
loc_825D9D70:
	// lwz r30,16(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplwi cr6,r30,4096
	cr6.compare<uint32_t>(r30.u32, 4096, xer);
	// ble cr6,0x825d9d80
	if (!cr6.gt) goto loc_825D9D80;
	// li r30,4096
	r30.s64 = 4096;
loc_825D9D80:
	// addi r29,r31,20
	r29.s64 = r31.s64 + 20;
	// add r11,r31,r20
	r11.u64 = r31.u64 + r20.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r11,116
	ctx.r3.s64 = r11.s64 + 116;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// bge cr6,0x825d9e4c
	if (!cr6.lt) goto loc_825D9E4C;
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825d9e4c
	if (!cr6.eq) goto loc_825D9E4C;
	// addi r27,r31,12
	r27.s64 = r31.s64 + 12;
	// mr r28,r30
	r28.u64 = r30.u64;
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mtctr r23
	ctr.u64 = r23.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d9cf8
	if (cr6.lt) goto loc_825D9CF8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d9df8
	if (cr6.eq) goto loc_825D9DF8;
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,72(r31)
	PPC_STORE_U64(r31.u32 + 72, r11.u64);
loc_825D9DF8:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825d9e08
	if (cr6.eq) goto loc_825D9E08;
	// stw r21,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r21.u32);
loc_825D9E08:
	// lwz r30,0(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r30,4096
	cr6.compare<uint32_t>(r30.u32, 4096, xer);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// ble cr6,0x825d9e1c
	if (!cr6.gt) goto loc_825D9E1C;
	// li r30,4096
	r30.s64 = 4096;
loc_825D9E1C:
	// add r11,r28,r31
	r11.u64 = r28.u64 + r31.u64;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// addi r3,r11,116
	ctx.r3.s64 = r11.s64 + 116;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r11,r28,r30
	r11.u64 = r28.u64 + r30.u64;
	// addi r10,r31,116
	ctx.r10.s64 = r31.s64 + 116;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// b 0x825d9e5c
	goto loc_825D9E5C;
loc_825D9E4C:
	// addi r11,r31,116
	r11.s64 = r31.s64 + 116;
	// add r10,r30,r20
	ctx.r10.u64 = r30.u64 + r20.u64;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
loc_825D9E5C:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825d9e9c
	if (cr6.eq) goto loc_825D9E9C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825d9e9c
	if (!cr6.eq) goto loc_825D9E9C;
	// stw r26,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r26.u32);
	// li r3,10
	ctx.r3.s64 = 10;
	// stw r26,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r26.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
loc_825D9E9C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
loc_825D9EA8:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x825d9eb4
	if (cr6.eq) goto loc_825D9EB4;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
loc_825D9EB4:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825d9ec0
	if (cr6.eq) goto loc_825D9EC0;
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
loc_825D9EC0:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x825d9ecc
	if (cr6.eq) goto loc_825D9ECC;
	// stw r26,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r26.u32);
loc_825D9ECC:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_825D9ED8"))) PPC_WEAK_FUNC(sub_825D9ED8);
PPC_FUNC_IMPL(__imp__sub_825D9ED8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,32(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// bl 0x825e6e98
	sub_825E6E98(ctx, base);
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d9f1c
	if (cr6.lt) goto loc_825D9F1C;
	// li r11,0
	r11.s64 = 0;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// std r11,72(r31)
	PPC_STORE_U64(r31.u32 + 72, r11.u64);
loc_825D9F1C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D9F30"))) PPC_WEAK_FUNC(sub_825D9F30);
PPC_FUNC_IMPL(__imp__sub_825D9F30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r3,68(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 68);
	// bl 0x825e6998
	sub_825E6998(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r5,r11,84
	ctx.r5.s64 = r11.s64 + 84;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825d9f78
	if (cr6.eq) goto loc_825D9F78;
	// li r4,25
	ctx.r4.s64 = 25;
	// lwz r3,4228(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4228);
	// bl 0x825aa410
	sub_825AA410(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_825D9F78:
	// li r30,0
	r30.s64 = 0;
	// stw r30,84(r11)
	PPC_STORE_U32(r11.u32 + 84, r30.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r4,4232(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4232);
	// lwz r3,4228(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4228);
	// bl 0x825a9fb0
	sub_825A9FB0(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825d9fac
	if (cr6.eq) goto loc_825D9FAC;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,4228(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4228);
	// li r4,25
	ctx.r4.s64 = 25;
	// bl 0x825aa410
	sub_825AA410(ctx, base);
loc_825D9FAC:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825d9fd8
	if (cr6.lt) goto loc_825D9FD8;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
loc_825D9FD8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825D9FE0"))) PPC_WEAK_FUNC(sub_825D9FE0);
PPC_FUNC_IMPL(__imp__sub_825D9FE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// lwz r31,32(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r5,15
	ctx.r5.s64 = 15;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// bl 0x825ead58
	sub_825EAD58(ctx, base);
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// lhz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DA040"))) PPC_WEAK_FUNC(sub_825DA040);
PPC_FUNC_IMPL(__imp__sub_825DA040) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,32(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// lwz r10,108(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// ble cr6,0x825da080
	if (!cr6.gt) goto loc_825DA080;
loc_825DA070:
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,6
	ctx.r3.u64 = ctx.r3.u64 | 6;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_825DA080:
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825da070
	if (!cr6.eq) goto loc_825DA070;
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r4,1
	ctx.r4.s64 = 1;
	// rlwinm r10,r8,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// mullw r7,r9,r8
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// rlwinm r6,r9,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// add r30,r6,r7
	r30.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + r11.u64;
	// add r7,r30,r11
	ctx.r7.u64 = r30.u64 + r11.u64;
	// bl 0x825eaad0
	sub_825EAAD0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x825da0e8
	if (cr6.eq) goto loc_825DA0E8;
	// cmpwi cr6,r3,9
	cr6.compare<int32_t>(ctx.r3.s32, 9, xer);
	// bne cr6,0x825da0f8
	if (!cr6.eq) goto loc_825DA0F8;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// b 0x825da0f8
	goto loc_825DA0F8;
loc_825DA0E8:
	// lwz r11,108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
loc_825DA0F8:
	// ld r11,72(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 72);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// std r11,0(r26)
	PPC_STORE_U64(r26.u32 + 0, r11.u64);
	// beq cr6,0x825da110
	if (cr6.eq) goto loc_825DA110;
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_825DA110:
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825DA11C"))) PPC_WEAK_FUNC(sub_825DA11C);
PPC_FUNC_IMPL(__imp__sub_825DA11C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DA120"))) PPC_WEAK_FUNC(sub_825DA120);
PPC_FUNC_IMPL(__imp__sub_825DA120) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r29,r31,104
	r29.s64 = r31.s64 + 104;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// li r11,12
	r11.s64 = 12;
	// addi r7,r31,100
	ctx.r7.s64 = r31.s64 + 100;
	// lwz r4,4228(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4228);
	// li r6,12
	ctx.r6.s64 = 12;
	// lwz r28,28(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// li r11,2
	r11.s64 = 2;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// li r11,3
	r11.s64 = 3;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// li r11,4
	r11.s64 = 4;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// li r11,5
	r11.s64 = 5;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// li r11,6
	r11.s64 = 6;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// li r11,7
	r11.s64 = 7;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// li r11,8
	r11.s64 = 8;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// li r11,9
	r11.s64 = 9;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// li r11,10
	r11.s64 = 10;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// li r11,11
	r11.s64 = 11;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// bl 0x825eae48
	sub_825EAE48(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da224
	if (cr6.lt) goto loc_825DA224;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r8,r31,108
	ctx.r8.s64 = r31.s64 + 108;
	// lwz r4,4228(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4228);
	// addi r7,r31,36
	ctx.r7.s64 = r31.s64 + 36;
	// addi r6,r31,32
	ctx.r6.s64 = r31.s64 + 32;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb0a8
	sub_825EB0A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da224
	if (cr6.lt) goto loc_825DA224;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,4228(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4228);
	// lis r5,9
	ctx.r5.s64 = 589824;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x825aa0d8
	sub_825AA0D8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da224
	if (cr6.lt) goto loc_825DA224;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825da224
	if (cr6.eq) goto loc_825DA224;
	// lis r3,80
	ctx.r3.s64 = 5242880;
	// ori r3,r3,9
	ctx.r3.u64 = ctx.r3.u64 | 9;
loc_825DA224:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825DA22C"))) PPC_WEAK_FUNC(sub_825DA22C);
PPC_FUNC_IMPL(__imp__sub_825DA22C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DA230"))) PPC_WEAK_FUNC(sub_825DA230);
PPC_FUNC_IMPL(__imp__sub_825DA230) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825da260
	if (!cr6.eq) goto loc_825DA260;
	// lis r11,8
	r11.s64 = 524288;
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// beq cr6,0x825da258
	if (cr6.eq) goto loc_825DA258;
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,178
	ctx.r3.u64 = ctx.r3.u64 | 178;
	// blr 
	return;
loc_825DA258:
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// b 0x825da120
	sub_825DA120(ctx, base);
	return;
loc_825DA260:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DA268"))) PPC_WEAK_FUNC(sub_825DA268);
PPC_FUNC_IMPL(__imp__sub_825DA268) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DA278"))) PPC_WEAK_FUNC(sub_825DA278);
PPC_FUNC_IMPL(__imp__sub_825DA278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r30,0
	r30.s64 = 0;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// li r5,4240
	ctx.r5.s64 = 4240;
	// li r4,25
	ctx.r4.s64 = 25;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// bl 0x825aa3a8
	sub_825AA3A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da5b8
	if (cr6.lt) goto loc_825DA5B8;
	// li r5,4240
	ctx.r5.s64 = 4240;
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,68(r11)
	PPC_STORE_U32(r11.u32 + 68, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r30.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,4224(r11)
	PPC_STORE_U32(r11.u32 + 4224, r30.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,28(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r11,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lhz r11,16(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 16);
	// sth r11,40(r10)
	PPC_STORE_U16(ctx.r10.u32 + 40, r11.u16);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lhz r11,18(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 18);
	// sth r11,42(r10)
	PPC_STORE_U16(ctx.r10.u32 + 42, r11.u16);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r11,44(r10)
	PPC_STORE_U32(ctx.r10.u32 + 44, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,56(r10)
	PPC_STORE_U32(ctx.r10.u32 + 56, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// stw r11,60(r10)
	PPC_STORE_U32(ctx.r10.u32 + 60, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r11,64(r10)
	PPC_STORE_U32(ctx.r10.u32 + 64, r11.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r11,92(r10)
	PPC_STORE_U32(ctx.r10.u32 + 92, r11.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r29,96(r11)
	PPC_STORE_U32(r11.u32 + 96, r29.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r30,80(r11)
	PPC_STORE_U32(r11.u32 + 80, r30.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lhz r11,48(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825da458
	if (cr6.eq) goto loc_825DA458;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r4,25
	ctx.r4.s64 = 25;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r6,r10,84
	ctx.r6.s64 = ctx.r10.s64 + 84;
	// lhz r5,48(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// bl 0x825aa3a8
	sub_825AA3A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da5b8
	if (cr6.lt) goto loc_825DA5B8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lhz r5,48(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// lwz r3,84(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 84);
	// lwz r4,52(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lhz r11,48(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 48);
	// sth r11,88(r10)
	PPC_STORE_U16(ctx.r10.u32 + 88, r11.u16);
loc_825DA458:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lis r9,22358
	ctx.r9.s64 = 1465253888;
	// ori r9,r9,17201
	ctx.r9.u64 = ctx.r9.u64 | 17201;
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22349
	ctx.r9.s64 = 1464664064;
	// ori r9,r9,22081
	ctx.r9.u64 = ctx.r9.u64 | 22081;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22349
	ctx.r9.s64 = 1464664064;
	// ori r9,r9,22067
	ctx.r9.u64 = ctx.r9.u64 | 22067;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22349
	ctx.r9.s64 = 1464664064;
	// ori r9,r9,22066
	ctx.r9.u64 = ctx.r9.u64 | 22066;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22358
	ctx.r9.s64 = 1465253888;
	// ori r9,r9,20530
	ctx.r9.u64 = ctx.r9.u64 | 20530;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22349
	ctx.r9.s64 = 1464664064;
	// ori r9,r9,22096
	ctx.r9.u64 = ctx.r9.u64 | 22096;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,22349
	ctx.r9.s64 = 1464664064;
	// ori r9,r9,22098
	ctx.r9.u64 = ctx.r9.u64 | 22098;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x825da4e0
	if (cr6.eq) goto loc_825DA4E0;
	// lis r9,19792
	ctx.r9.s64 = 1297088512;
	// ori r9,r9,13395
	ctx.r9.u64 = ctx.r9.u64 | 13395;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x825da4ec
	if (!cr6.eq) goto loc_825DA4EC;
loc_825DA4E0:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r10.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825DA4EC:
	// stw r11,32(r26)
	PPC_STORE_U32(r26.u32 + 32, r11.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r6,92(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// addi r4,r26,36
	ctx.r4.s64 = r26.s64 + 36;
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r3,r11,68
	ctx.r3.s64 = r11.s64 + 68;
	// lwz r31,84(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lfs f1,2480(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	ctx.f1.f64 = double(temp.f32);
	// lwz r8,32(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// std r6,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r6.u64);
	// lhz r7,48(r7)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + 48);
	// lwz r10,96(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// lwz r5,44(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r7,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r7.u32);
	// lfd f0,120(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f2,f0
	ctx.f2.f64 = double(float(f0.f64));
	// bl 0x825e6798
	sub_825E6798(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da5b8
	if (cr6.lt) goto loc_825DA5B8;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r9,80(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x825da570
	if (cr6.eq) goto loc_825DA570;
	// addi r3,r11,68
	ctx.r3.s64 = r11.s64 + 68;
	// bl 0x825e6a30
	sub_825E6A30(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825DA570:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r30,80(r11)
	PPC_STORE_U32(r11.u32 + 80, r30.u32);
	// bl 0x825d9b40
	sub_825D9B40(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da5b8
	if (cr6.lt) goto loc_825DA5B8;
	// lis r11,-32162
	r11.s64 = -2107768832;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r4,r11,-24016
	ctx.r4.s64 = r11.s64 + -24016;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r25,4228(r11)
	PPC_STORE_U32(r11.u32 + 4228, r25.u32);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r6,r5,4232
	ctx.r6.s64 = ctx.r5.s64 + 4232;
	// bl 0x825a9f10
	sub_825A9F10(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825da5b8
	if (cr6.lt) goto loc_825DA5B8;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,4232(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4232);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
loc_825DA5B8:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825DA5C0"))) PPC_WEAK_FUNC(sub_825DA5C0);
PPC_FUNC_IMPL(__imp__sub_825DA5C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lis r11,-32688
	r11.s64 = -2142240768;
	// ori r11,r11,3
	r11.u64 = r11.u64 | 3;
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// bne cr6,0x825da734
	if (!cr6.eq) goto loc_825DA734;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lis r10,22358
	ctx.r10.s64 = 1465253888;
	// ori r8,r10,17201
	ctx.r8.u64 = ctx.r10.u64 | 17201;
	// lwz r10,20(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22081
	ctx.r8.u64 = ctx.r8.u64 | 22081;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22067
	ctx.r8.u64 = ctx.r8.u64 | 22067;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22066
	ctx.r8.u64 = ctx.r8.u64 | 22066;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22065
	ctx.r8.u64 = ctx.r8.u64 | 22065;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22358
	ctx.r8.s64 = 1465253888;
	// ori r8,r8,20530
	ctx.r8.u64 = ctx.r8.u64 | 20530;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22096
	ctx.r8.u64 = ctx.r8.u64 | 22096;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,22349
	ctx.r8.s64 = 1464664064;
	// ori r8,r8,22098
	ctx.r8.u64 = ctx.r8.u64 | 22098;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,19792
	ctx.r8.s64 = 1297088512;
	// ori r8,r8,13395
	ctx.r8.u64 = ctx.r8.u64 | 13395;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,19792
	ctx.r8.s64 = 1297088512;
	// ori r8,r8,13363
	ctx.r8.u64 = ctx.r8.u64 | 13363;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x825da694
	if (cr6.eq) goto loc_825DA694;
	// lis r8,19792
	ctx.r8.s64 = 1297088512;
	// ori r8,r8,13362
	ctx.r8.u64 = ctx.r8.u64 | 13362;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bne cr6,0x825da734
	if (!cr6.eq) goto loc_825DA734;
loc_825DA694:
	// lwz r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825da734
	if (cr6.eq) goto loc_825DA734;
	// lwz r10,12(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825da734
	if (cr6.eq) goto loc_825DA734;
	// lhz r10,18(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 18);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x825da734
	if (cr6.eq) goto loc_825DA734;
	// lis r8,-32162
	ctx.r8.s64 = -2107768832;
	// stw r5,36(r5)
	PPC_STORE_U32(ctx.r5.u32 + 36, ctx.r5.u32);
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// addi r8,r8,-24608
	ctx.r8.s64 = ctx.r8.s64 + -24608;
	// addi r10,r10,-23960
	ctx.r10.s64 = ctx.r10.s64 + -23960;
	// lis r11,-32162
	r11.s64 = -2107768832;
	// lis r31,-32162
	r31.s64 = -2107768832;
	// lis r4,-32162
	ctx.r4.s64 = -2107768832;
	// lis r7,-32162
	ctx.r7.s64 = -2107768832;
	// stw r8,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r8.u32);
	// lis r6,-32162
	ctx.r6.s64 = -2107768832;
	// stw r10,28(r5)
	PPC_STORE_U32(ctx.r5.u32 + 28, ctx.r10.u32);
	// lis r3,-32216
	ctx.r3.s64 = -2111307776;
	// lis r9,-32216
	ctx.r9.s64 = -2111307776;
	// addi r8,r11,-25576
	ctx.r8.s64 = r11.s64 + -25576;
	// addi r31,r31,-23944
	r31.s64 = r31.s64 + -23944;
	// addi r4,r4,-24784
	ctx.r4.s64 = ctx.r4.s64 + -24784;
	// addi r7,r7,-24512
	ctx.r7.s64 = ctx.r7.s64 + -24512;
	// addi r6,r6,-24872
	ctx.r6.s64 = ctx.r6.s64 + -24872;
	// addi r3,r3,-1880
	ctx.r3.s64 = ctx.r3.s64 + -1880;
	// stw r8,40(r5)
	PPC_STORE_U32(ctx.r5.u32 + 40, ctx.r8.u32);
	// addi r9,r9,-1880
	ctx.r9.s64 = ctx.r9.s64 + -1880;
	// stw r31,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r31.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r4,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r4.u32);
	// stw r7,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r7.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r6,16(r5)
	PPC_STORE_U32(ctx.r5.u32 + 16, ctx.r6.u32);
	// stw r3,20(r5)
	PPC_STORE_U32(ctx.r5.u32 + 20, ctx.r3.u32);
	// stw r9,24(r5)
	PPC_STORE_U32(ctx.r5.u32 + 24, ctx.r9.u32);
	// stw r10,32(r5)
	PPC_STORE_U32(ctx.r5.u32 + 32, ctx.r10.u32);
loc_825DA734:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DA740"))) PPC_WEAK_FUNC(sub_825DA740);
PPC_FUNC_IMPL(__imp__sub_825DA740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// extsh r30,r5
	r30.s64 = ctx.r5.s16;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r4,64
	ctx.r4.s64 = 64;
	// li r28,0
	r28.s64 = 0;
	// stw r30,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r30.u32);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,28(r29)
	PPC_STORE_U32(r29.u32 + 28, ctx.r3.u32);
	// bne cr6,0x825da794
	if (!cr6.eq) goto loc_825DA794;
loc_825DA784:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825DA794:
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,64
	ctx.r4.s64 = 64;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,32(r29)
	PPC_STORE_U32(r29.u32 + 32, ctx.r3.u32);
	// beq cr6,0x825da784
	if (cr6.eq) goto loc_825DA784;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,64
	ctx.r4.s64 = 64;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,36(r29)
	PPC_STORE_U32(r29.u32 + 36, ctx.r3.u32);
	// beq cr6,0x825da784
	if (cr6.eq) goto loc_825DA784;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,64
	ctx.r4.s64 = 64;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,40(r29)
	PPC_STORE_U32(r29.u32 + 40, ctx.r3.u32);
	// beq cr6,0x825da784
	if (cr6.eq) goto loc_825DA784;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,64
	ctx.r4.s64 = 64;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,24(r29)
	PPC_STORE_U32(r29.u32 + 24, ctx.r3.u32);
	// beq cr6,0x825da784
	if (cr6.eq) goto loc_825DA784;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,64
	ctx.r4.s64 = 64;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,44(r29)
	PPC_STORE_U32(r29.u32 + 44, ctx.r3.u32);
	// beq cr6,0x825da784
	if (cr6.eq) goto loc_825DA784;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825DA8C4"))) PPC_WEAK_FUNC(sub_825DA8C4);
PPC_FUNC_IMPL(__imp__sub_825DA8C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DA8C8"))) PPC_WEAK_FUNC(sub_825DA8C8);
PPC_FUNC_IMPL(__imp__sub_825DA8C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mullw r10,r10,r10
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r10.s32);
	// mullw. r10,r10,r29
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble 0x825da920
	if (!cr0.gt) goto loc_825DA920;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_825DA8FC:
	// lwz r9,28(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r8,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mullw r9,r9,r9
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r9.s32);
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825da8fc
	if (cr6.lt) goto loc_825DA8FC;
loc_825DA920:
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// mullw. r10,r10,r10
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r10.s32);
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble 0x825da954
	if (!cr0.gt) goto loc_825DA954;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_825DA934:
	// lwz r9,36(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r8,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mullw r9,r9,r9
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r9.s32);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825da934
	if (cr6.lt) goto loc_825DA934;
loc_825DA954:
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825da984
	if (!cr6.gt) goto loc_825DA984;
loc_825DA964:
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r9,36(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// sthx r8,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u16);
	// lhz r10,34(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825da964
	if (cr6.lt) goto loc_825DA964;
loc_825DA984:
	// lwz r11,660(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 660);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825daa18
	if (!cr6.eq) goto loc_825DAA18;
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// lwz r9,664(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 664);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw. r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825da9dc
	if (!cr0.gt) goto loc_825DA9DC;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_825DA9AC:
	// lwz r9,696(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 696);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r7,692(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 692);
	// lhzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// sthx r9,r11,r7
	PPC_STORE_U16(r11.u32 + ctx.r7.u32, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// lwz r7,664(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 664);
	// mullw r7,r7,r9
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// mullw r9,r7,r9
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825da9ac
	if (cr6.lt) goto loc_825DA9AC;
loc_825DA9DC:
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mullw. r11,r11,r11
	r11.s64 = int64_t(r11.s32) * int64_t(r11.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825daa18
	if (!cr0.gt) goto loc_825DAA18;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_825DA9F0:
	// lwz r9,704(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 704);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,700(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 700);
	// lhzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// sthx r9,r8,r11
	PPC_STORE_U16(ctx.r8.u32 + r11.u32, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// mullw r9,r9,r9
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r9.s32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825da9f0
	if (cr6.lt) goto loc_825DA9F0;
loc_825DAA18:
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,24(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lhz r11,34(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,44(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r10,1
	ctx.r10.s64 = 1;
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// stw r9,20(r30)
	PPC_STORE_U32(r30.u32 + 20, ctx.r9.u32);
	// lhz r9,34(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 34);
	// sth r10,48(r30)
	PPC_STORE_U16(r30.u32 + 48, ctx.r10.u16);
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// stw r9,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r9.u32);
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825DAA80"))) PPC_WEAK_FUNC(sub_825DAA80);
PPC_FUNC_IMPL(__imp__sub_825DAA80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce0
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// li r26,0
	r26.s64 = 0;
	// lwz r10,24(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r29,28(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r28,36(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// add r25,r11,r10
	r25.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dabcc
	if (!cr6.gt) goto loc_825DABCC;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// subf r24,r5,r6
	r24.s64 = ctx.r6.s64 - ctx.r5.s64;
loc_825DAAB8:
	// lwzx r10,r24,r27
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + r27.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825daadc
	if (!cr6.eq) goto loc_825DAADC;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r9,r28
	r28.u64 = ctx.r9.u64 + r28.u64;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// b 0x825dabbc
	goto loc_825DABBC;
loc_825DAADC:
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r8,0
	ctx.r8.s64 = 0;
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dab40
	if (!cr6.gt) goto loc_825DAB40;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// addi r11,r29,2
	r11.s64 = r29.s64 + 2;
	// subf r30,r29,r25
	r30.s64 = r25.s64 - r29.s64;
loc_825DAAFC:
	// lhzx r6,r30,r11
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r23,-2(r11)
	r23.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r22,0(r9)
	r22.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r23,r23
	r23.s64 = r23.s16;
	// extsh r22,r22
	r22.s64 = r22.s16;
	// mullw r6,r6,r31
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r31.s32);
	// mullw r31,r23,r22
	r31.s64 = int64_t(r23.s32) * int64_t(r22.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825daafc
	if (!cr6.eq) goto loc_825DAAFC;
loc_825DAB40:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x825dab7c
	if (!cr6.gt) goto loc_825DAB7C;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_825DAB54:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// mullw r6,r6,r31
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r31.s32);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825dab54
	if (!cr6.eq) goto loc_825DAB54;
loc_825DAB7C:
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// sraw r11,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// stwx r11,r24,r27
	PPC_STORE_U32(r24.u32 + r27.u32, r11.u32);
	// beq cr6,0x825daba4
	if (cr6.eq) goto loc_825DABA4;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_825DABA4:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
loc_825DABBC:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// blt cr6,0x825daab8
	if (cr6.lt) goto loc_825DAAB8;
loc_825DABCC:
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825DABD0"))) PPC_WEAK_FUNC(sub_825DABD0);
PPC_FUNC_IMPL(__imp__sub_825DABD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// li r27,0
	r27.s64 = 0;
	// lwz r10,24(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r30,28(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// lwz r29,36(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// add r25,r11,r10
	r25.u64 = r11.u64 + ctx.r10.u64;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dad1c
	if (!cr6.gt) goto loc_825DAD1C;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// subf r26,r5,r6
	r26.s64 = ctx.r6.s64 - ctx.r5.s64;
loc_825DAC08:
	// lwzx r10,r26,r28
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + r28.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825dac30
	if (!cr6.eq) goto loc_825DAC30;
	// lhz r10,34(r3)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// add r30,r9,r30
	r30.u64 = ctx.r9.u64 + r30.u64;
	// b 0x825dad08
	goto loc_825DAD08;
loc_825DAC30:
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r8,0
	ctx.r8.s64 = 0;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825dac90
	if (!cr6.gt) goto loc_825DAC90;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_825DAC54:
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r24,4(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lwz r23,0(r10)
	r23.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// mullw r6,r6,r24
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// mullw r31,r31,r23
	r31.s64 = int64_t(r31.s32) * int64_t(r23.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// bne cr6,0x825dac54
	if (!cr6.eq) goto loc_825DAC54;
loc_825DAC90:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x825daccc
	if (!cr6.gt) goto loc_825DACCC;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_825DACA4:
	// lhz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// mullw r6,r6,r31
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r31.s32);
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825daca4
	if (!cr6.eq) goto loc_825DACA4;
loc_825DACCC:
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// sraw r11,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// stwx r11,r26,r28
	PPC_STORE_U32(r26.u32 + r28.u32, r11.u32);
	// beq cr6,0x825dacf4
	if (cr6.eq) goto loc_825DACF4;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
loc_825DACF4:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
loc_825DAD08:
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x825dac08
	if (cr6.lt) goto loc_825DAC08;
loc_825DAD1C:
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_825DAD20"))) PPC_WEAK_FUNC(sub_825DAD20);
PPC_FUNC_IMPL(__imp__sub_825DAD20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r29,28(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r28,36(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// add r27,r10,r9
	r27.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// ble cr6,0x825daee4
	if (!cr6.gt) goto loc_825DAEE4;
	// subf r26,r6,r5
	r26.s64 = ctx.r5.s64 - ctx.r6.s64;
loc_825DAD68:
	// lwzx r10,r26,r6
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + ctx.r6.u32);
	// lwz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dae1c
	if (!cr6.gt) goto loc_825DAE1C;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mullw. r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825dadc0
	if (!cr0.gt) goto loc_825DADC0;
	// mr r11,r29
	r11.u64 = r29.u64;
	// subf r7,r29,r27
	ctx.r7.s64 = r27.s64 - r29.s64;
loc_825DAD94:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r9,r7,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r11.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r9,34(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dad94
	if (cr6.lt) goto loc_825DAD94;
loc_825DADC0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x825daebc
	if (!cr6.gt) goto loc_825DAEBC;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
loc_825DADD4:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dadf0
	if (!cr6.gt) goto loc_825DADF0;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// b 0x825dae00
	goto loc_825DAE00;
loc_825DADF0:
	// bge cr6,0x825dae04
	if (!cr6.lt) goto loc_825DAE04;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
loc_825DAE00:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825DAE04:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x825dadd4
	if (!cr6.eq) goto loc_825DADD4;
	// b 0x825daebc
	goto loc_825DAEBC;
loc_825DAE1C:
	// bge cr6,0x825daebc
	if (!cr6.lt) goto loc_825DAEBC;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mullw. r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825dae64
	if (!cr0.gt) goto loc_825DAE64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// subf r9,r29,r27
	ctx.r9.s64 = r27.s64 - r29.s64;
loc_825DAE38:
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r8,34(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x825dae38
	if (cr6.lt) goto loc_825DAE38;
loc_825DAE64:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x825daebc
	if (!cr6.gt) goto loc_825DAEBC;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
loc_825DAE78:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dae94
	if (!cr6.gt) goto loc_825DAE94;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// b 0x825daea4
	goto loc_825DAEA4;
loc_825DAE94:
	// bge cr6,0x825daea8
	if (!cr6.lt) goto loc_825DAEA8;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_825DAEA4:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825DAEA8:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x825dae78
	if (!cr6.eq) goto loc_825DAE78;
loc_825DAEBC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// blt cr6,0x825dad68
	if (cr6.lt) goto loc_825DAD68;
loc_825DAEE4:
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x825dafa4
	if (cr6.lt) goto loc_825DAFA4;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
loc_825DAEFC:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r9,720(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 720);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x825daf24
	if (!cr6.gt) goto loc_825DAF24;
	// sthx r9,r11,r4
	PPC_STORE_U16(r11.u32 + ctx.r4.u32, ctx.r9.u16);
	// b 0x825daf3c
	goto loc_825DAF3C;
loc_825DAF24:
	// lwz r9,724(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 724);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825daf38
	if (!cr6.lt) goto loc_825DAF38;
	// sthx r9,r11,r4
	PPC_STORE_U16(r11.u32 + ctx.r4.u32, ctx.r9.u16);
	// b 0x825daf3c
	goto loc_825DAF3C;
loc_825DAF38:
	// sthx r10,r11,r4
	PPC_STORE_U16(r11.u32 + ctx.r4.u32, ctx.r10.u16);
loc_825DAF3C:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825daf60
	if (!cr6.gt) goto loc_825DAF60;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lhz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r9,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r9.u16);
	// b 0x825daf94
	goto loc_825DAF94;
loc_825DAF60:
	// bge cr6,0x825daf84
	if (!cr6.lt) goto loc_825DAF84;
	// lhz r11,48(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sthx r11,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r11.u16);
	// b 0x825daf94
	goto loc_825DAF94;
loc_825DAF84:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r25,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, r25.u16);
loc_825DAF94:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x825daefc
	if (!cr6.lt) goto loc_825DAEFC;
loc_825DAFA4:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825daffc
	if (!cr6.eq) goto loc_825DAFFC;
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
loc_825DAFFC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825DB004"))) PPC_WEAK_FUNC(sub_825DB004);
PPC_FUNC_IMPL(__imp__sub_825DB004) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DB008"))) PPC_WEAK_FUNC(sub_825DB008);
PPC_FUNC_IMPL(__imp__sub_825DB008) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// add r28,r10,r9
	r28.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r26,24(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// ble cr6,0x825db1cc
	if (!cr6.gt) goto loc_825DB1CC;
	// subf r27,r6,r5
	r27.s64 = ctx.r5.s64 - ctx.r6.s64;
loc_825DB050:
	// lwzx r10,r27,r6
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + ctx.r6.u32);
	// lwz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825db104
	if (!cr6.gt) goto loc_825DB104;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mullw. r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825db0a8
	if (!cr0.gt) goto loc_825DB0A8;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r7,r3,r28
	ctx.r7.s64 = r28.s64 - ctx.r3.s64;
loc_825DB07C:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r9,r7,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + r11.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r9,34(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825db07c
	if (cr6.lt) goto loc_825DB07C;
loc_825DB0A8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825db1a4
	if (!cr6.gt) goto loc_825DB1A4;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
loc_825DB0BC:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825db0d8
	if (!cr6.gt) goto loc_825DB0D8;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// b 0x825db0e8
	goto loc_825DB0E8;
loc_825DB0D8:
	// bge cr6,0x825db0ec
	if (!cr6.lt) goto loc_825DB0EC;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
loc_825DB0E8:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825DB0EC:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x825db0bc
	if (!cr6.eq) goto loc_825DB0BC;
	// b 0x825db1a4
	goto loc_825DB1A4;
loc_825DB104:
	// bge cr6,0x825db1a4
	if (!cr6.lt) goto loc_825DB1A4;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mullw. r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x825db14c
	if (!cr0.gt) goto loc_825DB14C;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// subf r9,r3,r28
	ctx.r9.s64 = r28.s64 - ctx.r3.s64;
loc_825DB120:
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lhz r8,34(r30)
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x825db120
	if (cr6.lt) goto loc_825DB120;
loc_825DB14C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825db1a4
	if (!cr6.gt) goto loc_825DB1A4;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
loc_825DB160:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825db17c
	if (!cr6.gt) goto loc_825DB17C;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// b 0x825db18c
	goto loc_825DB18C;
loc_825DB17C:
	// bge cr6,0x825db190
	if (!cr6.lt) goto loc_825DB190;
	// lhz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_825DB18C:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
loc_825DB190:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x825db160
	if (!cr6.eq) goto loc_825DB160;
loc_825DB1A4:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// blt cr6,0x825db050
	if (cr6.lt) goto loc_825DB050;
loc_825DB1CC:
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x825db28c
	if (cr6.lt) goto loc_825DB28C;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
loc_825DB1E4:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r9,720(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 720);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x825db20c
	if (!cr6.gt) goto loc_825DB20C;
	// stwx r9,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r9.u32);
	// b 0x825db224
	goto loc_825DB224;
loc_825DB20C:
	// lwz r9,724(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 724);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825db220
	if (!cr6.lt) goto loc_825DB220;
	// stwx r9,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r9.u32);
	// b 0x825db224
	goto loc_825DB224;
loc_825DB220:
	// stwx r10,r11,r26
	PPC_STORE_U32(r11.u32 + r26.u32, ctx.r10.u32);
loc_825DB224:
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825db248
	if (!cr6.gt) goto loc_825DB248;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// lhz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r9,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, ctx.r9.u16);
	// b 0x825db27c
	goto loc_825DB27C;
loc_825DB248:
	// bge cr6,0x825db26c
	if (!cr6.lt) goto loc_825DB26C;
	// lhz r11,48(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 48);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sthx r11,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r11.u16);
	// b 0x825db27c
	goto loc_825DB27C;
loc_825DB26C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r25,r11,r10
	PPC_STORE_U16(r11.u32 + ctx.r10.u32, r25.u16);
loc_825DB27C:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x825db1e4
	if (!cr6.lt) goto loc_825DB1E4;
loc_825DB28C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825db2e8
	if (!cr6.eq) goto loc_825DB2E8;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
loc_825DB2E8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825DB2F0"))) PPC_WEAK_FUNC(sub_825DB2F0);
PPC_FUNC_IMPL(__imp__sub_825DB2F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DB348"))) PPC_WEAK_FUNC(sub_825DB348);
PPC_FUNC_IMPL(__imp__sub_825DB348) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r30,r4,16
	r30.u64 = ctx.r4.u32 & 0xFFFF;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm r29,r30,1,0,30
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,64
	ctx.r4.s64 = 64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// li r28,0
	r28.s64 = 0;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// bne cr6,0x825db390
	if (!cr6.eq) goto loc_825DB390;
loc_825DB380:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_825DB390:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r4,64
	ctx.r4.s64 = 64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r3.u32);
	// beq cr6,0x825db380
	if (cr6.eq) goto loc_825DB380;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// rlwinm r29,r30,3,0,28
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// li r4,64
	ctx.r4.s64 = 64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r3.u32);
	// beq cr6,0x825db380
	if (cr6.eq) goto loc_825DB380;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,64
	ctx.r4.s64 = 64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// beq cr6,0x825db380
	if (cr6.eq) goto loc_825DB380;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r4,64
	ctx.r4.s64 = 64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825c5f00
	sub_825C5F00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r3.u32);
	// beq cr6,0x825db380
	if (cr6.eq) goto loc_825DB380;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825DB440"))) PPC_WEAK_FUNC(sub_825DB440);
PPC_FUNC_IMPL(__imp__sub_825DB440) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// bl 0x825c5fb8
	sub_825C5FB8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DB490"))) PPC_WEAK_FUNC(sub_825DB490);
PPC_FUNC_IMPL(__imp__sub_825DB490) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc4
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r4,36(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// li r5,0
	ctx.r5.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db5a8
	if (!cr6.gt) goto loc_825DB5A8;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
loc_825DB4D4:
	// lhz r29,0(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r25,0(r10)
	r25.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// extsh r26,r29
	r26.s64 = r29.s16;
	// lhz r29,6(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r22,r25
	r22.s64 = r25.s16;
	// lhz r25,2(r10)
	r25.u64 = PPC_LOAD_U16(ctx.r10.u32 + 2);
	// extsh r23,r29
	r23.s64 = r29.s16;
	// lhz r29,10(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// lhz r30,-6(r10)
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + -6);
	// extsh r19,r25
	r19.s64 = r25.s16;
	// extsh r20,r29
	r20.s64 = r29.s16;
	// lhz r29,8(r10)
	r29.u64 = PPC_LOAD_U16(ctx.r10.u32 + 8);
	// lhz r4,-4(r10)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + -4);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lhz r31,2(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r17,r29
	r17.s64 = r29.s16;
	// lhz r28,-2(r10)
	r28.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r27,4(r11)
	r27.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// lhz r25,14(r11)
	r25.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lhz r29,12(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// extsh r27,r27
	r27.s64 = r27.s16;
	// lhz r24,4(r10)
	r24.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// extsh r16,r25
	r16.s64 = r25.s16;
	// lhz r21,8(r11)
	r21.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r15,r29
	r15.s64 = r29.s16;
	// lhz r18,6(r10)
	r18.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// extsh r24,r24
	r24.s64 = r24.s16;
	// extsh r21,r21
	r21.s64 = r21.s16;
	// extsh r18,r18
	r18.s64 = r18.s16;
	// mullw r25,r30,r26
	r25.s64 = int64_t(r30.s32) * int64_t(r26.s32);
	// mullw r29,r4,r31
	r29.s64 = int64_t(ctx.r4.s32) * int64_t(r31.s32);
	// mullw r30,r28,r27
	r30.s64 = int64_t(r28.s32) * int64_t(r27.s32);
	// mullw r26,r23,r22
	r26.s64 = int64_t(r23.s32) * int64_t(r22.s32);
	// mullw r31,r24,r20
	r31.s64 = int64_t(r24.s32) * int64_t(r20.s32);
	// mullw r27,r19,r21
	r27.s64 = int64_t(r19.s32) * int64_t(r21.s32);
	// mullw r4,r17,r16
	ctx.r4.s64 = int64_t(r17.s32) * int64_t(r16.s32);
	// mullw r28,r18,r15
	r28.s64 = int64_t(r18.s32) * int64_t(r15.s32);
	// add r29,r29,r25
	r29.u64 = r29.u64 + r25.u64;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// add r8,r29,r8
	ctx.r8.u64 = r29.u64 + ctx.r8.u64;
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825db4d4
	if (!cr6.eq) goto loc_825DB4D4;
loc_825DB5A8:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// sraw r3,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	ctx.r3.s64 = r11.s32 >> temp.u32;
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_825DB5C8"))) PPC_WEAK_FUNC(sub_825DB5C8);
PPC_FUNC_IMPL(__imp__sub_825DB5C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// subf r10,r6,r28
	ctx.r10.s64 = r28.s64 - ctx.r6.s64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// ble cr6,0x825db63c
	if (!cr6.gt) goto loc_825DB63C;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db680
	if (!cr6.gt) goto loc_825DB680;
loc_825DB608:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r30.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825db608
	if (cr6.lt) goto loc_825DB608;
	// b 0x825db680
	goto loc_825DB680;
loc_825DB63C:
	// bge cr6,0x825db680
	if (!cr6.lt) goto loc_825DB680;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db680
	if (!cr6.gt) goto loc_825DB680;
loc_825DB650:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r30.u32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825db650
	if (cr6.lt) goto loc_825DB650;
loc_825DB680:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825db6b8
	if (!cr6.eq) goto loc_825DB6B8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r5,r29
	ctx.r3.u64 = ctx.r5.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r5,r30
	ctx.r3.u64 = ctx.r5.u64 + r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_825DB6B8:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lwz r11,720(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 720);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// ble cr6,0x825db6dc
	if (!cr6.gt) goto loc_825DB6DC;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, r11.u16);
	// b 0x825db704
	goto loc_825DB704;
loc_825DB6DC:
	// lwz r11,724(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 724);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x825db6f8
	if (!cr6.lt) goto loc_825DB6F8;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r10,r29
	PPC_STORE_U16(ctx.r10.u32 + r29.u32, r11.u16);
	// b 0x825db704
	goto loc_825DB704;
loc_825DB6F8:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r28,r11,r29
	PPC_STORE_U16(r11.u32 + r29.u32, r28.u16);
loc_825DB704:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825db714
	if (!cr6.gt) goto loc_825DB714;
	// lhz r10,30(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// b 0x825db738
	goto loc_825DB738;
loc_825DB714:
	// bge cr6,0x825db734
	if (!cr6.lt) goto loc_825DB734;
	// lhz r11,30(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sthx r11,r10,r30
	PPC_STORE_U16(ctx.r10.u32 + r30.u32, r11.u16);
	// b 0x825db744
	goto loc_825DB744;
loc_825DB734:
	// li r10,0
	ctx.r10.s64 = 0;
loc_825DB738:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
loc_825DB744:
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825DB78C"))) PPC_WEAK_FUNC(sub_825DB78C);
PPC_FUNC_IMPL(__imp__sub_825DB78C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DB790"))) PPC_WEAK_FUNC(sub_825DB790);
PPC_FUNC_IMPL(__imp__sub_825DB790) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bccc
	// lwz r11,32(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,36(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db87c
	if (!cr6.gt) goto loc_825DB87C;
loc_825DB7C8:
	// lhz r29,2(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r26,r29
	r26.s64 = r29.s16;
	// lhz r29,14(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// lwz r24,16(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r23,r29
	r23.s64 = r29.s16;
	// lhz r28,12(r11)
	r28.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// mullw r29,r4,r24
	r29.s64 = int64_t(ctx.r4.s32) * int64_t(r24.s32);
	// lhz r25,6(r11)
	r25.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// lhz r30,10(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// lhz r27,4(r11)
	r27.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lwz r22,0(r10)
	r22.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r24,24(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// lwz r20,20(r10)
	r20.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// lwz r19,8(r10)
	r19.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// lwz r18,28(r10)
	r18.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// extsh r28,r28
	r28.s64 = r28.s16;
	// lwz r17,12(r10)
	r17.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r21,r25
	r21.s64 = r25.s16;
	// extsh r27,r27
	r27.s64 = r27.s16;
	// mullw r25,r31,r22
	r25.s64 = int64_t(r31.s32) * int64_t(r22.s32);
	// mullw r26,r26,r4
	r26.s64 = int64_t(r26.s32) * int64_t(ctx.r4.s32);
	// mullw r31,r28,r24
	r31.s64 = int64_t(r28.s32) * int64_t(r24.s32);
	// mullw r30,r30,r20
	r30.s64 = int64_t(r30.s32) * int64_t(r20.s32);
	// mullw r27,r27,r19
	r27.s64 = int64_t(r27.s32) * int64_t(r19.s32);
	// mullw r4,r23,r18
	ctx.r4.s64 = int64_t(r23.s32) * int64_t(r18.s32);
	// mullw r28,r21,r17
	r28.s64 = int64_t(r21.s32) * int64_t(r17.s32);
	// add r29,r29,r25
	r29.u64 = r29.u64 + r25.u64;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// add r8,r29,r8
	ctx.r8.u64 = r29.u64 + ctx.r8.u64;
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825db7c8
	if (!cr6.eq) goto loc_825DB7C8;
loc_825DB87C:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// sraw r3,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	ctx.r3.s64 = r11.s32 >> temp.u32;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_825DB89C"))) PPC_WEAK_FUNC(sub_825DB89C);
PPC_FUNC_IMPL(__imp__sub_825DB89C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DB8A0"))) PPC_WEAK_FUNC(sub_825DB8A0);
PPC_FUNC_IMPL(__imp__sub_825DB8A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// subf r10,r6,r28
	ctx.r10.s64 = r28.s64 - ctx.r6.s64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r30,40(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// ble cr6,0x825db914
	if (!cr6.gt) goto loc_825DB914;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db958
	if (!cr6.gt) goto loc_825DB958;
loc_825DB8E0:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r30.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825db8e0
	if (cr6.lt) goto loc_825DB8E0;
	// b 0x825db958
	goto loc_825DB958;
loc_825DB914:
	// bge cr6,0x825db958
	if (!cr6.lt) goto loc_825DB958;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825db958
	if (!cr6.gt) goto loc_825DB958;
loc_825DB928:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r30.u32);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825db928
	if (cr6.lt) goto loc_825DB928;
loc_825DB958:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x825db990
	if (!cr6.eq) goto loc_825DB990;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r5,r29
	ctx.r3.u64 = ctx.r5.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r5,r30
	ctx.r3.u64 = ctx.r5.u64 + r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_825DB990:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lwz r11,720(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 720);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// ble cr6,0x825db9b4
	if (!cr6.gt) goto loc_825DB9B4;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r11.u32);
	// b 0x825db9dc
	goto loc_825DB9DC;
loc_825DB9B4:
	// lwz r11,724(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 724);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x825db9d0
	if (!cr6.lt) goto loc_825DB9D0;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + r29.u32, r11.u32);
	// b 0x825db9dc
	goto loc_825DB9DC;
loc_825DB9D0:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r11,r29
	PPC_STORE_U32(r11.u32 + r29.u32, r28.u32);
loc_825DB9DC:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x825db9ec
	if (!cr6.gt) goto loc_825DB9EC;
	// lhz r10,30(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// b 0x825dba10
	goto loc_825DBA10;
loc_825DB9EC:
	// bge cr6,0x825dba0c
	if (!cr6.lt) goto loc_825DBA0C;
	// lhz r11,30(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 30);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// sthx r11,r10,r30
	PPC_STORE_U16(ctx.r10.u32 + r30.u32, r11.u16);
	// b 0x825dba1c
	goto loc_825DBA1C;
loc_825DBA0C:
	// li r10,0
	ctx.r10.s64 = 0;
loc_825DBA10:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
loc_825DBA1C:
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// sthx r10,r11,r30
	PPC_STORE_U16(r11.u32 + r30.u32, ctx.r10.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825DBA64"))) PPC_WEAK_FUNC(sub_825DBA64);
PPC_FUNC_IMPL(__imp__sub_825DBA64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DBA68"))) PPC_WEAK_FUNC(sub_825DBA68);
PPC_FUNC_IMPL(__imp__sub_825DBA68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// srawi r10,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r10.s64 = r11.s32 >> 4;
	// srawi r9,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r9.s64 = r11.s32 >> 3;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,716(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 716);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x825dbaf0
	if (!cr6.eq) goto loc_825DBAF0;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dbaf0
	if (!cr6.gt) goto loc_825DBAF0;
	// li r11,0
	r11.s64 = 0;
loc_825DBACC:
	// lwz r9,52(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lhzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// sthx r9,r8,r11
	PPC_STORE_U16(ctx.r8.u32 + r11.u32, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// lwz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dbacc
	if (cr6.lt) goto loc_825DBACC;
loc_825DBAF0:
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,1024
	ctx.r5.s64 = 1024;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r10,28(r31)
	PPC_STORE_U16(r31.u32 + 28, ctx.r10.u16);
	// sth r9,30(r31)
	PPC_STORE_U16(r31.u32 + 30, ctx.r9.u16);
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DBB4C"))) PPC_WEAK_FUNC(sub_825DBB4C);
PPC_FUNC_IMPL(__imp__sub_825DBB4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DBB50"))) PPC_WEAK_FUNC(sub_825DBB50);
PPC_FUNC_IMPL(__imp__sub_825DBB50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,120(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	// lwz r11,40(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x825dbb6c
	if (!cr6.eq) goto loc_825DBB6C;
	// lwz r10,32(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_825DBB6C:
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// bne cr6,0x825dbbc0
	if (!cr6.eq) goto loc_825DBBC0;
	// li r10,2
	ctx.r10.s64 = 2;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r9,16
	ctx.r9.s64 = 16;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// sth r10,28(r4)
	PPC_STORE_U16(ctx.r4.u32 + 28, ctx.r10.u16);
	// sth r9,30(r4)
	PPC_STORE_U16(ctx.r4.u32 + 30, ctx.r9.u16);
	// blelr cr6
	if (!cr6.gt) return;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825DBB94:
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r8,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r8.u16);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dbb94
	if (cr6.lt) goto loc_825DBB94;
	// blr 
	return;
loc_825DBBC0:
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r8,8
	ctx.r8.s64 = 8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sth r10,28(r4)
	PPC_STORE_U16(ctx.r4.u32 + 28, ctx.r10.u16);
	// sth r8,30(r4)
	PPC_STORE_U16(ctx.r4.u32 + 30, ctx.r8.u16);
	// blelr cr6
	if (!cr6.gt) return;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825DBBE0:
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// sthx r8,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r8.u16);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dbbe0
	if (cr6.lt) goto loc_825DBBE0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DBC0C"))) PPC_WEAK_FUNC(sub_825DBC0C);
PPC_FUNC_IMPL(__imp__sub_825DBC0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DBC10"))) PPC_WEAK_FUNC(sub_825DBC10);
PPC_FUNC_IMPL(__imp__sub_825DBC10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
loc_825DBC24:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825dbc24
	if (!cr6.eq) goto loc_825DBC24;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DBC44"))) PPC_WEAK_FUNC(sub_825DBC44);
PPC_FUNC_IMPL(__imp__sub_825DBC44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DBC48"))) PPC_WEAK_FUNC(sub_825DBC48);
PPC_FUNC_IMPL(__imp__sub_825DBC48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bgt cr6,0x825dbc9c
	if (cr6.gt) goto loc_825DBC9C;
	// cmpwi cr6,r3,-72
	cr6.compare<int32_t>(ctx.r3.s32, -72, xer);
	// bge cr6,0x825dbc5c
	if (!cr6.lt) goto loc_825DBC5C;
	// li r3,-71
	ctx.r3.s64 = -71;
loc_825DBC5C:
	// neg r11,r3
	r11.s64 = -ctx.r3.s64;
	// rlwinm r8,r3,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r9.s64 = r11.s32 >> 2;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,-4648
	r11.s64 = r11.s64 + -4648;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// slw r11,r10,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 / ctx.f13.f64));
	// blr 
	return;
loc_825DBC9C:
	// cmpwi cr6,r3,62
	cr6.compare<int32_t>(ctx.r3.s32, 62, xer);
	// blt cr6,0x825dbca8
	if (cr6.lt) goto loc_825DBCA8;
	// li r3,62
	ctx.r3.s64 = 62;
loc_825DBCA8:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,-4360
	r11.s64 = r11.s64 + -4360;
	// li r9,1
	ctx.r9.s64 = 1;
	// srawi r8,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r3.s32 >> 2;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	f0.f64 = double(temp.f32);
	// slw r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DBCE4"))) PPC_WEAK_FUNC(sub_825DBCE4);
PPC_FUNC_IMPL(__imp__sub_825DBCE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DBCE8"))) PPC_WEAK_FUNC(sub_825DBCE8);
PPC_FUNC_IMPL(__imp__sub_825DBCE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// lwz r9,52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 52);
	// slw r11,r3,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r9.u8 & 0x3F));
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dbd38
	if (cr6.lt) goto loc_825DBD38;
loc_825DBD14:
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825dbd14
	if (!cr6.lt) goto loc_825DBD14;
loc_825DBD38:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r3,56(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 56);
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// bgelr cr6
	if (!cr6.lt) return;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r9,6
	cr6.compare<int32_t>(ctx.r9.s32, 6, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r6
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// ble cr6,0x825dbd78
	if (!cr6.gt) goto loc_825DBD78;
	// addi r10,r9,-7
	ctx.r10.s64 = ctx.r9.s64 + -7;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r9,r9,-6
	ctx.r9.s64 = ctx.r9.s64 + -6;
	// slw r10,r8,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r10.u8 & 0x3F));
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// sraw r3,r11,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	ctx.r3.s64 = r11.s32 >> temp.u32;
	// b 0x825dbd80
	goto loc_825DBD80;
loc_825DBD78:
	// subfic r10,r9,6
	xer.ca = ctx.r9.u32 <= 6;
	ctx.r10.s64 = 6 - ctx.r9.s64;
	// slw r3,r11,r10
	ctx.r3.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
loc_825DBD80:
	// cmpw cr6,r3,r5
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r5.s32, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DBD90"))) PPC_WEAK_FUNC(sub_825DBD90);
PPC_FUNC_IMPL(__imp__sub_825DBD90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r7,368(r1)
	PPC_STORE_U64(ctx.r1.u32 + 368, ctx.r7.u64);
	// li r24,0
	r24.s64 = 0;
	// lwz r19,12(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// li r21,1
	r21.s64 = 1;
	// lwz r20,16(r4)
	r20.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// lwz r28,4(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// std r8,376(r1)
	PPC_STORE_U64(ctx.r1.u32 + 376, ctx.r8.u64);
	// lwz r29,372(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmpwi cr6,r29,6
	cr6.compare<int32_t>(r29.s32, 6, xer);
	// beq cr6,0x825dbdf4
	if (cr6.eq) goto loc_825DBDF4;
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x825dbdec
	if (cr6.eq) goto loc_825DBDEC;
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// cmpwi cr6,r11,32000
	cr6.compare<int32_t>(r11.s32, 32000, xer);
	// blt cr6,0x825dbdec
	if (cr6.lt) goto loc_825DBDEC;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,44100
	ctx.r10.u64 = ctx.r10.u64 | 44100;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dbdf4
	if (cr6.lt) goto loc_825DBDF4;
loc_825DBDEC:
	// li r18,0
	r18.s64 = 0;
	// b 0x825dbdf8
	goto loc_825DBDF8;
loc_825DBDF4:
	// mr r18,r21
	r18.u64 = r21.u64;
loc_825DBDF8:
	// lwz r11,400(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 400);
	// li r30,0
	r30.s64 = 0;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x825dbe44
	if (!cr6.eq) goto loc_825DBE44;
	// lhz r10,118(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// lwz r9,276(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 276);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r8,256(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// twllei r8,0
	// slw r10,r10,r29
	ctx.r10.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// mullw r9,r10,r9
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// rotlwi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// divw r27,r9,r8
	r27.s32 = ctx.r9.s32 / ctx.r8.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r10.u64;
	// twlgei r10,-1
	// b 0x825dbe48
	goto loc_825DBE48;
loc_825DBE44:
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_825DBE48:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r23,404(r3)
	r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 404);
	// lwz r25,268(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// add r22,r11,r19
	r22.u64 = r11.u64 + r19.u64;
	// add r26,r10,r5
	r26.u64 = ctx.r10.u64 + ctx.r5.u64;
loc_825DBE5C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpw cr6,r11,r23
	cr6.compare<int32_t>(r11.s32, r23.s32, xer);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bgt cr6,0x825dbe70
	if (cr6.gt) goto loc_825DBE70;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
loc_825DBE70:
	// cmpw cr6,r4,r25
	cr6.compare<int32_t>(ctx.r4.s32, r25.s32, xer);
	// bge cr6,0x825dc070
	if (!cr6.lt) goto loc_825DC070;
	// lbz r11,0(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825dc064
	if (!cr6.eq) goto loc_825DC064;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// slw r10,r4,r29
	ctx.r10.u64 = r29.u8 & 0x20 ? 0 : (ctx.r4.u32 << (r29.u8 & 0x3F));
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825dbeb8
	if (cr6.gt) goto loc_825DBEB8;
loc_825DBEA4:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// ble cr6,0x825dbea4
	if (!cr6.gt) goto loc_825DBEA4;
loc_825DBEB8:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x825dbeec
	if (cr6.eq) goto loc_825DBEEC;
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r28.u32);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// clrlwi r24,r11,24
	r24.u64 = r11.u32 & 0xFF;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// stwx r10,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r10.u32);
	// b 0x825dbe5c
	goto loc_825DBE5C;
loc_825DBEEC:
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x825dbf00
	if (cr6.lt) goto loc_825DBF00;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
loc_825DBF00:
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r8,r6
	r11.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// blt cr6,0x825dbf1c
	if (cr6.lt) goto loc_825DBF1C;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
loc_825DBF1C:
	// slw r11,r5,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r5.u32 << (r29.u8 & 0x3F));
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x825dbf30
	if (cr6.lt) goto loc_825DBF30;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_825DBF30:
	// addi r7,r30,1
	ctx.r7.s64 = r30.s64 + 1;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825dbf60
	if (cr6.gt) goto loc_825DBF60;
loc_825DBF4C:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// ble cr6,0x825dbf4c
	if (!cr6.gt) goto loc_825DBF4C;
loc_825DBF60:
	// cmpw cr6,r30,r31
	cr6.compare<int32_t>(r30.s32, r31.s32, xer);
	// bne cr6,0x825dbf98
	if (!cr6.eq) goto loc_825DBF98;
	// lwzx r3,r8,r28
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r28.u32);
	// bl 0x825dbc48
	sub_825DBC48(ctx, base);
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// fmuls f0,f1,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// clrlwi r24,r11,24
	r24.u64 = r11.u32 & 0xFF;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// stfsx f0,r9,r10
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, temp.u32);
	// b 0x825dbe5c
	goto loc_825DBE5C;
loc_825DBF98:
	// cmpwi cr6,r29,6
	cr6.compare<int32_t>(r29.s32, 6, xer);
	// ble cr6,0x825dbfb8
	if (!cr6.gt) goto loc_825DBFB8;
	// addi r11,r29,-7
	r11.s64 = r29.s64 + -7;
	// addi r10,r29,-6
	ctx.r10.s64 = r29.s64 + -6;
	// slw r11,r21,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r21.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sraw r7,r11,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	ctx.r7.s64 = r11.s32 >> temp.u32;
	// b 0x825dbfc0
	goto loc_825DBFC0;
loc_825DBFB8:
	// subfic r11,r29,6
	xer.ca = r29.u32 <= 6;
	r11.s64 = 6 - r29.s64;
	// slw r7,r9,r11
	ctx.r7.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
loc_825DBFC0:
	// lwzx r3,r8,r28
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r28.u32);
	// bl 0x825dbc48
	sub_825DBC48(ctx, base);
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// fmr f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f1.f64;
	// lwzx r3,r11,r28
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r28.u32);
	// bl 0x825dbc48
	sub_825DBC48(ctx, base);
	// extsw r11,r7
	r11.s64 = ctx.r7.s32;
	// fmuls f11,f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// extsw r9,r5
	ctx.r9.s64 = ctx.r5.s32;
	// fmuls f12,f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// subf r10,r4,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r4.s64;
	// extsw r8,r4
	ctx.r8.s64 = ctx.r4.s32;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r24,r11,24
	r24.u64 = r11.u32 & 0xFF;
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f9,104(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f8,f13
	ctx.f8.f64 = double(float(ctx.f13.f64));
	// frsp f13,f10
	ctx.f13.f64 = double(float(ctx.f10.f64));
	// frsp f10,f9
	ctx.f10.f64 = double(float(ctx.f9.f64));
	// fsubs f9,f8,f0
	ctx.f9.f64 = double(float(ctx.f8.f64 - f0.f64));
	// fsubs f0,f0,f10
	f0.f64 = double(float(f0.f64 - ctx.f10.f64));
	// fdivs f10,f9,f13
	ctx.f10.f64 = double(float(ctx.f9.f64 / ctx.f13.f64));
	// fdivs f0,f0,f13
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fmuls f13,f10,f11
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// fmadds f0,f0,f12,f13
	f0.f64 = double(float(f0.f64 * ctx.f12.f64 + ctx.f13.f64));
	// stfsx f0,r10,r7
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, temp.u32);
loc_825DC064:
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// b 0x825dbe5c
	goto loc_825DBE5C;
loc_825DC070:
	// clrlwi r4,r24,24
	ctx.r4.u64 = r24.u32 & 0xFF;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dc0e8
	if (!cr6.gt) goto loc_825DC0E8;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
loc_825DC08C:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x825dc0b8
	if (cr6.eq) goto loc_825DC0B8;
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// lwzx r10,r7,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x825dbc48
	sub_825DBC48(ctx, base);
	// stfsx f1,r7,r20
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r7.u32 + r20.u32, temp.u32);
	// b 0x825dc0d8
	goto loc_825DC0D8;
loc_825DC0B8:
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// lfsx f0,r7,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lfs f13,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f0,f13
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fsqrts f0,f0
	f0.f64 = double(float(sqrt(f0.f64)));
	// stfsx f0,r7,r20
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + r20.u32, temp.u32);
loc_825DC0D8:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x825dc08c
	if (!cr6.eq) goto loc_825DC08C;
loc_825DC0E8:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x825dc104
	if (cr6.eq) goto loc_825DC104;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// add r11,r11,r20
	r11.u64 = r11.u64 + r20.u64;
	// lfs f0,2552(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f0.f64 = double(temp.f32);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
loc_825DC104:
	// stb r24,0(r19)
	PPC_STORE_U8(r19.u32 + 0, r24.u8);
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_825DC110"))) PPC_WEAK_FUNC(sub_825DC110);
PPC_FUNC_IMPL(__imp__sub_825DC110) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lwz r11,80(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 80);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// stw r9,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r9.u32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfs f11,5736(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5736);
	ctx.f11.f64 = double(temp.f32);
	// lfd f0,-16(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f11
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// bne cr6,0x825dc2b8
	if (!cr6.eq) goto loc_825DC2B8;
	// cmpwi cr6,r11,22050
	cr6.compare<int32_t>(r11.s32, 22050, xer);
	// bne cr6,0x825dc1a8
	if (!cr6.eq) goto loc_825DC1A8;
loc_825DC154:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f13,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-31948(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -31948);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// bge cr6,0x825dc2f4
	if (!cr6.lt) goto loc_825DC2F4;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f12,-31944(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -31944);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// blt cr6,0x825dc190
	if (cr6.lt) goto loc_825DC190;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,9192(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 9192);
	ctx.f13.f64 = double(temp.f32);
loc_825DC180:
	// fmuls f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC190:
	// lis r11,-32253
	r11.s64 = -2113732608;
	// lfs f13,-4584(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4584);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC1A8:
	// cmplwi cr6,r11,44100
	cr6.compare<uint32_t>(r11.u32, 44100, xer);
	// bne cr6,0x825dc1dc
	if (!cr6.eq) goto loc_825DC1DC;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f12,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,16284(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16284);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bge cr6,0x825dc2f4
	if (!cr6.lt) goto loc_825DC2F4;
loc_825DC1C4:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f13,-27472(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27472);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC1DC:
	// cmpwi cr6,r11,16000
	cr6.compare<int32_t>(r11.s32, 16000, xer);
	// bne cr6,0x825dc220
	if (!cr6.eq) goto loc_825DC220;
loc_825DC1E4:
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f11
	cr6.compare(ctx.f13.f64, ctx.f11.f64);
	// bgt cr6,0x825dc208
	if (cr6.gt) goto loc_825DC208;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,9196(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 9196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC208:
	// li r11,3
	r11.s64 = 3;
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// stw r11,408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 408, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC220:
	// cmpwi cr6,r11,11025
	cr6.compare<int32_t>(r11.s32, 11025, xer);
	// beq cr6,0x825dc31c
	if (cr6.eq) goto loc_825DC31C;
	// cmpwi cr6,r11,8000
	cr6.compare<int32_t>(r11.s32, 8000, xer);
	// bne cr6,0x825dc26c
	if (!cr6.eq) goto loc_825DC26C;
loc_825DC230:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,16280(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16280);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// ble cr6,0x825dc37c
	if (!cr6.gt) goto loc_825DC37C;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f12,22980(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 22980);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// bgt cr6,0x825dc2f4
	if (cr6.gt) goto loc_825DC2F4;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f13,16276(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16276);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC26C:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f12,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,27856(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27856);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// blt cr6,0x825dc298
	if (cr6.lt) goto loc_825DC298;
loc_825DC280:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,22980(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 22980);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC298:
	// lis r11,-32253
	r11.s64 = -2113732608;
	// lfs f13,-4584(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4584);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// blt cr6,0x825dc37c
	if (cr6.lt) goto loc_825DC37C;
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC2B8:
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bgt cr6,0x825dc2f4
	if (cr6.gt) goto loc_825DC2F4;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,48000
	ctx.r10.u64 = ctx.r10.u64 | 48000;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825dc2f4
	if (cr6.gt) goto loc_825DC2F4;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,44100
	ctx.r10.u64 = ctx.r10.u64 | 44100;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dc304
	if (cr6.lt) goto loc_825DC304;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f12,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,16284(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16284);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// blt cr6,0x825dc1c4
	if (cr6.lt) goto loc_825DC1C4;
loc_825DC2F4:
	// li r11,0
	r11.s64 = 0;
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC304:
	// cmpwi cr6,r11,22050
	cr6.compare<int32_t>(r11.s32, 22050, xer);
	// bge cr6,0x825dc154
	if (!cr6.lt) goto loc_825DC154;
	// cmpwi cr6,r11,16000
	cr6.compare<int32_t>(r11.s32, 16000, xer);
	// bge cr6,0x825dc1e4
	if (!cr6.lt) goto loc_825DC1E4;
	// cmpwi cr6,r11,11025
	cr6.compare<int32_t>(r11.s32, 11025, xer);
	// blt cr6,0x825dc350
	if (cr6.lt) goto loc_825DC350;
loc_825DC31C:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f12,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,9192(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 9192);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
	// lfs f0,-11844(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -11844);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f12,f0
	cr6.compare(ctx.f12.f64, f0.f64);
	// blt cr6,0x825dc384
	if (cr6.lt) goto loc_825DC384;
	// li r11,3
	r11.s64 = 3;
	// stw r11,408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 408, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825DC350:
	// cmpwi cr6,r11,8000
	cr6.compare<int32_t>(r11.s32, 8000, xer);
	// bge cr6,0x825dc230
	if (!cr6.lt) goto loc_825DC230;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f12,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,27856(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27856);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bge cr6,0x825dc280
	if (!cr6.lt) goto loc_825DC280;
	// lis r11,-32253
	r11.s64 = -2113732608;
	// lfs f13,-4584(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4584);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bge cr6,0x825dc180
	if (!cr6.lt) goto loc_825DC180;
loc_825DC37C:
	// fmuls f0,f0,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * ctx.f11.f64));
	// stfs f0,396(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 396, temp.u32);
loc_825DC384:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DC38C"))) PPC_WEAK_FUNC(sub_825DC38C);
PPC_FUNC_IMPL(__imp__sub_825DC38C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DC390"))) PPC_WEAK_FUNC(sub_825DC390);
PPC_FUNC_IMPL(__imp__sub_825DC390) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcfc
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x825dc3b4
	if (!cr6.eq) goto loc_825DC3B4;
	// b 0x8239bd4c
	return;
loc_825DC3B4:
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// lwz r8,60(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lwz r9,344(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 344);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// std r10,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r10.u64);
	// lfd f0,-48(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f0.f64 = double(temp.f32);
	// fdivs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 / ctx.f13.f64));
	// bne cr6,0x825dc488
	if (!cr6.eq) goto loc_825DC488;
	// lwz r10,340(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// lwz r8,412(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lwz r10,340(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dc45c
	if (!cr6.gt) goto loc_825DC45C;
	// addi r8,r1,-48
	ctx.r8.s64 = ctx.r1.s64 + -48;
	// lfs f0,396(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 396);
	f0.f64 = double(temp.f32);
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// addi r10,r10,-4112
	ctx.r10.s64 = ctx.r10.s64 + -4112;
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// lwz r8,-48(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
loc_825DC42C:
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r6,r8
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r8.u32, xer);
	// bgt cr6,0x825dc454
	if (cr6.gt) goto loc_825DC454;
	// lwz r6,340(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// blt cr6,0x825dc42c
	if (cr6.lt) goto loc_825DC42C;
	// b 0x825dc45c
	goto loc_825DC45C;
loc_825DC454:
	// lwz r10,412(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
loc_825DC45C:
	// lwz r10,340(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bne cr6,0x825dc470
	if (!cr6.eq) goto loc_825DC470;
	// stw r7,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r7.u32);
loc_825DC470:
	// lwz r10,412(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x825dc580
	if (cr6.gt) goto loc_825DC580;
loc_825DC480:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// b 0x8239bd4c
	return;
loc_825DC488:
	// lwz r10,244(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 244);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dc580
	if (!cr6.gt) goto loc_825DC580;
	// addi r5,r9,4
	ctx.r5.s64 = ctx.r9.s64 + 4;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f0,5736(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5736);
	f0.f64 = double(temp.f32);
loc_825DC4A8:
	// lwz r8,340(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// slw r29,r4,r7
	r29.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r4.u32 << (ctx.r7.u8 & 0x3F));
	// lwz r6,412(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// addi r31,r1,-48
	r31.s64 = ctx.r1.s64 + -48;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// twllei r29,0
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stwx r8,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r8.u32);
	// lfs f12,396(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 396);
	ctx.f12.f64 = double(temp.f32);
	// lwz r6,252(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 252);
	// lwz r30,340(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// rotlwi r8,r6,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// divw r6,r6,r29
	ctx.r6.s32 = ctx.r6.s32 / r29.s32;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// lwzx r30,r10,r30
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// andc r8,r29,r8
	ctx.r8.u64 = r29.u64 & ~ctx.r8.u64;
	// twlgei r8,-1
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// std r6,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r6.u64);
	// lfd f11,-40(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fmuls f12,f11,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmadds f12,f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + f0.f64));
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r31
	PPC_STORE_U32(r31.u32, ctx.f12.u32);
	// ble cr6,0x825dc558
	if (!cr6.gt) goto loc_825DC558;
	// lwz r6,-48(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
loc_825DC524:
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpw cr6,r31,r6
	cr6.compare<int32_t>(r31.s32, ctx.r6.s32, xer);
	// bgt cr6,0x825dc54c
	if (cr6.gt) goto loc_825DC54C;
	// lwz r31,340(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 340);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwzx r31,r10,r31
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// cmpw cr6,r9,r31
	cr6.compare<int32_t>(ctx.r9.s32, r31.s32, xer);
	// blt cr6,0x825dc524
	if (cr6.lt) goto loc_825DC524;
	// b 0x825dc558
	goto loc_825DC558;
loc_825DC54C:
	// lwz r8,412(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stwx r9,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u32);
loc_825DC558:
	// lwz r9,412(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825dc480
	if (!cr6.gt) goto loc_825DC480;
	// lwz r9,244(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 244);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r5,r5,116
	ctx.r5.s64 = ctx.r5.s64 + 116;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dc4a8
	if (cr6.lt) goto loc_825DC4A8;
loc_825DC580:
	// lwz r10,412(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r10,400(r11)
	PPC_STORE_U32(r11.u32 + 400, ctx.r10.u32);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825DC590"))) PPC_WEAK_FUNC(sub_825DC590);
PPC_FUNC_IMPL(__imp__sub_825DC590) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,424(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 424);
	// lhz r10,114(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 114);
	// lwz r6,52(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r10
	r11.s64 = ctx.r10.s16;
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825dc624
	if (!cr6.gt) goto loc_825DC624;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// twllei r10,0
	// andc r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r8.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// twlgei r10,-1
	// lhz r10,118(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
loc_825DC5FC:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lhz r7,118(r4)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// blt cr6,0x825dc5fc
	if (cr6.lt) goto loc_825DC5FC;
	// blr 
	return;
loc_825DC624:
	// bgelr cr6
	if (!cr6.lt) return;
	// divw r8,r10,r11
	ctx.r8.s32 = ctx.r10.s32 / r11.s32;
	// lhz r9,118(r4)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + 118);
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// twllei r11,0
	// andc r11,r11,r7
	r11.u64 = r11.u64 & ~ctx.r7.u64;
	// rotlwi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// twlgei r11,-1
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// divw r11,r9,r8
	r11.s32 = ctx.r9.s32 / ctx.r8.s32;
	// andc r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r10.u64;
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// twllei r8,0
	// twlgei r10,-1
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// mullw r11,r9,r8
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r5,r8
	ctx.r5.s64 = -ctx.r8.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r10,r6
	ctx.r7.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
loc_825DC688:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825dc6b0
	if (!cr6.gt) goto loc_825DC6B0;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_825DC698:
	// lwz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x825dc698
	if (!cr6.eq) goto loc_825DC698;
loc_825DC6B0:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x825dc688
	if (!cr6.lt) goto loc_825DC688;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DC6C8"))) PPC_WEAK_FUNC(sub_825DC6C8);
PPC_FUNC_IMPL(__imp__sub_825DC6C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5e8
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// mr r18,r25
	r18.u64 = r25.u64;
	// mr r19,r25
	r19.u64 = r25.u64;
	// lwz r11,172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// lwz r14,0(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// stw r31,476(r1)
	PPC_STORE_U32(ctx.r1.u32 + 476, r31.u32);
	// lwz r22,20(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r23,16(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwz r17,56(r31)
	r17.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r14,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r14.u32);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// beq cr6,0x825dc738
	if (cr6.eq) goto loc_825DC738;
	// lwz r24,176(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// b 0x825dc73c
	goto loc_825DC73C;
loc_825DC738:
	// mr r24,r25
	r24.u64 = r25.u64;
loc_825DC73C:
	// lwz r11,224(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 224);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dd684
	if (!cr6.gt) goto loc_825DD684;
	// lhz r11,118(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 118);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825dd684
	if (!cr6.gt) goto loc_825DD684;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825dc770
	if (cr6.eq) goto loc_825DC770;
	// li r5,100
	ctx.r5.s64 = 100;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_825DC770:
	// lhz r11,118(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 118);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// lwz r9,224(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 224);
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// rlwinm r11,r9,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0xFFFFFFC0;
	// twllei r7,0
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 1);
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// divw r11,r11,r7
	r11.s32 = r11.s32 / ctx.r7.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// andc r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 & ~ctx.r10.u64;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// twlgei r10,-1
	// ble cr6,0x825dc7c0
	if (!cr6.gt) goto loc_825DC7C0;
loc_825DC7AC:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srw r10,r11,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r8.u8 & 0x3F));
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bgt cr6,0x825dc7ac
	if (cr6.gt) goto loc_825DC7AC;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
loc_825DC7C0:
	// lwz r7,256(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + 256);
	// twllei r9,0
	// stw r8,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r8.u32);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// rotlwi r8,r7,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// divw r11,r7,r9
	r11.s32 = ctx.r7.s32 / ctx.r9.s32;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// andc r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r8.u64;
	// twlgei r9,-1
	// ble cr6,0x825dc7fc
	if (!cr6.gt) goto loc_825DC7FC;
loc_825DC7EC:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825dc7ec
	if (cr6.gt) goto loc_825DC7EC;
loc_825DC7FC:
	// lwz r9,344(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 344);
	// mulli r10,r10,116
	ctx.r10.s64 = ctx.r10.s64 * 116;
	// add r16,r10,r9
	r16.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x825dc824
	if (!cr6.gt) goto loc_825DC824;
loc_825DC814:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srw r9,r11,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 >> (ctx.r10.u8 & 0x3F));
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bgt cr6,0x825dc814
	if (cr6.gt) goto loc_825DC814;
loc_825DC824:
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,340(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 340);
	// lwz r30,308(r20)
	r30.u64 = PPC_LOAD_U32(r20.u32 + 308);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// ld r29,128(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwzx r15,r11,r10
	r15.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
	// rldicr r8,r15,32,63
	ctx.r8.u64 = __builtin_rotateleft64(r15.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// stw r15,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r15.u32);
	// bl 0x825dbd90
	sub_825DBD90(ctx, base);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r5,264(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 264);
	// rldicr r8,r15,32,63
	ctx.r8.u64 = __builtin_rotateleft64(r15.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x825dbce8
	sub_825DBCE8(ctx, base);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bgt cr6,0x825dc89c
	if (cr6.gt) goto loc_825DC89C;
loc_825DC888:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dc888
	if (!cr6.gt) goto loc_825DC888;
loc_825DC89C:
	// lis r11,25
	r11.s64 = 1638400;
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ori r30,r11,26125
	r30.u64 = r11.u64 | 26125;
	// lis r11,15470
	r11.s64 = 1013841920;
	// lis r26,-32244
	r26.s64 = -2113142784;
	// ori r31,r11,62303
	r31.u64 = r11.u64 | 62303;
	// ble cr6,0x825dc980
	if (!cr6.gt) goto loc_825DC980;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// lfs f28,0(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 0);
	f28.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r29,r20,540
	r29.s64 = r20.s64 + 540;
	// rlwinm r27,r21,2,0,29
	r27.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r17
	r28.u64 = r17.u64;
	// lfd f29,32128(r10)
	f29.u64 = PPC_LOAD_U64(ctx.r10.u32 + 32128);
	// lfs f30,-8088(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8088);
	f30.f64 = double(temp.f32);
loc_825DC8DC:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// stw r11,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r11.u32);
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lfs f13,292(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 292);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwzx r11,r27,r11
	r11.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,104(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f2,f0,f30
	ctx.f2.f64 = double(float(f0.f64 * f30.f64));
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f12,f0
	ctx.f12.f64 = double(float(f0.f64));
	// lfs f0,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	f0.f64 = double(temp.f32);
	// fmuls f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmuls f31,f0,f13
	f31.f64 = double(float(f0.f64 * ctx.f13.f64));
	// bl 0x8239e6a0
	sub_8239E6A0(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// fmul f0,f0,f28
	f0.f64 = f0.f64 * f28.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,0(r28)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// lwz r11,264(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 264);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x825dc8dc
	if (cr6.lt) goto loc_825DC8DC;
loc_825DC980:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r5,404(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 404);
	// li r28,1
	r28.s64 = 1;
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// lfs f31,16272(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16272);
	f31.f64 = double(temp.f32);
	// bge cr6,0x825dcd34
	if (!cr6.lt) goto loc_825DCD34;
	// ld r27,128(r1)
	r27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
loc_825DC99C:
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// rldicr r8,r15,32,31
	ctx.r8.u64 = __builtin_rotateleft64(r15.u64, 32) & 0xFFFFFFFF00000000;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825dbce8
	sub_825DBCE8(ctx, base);
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r4,476(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x825d3cb8
	sub_825D3CB8(ctx, base);
	// subf r11,r25,r29
	r11.s64 = r29.s64 - r25.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825dcc14
	if (cr6.lt) goto loc_825DCC14;
	// subf r10,r25,r29
	ctx.r10.s64 = r29.s64 - r25.s64;
	// addi r9,r19,2
	ctx.r9.s64 = r19.s64 + 2;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r8,r25,2
	ctx.r8.s64 = r25.s64 + 2;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r14
	ctx.r10.u64 = ctx.r10.u64 + r14.u64;
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// add r19,r8,r19
	r19.u64 = ctx.r8.u64 + r19.u64;
	// add r25,r8,r25
	r25.u64 = ctx.r8.u64 + r25.u64;
loc_825DCA10:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,-8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,-8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + -8, temp.u32);
	// beq cr6,0x825dca8c
	if (cr6.eq) goto loc_825DCA8C;
	// lwz r8,-8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dca8c
	if (cr6.eq) goto loc_825DCA8C;
	// rlwinm r8,r18,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r8,r24
	PPC_STORE_U32(ctx.r8.u32 + r24.u32, r28.u32);
loc_825DCA8C:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,232(r1)
	PPC_STORE_U64(ctx.r1.u32 + 232, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r8.u64);
	// lfd f0,232(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 232);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,160(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,-4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
	// beq cr6,0x825dcb08
	if (cr6.eq) goto loc_825DCB08;
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dcb08
	if (cr6.eq) goto loc_825DCB08;
	// rlwinm r8,r18,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r8,r24
	PPC_STORE_U32(ctx.r8.u32 + r24.u32, r28.u32);
loc_825DCB08:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r8.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// beq cr6,0x825dcb84
	if (cr6.eq) goto loc_825DCB84;
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dcb84
	if (cr6.eq) goto loc_825DCB84;
	// rlwinm r8,r18,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r8,r24
	PPC_STORE_U32(ctx.r8.u32 + r24.u32, r28.u32);
loc_825DCB84:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r8.u64);
	// lfd f0,184(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,216(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// beq cr6,0x825dcc00
	if (cr6.eq) goto loc_825DCC00;
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dcc00
	if (cr6.eq) goto loc_825DCC00;
	// rlwinm r8,r18,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r8,r24
	PPC_STORE_U32(ctx.r8.u32 + r24.u32, r28.u32);
loc_825DCC00:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825dca10
	if (!cr6.eq) goto loc_825DCA10;
loc_825DCC14:
	// cmpw cr6,r25,r29
	cr6.compare<int32_t>(r25.s32, r29.s32, xer);
	// bge cr6,0x825dcccc
	if (!cr6.lt) goto loc_825DCCCC;
	// subf r6,r25,r29
	ctx.r6.s64 = r29.s64 - r25.s64;
	// rlwinm r9,r19,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// add r8,r9,r14
	ctx.r8.u64 = ctx.r9.u64 + r14.u64;
	// add r7,r10,r17
	ctx.r7.u64 = ctx.r10.u64 + r17.u64;
	// add r19,r6,r19
	r19.u64 = ctx.r6.u64 + r19.u64;
	// add r25,r6,r25
	r25.u64 = ctx.r6.u64 + r25.u64;
loc_825DCC3C:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// srawi r10,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r9,r5,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r5.s64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r10.u64);
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// std r10,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r10.u64);
	// lfd f0,200(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// beq cr6,0x825dccb8
	if (cr6.eq) goto loc_825DCCB8;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825dccb8
	if (cr6.eq) goto loc_825DCCB8;
	// rlwinm r10,r18,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r10,r24
	PPC_STORE_U32(ctx.r10.u32 + r24.u32, r28.u32);
loc_825DCCB8:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825dcc3c
	if (!cr6.eq) goto loc_825DCC3C;
loc_825DCCCC:
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r25,1
	r11.s64 = r25.s64 + 1;
	// addi r10,r21,1
	ctx.r10.s64 = r21.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r16
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r16.u32);
	// slw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r9.u8 & 0x3F));
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dccf8
	if (cr6.lt) goto loc_825DCCF8;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
loc_825DCCF8:
	// addi r11,r18,1
	r11.s64 = r18.s64 + 1;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r25,r10
	cr6.compare<int32_t>(r25.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dcd28
	if (cr6.lt) goto loc_825DCD28;
loc_825DCD14:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r25,r10
	cr6.compare<int32_t>(r25.s32, ctx.r10.s32, xer);
	// bge cr6,0x825dcd14
	if (!cr6.lt) goto loc_825DCD14;
loc_825DCD28:
	// lwz r5,404(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 404);
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// blt cr6,0x825dc99c
	if (cr6.lt) goto loc_825DC99C;
loc_825DCD34:
	// lwz r5,268(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// bge cr6,0x825dd3ac
	if (!cr6.lt) goto loc_825DD3AC;
	// subf r22,r23,r22
	r22.s64 = r22.s64 - r23.s64;
loc_825DCD44:
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// lbzx r11,r11,r18
	r11.u64 = PPC_LOAD_U8(r11.u32 + r18.u32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825dd010
	if (!cr6.eq) goto loc_825DD010;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825dcd68
	if (cr6.eq) goto loc_825DCD68;
	// rlwinm r11,r18,2,0,29
	r11.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// stwx r10,r11,r24
	PPC_STORE_U32(r11.u32 + r24.u32, ctx.r10.u32);
loc_825DCD68:
	// lwzx r3,r22,r23
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + r23.u32);
	// bl 0x825cd8d8
	sub_825CD8D8(ctx, base);
	// addi r10,r18,1
	ctx.r10.s64 = r18.s64 + 1;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,268(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// lfs f0,0(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 0);
	f0.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f12,f0,f1
	ctx.f12.f64 = double(float(f0.f64 * ctx.f1.f64));
	// add r27,r10,r9
	r27.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// blt cr6,0x825dcda0
	if (cr6.lt) goto loc_825DCDA0;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
loc_825DCDA0:
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// bge cr6,0x825dd008
	if (!cr6.lt) goto loc_825DD008;
loc_825DCDA8:
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// rlwinm r11,r21,2,0,29
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x825dbc48
	sub_825DBC48(ctx, base);
	// rldicr r8,r15,32,31
	ctx.r8.u64 = __builtin_rotateleft64(r15.u64, 32) & 0xFFFFFFFF00000000;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// ld r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmuls f0,f1,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825dbce8
	sub_825DBCE8(ctx, base);
	// subf r11,r25,r3
	r11.s64 = ctx.r3.s64 - r25.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825dcf58
	if (cr6.lt) goto loc_825DCF58;
	// subf r10,r25,r3
	ctx.r10.s64 = ctx.r3.s64 - r25.s64;
	// addi r9,r25,2
	ctx.r9.s64 = r25.s64 + 2;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// add r10,r8,r17
	ctx.r10.u64 = ctx.r8.u64 + r17.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r8,r25
	r25.u64 = ctx.r8.u64 + r25.u64;
loc_825DCE10:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r8.u64);
	// lfd f13,152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-8(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r8.u64);
	// lfd f13,200(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r8.u64);
	// lfd f13,216(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r8.u64);
	// lfd f13,184(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825dce10
	if (!cr6.eq) goto loc_825DCE10;
	// lwz r15,136(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r14,240(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 240);
loc_825DCF58:
	// cmpw cr6,r25,r3
	cr6.compare<int32_t>(r25.s32, ctx.r3.s32, xer);
	// bge cr6,0x825dcfd0
	if (!cr6.lt) goto loc_825DCFD0;
	// rlwinm r9,r25,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r25,r3
	ctx.r10.s64 = ctx.r3.s64 - r25.s64;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// add r25,r10,r25
	r25.u64 = ctx.r10.u64 + r25.u64;
loc_825DCF74:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r8.u64);
	// lfd f13,176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825dcf74
	if (!cr6.eq) goto loc_825DCF74;
loc_825DCFD0:
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r25,1
	r11.s64 = r25.s64 + 1;
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r21,1
	ctx.r10.s64 = r21.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r16
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r16.u32);
	// slw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r9.u8 & 0x3F));
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dd000
	if (cr6.lt) goto loc_825DD000;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
loc_825DD000:
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// blt cr6,0x825dcda8
	if (cr6.lt) goto loc_825DCDA8;
loc_825DD008:
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// b 0x825dd350
	goto loc_825DD350;
loc_825DD010:
	// rldicr r8,r15,32,31
	ctx.r8.u64 = __builtin_rotateleft64(r15.u64, 32) & 0xFFFFFFFF00000000;
	// ld r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x825dbce8
	sub_825DBCE8(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r28,r18,2,0,29
	r28.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// add r11,r28,r11
	r11.u64 = r28.u64 + r11.u64;
	// addi r27,r11,4
	r27.s64 = r11.s64 + 4;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// ble cr6,0x825dd04c
	if (!cr6.gt) goto loc_825DD04C;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_825DD04C:
	// lwz r21,80(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,476(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// bl 0x825d3cb8
	sub_825D3CB8(ctx, base);
	// subf r11,r25,r29
	r11.s64 = r29.s64 - r25.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825dd29c
	if (cr6.lt) goto loc_825DD29C;
	// subf r10,r25,r29
	ctx.r10.s64 = r29.s64 - r25.s64;
	// addi r9,r19,2
	ctx.r9.s64 = r19.s64 + 2;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r8,r25,2
	ctx.r8.s64 = r25.s64 + 2;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r14
	ctx.r10.u64 = ctx.r10.u64 + r14.u64;
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
	// add r19,r8,r19
	r19.u64 = ctx.r8.u64 + r19.u64;
	// add r25,r8,r25
	r25.u64 = ctx.r8.u64 + r25.u64;
loc_825DD0A8:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,-8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r8.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,160(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,-8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + -8, temp.u32);
	// beq cr6,0x825dd120
	if (cr6.eq) goto loc_825DD120;
	// lwz r8,-8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dd120
	if (cr6.eq) goto loc_825DD120;
	// stwx r5,r28,r24
	PPC_STORE_U32(r28.u32 + r24.u32, ctx.r5.u32);
loc_825DD120:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,232(r1)
	PPC_STORE_U64(ctx.r1.u32 + 232, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f0,232(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 232);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,-4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
	// beq cr6,0x825dd198
	if (cr6.eq) goto loc_825DD198;
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dd198
	if (cr6.eq) goto loc_825DD198;
	// stwx r5,r28,r24
	PPC_STORE_U32(r28.u32 + r24.u32, ctx.r5.u32);
loc_825DD198:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,248(r1)
	PPC_STORE_U64(ctx.r1.u32 + 248, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,248(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 248);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// beq cr6,0x825dd210
	if (cr6.eq) goto loc_825DD210;
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dd210
	if (cr6.eq) goto loc_825DD210;
	// stwx r5,r28,r24
	PPC_STORE_U32(r28.u32 + r24.u32, ctx.r5.u32);
loc_825DD210:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r8.u64);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, ctx.r8.u64);
	// lfd f0,192(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// beq cr6,0x825dd288
	if (cr6.eq) goto loc_825DD288;
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x825dd288
	if (cr6.eq) goto loc_825DD288;
	// stwx r5,r28,r24
	PPC_STORE_U32(r28.u32 + r24.u32, ctx.r5.u32);
loc_825DD288:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825dd0a8
	if (!cr6.eq) goto loc_825DD0A8;
loc_825DD29C:
	// cmpw cr6,r25,r29
	cr6.compare<int32_t>(r25.s32, r29.s32, xer);
	// bge cr6,0x825dd350
	if (!cr6.lt) goto loc_825DD350;
	// subf r6,r25,r29
	ctx.r6.s64 = r29.s64 - r25.s64;
	// rlwinm r9,r19,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// add r8,r9,r14
	ctx.r8.u64 = ctx.r9.u64 + r14.u64;
	// add r7,r10,r17
	ctx.r7.u64 = ctx.r10.u64 + r17.u64;
	// add r19,r6,r19
	r19.u64 = ctx.r6.u64 + r19.u64;
	// add r25,r6,r25
	r25.u64 = ctx.r6.u64 + r25.u64;
loc_825DD2C4:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// srawi r10,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r9,r4,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r4.s64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,224(r1)
	PPC_STORE_U64(ctx.r1.u32 + 224, ctx.r10.u64);
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f0,224(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 224);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// beq cr6,0x825dd33c
	if (cr6.eq) goto loc_825DD33C;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x825dd33c
	if (cr6.eq) goto loc_825DD33C;
	// stwx r5,r28,r24
	PPC_STORE_U32(r28.u32 + r24.u32, ctx.r5.u32);
loc_825DD33C:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x825dd2c4
	if (!cr6.eq) goto loc_825DD2C4;
loc_825DD350:
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r25,1
	r11.s64 = r25.s64 + 1;
	// addi r10,r21,1
	ctx.r10.s64 = r21.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r16
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r16.u32);
	// slw r11,r11,r9
	r11.u64 = ctx.r9.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r9.u8 & 0x3F));
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dd37c
	if (cr6.lt) goto loc_825DD37C;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
loc_825DD37C:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x825dd3a0
	if (cr6.lt) goto loc_825DD3A0;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_825DD38C:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r25,r10
	cr6.compare<int32_t>(r25.s32, ctx.r10.s32, xer);
	// bge cr6,0x825dd38c
	if (!cr6.lt) goto loc_825DD38C;
loc_825DD3A0:
	// lwz r5,268(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// cmpw cr6,r25,r5
	cr6.compare<int32_t>(r25.s32, ctx.r5.s32, xer);
	// blt cr6,0x825dcd44
	if (cr6.lt) goto loc_825DCD44;
loc_825DD3AC:
	// lwz r11,476(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpw cr6,r25,r29
	cr6.compare<int32_t>(r25.s32, r29.s32, xer);
	// bge cr6,0x825dd600
	if (!cr6.lt) goto loc_825DD600;
	// lwz r10,268(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// rlwinm r11,r21,2,0,29
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r11,r11,r16
	r11.u64 = r11.u64 + r16.u64;
	// slw r10,r10,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srawi r10,r10,6
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 6;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825dd3fc
	if (!cr6.lt) goto loc_825DD3FC;
loc_825DD3E8:
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825dd3e8
	if (cr6.lt) goto loc_825DD3E8;
loc_825DD3FC:
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// lwz r4,476(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x825d3cb8
	sub_825D3CB8(ctx, base);
	// subf r11,r25,r29
	r11.s64 = r29.s64 - r25.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f0,16264(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16264);
	f0.f64 = double(temp.f32);
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// blt cr6,0x825dd58c
	if (cr6.lt) goto loc_825DD58C;
	// subf r10,r25,r29
	ctx.r10.s64 = r29.s64 - r25.s64;
	// addi r9,r25,2
	ctx.r9.s64 = r25.s64 + 2;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// add r10,r8,r17
	ctx.r10.u64 = ctx.r8.u64 + r17.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r25,r8,r25
	r25.u64 = ctx.r8.u64 + r25.u64;
loc_825DD44C:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-8(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,224(r1)
	PPC_STORE_U64(ctx.r1.u32 + 224, ctx.r8.u64);
	// lfd f13,224(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 224);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, ctx.r8.u64);
	// lfd f13,208(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r8.u64);
	// lfd f13,192(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825dd44c
	if (!cr6.eq) goto loc_825DD44C;
loc_825DD58C:
	// cmpw cr6,r25,r29
	cr6.compare<int32_t>(r25.s32, r29.s32, xer);
	// bge cr6,0x825dd600
	if (!cr6.lt) goto loc_825DD600;
	// rlwinm r9,r25,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r20,540
	r11.s64 = r20.s64 + 540;
	// subf r10,r25,r29
	ctx.r10.s64 = r29.s64 - r25.s64;
	// add r9,r9,r17
	ctx.r9.u64 = ctx.r9.u64 + r17.u64;
loc_825DD5A4:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,16268(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 16268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825dd5a4
	if (!cr6.eq) goto loc_825DD5A4;
loc_825DD600:
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x825dd670
	if (cr6.eq) goto loc_825DD670;
	// lwz r11,304(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 304);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x825dd670
	if (cr6.lt) goto loc_825DD670;
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,268(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 268);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
loc_825DD628:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x825dd640
	if (cr6.lt) goto loc_825DD640;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x825dd664
	if (cr6.lt) goto loc_825DD664;
loc_825DD640:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x825dd628
	if (!cr6.lt) goto loc_825DD628;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d634
	// b 0x8239bd10
	return;
loc_825DD664:
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// stwx r10,r11,r24
	PPC_STORE_U32(r11.u32 + r24.u32, ctx.r10.u32);
loc_825DD670:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d634
	// b 0x8239bd10
	return;
loc_825DD684:
	// lis r3,-32764
	ctx.r3.s64 = -2147221504;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d634
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825DD69C"))) PPC_WEAK_FUNC(sub_825DD69C);
PPC_FUNC_IMPL(__imp__sub_825DD69C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD6A0"))) PPC_WEAK_FUNC(sub_825DD6A0);
PPC_FUNC_IMPL(__imp__sub_825DD6A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// lwz r26,0(r23)
	r26.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwz r27,0(r22)
	r27.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// lwz r28,0(r21)
	r28.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmpw cr6,r26,r5
	cr6.compare<int32_t>(r26.s32, ctx.r5.s32, xer);
	// bge cr6,0x825dd760
	if (!cr6.lt) goto loc_825DD760;
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r26,r5
	r29.s64 = ctx.r5.s64 - r26.s64;
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// add r26,r29,r26
	r26.u64 = r29.u64 + r26.u64;
	// li r24,1
	r24.s64 = 1;
loc_825DD6EC:
	// lhz r11,110(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 110);
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r24,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r24.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// bge cr6,0x825dd714
	if (!cr6.lt) goto loc_825DD714;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// b 0x825dd720
	goto loc_825DD720;
loc_825DD714:
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// ble cr6,0x825dd720
	if (!cr6.gt) goto loc_825DD720;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
loc_825DD720:
	// lwz r11,520(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 520);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 0);
	// lwz r9,88(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mullw r11,r9,r25
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// sth r10,0(r27)
	PPC_STORE_U16(r27.u32 + 0, ctx.r10.u16);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825dd6ec
	if (!cr6.eq) goto loc_825DD6EC;
loc_825DD760:
	// stw r26,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r26.u32);
	// stw r27,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r27.u32);
	// stw r28,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r28.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_825DD774"))) PPC_WEAK_FUNC(sub_825DD774);
PPC_FUNC_IMPL(__imp__sub_825DD774) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD778"))) PPC_WEAK_FUNC(sub_825DD778);
PPC_FUNC_IMPL(__imp__sub_825DD778) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmpw cr6,r3,r5
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r5.s32, xer);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bge cr6,0x825dd818
	if (!cr6.lt) goto loc_825DD818;
	// subf r10,r3,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r3.s64;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r29,-128
	r29.s64 = -8388608;
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lis r9,127
	ctx.r9.s64 = 8323072;
	// ori r30,r9,65535
	r30.u64 = ctx.r9.u64 | 65535;
loc_825DD7BC:
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// stw r9,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, ctx.r9.u32);
	// bge cr6,0x825dd7d4
	if (!cr6.lt) goto loc_825DD7D4;
	// stw r29,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r29.u32);
	// b 0x825dd7e0
	goto loc_825DD7E0;
loc_825DD7D4:
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// ble cr6,0x825dd7e0
	if (!cr6.gt) goto loc_825DD7E0;
	// stw r30,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r30.u32);
loc_825DD7E0:
	// lbz r9,-63(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -63);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r28,-62(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + -62);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r27,-61(r1)
	r27.u64 = PPC_LOAD_U8(ctx.r1.u32 + -61);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// stb r28,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r28.u8);
	// stb r27,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r27.u8);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r9,0(r31)
	PPC_STORE_U16(r31.u32 + 0, ctx.r9.u16);
	// bne cr6,0x825dd7bc
	if (!cr6.eq) goto loc_825DD7BC;
loc_825DD818:
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825DD828"))) PPC_WEAK_FUNC(sub_825DD828);
PPC_FUNC_IMPL(__imp__sub_825DD828) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmpw cr6,r3,r5
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r5.s32, xer);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bge cr6,0x825dd8c4
	if (!cr6.lt) goto loc_825DD8C4;
	// subf r10,r3,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r3.s64;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r29,-8
	r29.s64 = -524288;
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lis r9,7
	ctx.r9.s64 = 458752;
	// ori r30,r9,65535
	r30.u64 = ctx.r9.u64 | 65535;
loc_825DD86C:
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// bge cr6,0x825dd880
	if (!cr6.lt) goto loc_825DD880;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// b 0x825dd88c
	goto loc_825DD88C;
loc_825DD880:
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// ble cr6,0x825dd88c
	if (!cr6.gt) goto loc_825DD88C;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_825DD88C:
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, ctx.r9.u32);
	// lhz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + -48);
	// lbz r28,-46(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + -46);
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// stb r28,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r28.u8);
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// sth r9,0(r31)
	PPC_STORE_U16(r31.u32 + 0, ctx.r9.u16);
	// bne cr6,0x825dd86c
	if (!cr6.eq) goto loc_825DD86C;
loc_825DD8C4:
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_825DD8D4"))) PPC_WEAK_FUNC(sub_825DD8D4);
PPC_FUNC_IMPL(__imp__sub_825DD8D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD8D8"))) PPC_WEAK_FUNC(sub_825DD8D8);
PPC_FUNC_IMPL(__imp__sub_825DD8D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmpw cr6,r3,r5
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r5.s32, xer);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bge cr6,0x825dd950
	if (!cr6.lt) goto loc_825DD950;
	// subf r11,r3,r5
	r11.s64 = ctx.r5.s64 - ctx.r3.s64;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r9,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
loc_825DD90C:
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r9,-32768
	cr6.compare<int32_t>(ctx.r9.s32, -32768, xer);
	// bge cr6,0x825dd920
	if (!cr6.lt) goto loc_825DD920;
	// li r9,-32768
	ctx.r9.s64 = -32768;
	// b 0x825dd92c
	goto loc_825DD92C;
loc_825DD920:
	// cmpwi cr6,r9,32767
	cr6.compare<int32_t>(ctx.r9.s32, 32767, xer);
	// ble cr6,0x825dd92c
	if (!cr6.gt) goto loc_825DD92C;
	// li r9,32767
	ctx.r9.s64 = 32767;
loc_825DD92C:
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lhz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// sth r9,0(r31)
	PPC_STORE_U16(r31.u32 + 0, ctx.r9.u16);
	// bne cr6,0x825dd90c
	if (!cr6.eq) goto loc_825DD90C;
loc_825DD950:
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD968"))) PPC_WEAK_FUNC(sub_825DD968);
PPC_FUNC_IMPL(__imp__sub_825DD968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,56(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// lhz r11,210(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 210);
	// lhz r8,0(r6)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// sth r7,0(r6)
	PPC_STORE_U16(ctx.r6.u32 + 0, ctx.r7.u16);
	// stw r5,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r5.u32);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,356(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 356);
	// extsh r10,r8
	ctx.r10.s64 = ctx.r8.s16;
	// stw r6,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r6.u32);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwzx r10,r5,r7
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r7.u32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825dda68
	if (!cr6.lt) goto loc_825DDA68;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// blt cr6,0x825dd9cc
	if (cr6.lt) goto loc_825DD9CC;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
loc_825DD9CC:
	// lwz r11,256(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x825dd9f0
	if (!cr6.gt) goto loc_825DD9F0;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825DD9F0:
	// lwz r11,96(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// cmpwi cr6,r11,61
	cr6.compare<int32_t>(r11.s32, 61, xer);
	// addi r6,r1,156
	ctx.r6.s64 = ctx.r1.s64 + 156;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// beq cr6,0x825dda64
	if (cr6.eq) goto loc_825DDA64;
	// cmpwi cr6,r11,78
	cr6.compare<int32_t>(r11.s32, 78, xer);
	// beq cr6,0x825dda4c
	if (cr6.eq) goto loc_825DDA4C;
	// cmpwi cr6,r11,94
	cr6.compare<int32_t>(r11.s32, 94, xer);
	// beq cr6,0x825dda34
	if (cr6.eq) goto loc_825DDA34;
	// bl 0x825dd6a0
	sub_825DD6A0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825DDA34:
	// bl 0x825dd778
	sub_825DD778(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825DDA4C:
	// bl 0x825dd828
	sub_825DD828(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_825DDA64:
	// bl 0x825dd8d8
	sub_825DD8D8(ctx, base);
loc_825DDA68:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DDA7C"))) PPC_WEAK_FUNC(sub_825DDA7C);
PPC_FUNC_IMPL(__imp__sub_825DDA7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DDA80"))) PPC_WEAK_FUNC(sub_825DDA80);
PPC_FUNC_IMPL(__imp__sub_825DDA80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// li r8,0
	ctx.r8.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ddafc
	if (cr6.eq) goto loc_825DDAFC;
	// li r31,0
	r31.s64 = 0;
loc_825DDAB4:
	// rlwinm r7,r31,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,320(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 320);
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// mulli r11,r31,1776
	r11.s64 = r31.s64 * 1776;
	// lwzx r5,r7,r28
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + r28.u32);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// add r6,r9,r29
	ctx.r6.u64 = ctx.r9.u64 + r29.u64;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825dd968
	sub_825DD968(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x825ddb34
	if (cr6.lt) goto loc_825DDB34;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lhz r10,34(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// extsh r8,r11
	ctx.r8.s64 = r11.s16;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ddab4
	if (cr6.lt) goto loc_825DDAB4;
loc_825DDAFC:
	// lhz r11,34(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 34);
	// lhz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x825ddb24
	if (cr6.eq) goto loc_825DDB24;
	// clrlwi r10,r11,16
	ctx.r10.u64 = r11.u32 & 0xFFFF;
	// li r11,0
	r11.s64 = 0;
loc_825DDB14:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ddb14
	if (cr6.lt) goto loc_825DDB14;
loc_825DDB24:
	// lhz r11,210(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 210);
	// clrlwi r10,r9,16
	ctx.r10.u64 = ctx.r9.u32 & 0xFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r11,210(r30)
	PPC_STORE_U16(r30.u32 + 210, r11.u16);
loc_825DDB34:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825DDB3C"))) PPC_WEAK_FUNC(sub_825DDB3C);
PPC_FUNC_IMPL(__imp__sub_825DDB3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DDB40"))) PPC_WEAK_FUNC(sub_825DDB40);
PPC_FUNC_IMPL(__imp__sub_825DDB40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// addi r4,r29,664
	ctx.r4.s64 = r29.s64 + 664;
	// bl 0x825da8c8
	sub_825DA8C8(ctx, base);
	// lhz r11,580(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ddc70
	if (!cr6.gt) goto loc_825DDC70;
	// li r28,0
	r28.s64 = 0;
	// li r25,-1
	r25.s64 = -1;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_825DDB78:
	// lwz r11,584(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 584);
	// rlwinm r10,r27,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// li r5,160
	ctx.r5.s64 = 160;
	// li r4,0
	ctx.r4.s64 = 0;
	// lhzx r11,r10,r11
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mulli r11,r11,1776
	r11.s64 = r11.s64 * 1776;
	// add r31,r11,r26
	r31.u64 = r11.u64 + r26.u64;
	// addi r3,r31,1616
	ctx.r3.s64 = r31.s64 + 1616;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// stw r28,468(r31)
	PPC_STORE_U32(r31.u32 + 468, r28.u32);
	// stw r28,472(r31)
	PPC_STORE_U32(r31.u32 + 472, r28.u32);
	// stw r28,476(r31)
	PPC_STORE_U32(r31.u32 + 476, r28.u32);
	// stw r28,480(r31)
	PPC_STORE_U32(r31.u32 + 480, r28.u32);
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ddbf0
	if (!cr6.gt) goto loc_825DDBF0;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_825DDBC4:
	// mulli r11,r30,56
	r11.s64 = r30.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r4,r11,200
	ctx.r4.s64 = r11.s64 + 200;
	// bl 0x825dba68
	sub_825DBA68(ctx, base);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ddbc4
	if (cr6.lt) goto loc_825DDBC4;
loc_825DDBF0:
	// lwz r11,448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 448);
	// stw r28,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r28.u32);
	// stw r28,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r28.u32);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// stw r25,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r25.u32);
	// stw r28,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r28.u32);
	// stw r28,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r28.u32);
	// bne cr6,0x825ddc58
	if (!cr6.eq) goto loc_825DDC58;
	// lhz r11,182(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// stw r28,448(r31)
	PPC_STORE_U32(r31.u32 + 448, r28.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825ddc58
	if (!cr6.gt) goto loc_825DDC58;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_825DDC28:
	// mulli r11,r30,56
	r11.s64 = r30.s64 * 56;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r4,r11,200
	ctx.r4.s64 = r11.s64 + 200;
	// bl 0x825dbb50
	sub_825DBB50(ctx, base);
	// lhz r10,182(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 182);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// clrlwi r30,r11,16
	r30.u64 = r11.u32 & 0xFFFF;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ddc28
	if (cr6.lt) goto loc_825DDC28;
loc_825DDC58:
	// lhz r10,580(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 580);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// blt cr6,0x825ddb78
	if (cr6.lt) goto loc_825DDB78;
loc_825DDC70:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825DDC7C"))) PPC_WEAK_FUNC(sub_825DDC7C);
PPC_FUNC_IMPL(__imp__sub_825DDC7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DDC80"))) PPC_WEAK_FUNC(sub_825DDC80);
PPC_FUNC_IMPL(__imp__sub_825DDC80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -72, f30.u64);
	// stfd f31,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,1
	r11.s64 = 1;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// cmpwi cr6,r5,16
	cr6.compare<int32_t>(ctx.r5.s32, 16, xer);
	// slw r27,r11,r5
	r27.u64 = ctx.r5.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r5.u8 & 0x3F));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f30,560(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	f30.f64 = double(temp.f32);
	// bge cr6,0x825ddcd8
	if (!cr6.lt) goto loc_825DDCD8;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,16360
	ctx.r10.s64 = ctx.r10.s64 + 16360;
	// addi r9,r9,16296
	ctx.r9.s64 = ctx.r9.s64 + 16296;
	// lfsx f0,r11,r10
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lfsx f31,r11,r9
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	f31.f64 = double(temp.f32);
	// fmuls f6,f0,f30
	ctx.f6.f64 = double(float(f0.f64 * f30.f64));
	// b 0x825ddd2c
	goto loc_825DDD2C;
loc_825DDCD8:
	// extsw r11,r27
	r11.s64 = r27.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfd f0,-31360(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fdiv f31,f0,f13
	f31.f64 = f0.f64 / ctx.f13.f64;
	// lfd f0,16288(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16288);
	// fmul f1,f31,f0
	ctx.f1.f64 = f31.f64 * f0.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,16424(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16424);
	// fmul f1,f31,f0
	ctx.f1.f64 = f31.f64 * f0.f64;
	// frsp f31,f13
	f31.f64 = double(float(ctx.f13.f64));
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f0,264(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f6,f0
	ctx.f6.f64 = double(float(f0.f64));
loc_825DDD2C:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// bne cr6,0x825ddd40
	if (!cr6.eq) goto loc_825DDD40;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,-25364(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25364);
	f0.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * f0.f64));
loc_825DDD40:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// rlwinm r4,r27,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// lfs f0,5736(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// lfs f3,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	ctx.f3.f64 = double(temp.f32);
	// ble cr6,0x825ddf38
	if (!cr6.gt) goto loc_825DDF38;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f4,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f4.f64 = double(temp.f32);
loc_825DDD6C:
	// fmuls f0,f6,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f6.f64 * f31.f64));
	// li r7,0
	ctx.r7.s64 = 0;
	// fnmsubs f13,f5,f6,f3
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f6.f64 - ctx.f3.f64)));
	// srawi r29,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	r29.s64 = ctx.r5.s32 >> 1;
	// fmr f8,f3
	ctx.f8.f64 = ctx.f3.f64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// fmr f7,f4
	ctx.f7.f64 = ctx.f4.f64;
	// fmr f5,f0
	ctx.f5.f64 = f0.f64;
	// fmuls f6,f0,f30
	ctx.f6.f64 = double(float(f0.f64 * f30.f64));
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
	// ble cr6,0x825dde38
	if (!cr6.gt) goto loc_825DDE38;
	// addi r11,r29,3
	r11.s64 = r29.s64 + 3;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r28,8
	r11.s64 = r28.s64 + 8;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
loc_825DDDAC:
	// lfs f11,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f11.f64 = double(temp.f32);
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// lfs f12,-8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// fsubs f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// fadds f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// stfs f12,-8(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// stfs f10,-12(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + -12, temp.u32);
	// cmpw cr6,r7,r4
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r4.s32, xer);
	// lfs f11,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// fadds f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// stfs f12,-4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// stfs f10,-8(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fsubs f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// fadds f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,0(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmuls f11,f9,f0
	ctx.f11.f64 = double(float(ctx.f9.f64 * f0.f64));
	// fmuls f10,f9,f13
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmsubs f11,f12,f13,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 - ctx.f11.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f12,f12,f0,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f10.f64));
	// stfs f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// blt cr6,0x825dddac
	if (cr6.lt) goto loc_825DDDAC;
loc_825DDE38:
	// li r30,4
	r30.s64 = 4;
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// ble cr6,0x825ddf2c
	if (!cr6.gt) goto loc_825DDF2C;
	// addi r11,r29,7
	r11.s64 = r29.s64 + 7;
	// addi r3,r28,24
	ctx.r3.s64 = r28.s64 + 24;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r28
	r31.u64 = r11.u64 + r28.u64;
loc_825DDE54:
	// fnmsubs f8,f0,f6,f8
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(-(f0.f64 * ctx.f6.f64 - ctx.f8.f64)));
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// fmadds f7,f13,f6,f7
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f7.f64));
	// cmpw cr6,r30,r4
	cr6.compare<int32_t>(r30.s32, ctx.r4.s32, xer);
	// fmadds f0,f8,f6,f0
	f0.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 + f0.f64));
	// fnmsubs f13,f7,f6,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f13.f64)));
	// bgt cr6,0x825ddf18
	if (cr6.gt) goto loc_825DDF18;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
loc_825DDE7C:
	// lfs f11,-12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f11.f64 = double(temp.f32);
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// lfs f12,-8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// fadds f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// stfs f10,-8(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// fsubs f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// lfs f10,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// fsubs f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f11.f64));
	// fadds f11,f10,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f11,-4(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// cmpw cr6,r7,r4
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r4.s32, xer);
	// fmuls f11,f9,f7
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f10,f9,f8
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// fmsubs f11,f12,f8,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f8.f64 - ctx.f11.f64));
	// stfs f11,-12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -12, temp.u32);
	// fmadds f12,f12,f7,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 + ctx.f10.f64));
	// stfs f12,-8(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f11,f12
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f12.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fsubs f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// fadds f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,0(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmuls f11,f9,f0
	ctx.f11.f64 = double(float(ctx.f9.f64 * f0.f64));
	// fmuls f10,f9,f13
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmsubs f11,f12,f13,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 - ctx.f11.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f12,f12,f0,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f10.f64));
	// stfs f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// ble cr6,0x825dde7c
	if (!cr6.gt) goto loc_825DDE7C;
loc_825DDF18:
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmpw cr6,r30,r29
	cr6.compare<int32_t>(r30.s32, r29.s32, xer);
	// blt cr6,0x825dde54
	if (cr6.lt) goto loc_825DDE54;
loc_825DDF2C:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// cmpwi cr6,r29,4
	cr6.compare<int32_t>(r29.s32, 4, xer);
	// bgt cr6,0x825ddd6c
	if (cr6.gt) goto loc_825DDD6C;
loc_825DDF38:
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// ble cr6,0x825ddfa0
	if (!cr6.gt) goto loc_825DDFA0;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x825ddfa0
	if (!cr6.gt) goto loc_825DDFA0;
	// addi r10,r4,-1
	ctx.r10.s64 = ctx.r4.s64 + -1;
	// addi r11,r28,8
	r11.s64 = r28.s64 + 8;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825DDF58:
	// lfs f13,-8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r11,-4
	ctx.r9.s64 = r11.s64 + -4;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - f0.f64));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f0,-8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 - ctx.f13.f64));
	// fadds f0,f0,f13
	f0.f64 = double(float(f0.f64 + ctx.f13.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f12,0(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// bne cr6,0x825ddf58
	if (!cr6.eq) goto loc_825DDF58;
loc_825DDFA0:
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// ble cr6,0x825de088
	if (!cr6.gt) goto loc_825DE088;
	// srawi r11,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r11.s64 = r27.s32 >> 1;
	// addi r3,r27,1
	ctx.r3.s64 = r27.s64 + 1;
	// addze r5,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r5.s64 = temp.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x825de088
	if (!cr6.gt) goto loc_825DE088;
	// addi r11,r28,4
	r11.s64 = r28.s64 + 4;
loc_825DDFC8:
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// bge cr6,0x825de028
	if (!cr6.lt) goto loc_825DE028;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	f0.f64 = double(temp.f32);
	// rlwinm r7,r3,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r9,r28
	ctx.r8.u64 = ctx.r9.u64 + r28.u64;
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + r11.u64;
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f0,0(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// lfs f0,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f0,4(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
loc_825DE028:
	// add r9,r10,r27
	ctx.r9.u64 = ctx.r10.u64 + r27.u64;
	// lfs f0,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r5,r10
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, xer);
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f0,4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// bgt cr6,0x825de074
	if (cr6.gt) goto loc_825DE074;
loc_825DE060:
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addze r8,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r8.s64 = temp.s64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// ble cr6,0x825de060
	if (!cr6.gt) goto loc_825DE060;
loc_825DE074:
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r6,r27
	cr6.compare<int32_t>(ctx.r6.s32, r27.s32, xer);
	// blt cr6,0x825ddfc8
	if (cr6.lt) goto loc_825DDFC8;
loc_825DE088:
	// cmpwi cr6,r26,1
	cr6.compare<int32_t>(r26.s32, 1, xer);
	// bne cr6,0x825de15c
	if (!cr6.eq) goto loc_825DE15C;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r4,4
	cr6.compare<int32_t>(ctx.r4.s32, 4, xer);
	// blt cr6,0x825de114
	if (cr6.lt) goto loc_825DE114;
	// extsw r9,r27
	ctx.r9.s64 = r27.s32;
	// addi r10,r4,-4
	ctx.r10.s64 = ctx.r4.s64 + -4;
	// addi r11,r28,8
	r11.s64 = r28.s64 + 8;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f0,f3,f0
	f0.f64 = double(float(ctx.f3.f64 / f0.f64));
loc_825DE0C8:
	// addi r9,r11,-8
	ctx.r9.s64 = r11.s64 + -8;
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r11,-4
	ctx.r8.s64 = r11.s64 + -4;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lfs f12,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f11,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(f0.f64 * ctx.f11.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f12,0(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// bne cr6,0x825de0c8
	if (!cr6.eq) goto loc_825DE0C8;
loc_825DE114:
	// cmpw cr6,r6,r4
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r4.s32, xer);
	// bge cr6,0x825de15c
	if (!cr6.lt) goto loc_825DE15C;
	// extsw r9,r27
	ctx.r9.s64 = r27.s32;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r6,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r6.s64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f0,f3,f0
	f0.f64 = double(float(ctx.f3.f64 / f0.f64));
loc_825DE140:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825de140
	if (!cr6.eq) goto loc_825DE140;
loc_825DE15C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f31,-64(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_825DE16C"))) PPC_WEAK_FUNC(sub_825DE16C);
PPC_FUNC_IMPL(__imp__sub_825DE16C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DE170"))) PPC_WEAK_FUNC(sub_825DE170);
PPC_FUNC_IMPL(__imp__sub_825DE170) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// addi r12,r1,-104
	r12.s64 = ctx.r1.s64 + -104;
	// bl 0x8239d5dc
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// li r23,0
	r23.s64 = 0;
	// addze r25,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r25.s64 = temp.s64;
	// cmplwi cr6,r28,1
	cr6.compare<uint32_t>(r28.u32, 1, xer);
	// ble cr6,0x825de1bc
	if (!cr6.gt) goto loc_825DE1BC;
loc_825DE1AC:
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// srw r11,r28,r23
	r11.u64 = r23.u8 & 0x20 ? 0 : (r28.u32 >> (r23.u8 & 0x3F));
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825de1ac
	if (cr6.gt) goto loc_825DE1AC;
loc_825DE1BC:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// addi r10,r25,-1
	ctx.r10.s64 = r25.s64 + -1;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// and r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 & r25.u64;
	// addi r30,r11,-4
	r30.s64 = r11.s64 + -4;
	// cntlzw r11,r10
	r11.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// mr r31,r27
	r31.u64 = r27.u64;
	// rlwinm r24,r11,27,31,31
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r28,64
	cr6.compare<int32_t>(r28.s32, 64, xer);
	// mr r29,r30
	r29.u64 = r30.u64;
	// blt cr6,0x825de24c
	if (cr6.lt) goto loc_825DE24C;
	// cmpwi cr6,r28,2048
	cr6.compare<int32_t>(r28.s32, 2048, xer);
	// bgt cr6,0x825de24c
	if (cr6.gt) goto loc_825DE24C;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x825de24c
	if (cr6.eq) goto loc_825DE24C;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// frsp f0,f31
	f0.f64 = double(float(f31.f64));
	// srawi r10,r28,7
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7F) != 0);
	ctx.r10.s64 = r28.s32 >> 7;
	// addi r11,r11,26480
	r11.s64 = r11.s64 + 26480;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lfs f12,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f30,f12,f0
	f30.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmuls f28,f11,f0
	f28.f64 = double(float(ctx.f11.f64 * f0.f64));
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f27,f12,f0
	f27.f64 = double(float(ctx.f12.f64 * f0.f64));
	// lfs f26,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	f26.f64 = double(temp.f32);
	// fneg f31,f11
	f31.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f25,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f25.f64 = double(temp.f32);
	// fneg f29,f13
	f29.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// b 0x825de314
	goto loc_825DE314;
loc_825DE24C:
	// extsw r11,r28
	r11.s64 = r28.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f29,f0
	f29.f64 = double(f0.s64);
	// lfd f0,-30984(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -30984);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fdiv f13,f0,f29
	ctx.f13.f64 = f0.f64 / f29.f64;
	// lfd f0,16440(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16440);
	// fmul f30,f13,f0
	f30.f64 = ctx.f13.f64 * f0.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// frsp f30,f0
	f30.f64 = double(float(f0.f64));
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fdiv f27,f0,f29
	f27.f64 = f0.f64 / f29.f64;
	// fmul f0,f1,f31
	f0.f64 = ctx.f1.f64 * f31.f64;
	// frsp f29,f0
	f29.f64 = double(float(f0.f64));
	// lfd f0,16432(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16432);
	// fmul f28,f27,f0
	f28.f64 = f27.f64 * f0.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// frsp f28,f0
	f28.f64 = double(float(f0.f64));
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,16424(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16424);
	// fmul f26,f27,f0
	f26.f64 = f27.f64 * f0.f64;
	// fmul f0,f1,f31
	f0.f64 = ctx.f1.f64 * f31.f64;
	// fmr f1,f26
	ctx.f1.f64 = f26.f64;
	// frsp f27,f0
	f27.f64 = double(float(f0.f64));
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmr f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64;
	// lis r11,-32251
	r11.s64 = -2113601536;
	// fmr f1,f26
	ctx.f1.f64 = f26.f64;
	// lfd f0,264(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// frsp f31,f0
	f31.f64 = double(float(f0.f64));
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// frsp f26,f1
	ctx.fpscr.disableFlushMode();
	f26.f64 = double(float(ctx.f1.f64));
	// lfs f0,6732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	f0.f64 = double(temp.f32);
	// fmuls f25,f31,f0
	f25.f64 = double(float(f31.f64 * f0.f64));
loc_825DE314:
	// srawi r11,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	r11.s64 = r25.s32 >> 1;
	// addze r26,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r26.s64 = temp.s64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// blt cr6,0x825de454
	if (cr6.lt) goto loc_825DE454;
	// addi r11,r8,-4
	r11.s64 = ctx.r8.s64 + -4;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
loc_825DE33C:
	// lfs f12,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f13,f31,f29,f28
	ctx.f13.f64 = double(float(-(f31.f64 * f29.f64 - f28.f64)));
	// fmuls f10,f12,f29
	ctx.f10.f64 = double(float(ctx.f12.f64 * f29.f64));
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f6,f12,f30
	ctx.f6.f64 = double(float(ctx.f12.f64 * f30.f64));
	// stfs f11,0(r29)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// fmadds f0,f31,f30,f27
	f0.f64 = double(float(f31.f64 * f30.f64 + f27.f64));
	// lfs f12,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmr f9,f30
	ctx.f9.f64 = f30.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fmr f8,f29
	ctx.f8.f64 = f29.f64;
	// addi r10,r31,28
	ctx.r10.s64 = r31.s64 + 28;
	// addi r9,r31,24
	ctx.r9.s64 = r31.s64 + 24;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmsubs f10,f12,f30,f10
	ctx.f10.f64 = double(float(ctx.f12.f64 * f30.f64 - ctx.f10.f64));
	// stfs f10,0(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// fmadds f12,f12,f29,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * f29.f64 + ctx.f6.f64));
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// lfs f12,-8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// fmr f4,f13
	ctx.f4.f64 = ctx.f13.f64;
	// stfs f5,-8(r29)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r29.u32 + -8, temp.u32);
	// fmuls f5,f13,f12
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f10,8(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmr f3,f0
	ctx.f3.f64 = f0.f64;
	// lfs f6,20(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f0,f31,f9
	ctx.f13.f64 = double(float(-(f0.f64 * f31.f64 - ctx.f9.f64)));
	// fmr f7,f0
	ctx.f7.f64 = f0.f64;
	// fmadds f0,f31,f11,f8
	f0.f64 = double(float(f31.f64 * ctx.f11.f64 + ctx.f8.f64));
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmsubs f12,f4,f10,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f10.f64 - ctx.f12.f64));
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 8, temp.u32);
	// fmadds f12,f3,f10,f5
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f5.f64));
	// stfs f12,12(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// lfs f12,-16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// stfs f6,-16(r29)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r29.u32 + -16, temp.u32);
	// fmr f5,f13
	ctx.f5.f64 = ctx.f13.f64;
	// lfs f11,16(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f4,f0,f12
	ctx.f4.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f6,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fnmsubs f13,f0,f31,f9
	ctx.f13.f64 = double(float(-(f0.f64 * f31.f64 - ctx.f9.f64)));
	// fmr f8,f0
	ctx.f8.f64 = f0.f64;
	// fmsubs f9,f5,f11,f4
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f4.f64));
	// stfs f9,16(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 16, temp.u32);
	// fmadds f0,f0,f11,f12
	f0.f64 = double(float(f0.f64 * ctx.f11.f64 + ctx.f12.f64));
	// stfs f0,20(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 20, temp.u32);
	// fmadds f0,f31,f10,f7
	f0.f64 = double(float(f31.f64 * ctx.f10.f64 + ctx.f7.f64));
	// lfs f12,-24(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + -24);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f10
	ctx.f11.f64 = ctx.f10.f64;
	// stfs f6,-24(r29)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r29.u32 + -24, temp.u32);
	// fmr f28,f13
	f28.f64 = ctx.f13.f64;
	// addi r31,r31,32
	r31.s64 = r31.s64 + 32;
	// addi r30,r30,-32
	r30.s64 = r30.s64 + -32;
	// addi r29,r29,-32
	r29.s64 = r29.s64 + -32;
	// fmuls f10,f0,f12
	ctx.f10.f64 = double(float(f0.f64 * ctx.f12.f64));
	// fmr f9,f0
	ctx.f9.f64 = f0.f64;
	// fmuls f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmr f27,f0
	f27.f64 = f0.f64;
	// fnmsubs f30,f0,f31,f11
	f30.f64 = double(float(-(f0.f64 * f31.f64 - ctx.f11.f64)));
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmadds f29,f31,f28,f8
	f29.f64 = double(float(f31.f64 * f28.f64 + ctx.f8.f64));
	// fmsubs f13,f13,f0,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64 - ctx.f10.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f0,f9,f0,f12
	f0.f64 = double(float(ctx.f9.f64 * f0.f64 + ctx.f12.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// bne cr6,0x825de33c
	if (!cr6.eq) goto loc_825DE33C;
loc_825DE454:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// ble cr6,0x825de4c0
	if (!cr6.gt) goto loc_825DE4C0;
loc_825DE45C:
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fnmsubs f13,f31,f29,f28
	ctx.f13.f64 = double(float(-(f31.f64 * f29.f64 - f28.f64)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fmuls f8,f0,f29
	ctx.f8.f64 = double(float(f0.f64 * f29.f64));
	// addi r30,r30,-8
	r30.s64 = r30.s64 + -8;
	// fmr f11,f30
	ctx.f11.f64 = f30.f64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// fmr f10,f29
	ctx.f10.f64 = f29.f64;
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f31,f30,f27
	ctx.f12.f64 = double(float(f31.f64 * f30.f64 + f27.f64));
	// stfs f9,0(r29)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// fmuls f9,f0,f30
	ctx.f9.f64 = double(float(f0.f64 * f30.f64));
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmr f28,f30
	f28.f64 = f30.f64;
	// fmr f27,f29
	f27.f64 = f29.f64;
	// addi r29,r29,-8
	r29.s64 = r29.s64 + -8;
	// fmr f30,f13
	f30.f64 = ctx.f13.f64;
	// fmsubs f13,f0,f11,f8
	ctx.f13.f64 = double(float(f0.f64 * ctx.f11.f64 - ctx.f8.f64));
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// fmr f29,f12
	f29.f64 = ctx.f12.f64;
	// fmadds f0,f0,f10,f9
	f0.f64 = double(float(f0.f64 * ctx.f10.f64 + ctx.f9.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// bgt cr6,0x825de45c
	if (cr6.gt) goto loc_825DE45C;
loc_825DE4C0:
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// blt cr6,0x825de5e4
	if (cr6.lt) goto loc_825DE5E4;
	// addi r10,r11,-4
	ctx.r10.s64 = r11.s64 + -4;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
loc_825DE4E0:
	// fnmsubs f13,f31,f29,f28
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(-(f31.f64 * f29.f64 - f28.f64)));
	// lfs f12,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f31,f30,f27
	f0.f64 = double(float(f31.f64 * f30.f64 + f27.f64));
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f11,f29
	ctx.f1.f64 = double(float(ctx.f11.f64 * f29.f64));
	// addi r9,r31,28
	ctx.r9.s64 = r31.s64 + 28;
	// fmr f4,f12
	ctx.f4.f64 = ctx.f12.f64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmr f9,f30
	ctx.f9.f64 = f30.f64;
	// fmr f6,f30
	ctx.f6.f64 = f30.f64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// fmuls f5,f12,f29
	ctx.f5.f64 = double(float(ctx.f12.f64 * f29.f64));
	// lfs f12,8(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f29
	ctx.f8.f64 = f29.f64;
	// fmr f28,f12
	f28.f64 = ctx.f12.f64;
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f3,f13
	ctx.f3.f64 = ctx.f13.f64;
	// fmr f2,f13
	ctx.f2.f64 = ctx.f13.f64;
	// fnmsubs f13,f0,f31,f30
	ctx.f13.f64 = double(float(-(f0.f64 * f31.f64 - f30.f64)));
	// fmr f30,f11
	f30.f64 = ctx.f11.f64;
	// lfs f11,12(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f9,f4,f9,f1
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 - ctx.f1.f64));
	// stfs f9,0(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// fmuls f29,f0,f12
	f29.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,16(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fmr f7,f0
	ctx.f7.f64 = f0.f64;
	// fmr f4,f13
	ctx.f4.f64 = ctx.f13.f64;
	// fmadds f9,f30,f6,f5
	ctx.f9.f64 = double(float(f30.f64 * ctx.f6.f64 + ctx.f5.f64));
	// stfs f9,4(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// fmr f6,f10
	ctx.f6.f64 = ctx.f10.f64;
	// fmr f9,f13
	ctx.f9.f64 = ctx.f13.f64;
	// fmr f5,f13
	ctx.f5.f64 = ctx.f13.f64;
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f11,f11,f2,f29
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + f29.f64));
	// stfs f11,12(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// fmr f30,f13
	f30.f64 = ctx.f13.f64;
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f0,f31,f6,f8
	f0.f64 = double(float(f31.f64 * ctx.f6.f64 + ctx.f8.f64));
	// fmsubs f8,f3,f28,f1
	ctx.f8.f64 = double(float(ctx.f3.f64 * f28.f64 - ctx.f1.f64));
	// stfs f8,8(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r31.u32 + 8, temp.u32);
	// fmr f6,f12
	ctx.f6.f64 = ctx.f12.f64;
	// fmr f3,f0
	ctx.f3.f64 = f0.f64;
	// fmuls f2,f13,f0
	ctx.f2.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f1,f0,f12
	ctx.f1.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f12,24(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f0
	ctx.f8.f64 = f0.f64;
	// fmadds f0,f31,f9,f7
	f0.f64 = double(float(f31.f64 * ctx.f9.f64 + ctx.f7.f64));
	// fnmsubs f13,f3,f31,f10
	ctx.f13.f64 = double(float(-(ctx.f3.f64 * f31.f64 - ctx.f10.f64)));
	// fmsubs f10,f5,f6,f2
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 - ctx.f2.f64));
	// stfs f10,16(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 16, temp.u32);
	// fmadds f10,f30,f4,f1
	ctx.f10.f64 = double(float(f30.f64 * ctx.f4.f64 + ctx.f1.f64));
	// stfs f10,20(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 20, temp.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fmr f27,f0
	f27.f64 = f0.f64;
	// fnmsubs f30,f0,f31,f9
	f30.f64 = double(float(-(f0.f64 * f31.f64 - ctx.f9.f64)));
	// fmr f28,f13
	f28.f64 = ctx.f13.f64;
	// fmsubs f10,f13,f12,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 - ctx.f10.f64));
	// stfs f10,24(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 24, temp.u32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// addi r31,r31,32
	r31.s64 = r31.s64 + 32;
	// fmadds f29,f31,f28,f8
	f29.f64 = double(float(f31.f64 * f28.f64 + ctx.f8.f64));
	// fmadds f0,f11,f13,f12
	f0.f64 = double(float(ctx.f11.f64 * ctx.f13.f64 + ctx.f12.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// bne cr6,0x825de4e0
	if (!cr6.eq) goto loc_825DE4E0;
loc_825DE5E4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825de640
	if (!cr6.gt) goto loc_825DE640;
loc_825DE5EC:
	// fnmsubs f12,f31,f29,f28
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(-(f31.f64 * f29.f64 - f28.f64)));
	// addi r10,r31,4
	ctx.r10.s64 = r31.s64 + 4;
	// fmadds f11,f31,f30,f27
	ctx.f11.f64 = double(float(f31.f64 * f30.f64 + f27.f64));
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmr f10,f30
	ctx.f10.f64 = f30.f64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// fmr f9,f30
	ctx.f9.f64 = f30.f64;
	// fmr f28,f30
	f28.f64 = f30.f64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f29
	ctx.f8.f64 = double(float(f0.f64 * f29.f64));
	// fmr f27,f29
	f27.f64 = f29.f64;
	// fmr f30,f12
	f30.f64 = ctx.f12.f64;
	// fmuls f12,f13,f29
	ctx.f12.f64 = double(float(ctx.f13.f64 * f29.f64));
	// fmr f29,f11
	f29.f64 = ctx.f11.f64;
	// fmsubs f0,f0,f10,f12
	f0.f64 = double(float(f0.f64 * ctx.f10.f64 - ctx.f12.f64));
	// stfs f0,0(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// fmadds f0,f13,f9,f8
	f0.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 + ctx.f8.f64));
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// bgt cr6,0x825de5ec
	if (cr6.gt) goto loc_825DE5EC;
loc_825DE640:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// addi r5,r23,-1
	ctx.r5.s64 = r23.s64 + -1;
	// bne cr6,0x825de650
	if (!cr6.eq) goto loc_825DE650;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
loc_825DE650:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mtctr r22
	ctr.u64 = r22.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r9,r28,-2
	ctx.r9.s64 = r28.s64 + -2;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// lfs f0,2552(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	f0.f64 = double(temp.f32);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// lfs f13,2480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2480);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x825de848
	if (cr6.lt) goto loc_825DE848;
	// addi r9,r29,-4
	ctx.r9.s64 = r29.s64 + -4;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r29,r8,r29
	r29.s64 = r29.s64 - ctx.r8.s64;
loc_825DE6A4:
	// fmadds f12,f31,f0,f25
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(f31.f64 * f0.f64 + f25.f64));
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f31,f13,f26
	ctx.f11.f64 = double(float(-(f31.f64 * ctx.f13.f64 - f26.f64)));
	// lfs f7,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// fneg f3,f13
	ctx.f3.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmr f5,f13
	ctx.f5.f64 = ctx.f13.f64;
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f13,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// addi r7,r11,20
	ctx.r7.s64 = r11.s64 + 20;
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * f0.f64));
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// fmr f2,f9
	ctx.f2.f64 = ctx.f9.f64;
	// addi r5,r10,-12
	ctx.r5.s64 = ctx.r10.s64 + -12;
	// fmr f4,f0
	ctx.f4.f64 = f0.f64;
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r4,r11,28
	ctx.r4.s64 = r11.s64 + 28;
	// fmuls f29,f12,f7
	f29.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// fneg f6,f12
	ctx.f6.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// addi r31,r10,-24
	r31.s64 = ctx.r10.s64 + -24;
	// fmuls f7,f11,f7
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// addi r30,r10,-20
	r30.s64 = ctx.r10.s64 + -20;
	// fmr f1,f12
	ctx.f1.f64 = ctx.f12.f64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// fmsubs f0,f10,f0,f13
	f0.f64 = double(float(ctx.f10.f64 * f0.f64 - ctx.f13.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fmsubs f0,f3,f10,f8
	f0.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 - ctx.f8.f64));
	// stfs f0,4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmr f30,f11
	f30.f64 = ctx.f11.f64;
	// fmadds f0,f9,f11,f29
	f0.f64 = double(float(ctx.f9.f64 * ctx.f11.f64 + f29.f64));
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f9,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f0,f6,f2,f7
	f0.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f7.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfs f13,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f31,f11,f5
	f0.f64 = double(float(f31.f64 * ctx.f11.f64 + ctx.f5.f64));
	// fmuls f7,f13,f12
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f10,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f13,f1,f31,f4
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * f31.f64 - ctx.f4.f64)));
	// fmr f5,f12
	ctx.f5.f64 = ctx.f12.f64;
	// fmsubs f12,f30,f8,f7
	ctx.f12.f64 = double(float(f30.f64 * ctx.f8.f64 - ctx.f7.f64));
	// stfs f12,8(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// fmsubs f12,f6,f8,f3
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f3.f64));
	// stfs f12,-4(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// fmuls f8,f0,f9
	ctx.f8.f64 = double(float(f0.f64 * ctx.f9.f64));
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// fmuls f7,f13,f9
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmr f9,f13
	ctx.f9.f64 = ctx.f13.f64;
	// fmr f3,f0
	ctx.f3.f64 = f0.f64;
	// fmr f2,f13
	ctx.f2.f64 = ctx.f13.f64;
	// fmr f6,f0
	ctx.f6.f64 = f0.f64;
	// fmadds f8,f10,f13,f8
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f8.f64));
	// stfs f8,12(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// fmadds f10,f12,f10,f7
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 + ctx.f7.f64));
	// stfs f10,-8(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lfs f8,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r10,r10,-32
	ctx.r10.s64 = ctx.r10.s64 + -32;
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * f0.f64));
	// lfs f10,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f7,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f0,f31,f9,f5
	f0.f64 = double(float(f31.f64 * ctx.f9.f64 + ctx.f5.f64));
	// fnmsubs f13,f3,f31,f11
	ctx.f13.f64 = double(float(-(ctx.f3.f64 * f31.f64 - ctx.f11.f64)));
	// fmr f5,f6
	ctx.f5.f64 = ctx.f6.f64;
	// fmsubs f11,f2,f10,f4
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fmsubs f10,f12,f10,f8
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f8.f64));
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f8,f0,f12
	ctx.f8.f64 = double(float(f0.f64 * ctx.f12.f64));
	// stfs f11,0(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f10,0(r5)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fmuls f6,f13,f12
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fneg f11,f0
	ctx.f11.u64 = f0.u64 ^ 0x8000000000000000;
	// fmr f25,f0
	f25.f64 = f0.f64;
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f3,f13
	ctx.f3.f64 = ctx.f13.f64;
	// fmadds f12,f7,f13,f8
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f8.f64));
	// fmadds f7,f11,f7,f6
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f7.f64 + ctx.f6.f64));
	// stfs f12,0(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f7,0(r6)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmr f26,f10
	f26.f64 = ctx.f10.f64;
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f7,f0
	ctx.f2.f64 = double(float(ctx.f7.f64 * f0.f64));
	// lfs f6,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f7,f13
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f12,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f31,f10,f5
	ctx.f13.f64 = double(float(f31.f64 * ctx.f10.f64 + ctx.f5.f64));
	// lfs f8,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f0,f0,f31,f9
	f0.f64 = double(float(-(f0.f64 * f31.f64 - ctx.f9.f64)));
	// fmsubs f10,f3,f6,f2
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 - ctx.f2.f64));
	// stfs f10,0(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// fmsubs f11,f11,f6,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f6.f64 - ctx.f7.f64));
	// stfs f11,0(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// fmuls f11,f13,f8
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fmuls f9,f0,f8
	ctx.f9.f64 = double(float(f0.f64 * ctx.f8.f64));
	// fmadds f11,f12,f0,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f11.f64));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmadds f12,f10,f12,f9
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f12,0(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// bne cr6,0x825de6a4
	if (!cr6.eq) goto loc_825DE6A4;
loc_825DE848:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x825de8c8
	if (!cr6.gt) goto loc_825DE8C8;
loc_825DE850:
	// fmadds f11,f31,f0,f25
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(f31.f64 * f0.f64 + f25.f64));
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f12,f31,f13,f26
	ctx.f12.f64 = double(float(-(f31.f64 * ctx.f13.f64 - f26.f64)));
	// lfs f7,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f3,f8,f13
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmr f6,f0
	ctx.f6.f64 = f0.f64;
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fneg f5,f13
	ctx.f5.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * f0.f64));
	// fmr f25,f13
	f25.f64 = ctx.f13.f64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// fmr f26,f0
	f26.f64 = f0.f64;
	// fmuls f2,f11,f7
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fneg f4,f11
	ctx.f4.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fmuls f7,f12,f7
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
	// fmsubs f11,f10,f6,f3
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 - ctx.f3.f64));
	// stfs f11,0(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fmr f0,f12
	f0.f64 = ctx.f12.f64;
	// fmsubs f11,f5,f10,f8
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f10.f64 - ctx.f8.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmadds f12,f9,f12,f2
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 + ctx.f2.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// fmadds f12,f4,f9,f7
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 + ctx.f7.f64));
	// stfs f12,0(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,-8
	ctx.r10.s64 = ctx.r10.s64 + -8;
	// bgt cr6,0x825de850
	if (cr6.gt) goto loc_825DE850;
loc_825DE8C8:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x825de8dc
	if (cr6.eq) goto loc_825DE8DC;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
loc_825DE8DC:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-104
	r12.s64 = ctx.r1.s64 + -104;
	// bl 0x8239d628
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_825DE8EC"))) PPC_WEAK_FUNC(sub_825DE8EC);
PPC_FUNC_IMPL(__imp__sub_825DE8EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DE8F0"))) PPC_WEAK_FUNC(sub_825DE8F0);
PPC_FUNC_IMPL(__imp__sub_825DE8F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// srawi r11,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	r11.s64 = ctx.r7.s32 >> 1;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// addze r10,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r10.s64 = temp.s64;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// subf r7,r5,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r5.s64;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
loc_825DE918:
	// lwzx r6,r7,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srawi r6,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 1;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// neg r6,r6
	ctx.r6.s64 = -ctx.r6.s64;
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r6,r7,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// srawi r6,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 1;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// subf r6,r6,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r6.s64;
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// bne cr6,0x825de918
	if (!cr6.eq) goto loc_825DE918;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DE968"))) PPC_WEAK_FUNC(sub_825DE968);
PPC_FUNC_IMPL(__imp__sub_825DE968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcec
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// mr r29,r26
	r29.u64 = r26.u64;
	// ble cr6,0x825de9f8
	if (!cr6.gt) goto loc_825DE9F8;
	// addi r11,r1,-464
	r11.s64 = ctx.r1.s64 + -464;
	// addi r28,r1,-464
	r28.s64 = ctx.r1.s64 + -464;
	// subf r27,r11,r6
	r27.s64 = ctx.r6.s64 - r11.s64;
loc_825DE98C:
	// cmpw cr6,r29,r7
	cr6.compare<int32_t>(r29.s32, ctx.r7.s32, xer);
	// bge cr6,0x825de9f8
	if (!cr6.lt) goto loc_825DE9F8;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r26,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r26.u32);
	// blt cr6,0x825de9e8
	if (cr6.lt) goto loc_825DE9E8;
	// mr r31,r26
	r31.u64 = r26.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// add r10,r27,r28
	ctx.r10.u64 = r27.u64 + r28.u64;
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
loc_825DE9B0:
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r25,0(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// extsw r25,r25
	r25.s64 = r25.s32;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// mulld r30,r30,r25
	r30.s64 = r30.s64 * r25.s64;
	// sradi r30,r30,30
	xer.ca = (r30.s64 < 0) & ((r30.u64 & 0x3FFFFFFF) != 0);
	r30.s64 = r30.s64 >> 30;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// extsw r30,r30
	r30.s64 = r30.s32;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// bne cr6,0x825de9b0
	if (!cr6.eq) goto loc_825DE9B0;
	// stw r31,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r31.u32);
loc_825DE9E8:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r29,r5
	cr6.compare<int32_t>(r29.s32, ctx.r5.s32, xer);
	// blt cr6,0x825de98c
	if (cr6.lt) goto loc_825DE98C;
loc_825DE9F8:
	// add r28,r5,r7
	r28.u64 = ctx.r5.u64 + ctx.r7.u64;
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x825dea88
	if (!cr6.lt) goto loc_825DEA88;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-464
	ctx.r7.s64 = ctx.r1.s64 + -464;
	// addi r3,r1,-464
	ctx.r3.s64 = ctx.r1.s64 + -464;
	// add r31,r10,r7
	r31.u64 = ctx.r10.u64 + ctx.r7.u64;
	// subf r30,r3,r6
	r30.s64 = ctx.r6.s64 - ctx.r3.s64;
	// subf r29,r5,r11
	r29.s64 = r11.s64 - ctx.r5.s64;
loc_825DEA24:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r26,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r26.u32);
	// ble cr6,0x825dea78
	if (!cr6.gt) goto loc_825DEA78;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// add r10,r31,r30
	ctx.r10.u64 = r31.u64 + r30.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_825DEA40:
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r27,0(r7)
	r27.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// extsw r27,r27
	r27.s64 = r27.s32;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// mulld r3,r3,r27
	ctx.r3.s64 = ctx.r3.s64 * r27.s64;
	// sradi r3,r3,30
	xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r3.s64 >> 30;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// add r6,r3,r6
	ctx.r6.u64 = ctx.r3.u64 + ctx.r6.u64;
	// bne cr6,0x825dea40
	if (!cr6.eq) goto loc_825DEA40;
	// stw r6,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r6.u32);
loc_825DEA78:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825dea24
	if (!cr6.eq) goto loc_825DEA24;
loc_825DEA88:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// srawi r7,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r7.s64 = r11.s32 >> 1;
	// addze. r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// ble 0x825deae4
	if (!cr0.gt) goto loc_825DEAE4;
	// addi r7,r1,-464
	ctx.r7.s64 = ctx.r1.s64 + -464;
	// addi r11,r1,-464
	r11.s64 = ctx.r1.s64 + -464;
	// subf r5,r7,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r7.s64;
loc_825DEAAC:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stwx r7,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + r11.u32, ctx.r7.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r7,r6,r8
	PPC_STORE_U32(ctx.r6.u32 + ctx.r8.u32, ctx.r7.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x825deaac
	if (cr6.lt) goto loc_825DEAAC;
loc_825DEAE4:
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r1,-464
	ctx.r10.s64 = ctx.r1.s64 + -464;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stwx r10,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r10.u32);
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825DEB04"))) PPC_WEAK_FUNC(sub_825DEB04);
PPC_FUNC_IMPL(__imp__sub_825DEB04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DEB08"))) PPC_WEAK_FUNC(sub_825DEB08);
PPC_FUNC_IMPL(__imp__sub_825DEB08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcec
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// mr r29,r26
	r29.u64 = r26.u64;
	// ble cr6,0x825deb98
	if (!cr6.gt) goto loc_825DEB98;
	// addi r11,r1,-464
	r11.s64 = ctx.r1.s64 + -464;
	// addi r28,r1,-464
	r28.s64 = ctx.r1.s64 + -464;
	// subf r27,r11,r6
	r27.s64 = ctx.r6.s64 - r11.s64;
loc_825DEB2C:
	// cmpw cr6,r29,r7
	cr6.compare<int32_t>(r29.s32, ctx.r7.s32, xer);
	// bge cr6,0x825deb98
	if (!cr6.lt) goto loc_825DEB98;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r26,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r26.u32);
	// blt cr6,0x825deb88
	if (cr6.lt) goto loc_825DEB88;
	// mr r31,r26
	r31.u64 = r26.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// add r10,r27,r28
	ctx.r10.u64 = r27.u64 + r28.u64;
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
loc_825DEB50:
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r25,0(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// extsw r25,r25
	r25.s64 = r25.s32;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// mulld r30,r30,r25
	r30.s64 = r30.s64 * r25.s64;
	// sradi r30,r30,30
	xer.ca = (r30.s64 < 0) & ((r30.u64 & 0x3FFFFFFF) != 0);
	r30.s64 = r30.s64 >> 30;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// extsw r30,r30
	r30.s64 = r30.s32;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// bne cr6,0x825deb50
	if (!cr6.eq) goto loc_825DEB50;
	// stw r31,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r31.u32);
loc_825DEB88:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmpw cr6,r29,r5
	cr6.compare<int32_t>(r29.s32, ctx.r5.s32, xer);
	// blt cr6,0x825deb2c
	if (cr6.lt) goto loc_825DEB2C;
loc_825DEB98:
	// add r28,r5,r7
	r28.u64 = ctx.r5.u64 + ctx.r7.u64;
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x825dec28
	if (!cr6.lt) goto loc_825DEC28;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r1,-464
	ctx.r7.s64 = ctx.r1.s64 + -464;
	// addi r3,r1,-464
	ctx.r3.s64 = ctx.r1.s64 + -464;
	// add r31,r10,r7
	r31.u64 = ctx.r10.u64 + ctx.r7.u64;
	// subf r30,r3,r6
	r30.s64 = ctx.r6.s64 - ctx.r3.s64;
	// subf r29,r5,r11
	r29.s64 = r11.s64 - ctx.r5.s64;
loc_825DEBC4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r26,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r26.u32);
	// ble cr6,0x825dec18
	if (!cr6.gt) goto loc_825DEC18;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// add r7,r31,r30
	ctx.r7.u64 = r31.u64 + r30.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_825DEBE0:
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r27,0(r7)
	r27.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// extsw r27,r27
	r27.s64 = r27.s32;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// mulld r3,r3,r27
	ctx.r3.s64 = ctx.r3.s64 * r27.s64;
	// sradi r3,r3,30
	xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r3.s64 >> 30;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// add r6,r3,r6
	ctx.r6.u64 = ctx.r3.u64 + ctx.r6.u64;
	// bne cr6,0x825debe0
	if (!cr6.eq) goto loc_825DEBE0;
	// stw r6,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r6.u32);
loc_825DEC18:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x825debc4
	if (!cr6.eq) goto loc_825DEBC4;
loc_825DEC28:
	// addi r11,r28,-1
	r11.s64 = r28.s64 + -1;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// srawi r7,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r7.s64 = r11.s32 >> 1;
	// addze. r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// ble 0x825dec84
	if (!cr0.gt) goto loc_825DEC84;
	// addi r7,r1,-464
	ctx.r7.s64 = ctx.r1.s64 + -464;
	// addi r11,r1,-464
	r11.s64 = ctx.r1.s64 + -464;
	// subf r5,r7,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r7.s64;
loc_825DEC4C:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stwx r7,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + r11.u32, ctx.r7.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r7,r6,r8
	PPC_STORE_U32(ctx.r6.u32 + ctx.r8.u32, ctx.r7.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x825dec4c
	if (cr6.lt) goto loc_825DEC4C;
loc_825DEC84:
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_825DEC88"))) PPC_WEAK_FUNC(sub_825DEC88);
PPC_FUNC_IMPL(__imp__sub_825DEC88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-608(r1)
	ea = -608 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,2
	r11.s64 = 2;
	// lis r25,2048
	r25.s64 = 134217728;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lis r11,16384
	r11.s64 = 1073741824;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// stw r25,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r25.u32);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// stw r25,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r25.u32);
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r27,r11,16448
	r27.s64 = r11.s64 + 16448;
	// ble cr6,0x825ded24
	if (!cr6.gt) goto loc_825DED24;
	// li r29,0
	r29.s64 = 0;
loc_825DECE0:
	// lbzx r11,r31,r28
	r11.u64 = PPC_LOAD_U8(r31.u32 + r28.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x825deb08
	sub_825DEB08(ctx, base);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// cmpw cr6,r31,r30
	cr6.compare<int32_t>(r31.s32, r30.s32, xer);
	// blt cr6,0x825dece0
	if (cr6.lt) goto loc_825DECE0;
loc_825DED24:
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addze r23,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r23.s64 = temp.s64;
	// cmpwi cr6,r23,1
	cr6.compare<int32_t>(r23.s32, 1, xer);
	// blt cr6,0x825ded44
	if (cr6.lt) goto loc_825DED44;
	// addi r3,r24,4
	ctx.r3.s64 = r24.s64 + 4;
	// addi r4,r1,116
	ctx.r4.s64 = ctx.r1.s64 + 116;
	// rlwinm r5,r23,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
loc_825DED44:
	// lbz r11,1(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 1);
	// addi r10,r27,64
	ctx.r10.s64 = r27.s64 + 64;
	// li r31,3
	r31.s64 = 3;
	// stw r25,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r25.u32);
	// rotlwi r11,r11,2
	r11.u64 = __builtin_rotateleft32(r11.u32, 2);
	// stw r25,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r25.u32);
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// ble cr6,0x825dedbc
	if (!cr6.gt) goto loc_825DEDBC;
	// li r29,48
	r29.s64 = 48;
loc_825DED78:
	// lbzx r11,r31,r28
	r11.u64 = PPC_LOAD_U8(r31.u32 + r28.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x825de968
	sub_825DE968(ctx, base);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// cmpw cr6,r31,r30
	cr6.compare<int32_t>(r31.s32, r30.s32, xer);
	// blt cr6,0x825ded78
	if (cr6.lt) goto loc_825DED78;
loc_825DEDBC:
	// cmpwi cr6,r23,1
	cr6.compare<int32_t>(r23.s32, 1, xer);
	// blt cr6,0x825dedf4
	if (cr6.lt) goto loc_825DEDF4;
	// addi r9,r22,4
	ctx.r9.s64 = r22.s64 + 4;
	// addi r11,r1,116
	r11.s64 = ctx.r1.s64 + 116;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_825DEDD0:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825dedd0
	if (!cr6.eq) goto loc_825DEDD0;
loc_825DEDF4:
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825DEDFC"))) PPC_WEAK_FUNC(sub_825DEDFC);
PPC_FUNC_IMPL(__imp__sub_825DEDFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DEE00"))) PPC_WEAK_FUNC(sub_825DEE00);
PPC_FUNC_IMPL(__imp__sub_825DEE00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x825dec88
	sub_825DEC88(ctx, base);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825de8f0
	sub_825DE8F0(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825DEE48"))) PPC_WEAK_FUNC(sub_825DEE48);
PPC_FUNC_IMPL(__imp__sub_825DEE48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-32126
	r11.s64 = -2105409536;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r5,r11,13696
	ctx.r5.s64 = r11.s64 + 13696;
loc_825DEE58:
	// li r10,128
	ctx.r10.s64 = 128;
	// li r11,0
	r11.s64 = 0;
loc_825DEE60:
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x825dee80
	if (!cr6.eq) goto loc_825DEE80;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r10,31,25,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7F;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// blt cr6,0x825dee60
	if (cr6.lt) goto loc_825DEE60;
loc_825DEE80:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// add r9,r6,r5
	ctx.r9.u64 = ctx.r6.u64 + ctx.r5.u64;
	// li r10,128
	ctx.r10.s64 = 128;
	// li r11,0
	r11.s64 = 0;
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// stb r4,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r4.u8);
loc_825DEE98:
	// clrlwi r4,r10,24
	ctx.r4.u64 = ctx.r10.u32 & 0xFF;
	// and r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 & ctx.r7.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x825deeb8
	if (!cr6.eq) goto loc_825DEEB8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r10,31,25,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7F;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// blt cr6,0x825dee98
	if (cr6.lt) goto loc_825DEE98;
loc_825DEEB8:
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpwi cr6,r8,256
	cr6.compare<int32_t>(ctx.r8.s32, 256, xer);
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// blt cr6,0x825dee58
	if (cr6.lt) goto loc_825DEE58;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DEEDC"))) PPC_WEAK_FUNC(sub_825DEEDC);
PPC_FUNC_IMPL(__imp__sub_825DEEDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DEEE0"))) PPC_WEAK_FUNC(sub_825DEEE0);
PPC_FUNC_IMPL(__imp__sub_825DEEE0) {
	PPC_FUNC_PROLOGUE();
	// b 0x825dee48
	sub_825DEE48(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825DEEE4"))) PPC_WEAK_FUNC(sub_825DEEE4);
PPC_FUNC_IMPL(__imp__sub_825DEEE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DEEE8"))) PPC_WEAK_FUNC(sub_825DEEE8);
PPC_FUNC_IMPL(__imp__sub_825DEEE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// extsw r11,r3
	r11.s64 = ctx.r3.s32;
	// extsw r9,r4
	ctx.r9.s64 = ctx.r4.s32;
	// mulld r8,r11,r11
	ctx.r8.s64 = r11.s64 * r11.s64;
	// mulld r11,r9,r9
	r11.s64 = ctx.r9.s64 * ctx.r9.s64;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// rldicl r9,r8,44,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u64, 44) & 0xFFFFFFFFFFF;
	// rldicl r11,r9,32,32
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825def2c
	if (!cr6.eq) goto loc_825DEF2C;
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// li r10,32
	ctx.r10.s64 = 32;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825def2c
	if (!cr6.eq) goto loc_825DEF2C;
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
loc_825DEF2C:
	// rlwinm r9,r11,0,0,7
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFF000000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825def4c
	if (!cr6.eq) goto loc_825DEF4C;
loc_825DEF38:
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// rlwinm r9,r11,0,0,7
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFF000000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x825def38
	if (cr6.eq) goto loc_825DEF38;
loc_825DEF4C:
	// rlwinm r11,r11,8,24,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFF;
	// lis r9,-32126
	ctx.r9.s64 = -2105409536;
	// rlwinm r7,r11,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,13696
	ctx.r9.s64 = ctx.r9.s64 + 13696;
	// rlwinm r11,r11,2,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x4;
	// lbzx r9,r7,r9
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// srw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r9,-20
	r11.s64 = ctx.r9.s64 + -20;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x825def94
	if (!cr6.gt) goto loc_825DEF94;
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// slw r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// b 0x825defa4
	goto loc_825DEFA4;
loc_825DEF94:
	// subfic r11,r11,32
	xer.ca = r11.u32 <= 32;
	r11.s64 = 32 - r11.s64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// srd r11,r8,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (r11.u8 & 0x7F));
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_825DEFA4:
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// clrldi r6,r10,32
	ctx.r6.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r10,r10,15824
	ctx.r10.s64 = ctx.r10.s64 + 15824;
	// addi r11,r11,14792
	r11.s64 = r11.s64 + 14792;
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// rlwinm r10,r8,10,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x3FC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r10,r10,r7
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// mulld r10,r10,r6
	ctx.r10.s64 = ctx.r10.s64 * ctx.r6.s64;
	// rldicl r10,r10,32,32
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// mulld r11,r11,r9
	r11.s64 = r11.s64 * ctx.r9.s64;
	// rldicl r11,r11,32,32
	r11.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DF004"))) PPC_WEAK_FUNC(sub_825DF004);
PPC_FUNC_IMPL(__imp__sub_825DF004) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DF008"))) PPC_WEAK_FUNC(sub_825DF008);
PPC_FUNC_IMPL(__imp__sub_825DF008) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// add r29,r7,r8
	r29.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// lwz r27,16(r4)
	r27.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// subf r30,r7,r8
	r30.s64 = ctx.r8.s64 - ctx.r7.s64;
	// lwz r28,4(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// extsw r10,r8
	ctx.r10.s64 = ctx.r8.s32;
	// lwz r8,20(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// extsw r9,r11
	ctx.r9.s64 = r11.s32;
	// extsw r25,r8
	r25.s64 = ctx.r8.s32;
	// extsw r11,r7
	r11.s64 = ctx.r7.s32;
	// extsw r8,r30
	ctx.r8.s64 = r30.s32;
	// extsw r30,r29
	r30.s64 = r29.s32;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lis r7,16383
	ctx.r7.s64 = 1073676288;
	// extsw r26,r27
	r26.s64 = r27.s32;
	// mulld r27,r8,r9
	r27.s64 = ctx.r8.s64 * ctx.r9.s64;
	// mulld r23,r30,r6
	r23.s64 = r30.s64 * ctx.r6.s64;
	// ori r24,r7,65535
	r24.u64 = ctx.r7.u64 | 65535;
	// mulld r8,r8,r6
	ctx.r8.s64 = ctx.r8.s64 * ctx.r6.s64;
	// mulld r9,r9,r30
	ctx.r9.s64 = ctx.r9.s64 * r30.s64;
	// extsw r7,r3
	ctx.r7.s64 = ctx.r3.s32;
	// extsw r3,r28
	ctx.r3.s64 = r28.s32;
	// sradi r6,r27,30
	xer.ca = (r27.s64 < 0) & ((r27.u64 & 0x3FFFFFFF) != 0);
	ctx.r6.s64 = r27.s64 >> 30;
	// mulld r28,r10,r11
	r28.s64 = ctx.r10.s64 * r11.s64;
	// sradi r30,r23,30
	xer.ca = (r23.s64 < 0) & ((r23.u64 & 0x3FFFFFFF) != 0);
	r30.s64 = r23.s64 >> 30;
	// sradi r27,r9,30
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x3FFFFFFF) != 0);
	r27.s64 = ctx.r9.s64 >> 30;
	// mulld r29,r11,r11
	r29.s64 = r11.s64 * r11.s64;
	// sradi r23,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	r23.s64 = ctx.r8.s64 >> 30;
	// sradi r9,r28,30
	xer.ca = (r28.s64 < 0) & ((r28.u64 & 0x3FFFFFFF) != 0);
	ctx.r9.s64 = r28.s64 >> 30;
	// sradi r8,r29,30
	xer.ca = (r29.s64 < 0) & ((r29.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = r29.s64 >> 30;
	// extsw r29,r27
	r29.s64 = r27.s32;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// extsw r28,r23
	r28.s64 = r23.s32;
	// subf r8,r8,r24
	ctx.r8.s64 = r24.s64 - ctx.r8.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// subf r29,r28,r29
	r29.s64 = r29.s64 - r28.s64;
	// subf r28,r9,r8
	r28.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r30,r6,r30
	r30.u64 = ctx.r6.u64 + r30.u64;
	// add r27,r8,r9
	r27.u64 = ctx.r8.u64 + ctx.r9.u64;
	// extsw r6,r9
	ctx.r6.s64 = ctx.r9.s32;
	// extsw r9,r8
	ctx.r9.s64 = ctx.r8.s32;
	// extsw r8,r28
	ctx.r8.s64 = r28.s32;
	// mulld r24,r6,r10
	r24.s64 = ctx.r6.s64 * ctx.r10.s64;
	// extsw r28,r27
	r28.s64 = r27.s32;
	// mulld r27,r9,r11
	r27.s64 = ctx.r9.s64 * r11.s64;
	// mulld r10,r9,r10
	ctx.r10.s64 = ctx.r9.s64 * ctx.r10.s64;
	// mulld r9,r7,r8
	ctx.r9.s64 = ctx.r7.s64 * ctx.r8.s64;
	// mulld r11,r6,r11
	r11.s64 = ctx.r6.s64 * r11.s64;
	// mulld r6,r3,r28
	ctx.r6.s64 = ctx.r3.s64 * r28.s64;
	// mulld r8,r3,r8
	ctx.r8.s64 = ctx.r3.s64 * ctx.r8.s64;
	// mulld r7,r7,r28
	ctx.r7.s64 = ctx.r7.s64 * r28.s64;
	// sradi r9,r9,30
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x3FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s64 >> 30;
	// sradi r6,r6,30
	xer.ca = (ctx.r6.s64 < 0) & ((ctx.r6.u64 & 0x3FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s64 >> 30;
	// sradi r3,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r8.s64 >> 30;
	// sradi r7,r7,30
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x3FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s64 >> 30;
	// sradi r8,r24,30
	xer.ca = (r24.s64 < 0) & ((r24.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = r24.s64 >> 30;
	// sradi r28,r27,30
	xer.ca = (r27.s64 < 0) & ((r27.u64 & 0x3FFFFFFF) != 0);
	r28.s64 = r27.s64 >> 30;
	// sradi r27,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r27.s64 = r11.s64 >> 30;
	// sradi r24,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	r24.s64 = ctx.r10.s64 >> 30;
	// extsw r10,r28
	ctx.r10.s64 = r28.s32;
	// extsw r11,r8
	r11.s64 = ctx.r8.s32;
	// extsw r8,r27
	ctx.r8.s64 = r27.s32;
	// extsw r28,r24
	r28.s64 = r24.s32;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r10,r8,r28
	ctx.r10.s64 = r28.s64 - ctx.r8.s64;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// extsw r8,r6
	ctx.r8.s64 = ctx.r6.s32;
	// extsw r6,r3
	ctx.r6.s64 = ctx.r3.s32;
	// add r28,r9,r8
	r28.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// extsw r11,r9
	r11.s64 = ctx.r9.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// subf r27,r6,r7
	r27.s64 = ctx.r7.s64 - ctx.r6.s64;
	// mulld r7,r26,r11
	ctx.r7.s64 = r26.s64 * r11.s64;
	// lwz r9,28(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// mulld r6,r25,r10
	ctx.r6.s64 = r25.s64 * ctx.r10.s64;
	// lwz r8,24(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// mulld r11,r25,r11
	r11.s64 = r25.s64 * r11.s64;
	// mulld r10,r26,r10
	ctx.r10.s64 = r26.s64 * ctx.r10.s64;
	// sradi r7,r7,30
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x3FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s64 >> 30;
	// sradi r6,r6,30
	xer.ca = (ctx.r6.s64 < 0) & ((ctx.r6.u64 & 0x3FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s64 >> 30;
	// sradi r11,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s64 >> 30;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// extsw r4,r11
	ctx.r4.s64 = r11.s32;
	// extsw r3,r10
	ctx.r3.s64 = ctx.r10.s32;
	// extsw r11,r7
	r11.s64 = ctx.r7.s32;
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// subf r26,r4,r3
	r26.s64 = ctx.r3.s64 - ctx.r4.s64;
	// subf r25,r9,r8
	r25.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r24,r11,r10
	r24.u64 = r11.u64 + ctx.r10.u64;
	// add r23,r8,r9
	r23.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r10,r25,r26
	ctx.r10.u64 = r25.u64 + r26.u64;
	// add r11,r23,r24
	r11.u64 = r23.u64 + r24.u64;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// srawi r4,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// subf r11,r25,r27
	r11.s64 = r27.s64 - r25.s64;
	// subf r9,r26,r23
	ctx.r9.s64 = r23.s64 - r26.s64;
	// subf r10,r24,r11
	ctx.r10.s64 = r11.s64 - r24.s64;
	// subf r11,r28,r9
	r11.s64 = ctx.r9.s64 - r28.s64;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// srawi r4,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// stwx r9,r8,r5
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, ctx.r9.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// subf r11,r24,r25
	r11.s64 = r25.s64 - r24.s64;
	// subf r9,r28,r23
	ctx.r9.s64 = r23.s64 - r28.s64;
	// subf r10,r27,r11
	ctx.r10.s64 = r11.s64 - r27.s64;
	// subf r8,r31,r22
	ctx.r8.s64 = r22.s64 - r31.s64;
	// subf r11,r29,r9
	r11.s64 = ctx.r9.s64 - r29.s64;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// srawi r4,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// stwx r9,r8,r5
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, ctx.r9.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// subf r11,r24,r23
	r11.s64 = r23.s64 - r24.s64;
	// subf r9,r25,r26
	ctx.r9.s64 = r26.s64 - r25.s64;
	// subf r10,r30,r11
	ctx.r10.s64 = r11.s64 - r30.s64;
	// add r8,r31,r22
	ctx.r8.u64 = r31.u64 + r22.u64;
	// subf r11,r27,r9
	r11.s64 = ctx.r9.s64 - r27.s64;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// srawi r4,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// stwx r9,r8,r5
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, ctx.r9.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// rlwinm r11,r22,1,0,30
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, ctx.r3.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_825DF274"))) PPC_WEAK_FUNC(sub_825DF274);
PPC_FUNC_IMPL(__imp__sub_825DF274) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DF278"))) PPC_WEAK_FUNC(sub_825DF278);
PPC_FUNC_IMPL(__imp__sub_825DF278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-560(r1)
	ea = -560 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// lwz r10,220(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r5,52(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// stw r11,596(r1)
	PPC_STORE_U32(ctx.r1.u32 + 596, r11.u32);
	// beq cr6,0x825df2a8
	if (cr6.eq) goto loc_825DF2A8;
	// lhz r11,118(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 118);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// b 0x825df2ac
	goto loc_825DF2AC;
loc_825DF2A8:
	// lwz r10,256(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
loc_825DF2AC:
	// li r9,2048
	ctx.r9.s64 = 2048;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// li r11,0
	r11.s64 = 0;
	// divw r9,r9,r10
	ctx.r9.s32 = ctx.r9.s32 / ctx.r10.s32;
	// twllei r10,0
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// ble cr6,0x825df2d8
	if (!cr6.gt) goto loc_825DF2D8;
loc_825DF2C8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srw r8,r9,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bgt cr6,0x825df2c8
	if (cr6.gt) goto loc_825DF2C8;
loc_825DF2D8:
	// srawi r27,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r27.s64 = ctx.r10.s32 >> 1;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lis r11,23170
	r11.s64 = 1518469120;
	// lwz r8,4(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// srawi r17,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	r17.s64 = r27.s32 >> 1;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// ori r11,r11,31128
	r11.u64 = r11.u64 | 31128;
	// lwz r7,12(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// srawi r20,r17,1
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0x1) != 0);
	r20.s64 = r17.s32 >> 1;
	// lwz r6,16(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// mr r26,r11
	r26.u64 = r11.u64;
	// lwz r3,20(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// srawi r18,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r18.s64 = r20.s32 >> 1;
	// lwz r31,24(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// srawi r24,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r24.s64 = ctx.r9.s32 >> 2;
	// lwz r30,28(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	// srawi r9,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 2;
	// lwz r29,32(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// mr r19,r11
	r19.u64 = r11.u64;
	// lwz r4,36(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// mr r15,r11
	r15.u64 = r11.u64;
	// stw r20,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r20.u32);
	// mr r14,r11
	r14.u64 = r11.u64;
	// stw r18,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r18.u32);
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// srawi r10,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 2;
	// srawi r7,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 2;
	// lis r28,512
	r28.s64 = 33554432;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// neg r11,r10
	r11.s64 = -ctx.r10.s64;
	// srawi r10,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 2;
	// srawi r8,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r8.s64 = r31.s32 >> 2;
	// neg r25,r10
	r25.s64 = -ctx.r10.s64;
	// srawi r10,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	ctx.r10.s64 = r30.s32 >> 2;
	// srawi r6,r29,2
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x3) != 0);
	ctx.r6.s64 = r29.s32 >> 2;
	// neg r16,r10
	r16.s64 = -ctx.r10.s64;
	// neg r10,r7
	ctx.r10.s64 = -ctx.r7.s64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// srawi r4,r4,2
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 2;
	// lwz r29,84(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// neg r31,r9
	r31.s64 = -ctx.r9.s64;
	// neg r9,r4
	ctx.r9.s64 = -ctx.r4.s64;
	// neg r3,r6
	ctx.r3.s64 = -ctx.r6.s64;
	// subf r6,r9,r31
	ctx.r6.s64 = r31.s64 - ctx.r9.s64;
	// std r7,304(r1)
	PPC_STORE_U64(ctx.r1.u32 + 304, ctx.r7.u64);
	// neg r30,r24
	r30.s64 = -r24.s64;
	// add r21,r9,r31
	r21.u64 = ctx.r9.u64 + r31.u64;
	// stw r9,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r9.u32);
	// neg r9,r29
	ctx.r9.s64 = -r29.s64;
	// stw r3,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, ctx.r3.u32);
	// subf r4,r3,r30
	ctx.r4.s64 = r30.s64 - ctx.r3.s64;
	// add r24,r3,r30
	r24.u64 = ctx.r3.u64 + r30.u64;
	// lwz r29,92(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// extsw r3,r9
	ctx.r3.s64 = ctx.r9.s32;
	// stw r6,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r6.u32);
	// extsw r23,r11
	r23.s64 = r11.s32;
	// extsw r22,r25
	r22.s64 = r25.s32;
	// subf r7,r16,r28
	ctx.r7.s64 = r28.s64 - r16.s64;
	// add r8,r16,r28
	ctx.r8.u64 = r16.u64 + r28.u64;
	// std r3,312(r1)
	PPC_STORE_U64(ctx.r1.u32 + 312, ctx.r3.u64);
	// mulld r3,r23,r26
	ctx.r3.s64 = r23.s64 * r26.s64;
	// std r23,320(r1)
	PPC_STORE_U64(ctx.r1.u32 + 320, r23.u64);
	// std r22,328(r1)
	PPC_STORE_U64(ctx.r1.u32 + 328, r22.u64);
	// stw r7,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, ctx.r7.u32);
	// stw r7,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r7.u32);
	// mulld r26,r22,r19
	r26.s64 = r22.s64 * r19.s64;
	// sradi r3,r3,30
	xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = ctx.r3.s64 >> 30;
	// sradi r26,r26,30
	xer.ca = (r26.s64 < 0) & ((r26.u64 & 0x3FFFFFFF) != 0);
	r26.s64 = r26.s64 >> 30;
	// neg r29,r29
	r29.s64 = -r29.s64;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// ld r19,304(r1)
	r19.u64 = PPC_LOAD_U64(ctx.r1.u32 + 304);
	// mulld r19,r19,r15
	r19.s64 = r19.s64 * r15.s64;
	// add r15,r7,r11
	r15.u64 = ctx.r7.u64 + r11.u64;
	// sradi r19,r19,30
	xer.ca = (r19.s64 < 0) & ((r19.u64 & 0x3FFFFFFF) != 0);
	r19.s64 = r19.s64 >> 30;
	// stw r15,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r15.u32);
	// subf r15,r11,r7
	r15.s64 = ctx.r7.s64 - r11.s64;
	// add r7,r6,r25
	ctx.r7.u64 = ctx.r6.u64 + r25.u64;
	// stw r15,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r15.u32);
	// stw r7,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r7.u32);
	// subf r7,r25,r6
	ctx.r7.s64 = ctx.r6.s64 - r25.s64;
	// stw r7,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r7.u32);
	// add r7,r4,r10
	ctx.r7.u64 = ctx.r4.u64 + ctx.r10.u64;
	// stw r6,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r6.u32);
	// subf r15,r9,r10
	r15.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r4,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r4.u32);
	// stw r4,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r4.u32);
	// stw r29,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r29.u32);
	// stw r7,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r7.u32);
	// subf r7,r10,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r10.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r29,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r29.u32);
	// lis r4,11585
	ctx.r4.s64 = 759234560;
	// stw r7,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r7.u32);
	// add r7,r3,r8
	ctx.r7.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r7,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r7.u32);
	// subf r7,r3,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r3.s64;
	// stw r7,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r7.u32);
	// extsw r7,r26
	ctx.r7.s64 = r26.s32;
	// ori r26,r4,15564
	r26.u64 = ctx.r4.u64 | 15564;
	// add r6,r21,r7
	ctx.r6.u64 = r21.u64 + ctx.r7.u64;
	// subf r7,r7,r21
	ctx.r7.s64 = r21.s64 - ctx.r7.s64;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r6,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r6.u32);
	// stw r7,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r7.u32);
	// extsw r7,r19
	ctx.r7.s64 = r19.s32;
	// add r6,r24,r7
	ctx.r6.u64 = r24.u64 + ctx.r7.u64;
	// subf r7,r7,r24
	ctx.r7.s64 = r24.s64 - ctx.r7.s64;
	// stw r6,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r6.u32);
	// add r6,r25,r21
	ctx.r6.u64 = r25.u64 + r21.u64;
	// stw r7,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, ctx.r7.u32);
	// ld r7,312(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 312);
	// mulld r7,r7,r14
	ctx.r7.s64 = ctx.r7.s64 * r14.s64;
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// add r14,r11,r8
	r14.u64 = r11.u64 + ctx.r8.u64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r9,r29
	ctx.r8.u64 = ctx.r9.u64 + r29.u64;
	// sradi r7,r7,30
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x3FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s64 >> 30;
	// subf r9,r9,r29
	ctx.r9.s64 = r29.s64 - ctx.r9.s64;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// stw r8,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r8.u32);
	// add r8,r29,r7
	ctx.r8.u64 = r29.u64 + ctx.r7.u64;
	// stw r9,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, ctx.r9.u32);
	// subf r9,r15,r24
	ctx.r9.s64 = r24.s64 - r15.s64;
	// subf r9,r29,r9
	ctx.r9.s64 = ctx.r9.s64 - r29.s64;
	// stw r8,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r8.u32);
	// subf r8,r7,r29
	ctx.r8.s64 = r29.s64 - ctx.r7.s64;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// subf r7,r10,r29
	ctx.r7.s64 = r29.s64 - ctx.r10.s64;
	// mulld r9,r9,r26
	ctx.r9.s64 = ctx.r9.s64 * r26.s64;
	// stw r8,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r8.u32);
	// add r8,r6,r14
	ctx.r8.u64 = ctx.r6.u64 + r14.u64;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// sradi r9,r9,30
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x3FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s64 >> 30;
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// mulld r8,r8,r26
	ctx.r8.s64 = ctx.r8.s64 * r26.s64;
	// add r3,r10,r24
	ctx.r3.u64 = ctx.r10.u64 + r24.u64;
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// sradi r8,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s64 >> 30;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// subf r11,r25,r21
	r11.s64 = r21.s64 - r25.s64;
	// rlwinm r19,r17,2,0,29
	r19.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r9
	ctx.r4.u64 = r11.u64 + ctx.r9.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r11,r19,r5
	r11.u64 = r19.u64 + ctx.r5.u64;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// subf r11,r29,r15
	r11.s64 = r15.s64 - r29.s64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// subf r3,r9,r14
	ctx.r3.s64 = r14.s64 - ctx.r9.s64;
	// add r4,r11,r24
	ctx.r4.u64 = r11.u64 + r24.u64;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r24,r19,r11
	r24.u64 = r19.u64 + r11.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// subf r11,r21,r25
	r11.s64 = r25.s64 - r21.s64;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r4,r11,r9
	ctx.r4.u64 = r11.u64 + ctx.r9.u64;
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
	// bl 0x825deee8
	sub_825DEEE8(ctx, base);
	// lis r8,16069
	ctx.r8.s64 = 1053097984;
	// stwx r3,r19,r24
	PPC_STORE_U32(r19.u32 + r24.u32, ctx.r3.u32);
	// lis r7,3196
	ctx.r7.s64 = 209453056;
	// ori r8,r8,12190
	ctx.r8.u64 = ctx.r8.u64 | 12190;
	// ori r7,r7,23582
	ctx.r7.u64 = ctx.r7.u64 | 23582;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// lis r8,15136
	ctx.r8.s64 = 991952896;
	// lis r7,6269
	ctx.r7.s64 = 410845184;
	// ori r8,r8,55197
	ctx.r8.u64 = ctx.r8.u64 | 55197;
	// ori r7,r7,58022
	ctx.r7.u64 = ctx.r7.u64 | 58022;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r4,r1,240
	ctx.r4.s64 = ctx.r1.s64 + 240;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// lis r8,13622
	ctx.r8.s64 = 892731392;
	// lis r7,9102
	ctx.r7.s64 = 596508672;
	// ori r8,r8,52305
	ctx.r8.u64 = ctx.r8.u64 | 52305;
	// ori r7,r7,30322
	ctx.r7.u64 = ctx.r7.u64 | 30322;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r4,r1,272
	ctx.r4.s64 = ctx.r1.s64 + 272;
	// add r3,r18,r20
	ctx.r3.u64 = r18.u64 + r20.u64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r9,0
	ctx.r9.s64 = 0;
	// srawi r10,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r10.s64 = r11.s32 >> 7;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,26480
	r11.s64 = r11.s64 + 26480;
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lis r10,16383
	ctx.r10.s64 = 1073676288;
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// lfs f0,17092(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 17092);
	f0.f64 = double(temp.f32);
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r24,1
	r24.s64 = 1;
	// lfs f13,44(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// cmpwi cr6,r18,1
	cr6.compare<int32_t>(r18.s32, 1, xer);
	// lfs f12,48(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lfs f11,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// lfs f10,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * f0.f64));
	// lfs f9,40(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * f0.f64));
	// fmuls f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 * f0.f64));
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// fctiwz f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// fctiwz f13,f10
	ctx.f13.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// rlwinm r11,r25,1,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// stfiwx f0,0,r3
	PPC_STORE_U32(ctx.r3.u32, f0.u32);
	// ble cr6,0x825dfaf4
	if (!cr6.gt) goto loc_825DFAF4;
	// extsw r3,r11
	ctx.r3.s64 = r11.s32;
	// extsw r11,r16
	r11.s64 = r16.s32;
	// std r3,336(r1)
	PPC_STORE_U64(ctx.r1.u32 + 336, ctx.r3.u64);
	// std r11,352(r1)
	PPC_STORE_U64(ctx.r1.u32 + 352, r11.u64);
	// lwz r11,404(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,344(r1)
	PPC_STORE_U64(ctx.r1.u32 + 344, r11.u64);
	// lwz r11,400(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,360(r1)
	PPC_STORE_U64(ctx.r1.u32 + 360, r11.u64);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// addi r11,r17,-1
	r11.s64 = r17.s64 + -1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// subf r11,r17,r20
	r11.s64 = r20.s64 - r17.s64;
	// stw r11,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r11.u32);
	// b 0x825df700
	goto loc_825DF700;
loc_825DF6EC:
	// lis r11,16383
	r11.s64 = 1073676288;
	// ld r23,320(r1)
	r23.u64 = PPC_LOAD_U64(ctx.r1.u32 + 320);
	// ld r22,328(r1)
	r22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 328);
	// ld r3,336(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 336);
	// ori r10,r11,65535
	ctx.r10.u64 = r11.u64 | 65535;
loc_825DF700:
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// extsw r11,r25
	r11.s64 = r25.s32;
	// mr r21,r10
	r21.u64 = ctx.r10.u64;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// mulld r7,r11,r11
	ctx.r7.s64 = r11.s64 * r11.s64;
	// mulld r4,r11,r8
	ctx.r4.s64 = r11.s64 * ctx.r8.s64;
	// mulld r11,r3,r11
	r11.s64 = ctx.r3.s64 * r11.s64;
	// sradi r4,r4,30
	xer.ca = (ctx.r4.s64 < 0) & ((ctx.r4.u64 & 0x3FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s64 >> 30;
	// sradi r20,r7,30
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x3FFFFFFF) != 0);
	r20.s64 = ctx.r7.s64 >> 30;
	// sradi r11,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s64 >> 30;
	// mulld r8,r3,r8
	ctx.r8.s64 = ctx.r3.s64 * ctx.r8.s64;
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// subf r6,r9,r25
	ctx.r6.s64 = r25.s64 - ctx.r9.s64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// rlwinm r20,r20,1,0,30
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// sradi r8,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s64 >> 30;
	// stw r25,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r25.u32);
	// add r10,r9,r25
	ctx.r10.u64 = ctx.r9.u64 + r25.u64;
	// subf r6,r20,r21
	ctx.r6.s64 = r21.s64 - r20.s64;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// add r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 + ctx.r7.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// ld r11,352(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 352);
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// std r4,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r4.u64);
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// extsw r6,r9
	ctx.r6.s64 = ctx.r9.s32;
	// mulld r3,r10,r23
	ctx.r3.s64 = ctx.r10.s64 * r23.s64;
	// std r6,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r6.u64);
	// mulld r21,r10,r22
	r21.s64 = ctx.r10.s64 * r22.s64;
	// mulld r23,r4,r23
	r23.s64 = ctx.r4.s64 * r23.s64;
	// mulld r22,r4,r22
	r22.s64 = ctx.r4.s64 * r22.s64;
	// extsw r4,r7
	ctx.r4.s64 = ctx.r7.s32;
	// mulld r9,r11,r6
	ctx.r9.s64 = r11.s64 * ctx.r6.s64;
	// std r4,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r4.u64);
	// mulld r11,r4,r11
	r11.s64 = ctx.r4.s64 * r11.s64;
	// sradi r9,r9,30
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x3FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s64 >> 30;
	// sradi r7,r3,30
	xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x3FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r3.s64 >> 30;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// sradi r3,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = r11.s64 >> 30;
	// sradi r8,r23,30
	xer.ca = (r23.s64 < 0) & ((r23.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = r23.s64 >> 30;
	// extsw r11,r9
	r11.s64 = ctx.r9.s32;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// extsw r9,r7
	ctx.r9.s64 = ctx.r7.s32;
	// subf r23,r8,r28
	r23.s64 = r28.s64 - ctx.r8.s64;
	// extsw r7,r3
	ctx.r7.s64 = ctx.r3.s32;
	// ld r3,344(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 344);
	// subf r20,r9,r11
	r20.s64 = r11.s64 - ctx.r9.s64;
	// mulld r6,r3,r6
	ctx.r6.s64 = ctx.r3.s64 * ctx.r6.s64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r23.u32);
	// stw r7,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r7.u32);
	// mulld r4,r3,r4
	ctx.r4.s64 = ctx.r3.s64 * ctx.r4.s64;
	// sradi r6,r6,30
	xer.ca = (ctx.r6.s64 < 0) & ((ctx.r6.u64 & 0x3FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s64 >> 30;
	// sradi r3,r21,30
	xer.ca = (r21.s64 < 0) & ((r21.u64 & 0x3FFFFFFF) != 0);
	ctx.r3.s64 = r21.s64 >> 30;
	// add r21,r7,r9
	r21.u64 = ctx.r7.u64 + ctx.r9.u64;
	// subf r19,r9,r7
	r19.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// add r23,r9,r11
	r23.u64 = ctx.r9.u64 + r11.u64;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// extsw r9,r3
	ctx.r9.s64 = ctx.r3.s32;
	// add r3,r23,r28
	ctx.r3.u64 = r23.u64 + r28.u64;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// sradi r4,r4,30
	xer.ca = (ctx.r4.s64 < 0) & ((ctx.r4.u64 & 0x3FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r4.s64 >> 30;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// stw r7,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, ctx.r7.u32);
	// add r7,r20,r28
	ctx.r7.u64 = r20.u64 + r28.u64;
	// sradi r22,r22,30
	xer.ca = (r22.s64 < 0) & ((r22.u64 & 0x3FFFFFFF) != 0);
	r22.s64 = r22.s64 >> 30;
	// stw r3,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r3.u32);
	// add r3,r21,r28
	ctx.r3.u64 = r21.u64 + r28.u64;
	// stw r8,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r8.u32);
	// stw r7,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r7.u32);
	// add r7,r19,r28
	ctx.r7.u64 = r19.u64 + r28.u64;
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stw r7,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r7.u32);
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// lwz r16,92(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// extsw r23,r4
	r23.s64 = ctx.r4.s32;
	// lwz r15,84(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r11,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r11.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// add r8,r23,r9
	ctx.r8.u64 = r23.u64 + ctx.r9.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// ld r6,160(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// stw r8,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r8.u32);
	// extsw r8,r22
	ctx.r8.s64 = r22.s32;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// ld r22,120(r1)
	r22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// subf r20,r11,r8
	r20.s64 = ctx.r8.s64 - r11.s64;
	// subf r21,r8,r31
	r21.s64 = r31.s64 - ctx.r8.s64;
	// subf r19,r23,r8
	r19.s64 = ctx.r8.s64 - r23.s64;
	// ld r8,312(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 312);
	// stw r7,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r7.u32);
	// add r19,r19,r31
	r19.u64 = r19.u64 + r31.u64;
	// ld r7,360(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 360);
	// mulld r4,r7,r6
	ctx.r4.s64 = ctx.r7.s64 * ctx.r6.s64;
	// ld r6,304(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 304);
	// stw r19,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r19.u32);
	// mulld r3,r10,r6
	ctx.r3.s64 = ctx.r10.s64 * ctx.r6.s64;
	// sradi r18,r4,30
	xer.ca = (ctx.r4.s64 < 0) & ((ctx.r4.u64 & 0x3FFFFFFF) != 0);
	r18.s64 = ctx.r4.s64 >> 30;
	// sradi r4,r3,30
	xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x3FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r3.s64 >> 30;
	// subf r3,r9,r11
	ctx.r3.s64 = r11.s64 - ctx.r9.s64;
	// subf r9,r9,r23
	ctx.r9.s64 = r23.s64 - ctx.r9.s64;
	// mulld r22,r7,r22
	r22.s64 = ctx.r7.s64 * r22.s64;
	// ld r7,144(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// mulld r6,r7,r6
	ctx.r6.s64 = ctx.r7.s64 * ctx.r6.s64;
	// mulld r10,r10,r8
	ctx.r10.s64 = ctx.r10.s64 * ctx.r8.s64;
	// sradi r17,r22,30
	xer.ca = (r22.s64 < 0) & ((r22.u64 & 0x3FFFFFFF) != 0);
	r17.s64 = r22.s64 >> 30;
	// mulld r8,r7,r8
	ctx.r8.s64 = ctx.r7.s64 * ctx.r8.s64;
	// lwz r19,104(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// sradi r6,r6,30
	xer.ca = (ctx.r6.s64 < 0) & ((ctx.r6.u64 & 0x3FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r6.s64 >> 30;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// sradi r8,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s64 >> 30;
	// subf r11,r11,r21
	r11.s64 = r21.s64 - r11.s64;
	// add r19,r19,r31
	r19.u64 = r19.u64 + r31.u64;
	// extsw r9,r4
	ctx.r9.s64 = ctx.r4.s32;
	// extsw r22,r6
	r22.s64 = ctx.r6.s32;
	// std r10,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r10.u64);
	// extsw r10,r17
	ctx.r10.s64 = r17.s32;
	// std r8,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r8.u64);
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r11.u32);
	// subf r11,r23,r21
	r11.s64 = r21.s64 - r23.s64;
	// stw r19,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, r19.u32);
	// add r23,r20,r31
	r23.u64 = r20.u64 + r31.u64;
	// lwz r19,100(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r17,r10,r9
	r17.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r20,r9,r10
	r20.s64 = ctx.r10.s64 - ctx.r9.s64;
	// add r19,r19,r31
	r19.u64 = r19.u64 + r31.u64;
	// stw r11,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r11.u32);
	// extsw r11,r18
	r11.s64 = r18.s32;
	// add r17,r17,r30
	r17.u64 = r17.u64 + r30.u64;
	// stw r23,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r23.u32);
	// add r14,r9,r11
	r14.u64 = ctx.r9.u64 + r11.u64;
	// subf r23,r22,r30
	r23.s64 = r30.s64 - r22.s64;
	// add r14,r14,r30
	r14.u64 = r14.u64 + r30.u64;
	// stw r19,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r19.u32);
	// subf r18,r10,r22
	r18.s64 = r22.s64 - ctx.r10.s64;
	// subf r19,r9,r11
	r19.s64 = r11.s64 - ctx.r9.s64;
	// stw r17,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r17.u32);
	// subf r21,r11,r22
	r21.s64 = r22.s64 - r11.s64;
	// mr r8,r16
	ctx.r8.u64 = r16.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r14,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r14.u32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// ld r9,144(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// ld r22,120(r1)
	r22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// extsw r22,r22
	r22.s64 = r22.s32;
	// subf r11,r11,r23
	r11.s64 = r23.s64 - r11.s64;
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r11.u32);
	// add r11,r18,r30
	r11.u64 = r18.u64 + r30.u64;
	// stw r11,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, r11.u32);
	// add r11,r19,r30
	r11.u64 = r19.u64 + r30.u64;
	// stw r11,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r11.u32);
	// add r11,r20,r30
	r11.u64 = r20.u64 + r30.u64;
	// stw r11,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r11.u32);
	// add r11,r21,r30
	r11.u64 = r21.u64 + r30.u64;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
	// subf r11,r10,r23
	r11.s64 = r23.s64 - ctx.r10.s64;
	// subf r10,r22,r29
	ctx.r10.s64 = r29.s64 - r22.s64;
	// add r23,r22,r29
	r23.u64 = r22.u64 + r29.u64;
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// subf r11,r9,r29
	r11.s64 = r29.s64 - ctx.r9.s64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// stw r10,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r10.u32);
	// stw r23,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r23.u32);
	// stw r23,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r23.u32);
	// stw r10,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r10.u32);
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// stw r9,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r9.u32);
	// stw r11,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r11.u32);
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// lis r11,6269
	r11.s64 = 410845184;
	// lwz r17,80(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r23,r16
	r23.s64 = r16.s32;
	// ori r10,r11,58022
	ctx.r10.u64 = r11.u64 | 58022;
	// lis r11,15136
	r11.s64 = 991952896;
	// extsw r22,r15
	r22.s64 = r15.s32;
	// ori r11,r11,55197
	r11.u64 = r11.u64 | 55197;
	// mulld r9,r23,r10
	ctx.r9.s64 = r23.s64 * ctx.r10.s64;
	// mulld r8,r22,r11
	ctx.r8.s64 = r22.s64 * r11.s64;
	// mulld r11,r23,r11
	r11.s64 = r23.s64 * r11.s64;
	// sradi r9,r9,30
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x3FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s64 >> 30;
	// mulld r10,r22,r10
	ctx.r10.s64 = r22.s64 * ctx.r10.s64;
	// sradi r8,r8,30
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x3FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s64 >> 30;
	// sradi r11,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s64 >> 30;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// extsw r19,r11
	r19.s64 = r11.s32;
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// extsw r20,r8
	r20.s64 = ctx.r8.s32;
	// extsw r21,r9
	r21.s64 = ctx.r9.s32;
	// extsw r18,r10
	r18.s64 = ctx.r10.s32;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// subf r7,r20,r21
	ctx.r7.s64 = r21.s64 - r20.s64;
	// add r8,r18,r19
	ctx.r8.u64 = r18.u64 + r19.u64;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// add r3,r11,r17
	ctx.r3.u64 = r11.u64 + r17.u64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// subf r8,r18,r19
	ctx.r8.s64 = r19.s64 - r18.s64;
	// add r7,r20,r21
	ctx.r7.u64 = r20.u64 + r21.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r4,r1,240
	ctx.r4.s64 = ctx.r1.s64 + 240;
	// add r3,r24,r11
	ctx.r3.u64 = r24.u64 + r11.u64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// mulld r11,r23,r26
	r11.s64 = r23.s64 * r26.s64;
	// mulld r10,r22,r26
	ctx.r10.s64 = r22.s64 * r26.s64;
	// sradi r11,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s64 >> 30;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// addi r4,r1,272
	ctx.r4.s64 = ctx.r1.s64 + 272;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x825df008
	sub_825DF008(ctx, base);
	// ld r11,128(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// mulld r10,r11,r22
	ctx.r10.s64 = r11.s64 * r22.s64;
	// mulld r11,r11,r23
	r11.s64 = r11.s64 * r23.s64;
	// sradi r10,r10,30
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 30;
	// sradi r11,r11,30
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s64 >> 30;
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r17,-1
	ctx.r9.s64 = r17.s64 + -1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// lwz r11,172(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// stw r16,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r16.u32);
	// stw r15,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r15.u32);
	// cmpw cr6,r24,r11
	cr6.compare<int32_t>(r24.s32, r11.s32, xer);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// blt cr6,0x825df6ec
	if (cr6.lt) goto loc_825DF6EC;
loc_825DFAF4:
	// lwz r31,596(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	// li r30,0
	r30.s64 = 0;
	// lhz r11,118(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 118);
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x825dfb30
	if (!cr6.gt) goto loc_825DFB30;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_825DFB10:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// ble cr6,0x825dfb20
	if (!cr6.gt) goto loc_825DFB20;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_825DFB20:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825dfb10
	if (!cr6.eq) goto loc_825DFB10;
loc_825DFB30:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,148(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 148);
	// rlwinm r10,r7,0,16,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFF0;
	// lwz r8,52(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lfs f0,17088(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17088);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// stfs f0,128(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// subf r6,r10,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r10.s64;
	// vspltw v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// ble cr6,0x825dfbe0
	if (!cr6.gt) goto loc_825DFBE0;
	// addi r11,r10,-1
	r11.s64 = ctx.r10.s64 + -1;
	// rlwinm r11,r11,28,4,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0xFFFFFFF;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// rlwinm r11,r7,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
loc_825DFB74:
	// addi r5,r8,16
	ctx.r5.s64 = ctx.r8.s64 + 16;
	// lvx128 v13,r0,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r8,32
	ctx.r4.s64 = ctx.r8.s64 + 32;
	// vcfux v13,v13,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// addi r3,r8,48
	ctx.r3.s64 = ctx.r8.s64 + 48;
	// addi r29,r9,16
	r29.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lvx128 v12,r0,r5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r9,32
	ctx.r5.s64 = ctx.r9.s64 + 32;
	// lvx128 v11,r0,r4
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfux v12,v12,0
	_mm_store_ps(ctx.v12.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v12.u32)));
	// lvx128 v10,r0,r3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfux v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// vcfux v10,v10,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// addi r4,r9,48
	ctx.r4.s64 = ctx.r9.s64 + 48;
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmulfp128 v13,v13,v0
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v12,v12,v0
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v11,v11,v0
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v10,v10,v0
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// stvx v12,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x825dfb74
	if (!cr6.eq) goto loc_825DFB74;
loc_825DFBE0:
	// add r4,r6,r10
	ctx.r4.u64 = ctx.r6.u64 + ctx.r10.u64;
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x825dfca8
	if (cr6.lt) goto loc_825DFCA8;
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - r11.s64;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r3,r9,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
loc_825DFC24:
	// lwz r5,-12(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + -12);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwzx r29,r3,r10
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	// lwz r28,-4(r7)
	r28.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// lwz r27,0(r7)
	r27.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// std r5,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r5.u64);
	// std r29,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r29.u64);
	// std r28,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r28.u64);
	// std r27,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r27.u64);
	// lfd f13,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// lfd f12,144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f11,120(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f10,160(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// fmuls f13,f12,f0
	ctx.f13.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmuls f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f11.f64 * f0.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f13,f10,f0
	ctx.f13.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f13,8(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825dfc24
	if (!cr6.eq) goto loc_825DFC24;
loc_825DFCA8:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// bge cr6,0x825dfcec
	if (!cr6.lt) goto loc_825DFCEC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
loc_825DFCC0:
	// lwzx r9,r10,r8
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// lfd f13,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x825dfcc0
	if (!cr6.eq) goto loc_825DFCC0;
loc_825DFCEC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x825dfd04
	if (!cr6.eq) goto loc_825DFD04;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r1,560
	ctx.r1.s64 = ctx.r1.s64 + 560;
	// b 0x8239bd10
	return;
loc_825DFD04:
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// li r3,0
	ctx.r3.s64 = 0;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f13,128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,156(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 156, temp.u32);
	// addi r1,r1,560
	ctx.r1.s64 = ctx.r1.s64 + 560;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_825DFD2C"))) PPC_WEAK_FUNC(sub_825DFD2C);
PPC_FUNC_IMPL(__imp__sub_825DFD2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DFD30"))) PPC_WEAK_FUNC(sub_825DFD30);
PPC_FUNC_IMPL(__imp__sub_825DFD30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// stw r4,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, ctx.r4.u32);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// lfs f0,17096(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 17096);
	f0.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stfs f0,-48(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stw r11,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, r11.u32);
	// stfs f0,-44(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stw r3,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, ctx.r3.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// stfs f0,-36(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// lfs f0,-12632(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12632);
	f0.f64 = double(temp.f32);
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// stfs f0,-64(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f0,-60(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// stfs f0,-56(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// stfs f0,-52(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// beq cr6,0x825e0094
	if (cr6.eq) goto loc_825E0094;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x825dfe7c
	if (cr6.eq) goto loc_825DFE7C;
	// clrlwi r29,r5,16
	r29.u64 = ctx.r5.u32 & 0xFFFF;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x825e02c0
	if (!cr6.gt) goto loc_825E02C0;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r31,0
	r31.s64 = 0;
	// li r30,1
	r30.s64 = 1;
	// lfs f13,5736(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5736);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,2480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2480);
	ctx.f12.f64 = double(temp.f32);
loc_825DFDB8:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825dfe68
	if (!cr6.gt) goto loc_825DFE68;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r10,r4
	ctx.r7.u64 = ctx.r10.u64 + ctx.r4.u64;
loc_825DFDD0:
	// lwz r8,320(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// mulli r10,r9,1776
	ctx.r10.s64 = ctx.r9.s64 * 1776;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r8,r1,-88
	ctx.r8.s64 = ctx.r1.s64 + -88;
	// lwz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// lfsx f0,r5,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lhz r10,110(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 110);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r10,r30,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r30.u32 << (ctx.r10.u8 & 0x3F));
	// bge cr6,0x825dfe20
	if (!cr6.lt) goto loc_825DFE20;
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// not r8,r10
	ctx.r8.u64 = ~ctx.r10.u64;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x825dfe44
	if (!cr6.lt) goto loc_825DFE44;
	// b 0x825dfe3c
	goto loc_825DFE3C;
loc_825DFE20:
	// fadds f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + ctx.f13.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// ble cr6,0x825dfe44
	if (!cr6.gt) goto loc_825DFE44;
loc_825DFE3C:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// stw r10,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r10.u32);
loc_825DFE44:
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// extsh r9,r10
	ctx.r9.s64 = ctx.r10.s16;
	// sth r8,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r8.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dfdd0
	if (cr6.lt) goto loc_825DFDD0;
loc_825DFE68:
	// addi r9,r31,1
	ctx.r9.s64 = r31.s64 + 1;
	// extsh r31,r9
	r31.s64 = ctx.r9.s16;
	// cmpw cr6,r31,r29
	cr6.compare<int32_t>(r31.s32, r29.s32, xer);
	// blt cr6,0x825dfdb8
	if (cr6.lt) goto loc_825DFDB8;
	// b 0x8239bd4c
	return;
loc_825DFE7C:
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r8,r10,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// sth r9,-96(r1)
	PPC_STORE_U16(ctx.r1.u32 + -96, ctx.r9.u16);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// sth r8,-94(r1)
	PPC_STORE_U16(ctx.r1.u32 + -94, ctx.r8.u16);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// sth r10,-92(r1)
	PPC_STORE_U16(ctx.r1.u32 + -92, ctx.r10.u16);
	// ble cr6,0x825dffa8
	if (!cr6.gt) goto loc_825DFFA8;
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// li r10,0
	ctx.r10.s64 = 0;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825DFEBC:
	// lwz r9,320(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// addi r11,r10,2
	r11.s64 = ctx.r10.s64 + 2;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,3
	ctx.r7.s64 = ctx.r10.s64 + 3;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,60(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// li r5,16
	ctx.r5.s64 = 16;
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + r11.u64;
	// lfsx f0,r10,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// stfs f0,-64(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// lwz r11,1836(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 1836);
	// stfs f0,-56(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// lfsx f0,r10,r11
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// stfs f0,-60(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// stfs f0,-52(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,-48(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// lfs f0,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,-40(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,-44(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,-36(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmaxfp v0,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_max_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// vmaxfp v13,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_max_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// vminfp v0,v11,v0
	_mm_store_ps(ctx.v0.f32, _mm_min_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// vminfp v13,v11,v13
	_mm_store_ps(ctx.v13.f32, _mm_min_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vctsxs v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_vctsxs(_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// vpkswss v0,v13,v0
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v0,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stvrx v0,r11,r5
	ea = r11.u32 + ctx.r5.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lhz r11,-96(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -96);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lhz r8,-94(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -94);
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r4,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, ctx.r4.u32);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// sth r9,-96(r1)
	PPC_STORE_U16(ctx.r1.u32 + -96, ctx.r9.u16);
	// blt cr6,0x825dfebc
	if (cr6.lt) goto loc_825DFEBC;
	// lwz r3,-80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
loc_825DFFA8:
	// lhz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -92);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// bge cr6,0x825e02c0
	if (!cr6.lt) goto loc_825E02C0;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r30,1
	r30.s64 = 1;
	// lfs f13,5736(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5736);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,2480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2480);
	ctx.f12.f64 = double(temp.f32);
loc_825DFFDC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e0080
	if (!cr6.gt) goto loc_825E0080;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,0
	ctx.r8.s64 = 0;
loc_825DFFEC:
	// lwz r9,320(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// mulli r10,r8,1776
	ctx.r10.s64 = ctx.r8.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r1,-88
	ctx.r9.s64 = ctx.r1.s64 + -88;
	// lwz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// lfsx f0,r7,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lhz r10,110(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 110);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r10,r30,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r30.u32 << (ctx.r10.u8 & 0x3F));
	// bge cr6,0x825e003c
	if (!cr6.lt) goto loc_825E003C;
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825e0060
	if (!cr6.lt) goto loc_825E0060;
	// b 0x825e0058
	goto loc_825E0058;
loc_825E003C:
	// fadds f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + ctx.f13.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x825e0060
	if (!cr6.gt) goto loc_825E0060;
loc_825E0058:
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r10,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r10.u32);
loc_825E0060:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// sth r9,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, ctx.r9.u16);
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x825dffec
	if (cr6.lt) goto loc_825DFFEC;
loc_825E0080:
	// addi r9,r6,1
	ctx.r9.s64 = ctx.r6.s64 + 1;
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x825dffdc
	if (cr6.lt) goto loc_825DFFDC;
	// b 0x8239bd4c
	return;
loc_825E0094:
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r8,r10,0,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// sth r9,-96(r1)
	PPC_STORE_U16(ctx.r1.u32 + -96, ctx.r9.u16);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// sth r8,-94(r1)
	PPC_STORE_U16(ctx.r1.u32 + -94, ctx.r8.u16);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// sth r10,-92(r1)
	PPC_STORE_U16(ctx.r1.u32 + -92, ctx.r10.u16);
	// ble cr6,0x825e01d8
	if (!cr6.gt) goto loc_825E01D8;
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// li r10,0
	ctx.r10.s64 = 0;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E00D4:
	// lwz r11,320(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// addi r7,r10,3
	ctx.r7.s64 = ctx.r10.s64 + 3;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r10,4
	ctx.r6.s64 = ctx.r10.s64 + 4;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// addi r5,r10,5
	ctx.r5.s64 = ctx.r10.s64 + 5;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r10,6
	ctx.r3.s64 = ctx.r10.s64 + 6;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// addi r10,r10,7
	ctx.r10.s64 = ctx.r10.s64 + 7;
	// stfs f0,-40(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// rlwinm r3,r3,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,-36(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lfsx f0,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// li r8,16
	ctx.r8.s64 = 16;
	// stfs f0,-64(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// lfsx f0,r5,r11
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// stfs f0,-60(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// lfsx f0,r3,r11
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// stfs f0,-56(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// lfsx f0,r10,r11
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// stfs f0,-52(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -52, temp.u32);
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,-48(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// lfs f0,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,-44(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// vmaxfp v0,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_max_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-64
	r11.s64 = ctx.r1.s64 + -64;
	// vmaxfp v13,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_max_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// vminfp v0,v11,v0
	_mm_store_ps(ctx.v0.f32, _mm_min_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// vminfp v13,v11,v13
	_mm_store_ps(ctx.v13.f32, _mm_min_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vctsxs v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.s32, _mm_vctsxs(_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// vpkswss v0,v13,v0
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvlx v0,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stvrx v0,r11,r8
	ea = r11.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lhz r11,-96(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + -96);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lhz r8,-94(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + -94);
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// extsh r7,r8
	ctx.r7.s64 = ctx.r8.s16;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r4,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, ctx.r4.u32);
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// sth r9,-96(r1)
	PPC_STORE_U16(ctx.r1.u32 + -96, ctx.r9.u16);
	// blt cr6,0x825e00d4
	if (cr6.lt) goto loc_825E00D4;
	// lwz r3,-80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
loc_825E01D8:
	// lhz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + -92);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// bge cr6,0x825e02c0
	if (!cr6.lt) goto loc_825E02C0;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r30,1
	r30.s64 = 1;
	// lfs f13,5736(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5736);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,2480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2480);
	ctx.f12.f64 = double(temp.f32);
loc_825E020C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e02b0
	if (!cr6.gt) goto loc_825E02B0;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,0
	ctx.r8.s64 = 0;
loc_825E021C:
	// lwz r9,320(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 320);
	// mulli r10,r8,1776
	ctx.r10.s64 = ctx.r8.s64 * 1776;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r1,-88
	ctx.r9.s64 = ctx.r1.s64 + -88;
	// lwz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// lfsx f0,r7,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lhz r10,110(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 110);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// slw r10,r30,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r30.u32 << (ctx.r10.u8 & 0x3F));
	// bge cr6,0x825e026c
	if (!cr6.lt) goto loc_825E026C;
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x825e0290
	if (!cr6.lt) goto loc_825E0290;
	// b 0x825e0288
	goto loc_825E0288;
loc_825E026C:
	// fadds f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + ctx.f13.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// ble cr6,0x825e0290
	if (!cr6.gt) goto loc_825E0290;
loc_825E0288:
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r10,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r10.u32);
loc_825E0290:
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// extsh r8,r10
	ctx.r8.s64 = ctx.r10.s16;
	// sth r9,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, ctx.r9.u16);
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// lhz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 34);
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e021c
	if (cr6.lt) goto loc_825E021C;
loc_825E02B0:
	// addi r9,r6,1
	ctx.r9.s64 = ctx.r6.s64 + 1;
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// cmpw cr6,r6,r5
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r5.s32, xer);
	// blt cr6,0x825e020c
	if (cr6.lt) goto loc_825E020C;
loc_825E02C0:
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825E02C4"))) PPC_WEAK_FUNC(sub_825E02C4);
PPC_FUNC_IMPL(__imp__sub_825E02C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E02C8"))) PPC_WEAK_FUNC(sub_825E02C8);
PPC_FUNC_IMPL(__imp__sub_825E02C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce8
	// lis r11,-32244
	r11.s64 = -2113142784;
	// li r7,0
	ctx.r7.s64 = 0;
	// lfs f0,17104(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17104);
	f0.f64 = double(temp.f32);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// stfs f0,-112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// stfs f0,-108(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// stfs f0,-104(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// stfs f0,-100(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// lfs f0,17100(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17100);
	f0.f64 = double(temp.f32);
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// stfs f0,-96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -96, temp.u32);
	// stfs f0,-92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -92, temp.u32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stfs f0,-88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -88, temp.u32);
	// stfs f0,-84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -84, temp.u32);
	// beq cr6,0x825e0418
	if (cr6.eq) goto loc_825E0418;
	// clrlwi r30,r5,16
	r30.u64 = ctx.r5.u32 & 0xFFFF;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x825e0718
	if (!cr6.gt) goto loc_825E0718;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r31,0
	r31.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f13,5736(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 5736);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,2480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	ctx.f12.f64 = double(temp.f32);
loc_825E0334:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e0400
	if (!cr6.gt) goto loc_825E0400;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
loc_825E0354:
	// lwz r8,320(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// mulli r11,r9,1776
	r11.s64 = ctx.r9.s64 * 1776;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// addi r8,r1,-128
	ctx.r8.s64 = ctx.r1.s64 + -128;
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// lfsx f0,r11,r5
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
	f0.f64 = double(temp.f32);
	// lhz r11,110(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 110);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// bge cr6,0x825e03a4
	if (!cr6.lt) goto loc_825E03A4;
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// not r8,r11
	ctx.r8.u64 = ~r11.u64;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x825e03c4
	if (!cr6.lt) goto loc_825E03C4;
	// b 0x825e03c0
	goto loc_825E03C0;
loc_825E03A4:
	// fadds f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 + ctx.f13.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x825e03c4
	if (!cr6.gt) goto loc_825E03C4;
loc_825E03C0:
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_825E03C4:
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mr r29,r11
	r29.u64 = r11.u64;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// stb r8,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, ctx.r8.u8);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stb r29,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r29.u8);
	// stb r11,-1(r10)
	PPC_STORE_U8(ctx.r10.u32 + -1, r11.u8);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// lhz r11,34(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// blt cr6,0x825e0354
	if (cr6.lt) goto loc_825E0354;
loc_825E0400:
	// addi r10,r31,1
	ctx.r10.s64 = r31.s64 + 1;
	// extsh r31,r10
	r31.s64 = ctx.r10.s16;
	// cmpw cr6,r31,r30
	cr6.compare<int32_t>(r31.s32, r30.s32, xer);
	// blt cr6,0x825e0334
	if (cr6.lt) goto loc_825E0334;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd38
	return;
loc_825E0418:
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// li r6,0
	ctx.r6.s64 = 0;
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// ble cr6,0x825e05b0
	if (!cr6.gt) goto loc_825E05B0;
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E0450:
	// lwz r9,320(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r7,1
	r31.s64 = ctx.r7.s64 + 1;
	// addi r28,r1,-128
	r28.s64 = ctx.r1.s64 + -128;
	// addi r27,r1,-124
	r27.s64 = ctx.r1.s64 + -124;
	// lwz r5,60(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// addi r26,r1,-120
	r26.s64 = ctx.r1.s64 + -120;
	// lwz r9,1836(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 1836);
	// addi r25,r1,-116
	r25.s64 = ctx.r1.s64 + -116;
	// add r7,r8,r5
	ctx.r7.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r5,r31,1
	ctx.r5.s64 = r31.s64 + 1;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	f0.f64 = double(temp.f32);
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// stfs f0,-112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -112, temp.u32);
	// lfs f0,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	f0.f64 = double(temp.f32);
	// addi r7,r5,1
	ctx.r7.s64 = ctx.r5.s64 + 1;
	// stfs f0,-104(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -104, temp.u32);
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,-108(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -108, temp.u32);
	// lfs f0,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,-100(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -100, temp.u32);
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// vmaxfp v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_max_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vminfp v0,v12,v0
	_mm_store_ps(ctx.v0.f32, _mm_min_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// addi r9,r1,-112
	ctx.r9.s64 = ctx.r1.s64 + -112;
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,-112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r28
	PPC_STORE_U32(r28.u32, f0.u32);
	// lwz r9,-128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// lfs f0,-108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r27
	PPC_STORE_U32(r27.u32, f0.u32);
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// lfs f0,-104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	f0.f64 = double(temp.f32);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r26
	PPC_STORE_U32(r26.u32, f0.u32);
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// lfs f0,-100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	f0.f64 = double(temp.f32);
	// srawi r31,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r31.s64 = ctx.r9.s32 >> 8;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stb r8,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r8.u8);
	// stfiwx f0,0,r25
	PPC_STORE_U32(r25.u32, f0.u32);
	// stw r9,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, ctx.r9.u32);
	// mr r8,r31
	ctx.r8.u64 = r31.u64;
	// lwz r9,-124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -124);
	// stb r5,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r5.u8);
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// srawi r8,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 8;
	// stw r9,-124(r1)
	PPC_STORE_U32(ctx.r1.u32 + -124, ctx.r9.u32);
	// lwz r9,-120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -120);
	// stb r31,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r31.u8);
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// stb r5,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r5.u8);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// srawi r26,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	r26.s64 = ctx.r9.s32 >> 8;
	// stw r9,-120(r1)
	PPC_STORE_U32(ctx.r1.u32 + -120, ctx.r9.u32);
	// lwz r9,-116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -116);
	// stb r28,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r28.u8);
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// stb r27,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r27.u8);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stb r26,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r26.u8);
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// stw r9,-116(r1)
	PPC_STORE_U32(ctx.r1.u32 + -116, ctx.r9.u32);
	// srawi r9,r9,8
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// stb r25,2(r11)
	PPC_STORE_U8(r11.u32 + 2, r25.u8);
	// stb r24,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r24.u8);
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// blt cr6,0x825e0450
	if (cr6.lt) goto loc_825E0450;
loc_825E05B0:
	// extsh r11,r29
	r11.s64 = r29.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x825e0718
	if (cr6.eq) goto loc_825E0718;
	// extsh r11,r6
	r11.s64 = ctx.r6.s16;
	// li r6,1
	ctx.r6.s64 = 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,320(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// lfsx f13,r11,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f12,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// addi r11,r11,5736
	r11.s64 = r11.s64 + 5736;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// bge cr6,0x825e0624
	if (!cr6.lt) goto loc_825E0624;
	// fsubs f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 - f0.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lhz r11,110(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 110);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e0650
	if (!cr6.lt) goto loc_825E0650;
	// b 0x825e064c
	goto loc_825E064C;
loc_825E0624:
	// fadds f13,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f13.f64 + f0.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lhz r11,110(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 110);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825e0650
	if (!cr6.gt) goto loc_825E0650;
loc_825E064C:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_825E0650:
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r31,r11
	r31.u64 = r11.u64;
	// addi r9,r7,1
	ctx.r9.s64 = ctx.r7.s64 + 1;
	// stw r11,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r11.u32);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// rlwinm r11,r10,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// stb r5,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r5.u8);
	// stb r31,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r31.u8);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// lwz r11,320(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// lwz r11,1836(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1836);
	// lfsx f13,r11,r8
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	ctx.f13.f64 = double(temp.f32);
	// lhz r11,110(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 110);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// slw r11,r6,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r6.u32 << (r11.u8 & 0x3F));
	// bge cr6,0x825e06d0
	if (!cr6.lt) goto loc_825E06D0;
	// fsubs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 - f0.f64));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// not r10,r11
	ctx.r10.u64 = ~r11.u64;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e06f0
	if (!cr6.lt) goto loc_825E06F0;
	// b 0x825e06ec
	goto loc_825E06EC;
loc_825E06D0:
	// fadds f0,f13,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// lwz r11,-128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x825e06f0
	if (!cr6.gt) goto loc_825E06F0;
loc_825E06EC:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_825E06F0:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// srawi r7,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r7.s64 = r11.s32 >> 8;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// stb r10,2(r11)
	PPC_STORE_U8(r11.u32 + 2, ctx.r10.u8);
	// stb r8,1(r11)
	PPC_STORE_U8(r11.u32 + 1, ctx.r8.u8);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
loc_825E0718:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_825E0720"))) PPC_WEAK_FUNC(sub_825E0720);
PPC_FUNC_IMPL(__imp__sub_825E0720) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bccc
	// lhz r11,110(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 110);
	// li r10,1
	ctx.r10.s64 = 1;
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// li r6,0
	ctx.r6.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, r11.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,-192(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// fdivs f0,f0,f13
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
	// stfs f0,-144(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// stfs f0,-140(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// stfs f0,-136(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// stfs f0,-132(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// beq cr6,0x825e0a10
	if (cr6.eq) goto loc_825E0A10;
	// cmpwi cr6,r9,2
	cr6.compare<int32_t>(ctx.r9.s32, 2, xer);
	// beq cr6,0x825e0804
	if (cr6.eq) goto loc_825E0804;
	// clrlwi r31,r5,16
	r31.u64 = ctx.r5.u32 & 0xFFFF;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x825e0c4c
	if (!cr6.gt) goto loc_825E0C4C;
	// li r4,0
	ctx.r4.s64 = 0;
loc_825E079C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825e07ec
	if (!cr6.gt) goto loc_825E07EC;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,0
	r11.s64 = 0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_825E07B4:
	// lwz r6,320(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = r11.s64 * 1776;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// extsh r11,r30
	r11.s64 = r30.s16;
	// lwz r9,60(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// lfsx f13,r9,r5
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e07b4
	if (cr6.lt) goto loc_825E07B4;
loc_825E07EC:
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// blt cr6,0x825e079c
	if (cr6.lt) goto loc_825E079C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd1c
	return;
loc_825E0804:
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// lwz r9,320(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// clrlwi r10,r8,28
	ctx.r10.u64 = ctx.r8.u32 & 0xF;
	// rlwinm r5,r11,0,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// extsh r25,r5
	r25.s64 = ctx.r5.s16;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// subf r11,r25,r11
	r11.s64 = r11.s64 - r25.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwz r10,60(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// lwz r9,1836(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 1836);
	// extsh r24,r11
	r24.s64 = r11.s16;
	// beq cr6,0x825e0990
	if (cr6.eq) goto loc_825E0990;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x825e0990
	if (!cr6.gt) goto loc_825E0990;
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// li r11,0
	r11.s64 = 0;
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E0850:
	// addi r6,r11,2
	ctx.r6.s64 = r11.s64 + 2;
	// addi r31,r11,3
	r31.s64 = r11.s64 + 3;
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r11,4
	r30.s64 = r11.s64 + 4;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r29,r11,5
	r29.s64 = r11.s64 + 5;
	// rlwinm r30,r30,2,0,29
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r28,r11,6
	r28.s64 = r11.s64 + 6;
	// lfsx f13,r4,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r29,r29,2,0,29
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,-176(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// addi r27,r11,7
	r27.s64 = r11.s64 + 7;
	// lfsx f13,r31,r10
	temp.u32 = PPC_LOAD_U32(r31.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,-168(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// lfsx f13,r30,r10
	temp.u32 = PPC_LOAD_U32(r30.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r27,r27,2,0,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,-160(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r29,r10
	temp.u32 = PPC_LOAD_U32(r29.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stfs f13,-152(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// add r26,r5,r10
	r26.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lfsx f13,r28,r10
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// stfs f13,-144(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// addi r23,r8,16
	r23.s64 = ctx.r8.s64 + 16;
	// lfsx f13,r27,r10
	temp.u32 = PPC_LOAD_U32(r27.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r22,r8,32
	r22.s64 = ctx.r8.s64 + 32;
	// stfs f13,-136(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// addi r21,r8,48
	r21.s64 = ctx.r8.s64 + 48;
	// lfsx f13,r4,r9
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// extsh r6,r11
	ctx.r6.s64 = r11.s16;
	// stfs f13,-172(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// lfsx f13,r31,r9
	temp.u32 = PPC_LOAD_U32(r31.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// stfs f13,-164(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// lfsx f13,r30,r9
	temp.u32 = PPC_LOAD_U32(r30.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// stfs f13,-156(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// lfsx f13,r29,r9
	temp.u32 = PPC_LOAD_U32(r29.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-148(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// lfsx f13,r28,r9
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-140(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// lfsx f13,r27,r9
	temp.u32 = PPC_LOAD_U32(r27.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-132(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// lfs f13,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-192(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -192, temp.u32);
	// lfs f13,4(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-184(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -184, temp.u32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-188(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -188, temp.u32);
	// lfs f13,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-180(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -180, temp.u32);
	// addi r5,r1,-176
	ctx.r5.s64 = ctx.r1.s64 + -176;
	// lvx128 v12,r0,r5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-160
	ctx.r5.s64 = ctx.r1.s64 + -160;
	// vmaddfp v12,v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v11,r0,r5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-144
	ctx.r5.s64 = ctx.r1.s64 + -144;
	// vmaddfp v11,v11,v13,v0
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-176
	ctx.r5.s64 = ctx.r1.s64 + -176;
	// vmaddfp v10,v10,v13,v0
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r23
	_mm_store_si128((__m128i*)(base + ((r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-160
	ctx.r5.s64 = ctx.r1.s64 + -160;
	// stvx v11,r0,r22
	_mm_store_si128((__m128i*)(base + ((r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-144
	ctx.r5.s64 = ctx.r1.s64 + -144;
	// stvx v10,r0,r21
	_mm_store_si128((__m128i*)(base + ((r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-192
	ctx.r5.s64 = ctx.r1.s64 + -192;
	// lvx128 v9,r0,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-192
	ctx.r5.s64 = ctx.r1.s64 + -192;
	// vmaddfp v9,v9,v13,v0
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// stvx v9,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x825e0850
	if (cr6.lt) goto loc_825E0850;
loc_825E0990:
	// extsh r11,r24
	r11.s64 = r24.s16;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// add r31,r11,r25
	r31.u64 = r11.u64 + r25.u64;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// bge cr6,0x825e0c4c
	if (!cr6.lt) goto loc_825E0C4C;
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
loc_825E09A8:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825e09f8
	if (!cr6.gt) goto loc_825E09F8;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,0
	r11.s64 = 0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_825E09C0:
	// lwz r6,320(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = r11.s64 * 1776;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// extsh r11,r30
	r11.s64 = r30.s16;
	// lwz r9,60(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// lfsx f13,r9,r5
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e09c0
	if (cr6.lt) goto loc_825E09C0;
loc_825E09F8:
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// blt cr6,0x825e09a8
	if (cr6.lt) goto loc_825E09A8;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd1c
	return;
loc_825E0A10:
	// extsh r11,r5
	r11.s64 = ctx.r5.s16;
	// lwz r5,320(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// clrlwi r10,r8,28
	ctx.r10.u64 = ctx.r8.u32 & 0xF;
	// rlwinm r9,r11,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r4,r10,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// lwz r10,60(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 60);
	// extsh r5,r9
	ctx.r5.s64 = ctx.r9.s16;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// beq cr6,0x825e0bd4
	if (cr6.eq) goto loc_825E0BD4;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x825e0bd4
	if (!cr6.gt) goto loc_825E0BD4;
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// li r11,0
	r11.s64 = 0;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E0A58:
	// addi r6,r11,2
	ctx.r6.s64 = r11.s64 + 2;
	// addi r31,r11,3
	r31.s64 = r11.s64 + 3;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r30,r11,4
	r30.s64 = r11.s64 + 4;
	// lfsx f13,r6,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r11,5
	r29.s64 = r11.s64 + 5;
	// stfs f13,-184(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -184, temp.u32);
	// addi r28,r11,6
	r28.s64 = r11.s64 + 6;
	// lfsx f13,r31,r10
	temp.u32 = PPC_LOAD_U32(r31.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r31,r30,2,0,29
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,-180(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -180, temp.u32);
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r11,7
	ctx.r6.s64 = r11.s64 + 7;
	// stfs f13,-192(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -192, temp.u32);
	// rlwinm r29,r28,2,0,29
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r28,r6,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f13,-188(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -188, temp.u32);
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// addi r27,r11,9
	r27.s64 = r11.s64 + 9;
	// addi r24,r8,16
	r24.s64 = ctx.r8.s64 + 16;
	// rlwinm r23,r6,2,0,29
	r23.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r26,r11,10
	r26.s64 = r11.s64 + 10;
	// rlwinm r27,r27,2,0,29
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r25,r11,11
	r25.s64 = r11.s64 + 11;
	// rlwinm r26,r26,2,0,29
	r26.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r25,2,0,29
	r25.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r11,12
	ctx.r6.s64 = r11.s64 + 12;
	// addi r19,r8,32
	r19.s64 = ctx.r8.s64 + 32;
	// addi r22,r11,13
	r22.s64 = r11.s64 + 13;
	// addi r21,r11,14
	r21.s64 = r11.s64 + 14;
	// addi r20,r11,15
	r20.s64 = r11.s64 + 15;
	// rlwinm r18,r6,2,0,29
	r18.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r17,r8,48
	r17.s64 = ctx.r8.s64 + 48;
	// extsh r6,r11
	ctx.r6.s64 = r11.s16;
	// rlwinm r22,r22,2,0,29
	r22.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r21,r21,2,0,29
	r21.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r20,r20,2,0,29
	r20.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// addi r9,r1,-192
	ctx.r9.s64 = ctx.r1.s64 + -192;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-192
	ctx.r9.s64 = ctx.r1.s64 + -192;
	// vmaddfp v12,v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// lfsx f13,r31,r10
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r31.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-176(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// lfsx f13,r30,r10
	temp.u32 = PPC_LOAD_U32(r30.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-172(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// lfsx f13,r29,r10
	temp.u32 = PPC_LOAD_U32(r29.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-168(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// lfsx f13,r28,r10
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-164(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// vmaddfp v12,v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r24
	_mm_store_si128((__m128i*)(base + ((r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfsx f13,r23,r10
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r23.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-160(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// lfsx f13,r27,r10
	temp.u32 = PPC_LOAD_U32(r27.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-156(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// lfsx f13,r26,r10
	temp.u32 = PPC_LOAD_U32(r26.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-152(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// lfsx f13,r25,r10
	temp.u32 = PPC_LOAD_U32(r25.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-148(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// vmaddfp v12,v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r19
	_mm_store_si128((__m128i*)(base + ((r19.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfsx f13,r18,r10
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r18.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-144(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfsx f13,r22,r10
	temp.u32 = PPC_LOAD_U32(r22.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// stfs f13,-140(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// lfsx f13,r21,r10
	temp.u32 = PPC_LOAD_U32(r21.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-136(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// lfsx f13,r20,r10
	temp.u32 = PPC_LOAD_U32(r20.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,-132(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// vmaddfp v12,v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r17
	_mm_store_si128((__m128i*)(base + ((r17.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blt cr6,0x825e0a58
	if (cr6.lt) goto loc_825E0A58;
loc_825E0BD4:
	// extsh r11,r4
	r11.s64 = ctx.r4.s16;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// add r31,r11,r5
	r31.u64 = r11.u64 + ctx.r5.u64;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// bge cr6,0x825e0c4c
	if (!cr6.lt) goto loc_825E0C4C;
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
loc_825E0BEC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x825e0c3c
	if (!cr6.gt) goto loc_825E0C3C;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,0
	r11.s64 = 0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_825E0C04:
	// lwz r6,320(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 320);
	// mulli r9,r11,1776
	ctx.r9.s64 = r11.s64 * 1776;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// extsh r11,r30
	r11.s64 = r30.s16;
	// lwz r9,60(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// lfsx f13,r9,r5
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lhz r9,34(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 34);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e0c04
	if (cr6.lt) goto loc_825E0C04;
loc_825E0C3C:
	// addi r11,r4,1
	r11.s64 = ctx.r4.s64 + 1;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// cmpw cr6,r4,r31
	cr6.compare<int32_t>(ctx.r4.s32, r31.s32, xer);
	// blt cr6,0x825e0bec
	if (cr6.lt) goto loc_825E0BEC;
loc_825E0C4C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_825E0C54"))) PPC_WEAK_FUNC(sub_825E0C54);
PPC_FUNC_IMPL(__imp__sub_825E0C54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E0C58"))) PPC_WEAK_FUNC(sub_825E0C58);
PPC_FUNC_IMPL(__imp__sub_825E0C58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,1
	r11.s64 = 1;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// cmpwi cr6,r5,16
	cr6.compare<int32_t>(ctx.r5.s32, 16, xer);
	// slw r22,r11,r5
	r22.u64 = ctx.r5.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r5.u8 & 0x3F));
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r23,r11,17120
	r23.s64 = r11.s64 + 17120;
	// bge cr6,0x825e0cb4
	if (!cr6.lt) goto loc_825E0CB4;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// rlwinm r11,r5,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,16360
	ctx.r10.s64 = ctx.r10.s64 + 16360;
	// addi r9,r9,16296
	ctx.r9.s64 = ctx.r9.s64 + 16296;
	// lfsx f13,r11,r10
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f31,r11,r9
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	f31.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,560(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x825e0d04
	goto loc_825E0D04;
loc_825E0CB4:
	// extsw r11,r22
	r11.s64 = r22.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfd f0,-31360(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fdiv f31,f0,f13
	f31.f64 = f0.f64 / ctx.f13.f64;
	// lfd f0,0(r23)
	f0.u64 = PPC_LOAD_U64(r23.u32 + 0);
	// fmul f1,f31,f0
	ctx.f1.f64 = f31.f64 * f0.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,16424(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16424);
	// fmul f1,f31,f0
	ctx.f1.f64 = f31.f64 * f0.f64;
	// frsp f31,f13
	f31.f64 = double(float(ctx.f13.f64));
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f0,264(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
loc_825E0D04:
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x825e0d18
	if (!cr6.eq) goto loc_825E0D18;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,-25364(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25364);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
loc_825E0D18:
	// li r11,48
	r11.s64 = 48;
	// stfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f31,96(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// rlwinm r24,r22,1,0,30
	r24.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lvx128 v26,r11,r23
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,5736(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vspltw v1,v0,0
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor v31,v1,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v26.u8)));
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// vspltw v27,v13,0
	_mm_store_si128((__m128i*)v27.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,64
	r11.s64 = 64;
	// vspltw v28,v12,0
	_mm_store_si128((__m128i*)v28.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v29,r11,r23
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,16
	r11.s64 = 16;
	// lvx128 v0,r11,r23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,80
	r11.s64 = 80;
	// lvx128 v25,r11,r23
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,32
	r11.s64 = 32;
	// lvx128 v30,r11,r23
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E0D88:
	// cmpwi cr6,r3,8
	cr6.compare<int32_t>(ctx.r3.s32, 8, xer);
	// bgt cr6,0x825e0da0
	if (cr6.gt) goto loc_825E0DA0;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x825e0fdc
	if (!cr6.eq) goto loc_825E0FDC;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// ble cr6,0x825e0f6c
	if (!cr6.gt) goto loc_825E0F6C;
loc_825E0DA0:
	// vmaddfp v5,v1,v27,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(v27.f32)), _mm_load_ps(ctx.v0.f32)));
	// li r9,0
	ctx.r9.s64 = 0;
	// vmaddfp v4,v31,v28,v29
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(v28.f32)), _mm_load_ps(v29.f32)));
	// srawi r26,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	r26.s64 = ctx.r3.s32 >> 1;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// vor v3,v29,v29
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)v29.u8));
	// vor v2,v0,v0
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmaddfp v1,v5,v25,v0
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(v25.f32)), _mm_load_ps(ctx.v0.f32)));
	// vsldoi v13,v0,v5,8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 8));
	// vor v28,v5,v5
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vsldoi v10,v29,v4,8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 8));
	// vor v27,v4,v4
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vxor v9,v13,v30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vxor v31,v1,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)v26.u8)));
	// ble cr6,0x825e0e24
	if (!cr6.gt) goto loc_825E0E24;
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r3,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r31
	r11.u64 = r31.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
loc_825E0DEC:
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// cmpw cr6,r9,r24
	cr6.compare<int32_t>(ctx.r9.s32, r24.s32, xer);
	// vaddfp v13,v13,v12
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v12,v10,v11,v0
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v13,v11,78
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xB1));
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// vmaddfp v13,v9,v13,v12
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v12.f32)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// blt cr6,0x825e0dec
	if (cr6.lt) goto loc_825E0DEC;
loc_825E0E24:
	// li r25,4
	r25.s64 = 4;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// ble cr6,0x825e0f64
	if (!cr6.gt) goto loc_825E0F64;
	// add r11,r26,r3
	r11.u64 = r26.u64 + ctx.r3.u64;
	// addi r10,r3,4
	ctx.r10.s64 = ctx.r3.s64 + 4;
	// addi r8,r26,4
	ctx.r8.s64 = r26.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r28,r31,16
	r28.s64 = r31.s64 + 16;
	// add r27,r9,r31
	r27.u64 = ctx.r9.u64 + r31.u64;
	// add r29,r10,r31
	r29.u64 = ctx.r10.u64 + r31.u64;
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
loc_825E0E5C:
	// vmaddfp v3,v31,v5,v3
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v3.f32)));
	// add r6,r25,r3
	ctx.r6.u64 = r25.u64 + ctx.r3.u64;
	// vmaddfp v2,v1,v4,v2
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v2.f32)));
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// cmpw cr6,r6,r24
	cr6.compare<int32_t>(ctx.r6.s32, r24.s32, xer);
	// vmaddfp v5,v1,v3,v5
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v5.f32)));
	// vmaddfp v4,v31,v2,v4
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(ctx.v4.f32)));
	// vsldoi v13,v2,v5,8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v5.u8), 8));
	// vsldoi v7,v3,v4,8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8), 8));
	// vxor v6,v13,v30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)v30.u8)));
	// bgt cr6,0x825e0f08
	if (cr6.gt) goto loc_825E0F08;
	// rlwinm r5,r3,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r3,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_825E0EA0:
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r4,r5,r4
	ctx.r4.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpw cr6,r6,r24
	cr6.compare<int32_t>(ctx.r6.s32, r24.s32, xer);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v8,v11,v10
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v10.f32)));
	// vsubfp v9,v13,v12
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vaddfp v13,v13,v12
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vaddfp v12,v11,v10
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v10,v7,v8,v0
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v11,v7,v9,v0
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v13,v9,78
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xB1));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v12,v8,78
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), 0xB1));
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// vmaddfp v12,v6,v12,v10
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v13,v6,v13,v11
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v11.f32)));
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// ble cr6,0x825e0ea0
	if (!cr6.gt) goto loc_825E0EA0;
loc_825E0F08:
	// cmpw cr6,r4,r24
	cr6.compare<int32_t>(ctx.r4.s32, r24.s32, xer);
	// bgt cr6,0x825e0f48
	if (cr6.gt) goto loc_825E0F48;
	// add r10,r4,r26
	ctx.r10.u64 = ctx.r4.u64 + r26.u64;
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vaddfp v13,v13,v12
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v12,v7,v11,v0
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v13,v11,78
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xB1));
	// vmaddfp v13,v6,v13,v12
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v12.f32)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E0F48:
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// addi r29,r29,16
	r29.s64 = r29.s64 + 16;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// addi r27,r27,16
	r27.s64 = r27.s64 + 16;
	// cmpw cr6,r25,r26
	cr6.compare<int32_t>(r25.s32, r26.s32, xer);
	// blt cr6,0x825e0e5c
	if (cr6.lt) goto loc_825E0E5C;
loc_825E0F64:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// b 0x825e0d88
	goto loc_825E0D88;
loc_825E0F6C:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x825e108c
	if (!cr6.gt) goto loc_825E108C;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x825e108c
	if (!cr6.gt) goto loc_825E108C;
	// addi r10,r24,-1
	ctx.r10.s64 = r24.s64 + -1;
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825E0F8C:
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r8,r11,-4
	ctx.r8.s64 = r11.s64 + -4;
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// fadds f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 + f0.f64));
	// lfs f12,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fadds f13,f11,f12
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f12.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// fsubs f0,f12,f11
	f0.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// stfs f13,0(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// bne cr6,0x825e0f8c
	if (!cr6.eq) goto loc_825E0F8C;
	// b 0x825e108c
	goto loc_825E108C;
loc_825E0FDC:
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x825e108c
	if (!cr6.gt) goto loc_825E108C;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x825e108c
	if (!cr6.gt) goto loc_825E108C;
	// addi r10,r24,-1
	ctx.r10.s64 = r24.s64 + -1;
	// mr r11,r31
	r11.u64 = r31.u64;
	// rlwinm r10,r10,28,4,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0xFFFFFFF;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// li r10,96
	ctx.r10.s64 = 96;
	// lvx128 v9,r10,r23
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,112
	ctx.r10.s64 = 112;
	// lvx128 v8,r10,r23
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E100C:
	// li r10,16
	ctx.r10.s64 = 16;
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,32
	ctx.r8.s64 = 32;
	// li r7,48
	ctx.r7.s64 = 48;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvlx v13,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,32
	ctx.r10.s64 = r11.s64 + 32;
	// lvlx v12,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vaddfp v10,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvlx v11,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vsubfp v13,v12,v11
	_mm_store_ps(ctx.v13.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v11.f32)));
	// addi r8,r10,-16
	ctx.r8.s64 = ctx.r10.s64 + -16;
	// vaddfp v12,v12,v11
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v11.f32)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vpermwi128 v11,v10,177
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0x4E));
	// vpermwi128 v7,v0,238
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x11));
	// vpermwi128 v6,v13,238
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x11));
	// vpermwi128 v0,v0,17
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xEE));
	// vpermwi128 v13,v13,17
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xEE));
	// vmaddfp v11,v10,v9,v11
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v11.f32)));
	// vpermwi128 v5,v12,177
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0x4E));
	// vmaddfp v0,v7,v8,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v13,v6,v8,v13
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v12,v12,v9,v5
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v5.f32)));
	// stvx v11,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x825e100c
	if (!cr6.eq) goto loc_825E100C;
loc_825E108C:
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// ble cr6,0x825e1194
	if (!cr6.gt) goto loc_825E1194;
	// srawi r11,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	r11.s64 = r22.s32 >> 1;
	// addi r3,r22,1
	ctx.r3.s64 = r22.s64 + 1;
	// addze r30,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r30.s64 = temp.s64;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// ble cr6,0x825e1194
	if (!cr6.gt) goto loc_825E1194;
	// addi r9,r3,1
	ctx.r9.s64 = ctx.r3.s64 + 1;
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r9,r31
	ctx.r5.u64 = ctx.r9.u64 + r31.u64;
loc_825E10C0:
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e1130
	if (!cr6.lt) goto loc_825E1130;
	// add r7,r3,r10
	ctx.r7.u64 = ctx.r3.u64 + ctx.r10.u64;
	// lfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r8,r3,2
	ctx.r8.s64 = ctx.r3.s64 + 2;
	// lfs f13,-8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// lfs f12,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r8,r4
	ctx.r7.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f10,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfsx f11,r7,r31
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,-8(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// lfs f10,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,-4(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfsx f10,r6,r31
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,0(r5)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfsx f10,r8,r31
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	ctx.f10.f64 = double(temp.f32);
	// stfsx f10,r7,r31
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + r31.u32, temp.u32);
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfsx f0,r6,r31
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + r31.u32, temp.u32);
	// stfsx f11,r8,r31
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + r31.u32, temp.u32);
loc_825E1130:
	// add r9,r10,r22
	ctx.r9.u64 = ctx.r10.u64 + r22.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lfs f12,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// bgt cr6,0x825e117c
	if (cr6.gt) goto loc_825E117C;
loc_825E1168:
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addze r8,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r8.s64 = temp.s64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// ble cr6,0x825e1168
	if (!cr6.gt) goto loc_825E1168;
loc_825E117C:
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmpw cr6,r4,r22
	cr6.compare<int32_t>(ctx.r4.s32, r22.s32, xer);
	// blt cr6,0x825e10c0
	if (cr6.lt) goto loc_825E10C0;
loc_825E1194:
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x825e1270
	if (!cr6.eq) goto loc_825E1270;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r24,4
	cr6.compare<int32_t>(r24.s32, 4, xer);
	// lfs f13,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x825e1228
	if (cr6.lt) goto loc_825E1228;
	// extsw r9,r22
	ctx.r9.s64 = r22.s32;
	// addi r10,r24,-4
	ctx.r10.s64 = r24.s64 + -4;
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 / f0.f64));
loc_825E11DC:
	// addi r9,r11,-8
	ctx.r9.s64 = r11.s64 + -8;
	// lfs f12,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r8,r11,-4
	ctx.r8.s64 = r11.s64 + -4;
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lfs f12,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lfs f10,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmuls f10,f0,f10
	ctx.f10.f64 = double(float(f0.f64 * ctx.f10.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f11,0(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f10,0(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// bne cr6,0x825e11dc
	if (!cr6.eq) goto loc_825E11DC;
loc_825E1228:
	// cmpw cr6,r6,r24
	cr6.compare<int32_t>(ctx.r6.s32, r24.s32, xer);
	// bge cr6,0x825e1270
	if (!cr6.lt) goto loc_825E1270;
	// extsw r9,r22
	ctx.r9.s64 = r22.s32;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r6,r24
	ctx.r10.s64 = r24.s64 - ctx.r6.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 / f0.f64));
loc_825E1254:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x825e1254
	if (!cr6.eq) goto loc_825E1254;
loc_825E1270:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_825E127C"))) PPC_WEAK_FUNC(sub_825E127C);
PPC_FUNC_IMPL(__imp__sub_825E127C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E1280"))) PPC_WEAK_FUNC(sub_825E1280);
PPC_FUNC_IMPL(__imp__sub_825E1280) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stfd f29,-128(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -128, f29.u64);
	// stfd f30,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, f30.u64);
	// stfd f31,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, f31.u64);
	// addi r12,r1,-128
	r12.s64 = ctx.r1.s64 + -128;
	// bl 0x826a7c64
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// srawi r11,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r11.s64 = ctx.r6.s32 >> 1;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// li r24,0
	r24.s64 = 0;
	// addze r26,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r26.s64 = temp.s64;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// ble cr6,0x825e12d4
	if (!cr6.gt) goto loc_825E12D4;
loc_825E12C4:
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// srw r11,r6,r24
	r11.u64 = r24.u8 & 0x20 ? 0 : (ctx.r6.u32 >> (r24.u8 & 0x3F));
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x825e12c4
	if (cr6.gt) goto loc_825E12C4;
loc_825E12D4:
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// addi r10,r26,-1
	ctx.r10.s64 = r26.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// and r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 & r26.u64;
	// add r25,r11,r27
	r25.u64 = r11.u64 + r27.u64;
	// cntlzw r11,r10
	r11.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// mr r30,r27
	r30.u64 = r27.u64;
	// rlwinm r23,r11,27,31,31
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// cmpwi cr6,r6,64
	cr6.compare<int32_t>(ctx.r6.s32, 64, xer);
	// mr r29,r25
	r29.u64 = r25.u64;
	// blt cr6,0x825e1380
	if (cr6.lt) goto loc_825E1380;
	// cmpwi cr6,r6,2048
	cr6.compare<int32_t>(ctx.r6.s32, 2048, xer);
	// bgt cr6,0x825e1380
	if (cr6.gt) goto loc_825E1380;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// beq cr6,0x825e1380
	if (cr6.eq) goto loc_825E1380;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// frsp f0,f31
	f0.f64 = double(float(f31.f64));
	// srawi r10,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 7;
	// addi r11,r11,26480
	r11.s64 = r11.s64 + 26480;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lfs f12,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * f0.f64));
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f11,144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// lfs f0,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	f0.f64 = double(temp.f32);
	// lfs f11,40(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fneg f0,f11
	f0.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f0,128(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lfs f0,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f0.f64 = double(temp.f32);
	// stfs f0,176(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// fneg f0,f13
	f0.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// b 0x825e1464
	goto loc_825E1464;
loc_825E1380:
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f30,f0
	f30.f64 = double(f0.s64);
	// lfd f0,-30984(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -30984);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fdiv f13,f0,f30
	ctx.f13.f64 = f0.f64 / f30.f64;
	// lfd f0,16440(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16440);
	// fmul f29,f13,f0
	f29.f64 = ctx.f13.f64 * f0.f64;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f0,-31360(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fdiv f30,f0,f30
	f30.f64 = f0.f64 / f30.f64;
	// fmul f0,f1,f31
	f0.f64 = ctx.f1.f64 * f31.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfd f0,16432(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16432);
	// fmul f29,f30,f0
	f29.f64 = f30.f64 * f0.f64;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,144(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,16424(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16424);
	// fmul f30,f30,f0
	f30.f64 = f30.f64 * f0.f64;
	// fmul f0,f1,f31
	f0.f64 = ctx.f1.f64 * f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmr f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64;
	// lis r11,-32251
	r11.s64 = -2113601536;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// lfd f0,264(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// frsp f31,f0
	f31.f64 = double(float(f0.f64));
	// stfs f31,128(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,6732(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	f0.f64 = double(temp.f32);
	// fmuls f0,f31,f0
	f0.f64 = double(float(f31.f64 * f0.f64));
	// stfs f0,176(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// frsp f0,f1
	f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
loc_825E1464:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// srawi r11,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r11.s64 = r26.s32 >> 2;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// addze r28,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r28.s64 = temp.s64;
	// vspltw v12,v13,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// vspltw v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// vspltw128 v127,v10,0
	_mm_store_si128((__m128i*)v127.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// vspltw v13,v9,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32244
	r11.s64 = -2113142784;
	// vspltw v10,v8,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), 0xFF));
	// addi r31,r11,17136
	r31.s64 = r11.s64 + 17136;
	// li r11,32
	r11.s64 = 32;
	// lvx128 v11,r11,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// vxor128 v126,v127,v11
	_mm_store_si128((__m128i*)v126.u8, _mm_xor_si128(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsldoi v11,v12,v13,8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 8));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// vspltw128 v125,v7,0
	_mm_store_si128((__m128i*)v125.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xFF));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,16
	r11.s64 = 16;
	// vspltw128 v124,v9,0
	_mm_store_si128((__m128i*)v124.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// vor v9,v0,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmaddcfp128 v9,v127,v9,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// vsldoi v12,v10,v0,8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 8));
	// vmaddfp128 v10,v126,v13,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v10.f32)));
	// lvx128 v7,r11,r31
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vsldoi v13,v13,v9,8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v9.u8), 8));
	// vsldoi v0,v0,v10,8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 8));
	// vxor v10,v13,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x825e159c
	if (!cr6.gt) goto loc_825E159C;
loc_825E1518:
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v6,v9,v9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// lvx128 v8,r0,r29
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v5,v8,v8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vmaddfp128 v11,v127,v0,v11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp128 v12,v126,v13,v12
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v12.f32)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stvx v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v6,v8,5,2
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 78), 5));
	// vrlimi128 v5,v9,5,2
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 78), 5));
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vor v8,v6,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v9,v5,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vor v5,v0,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmulfp128 v6,v0,v8
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32)));
	// stvx v9,r0,r29
	_mm_store_si128((__m128i*)(base + ((r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// vmaddfp128 v5,v126,v11,v5
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v5.f32)));
	// vmaddfp128 v13,v127,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v13.f32)));
	// vor v0,v5,v5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stvx v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v8,v8,78
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), 0xB1));
	// vmaddfp v9,v10,v8,v6
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v6.f32)));
	// vxor v10,v13,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v9,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bgt cr6,0x825e1518
	if (cr6.gt) goto loc_825E1518;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E159C:
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e15f4
	if (!cr6.gt) goto loc_825E15F4;
loc_825E15A8:
	// vmaddfp128 v12,v126,v13,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v12.f32)));
	// lvx128 v9,r0,r30
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v8,v0,v9
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)));
	// vpermwi128 v9,v9,78
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xB1));
	// vmaddfp128 v11,v127,v0,v11
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v11.f32)));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vmaddfp128 v13,v127,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v10,v10,v9,v8
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v8.f32)));
	// vmaddfp128 v0,v126,v11,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// vxor v9,v13,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v10,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// vor v10,v9,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// bgt cr6,0x825e15a8
	if (cr6.gt) goto loc_825E15A8;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_825E15F4:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// addi r5,r24,-1
	ctx.r5.s64 = r24.s64 + -1;
	// bne cr6,0x825e1604
	if (!cr6.eq) goto loc_825E1604;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_825E1604:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mtctr r22
	ctr.u64 = r22.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r11,48
	r11.s64 = 48;
	// lvx128 v12,r0,r31
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// vor v0,v12,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vsldoi128 v7,v124,v12,12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)ctx.v12.u8), 4));
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// lvx128 v13,r11,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// vor v10,v13,v13
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vmaddcfp128 v0,v126,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(v125.f32)));
	// vsldoi128 v9,v125,v13,12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v13.u8), 4));
	// vmaddcfp128 v10,v127,v10,v124
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(v124.f32)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,16
	r11.s64 = 16;
	// vor v8,v0,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vsldoi v0,v13,v0,12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8), 4));
	// lvx128 v11,r11,r31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,32
	r11.s64 = 32;
	// vor v6,v10,v10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v9,v8,1,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 228), 1));
	// vrlimi128 v7,v10,1,0
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 228), 1));
	// vmaddcfp128 v6,v126,v6,v13
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v13.f32)));
	// vsldoi v13,v12,v10,12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_alignr_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8), 4));
	// vmaddfp128 v12,v127,v8,v12
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v12.f32)));
	// lvx128 v2,r11,r31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// vrlimi128 v0,v6,1,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 228), 1));
	// vrlimi128 v13,v12,1,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 228), 1));
	// vxor v6,v0,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vxor v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vxor v4,v13,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// ble cr6,0x825e1738
	if (!cr6.gt) goto loc_825E1738;
loc_825E16A4:
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v8,v13,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v1,v0,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmaddcfp128 v8,v126,v8,v9
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v9.f32)));
	// vor v9,v0,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmaddcfp128 v9,v127,v9,v7
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v7.f32)));
	// vor v7,v12,v12
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vpermwi128 v12,v12,78
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xB1));
	// vrlimi128 v7,v10,5,1
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 147), 5));
	// vrlimi128 v12,v10,5,2
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 78), 5));
	// vmaddfp128 v13,v127,v8,v13
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v10,v4,v7
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v4,v0,v7
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v7.f32)));
	// vmaddfp128 v1,v126,v9,v1
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v1.f32)));
	// vor v3,v9,v9
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v9.u8));
	// vor v9,v8,v8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vmaddfp v10,v6,v12,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v12,v5,v12,v4
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v4.f32)));
	// vxor v5,v13,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vxor v4,v13,v2
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v0,v1,v1
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vxor v8,v0,v11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vpermwi128 v10,v10,228
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0x1B));
	// vor v6,v8,v8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stvx v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// bgt cr6,0x825e16a4
	if (cr6.gt) goto loc_825E16A4;
loc_825E1738:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x825e174c
	if (cr6.eq) goto loc_825E174C;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
loc_825E174C:
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-128
	r12.s64 = ctx.r1.s64 + -128;
	// bl 0x826a7efc
	// lfd f29,-128(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// lfd f30,-120(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f31,-112(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_825E1768"))) PPC_WEAK_FUNC(sub_825E1768);
PPC_FUNC_IMPL(__imp__sub_825E1768) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// addi r12,r1,-128
	r12.s64 = ctx.r1.s64 + -128;
	// bl 0x8239d5e8
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r18,r4
	r18.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r3,296(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 296);
	// bl 0x825cd8d8
	sub_825CD8D8(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,156(r18)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r18.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,20(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + 20);
	// li r24,0
	r24.s64 = 0;
	// lwz r19,0(r18)
	r19.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// li r21,0
	r21.s64 = 0;
	// lwz r22,144(r18)
	r22.u64 = PPC_LOAD_U32(r18.u32 + 144);
	// li r29,0
	r29.s64 = 0;
	// lwz r23,12(r18)
	r23.u64 = PPC_LOAD_U32(r18.u32 + 12);
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// fdivs f29,f0,f13
	f29.f64 = double(float(f0.f64 / ctx.f13.f64));
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// fmuls f31,f29,f1
	f31.f64 = double(float(f29.f64 * ctx.f1.f64));
	// bne cr6,0x825e187c
	if (!cr6.eq) goto loc_825E187C;
	// lwz r11,264(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r10,264(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// lwz r11,268(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x825e1844
	if (!cr6.lt) goto loc_825E1844;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// subf r8,r22,r26
	ctx.r8.s64 = r26.s64 - r22.s64;
loc_825E1804:
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfsx f0,r8,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	f0.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r7,268(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// blt cr6,0x825e1804
	if (cr6.lt) goto loc_825E1804;
loc_825E1844:
	// lhz r10,118(r18)
	ctx.r10.u64 = PPC_LOAD_U16(r18.u32 + 118);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,268(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r22
	ctx.r3.u64 = r11.u64 + r22.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-128
	r12.s64 = ctx.r1.s64 + -128;
	// bl 0x8239d634
	// b 0x8239bd1c
	return;
loc_825E187C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r6,264(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// lfs f0,292(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + 292);
	f0.f64 = double(temp.f32);
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// lfs f28,17248(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17248);
	f28.f64 = double(temp.f32);
	// lis r11,25
	r11.s64 = 1638400;
	// fmuls f30,f0,f28
	f30.f64 = double(float(f0.f64 * f28.f64));
	// ori r30,r11,26125
	r30.u64 = r11.u64 | 26125;
	// lis r11,15470
	r11.s64 = 1013841920;
	// ori r31,r11,62303
	r31.u64 = r11.u64 | 62303;
	// blt cr6,0x825e1a30
	if (cr6.lt) goto loc_825E1A30;
	// addi r10,r6,-4
	ctx.r10.s64 = ctx.r6.s64 + -4;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// rlwinm r9,r10,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r22,8
	ctx.r10.s64 = r22.s64 + 8;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r29,r9,2,0,29
	r29.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825E18C0:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,264(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r26
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r26.u32);
	f0.f64 = double(temp.f32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,-8(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,264(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r26
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r26.u32);
	f0.f64 = double(temp.f32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,-4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,264(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r26
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r26.u32);
	f0.f64 = double(temp.f32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,264(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r26
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r26.u32);
	f0.f64 = double(temp.f32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825e18c0
	if (!cr6.eq) goto loc_825E18C0;
loc_825E1A30:
	// cmpw cr6,r29,r6
	cr6.compare<int32_t>(r29.s32, ctx.r6.s32, xer);
	// bge cr6,0x825e1ab4
	if (!cr6.lt) goto loc_825E1AB4;
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r29,r6
	ctx.r10.s64 = ctx.r6.s64 - r29.s64;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
loc_825E1A4C:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,264(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 264);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f0,r8,r26
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r26.u32);
	f0.f64 = double(temp.f32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825e1a4c
	if (!cr6.eq) goto loc_825E1A4C;
loc_825E1AB4:
	// lwz r25,404(r28)
	r25.u64 = PPC_LOAD_U32(r28.u32 + 404);
	// subf r11,r29,r25
	r11.s64 = r25.s64 - r29.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825e1d4c
	if (cr6.lt) goto loc_825E1D4C;
	// addi r27,r25,-3
	r27.s64 = r25.s64 + -3;
	// addi r10,r29,3
	ctx.r10.s64 = r29.s64 + 3;
	// subf r6,r29,r27
	ctx.r6.s64 = r27.s64 - r29.s64;
	// addi r9,r29,1
	ctx.r9.s64 = r29.s64 + 1;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,30,2,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// add r4,r8,r26
	ctx.r4.u64 = ctx.r8.u64 + r26.u64;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r29,2
	ctx.r7.s64 = r29.s64 + 2;
	// addi r8,r19,8
	ctx.r8.s64 = r19.s64 + 8;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
	// subf r3,r22,r26
	ctx.r3.s64 = r26.s64 - r22.s64;
	// rlwinm r21,r6,2,0,29
	r21.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
loc_825E1B08:
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r20,0(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r20,r6
	ctx.r5.s64 = ctx.r6.s64 - r20.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r6,308(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r6,4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpw cr6,r29,r6
	cr6.compare<int32_t>(r29.s32, ctx.r6.s32, xer);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r6.u64);
	// lfd f0,112(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e1b64
	if (cr6.lt) goto loc_825E1B64;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_825E1B64:
	// lwz r6,-8(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + -8);
	// lfs f13,-12(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// addi r20,r7,-1
	r20.s64 = ctx.r7.s64 + -1;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r6.u64);
	// lfd f12,104(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,-4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r17,0(r11)
	r17.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r17,r6
	ctx.r5.s64 = ctx.r6.s64 - r17.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r6,308(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r6,4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpw cr6,r20,r6
	cr6.compare<int32_t>(r20.s32, ctx.r6.s32, xer);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r6.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e1bf0
	if (cr6.lt) goto loc_825E1BF0;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_825E1BF0:
	// lwz r6,-4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// lfsx f13,r3,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r6.u64);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r20,0(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r20,r6
	ctx.r5.s64 = ctx.r6.s64 - r20.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r6,308(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r6,4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpw cr6,r7,r6
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r6.s32, xer);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r6.u64);
	// lfd f0,120(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e1c78
	if (cr6.lt) goto loc_825E1C78;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_825E1C78:
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lfs f13,-4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// addi r20,r7,1
	r20.s64 = ctx.r7.s64 + 1;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r6.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r17,0(r11)
	r17.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r17,r6
	ctx.r5.s64 = ctx.r6.s64 - r17.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lwz r6,308(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r6,4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpw cr6,r20,r6
	cr6.compare<int32_t>(r20.s32, ctx.r6.s32, xer);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r6.u64);
	// lfd f0,136(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e1d04
	if (cr6.lt) goto loc_825E1D04;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_825E1D04:
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lfs f13,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// std r6,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r6.u64);
	// lfd f12,144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// blt cr6,0x825e1b08
	if (cr6.lt) goto loc_825E1B08;
loc_825E1D4C:
	// cmpw cr6,r29,r25
	cr6.compare<int32_t>(r29.s32, r25.s32, xer);
	// bge cr6,0x825e1e14
	if (!cr6.lt) goto loc_825E1E14;
	// rlwinm r9,r21,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r29,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r29,r25
	ctx.r10.s64 = r25.s64 - r29.s64;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// rlwinm r8,r24,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// add r6,r9,r19
	ctx.r6.u64 = ctx.r9.u64 + r19.u64;
	// subf r5,r22,r26
	ctx.r5.s64 = r26.s64 - r22.s64;
	// add r21,r10,r21
	r21.u64 = ctx.r10.u64 + r21.u64;
loc_825E1D78:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// srawi r10,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r9,r4,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r4.s64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r10,308(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// std r10,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r10.u64);
	// lfd f0,144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e1dd4
	if (cr6.lt) goto loc_825E1DD4;
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
loc_825E1DD4:
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lfsx f13,r5,r7
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r7.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmpw cr6,r29,r25
	cr6.compare<int32_t>(r29.s32, r25.s32, xer);
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// lfd f12,136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// blt cr6,0x825e1d78
	if (cr6.lt) goto loc_825E1D78;
loc_825E1E14:
	// lwz r10,268(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e2364
	if (!cr6.lt) goto loc_825E2364;
	// li r20,0
	r20.s64 = 0;
	// add r23,r23,r24
	r23.u64 = r23.u64 + r24.u64;
	// rlwinm r25,r24,2,0,29
	r25.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
loc_825E1E2C:
	// lwz r11,308(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 308);
	// add r9,r11,r25
	ctx.r9.u64 = r11.u64 + r25.u64;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmpw cr6,r29,r9
	cr6.compare<int32_t>(r29.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e1e48
	if (cr6.lt) goto loc_825E1E48;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
loc_825E1E48:
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// mr r27,r11
	r27.u64 = r11.u64;
	// blt cr6,0x825e1e60
	if (cr6.lt) goto loc_825E1E60;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
loc_825E1E60:
	// lbz r11,0(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x825e2098
	if (!cr6.eq) goto loc_825E2098;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwzx r3,r20,r11
	ctx.r3.u64 = PPC_LOAD_U32(r20.u32 + r11.u32);
	// bl 0x825cd8d8
	sub_825CD8D8(ctx, base);
	// lwz r11,16(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 16);
	// subf r10,r29,r27
	ctx.r10.s64 = r27.s64 - r29.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// lfsx f0,r11,r20
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + r20.u32);
	f0.f64 = double(temp.f32);
	// fmuls f0,f0,f1
	f0.f64 = double(float(f0.f64 * ctx.f1.f64));
	// fmuls f0,f0,f29
	f0.f64 = double(float(f0.f64 * f29.f64));
	// fmuls f0,f0,f28
	f0.f64 = double(float(f0.f64 * f28.f64));
	// blt cr6,0x825e2014
	if (cr6.lt) goto loc_825E2014;
	// subf r10,r29,r27
	ctx.r10.s64 = r27.s64 - r29.s64;
	// addi r7,r29,1
	ctx.r7.s64 = r29.s64 + 1;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r9,r29,3
	ctx.r9.s64 = r29.s64 + 3;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// add r9,r9,r26
	ctx.r9.u64 = ctx.r9.u64 + r26.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// subf r5,r22,r26
	ctx.r5.s64 = r26.s64 - r22.s64;
	// add r29,r7,r29
	r29.u64 = ctx.r7.u64 + r29.u64;
loc_825E1ED0:
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// srawi r6,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 2;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// srawi r7,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// extsw r7,r6
	ctx.r7.s64 = ctx.r6.s32;
	// lfs f13,-12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// std r7,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r7.u64);
	// lfd f12,144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// srawi r6,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 2;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// srawi r7,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// extsw r7,r6
	ctx.r7.s64 = ctx.r6.s32;
	// lfsx f13,r5,r10
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// std r7,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r7.u64);
	// lfd f12,136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// srawi r6,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 2;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// srawi r7,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// extsw r7,r6
	ctx.r7.s64 = ctx.r6.s32;
	// lfs f13,-4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// srawi r7,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// srawi r6,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 2;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// extsw r7,r6
	ctx.r7.s64 = ctx.r6.s32;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// std r7,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r7.u64);
	// lfd f12,120(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,8(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825e1ed0
	if (!cr6.eq) goto loc_825E1ED0;
loc_825E2014:
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// bge cr6,0x825e2090
	if (!cr6.lt) goto loc_825E2090;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r29,r27
	ctx.r9.s64 = r27.s64 - r29.s64;
	// addi r10,r28,540
	ctx.r10.s64 = r28.s64 + 540;
	// subf r6,r22,r26
	ctx.r6.s64 = r26.s64 - r22.s64;
	// add r11,r11,r22
	r11.u64 = r11.u64 + r22.u64;
	// add r29,r9,r29
	r29.u64 = ctx.r9.u64 + r29.u64;
loc_825E2034:
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// lfsx f13,r11,r6
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	ctx.f13.f64 = double(temp.f32);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f12,112(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne cr6,0x825e2034
	if (!cr6.eq) goto loc_825E2034;
loc_825E2090:
	// addi r20,r20,4
	r20.s64 = r20.s64 + 4;
	// b 0x825e2358
	goto loc_825E2358;
loc_825E2098:
	// cmpw cr6,r10,r27
	cr6.compare<int32_t>(ctx.r10.s32, r27.s32, xer);
	// bge cr6,0x825e20a4
	if (!cr6.lt) goto loc_825E20A4;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
loc_825E20A4:
	// subf r11,r29,r27
	r11.s64 = r27.s64 - r29.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x825e22b0
	if (cr6.lt) goto loc_825E22B0;
	// subf r10,r29,r27
	ctx.r10.s64 = r27.s64 - r29.s64;
	// addi r9,r29,3
	ctx.r9.s64 = r29.s64 + 3;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// addi r6,r21,2
	ctx.r6.s64 = r21.s64 + 2;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// addi r10,r29,1
	ctx.r10.s64 = r29.s64 + 1;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// add r8,r8,r26
	ctx.r8.u64 = ctx.r8.u64 + r26.u64;
	// add r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 + r19.u64;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// subf r4,r22,r26
	ctx.r4.s64 = r26.s64 - r22.s64;
	// add r21,r6,r21
	r21.u64 = ctx.r6.u64 + r21.u64;
	// add r29,r6,r29
	r29.u64 = ctx.r6.u64 + r29.u64;
loc_825E20F8:
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lfs f0,-12(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -12);
	f0.f64 = double(temp.f32);
	// lwz r6,-8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r6.u64);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r6.u64);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f13,f12,f30,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * f30.f64 + ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,-4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lfsx f0,r4,r10
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// lwz r6,-4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r6.u64);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r6.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f13,f12,f30,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * f30.f64 + ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lfs f0,-4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	f0.f64 = double(temp.f32);
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// std r6,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r6.u64);
	// std r5,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r5.u64);
	// lfd f13,160(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,168(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f13,f13,f30,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64 + ctx.f12.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,4(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mullw r6,r6,r30
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r30.s32);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// srawi r5,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 2;
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// srawi r6,r5,2
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r5.s32 >> 2;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r3,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r3.s64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// lfs f0,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	f0.f64 = double(temp.f32);
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// std r6,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r6.u64);
	// extsw r6,r5
	ctx.r6.s64 = ctx.r5.s32;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,184(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f13,f12,f30,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * f30.f64 + ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,8(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825e20f8
	if (!cr6.eq) goto loc_825E20F8;
loc_825E22B0:
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// bge cr6,0x825e2358
	if (!cr6.lt) goto loc_825E2358;
	// subf r11,r29,r27
	r11.s64 = r27.s64 - r29.s64;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r21,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r28,540
	ctx.r9.s64 = r28.s64 + 540;
	// add r10,r10,r22
	ctx.r10.u64 = ctx.r10.u64 + r22.u64;
	// add r8,r8,r19
	ctx.r8.u64 = ctx.r8.u64 + r19.u64;
	// subf r5,r22,r26
	ctx.r5.s64 = r26.s64 - r22.s64;
	// add r21,r11,r21
	r21.u64 = r11.u64 + r21.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
loc_825E22DC:
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mullw r7,r7,r30
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r30.s32);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// srawi r6,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 2;
	// stw r7,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r7.u32);
	// srawi r7,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 2;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lfsx f0,r10,r5
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
	f0.f64 = double(temp.f32);
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// std r7,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r7.u64);
	// extsw r7,r6
	ctx.r7.s64 = ctx.r6.s32;
	// std r7,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r7.u64);
	// lfd f13,192(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f12,200(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f13,f12,f30,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * f30.f64 + ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x825e22dc
	if (!cr6.eq) goto loc_825E22DC;
loc_825E2358:
	// lwz r10,268(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x825e1e2c
	if (cr6.lt) goto loc_825E1E2C;
loc_825E2364:
	// lhz r11,118(r18)
	r11.u64 = PPC_LOAD_U16(r18.u32 + 118);
	// lwz r10,268(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 268);
	// extsh r6,r11
	ctx.r6.s64 = r11.s16;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	f0.f64 = double(temp.f32);
	// subf r11,r29,r6
	r11.s64 = ctx.r6.s64 - r29.s64;
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// fmuls f0,f0,f30
	f0.f64 = double(float(f0.f64 * f30.f64));
	// blt cr6,0x825e24d8
	if (cr6.lt) goto loc_825E24D8;
	// subf r10,r29,r6
	ctx.r10.s64 = ctx.r6.s64 - r29.s64;
	// addi r9,r29,2
	ctx.r9.s64 = r29.s64 + 2;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// add r10,r8,r22
	ctx.r10.u64 = ctx.r8.u64 + r22.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r8,r29
	r29.u64 = ctx.r8.u64 + r29.u64;
loc_825E23B8:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r8.u64);
	// lfd f13,200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-8(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r8.u64);
	// lfd f13,192(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,-4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r5,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r8.u64);
	// lfd f13,184(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r8.u64);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne cr6,0x825e23b8
	if (!cr6.eq) goto loc_825E23B8;
loc_825E24D8:
	// cmpw cr6,r29,r6
	cr6.compare<int32_t>(r29.s32, ctx.r6.s32, xer);
	// bge cr6,0x825e2544
	if (!cr6.lt) goto loc_825E2544;
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r28,540
	r11.s64 = r28.s64 + 540;
	// subf r10,r29,r6
	ctx.r10.s64 = ctx.r6.s64 - r29.s64;
	// add r9,r9,r22
	ctx.r9.u64 = ctx.r9.u64 + r22.u64;
loc_825E24F0:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mullw r8,r8,r30
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r30.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// srawi r7,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// srawi r8,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r7,r6,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// extsw r8,r7
	ctx.r8.s64 = ctx.r7.s32;
	// std r8,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r8.u64);
	// lfd f13,200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825e24f0
	if (!cr6.eq) goto loc_825E24F0;
loc_825E2544:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// addi r12,r1,-128
	r12.s64 = ctx.r1.s64 + -128;
	// bl 0x8239d634
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_825E2558"))) PPC_WEAK_FUNC(sub_825E2558);
PPC_FUNC_IMPL(__imp__sub_825E2558) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// li r10,6
	ctx.r10.s64 = 6;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_825E2564:
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bdnz 0x825e2564
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_825E2564;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2574"))) PPC_WEAK_FUNC(sub_825E2574);
PPC_FUNC_IMPL(__imp__sub_825E2574) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2578"))) PPC_WEAK_FUNC(sub_825E2578);
PPC_FUNC_IMPL(__imp__sub_825E2578) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e25a8
	if (cr6.eq) goto loc_825E25A8;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_825E25A8:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e25bc
	if (cr6.eq) goto loc_825E25BC;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
loc_825E25BC:
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e25d0
	if (cr6.eq) goto loc_825E25D0;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
loc_825E25D0:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// li r11,6
	r11.s64 = 6;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_825E25DC:
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// bdnz 0x825e25dc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_825E25DC;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2600"))) PPC_WEAK_FUNC(sub_825E2600);
PPC_FUNC_IMPL(__imp__sub_825E2600) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// li r27,0
	r27.s64 = 0;
	// bl 0x825e2578
	sub_825E2578(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// mr r11,r30
	r11.u64 = r30.u64;
	// bgt cr6,0x825e263c
	if (cr6.gt) goto loc_825E263C;
	// neg r11,r30
	r11.s64 = -r30.s64;
loc_825E263C:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r10,r10,34464
	ctx.r10.u64 = ctx.r10.u64 | 34464;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e27f4
	if (cr6.gt) goto loc_825E27F4;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// bgt cr6,0x825e265c
	if (cr6.gt) goto loc_825E265C;
	// neg r11,r29
	r11.s64 = -r29.s64;
loc_825E265C:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x825e27f4
	if (cr6.gt) goto loc_825E27F4;
	// subf r11,r30,r29
	r11.s64 = r29.s64 - r30.s64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bgt cr6,0x825e27f4
	if (cr6.gt) goto loc_825E27F4;
	// subf. r10,r30,r11
	ctx.r10.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble 0x825e27f4
	if (!cr0.gt) goto loc_825E27F4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x825e27f4
	if (!cr6.gt) goto loc_825E27F4;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// bne cr6,0x825e26b0
	if (!cr6.eq) goto loc_825E26B0;
loc_825E269C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8239bd44
	return;
loc_825E26B0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x825e2708
	if (!cr6.gt) goto loc_825E2708;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_825E26D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lfd f0,0(r10)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f0,r9,r8
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, temp.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e26d4
	if (cr6.lt) goto loc_825E26D4;
loc_825E2708:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r3.u32);
	// beq cr6,0x825e269c
	if (cr6.eq) goto loc_825E269C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r11,0
	r11.s64 = 0;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f31,2480(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2480);
	f31.f64 = double(temp.f32);
	// ble 0x825e2774
	if (!cr0.gt) goto loc_825E2774;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825E2754:
	// lwz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stfsx f31,r9,r10
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, temp.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e2754
	if (cr6.lt) goto loc_825E2754;
loc_825E2774:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x825e27d8
	if (!cr6.lt) goto loc_825E27D8;
	// mulli r3,r11,-4
	ctx.r3.s64 = r11.s64 * -4;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// beq cr6,0x825e269c
	if (cr6.eq) goto loc_825E269C;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// mulli r5,r11,-4
	ctx.r5.s64 = r11.s64 * -4;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// neg. r10,r11
	ctx.r10.s64 = -r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// ble 0x825e27d8
	if (!cr0.gt) goto loc_825E27D8;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825E27B8:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stfsx f31,r10,r9
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, temp.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e27b8
	if (cr6.lt) goto loc_825E27B8;
loc_825E27D8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8239bd44
	return;
loc_825E27F4:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825E2808"))) PPC_WEAK_FUNC(sub_825E2808);
PPC_FUNC_IMPL(__imp__sub_825E2808) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x825e2844
	if (!cr6.lt) goto loc_825E2844;
	// li r30,0
	r30.s64 = 0;
loc_825E2844:
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x825e2914
	if (!cr6.gt) goto loc_825E2914;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f13,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f13.f64 = double(temp.f32);
loc_825E285C:
	// lwz r5,16(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// fmr f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f13.f64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpw cr6,r7,r5
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r5.s32, xer);
	// bge cr6,0x825e28a8
	if (!cr6.lt) goto loc_825E28A8;
	// rotlwi r11,r5,0
	r11.u64 = __builtin_rotateleft32(ctx.r5.u32, 0);
	// lwz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_825E2888:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fmadds f0,f12,f11,f0
	f0.f64 = double(float(ctx.f12.f64 * ctx.f11.f64 + f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e2888
	if (!cr6.eq) goto loc_825E2888;
loc_825E28A8:
	// subf r11,r5,r7
	r11.s64 = ctx.r7.s64 - ctx.r5.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x825e28b8
	if (!cr6.lt) goto loc_825E28B8;
	// li r11,0
	r11.s64 = 0;
loc_825E28B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// bge cr6,0x825e2900
	if (!cr6.lt) goto loc_825E2900;
	// rotlwi r5,r10,0
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r8,r5
	r11.s64 = ctx.r5.s64 - ctx.r8.s64;
loc_825E28E0:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fmadds f0,f12,f11,f0
	f0.f64 = double(float(ctx.f12.f64 * ctx.f11.f64 + f0.f64));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x825e28e0
	if (!cr6.eq) goto loc_825E28E0;
loc_825E2900:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stfsx f0,r4,r6
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + ctx.r6.u32, temp.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// cmpw cr6,r7,r30
	cr6.compare<int32_t>(ctx.r7.s32, r30.s32, xer);
	// blt cr6,0x825e285c
	if (cr6.lt) goto loc_825E285C;
loc_825E2914:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// subf r10,r30,r11
	ctx.r10.s64 = r11.s64 - r30.s64;
	// blt cr6,0x825e2940
	if (cr6.lt) goto loc_825E2940;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// b 0x825e296c
	goto loc_825E296C;
loc_825E2940:
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r3
	ctx.r4.u64 = r11.u64 + ctx.r3.u64;
	// bl 0x8239d800
	sub_8239D800(ctx, base);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
loc_825E296C:
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r30,r28
	r11.s64 = r28.s64 - r30.s64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// beq cr6,0x825e2990
	if (cr6.eq) goto loc_825E2990;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
loc_825E2990:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_825E2998"))) PPC_WEAK_FUNC(sub_825E2998);
PPC_FUNC_IMPL(__imp__sub_825E2998) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e29c8
	if (cr6.eq) goto loc_825E29C8;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
loc_825E29C8:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x825e29dc
	if (cr6.eq) goto loc_825E29DC;
	// bl 0x825c5ef0
	sub_825C5EF0(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
loc_825E29DC:
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E2A08"))) PPC_WEAK_FUNC(sub_825E2A08);
PPC_FUNC_IMPL(__imp__sub_825E2A08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,1
	r11.s64 = 65536;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// ori r11,r11,34464
	r11.u64 = r11.u64 | 34464;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x825e2ac0
	if (cr6.gt) goto loc_825E2AC0;
	// bl 0x825e2998
	sub_825E2998(ctx, base);
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// frsp f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f31.f64));
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stfs f0,0(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
	// bne cr6,0x825e2a70
	if (!cr6.eq) goto loc_825E2A70;
loc_825E2A5C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
loc_825E2A70:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825c5ee0
	sub_825C5EE0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r3.u32);
	// beq cr6,0x825e2a5c
	if (cr6.eq) goto loc_825E2A5C;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
loc_825E2AC0:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_825E2AD4"))) PPC_WEAK_FUNC(sub_825E2AD4);
PPC_FUNC_IMPL(__imp__sub_825E2AD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E2AD8"))) PPC_WEAK_FUNC(sub_825E2AD8);
PPC_FUNC_IMPL(__imp__sub_825E2AD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// add r30,r10,r5
	r30.u64 = ctx.r10.u64 + ctx.r5.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x825e2b0c
	if (!cr6.lt) goto loc_825E2B0C;
	// li r30,0
	r30.s64 = 0;
loc_825E2B0C:
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825e2b20
	if (cr6.lt) goto loc_825E2B20;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// add r28,r11,r5
	r28.u64 = r11.u64 + ctx.r5.u64;
	// b 0x825e2b24
	goto loc_825E2B24;
loc_825E2B20:
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_825E2B24:
	// subf r11,r28,r5
	r11.s64 = ctx.r5.s64 - r28.s64;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r27,r28,2,0,29
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// rlwinm r6,r30,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r11,r6,r29
	r11.u64 = ctx.r6.u64 + r29.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// ble 0x825e2b90
	if (!cr0.gt) goto loc_825E2B90;
loc_825E2B5C:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// subf r9,r9,r30
	ctx.r9.s64 = r30.s64 - ctx.r9.s64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x825e2b5c
	if (cr6.lt) goto loc_825E2B5C;
loc_825E2B90:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x825e2ba4
	if (!cr6.lt) goto loc_825E2BA4;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825E2BA4:
	// subf r8,r10,r30
	ctx.r8.s64 = r30.s64 - ctx.r10.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r7,0
	ctx.r7.s64 = 0;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// blt cr6,0x825e2c28
	if (cr6.lt) goto loc_825E2C28;
	// addi r9,r8,-4
	ctx.r9.s64 = ctx.r8.s64 + -4;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_825E2BD4:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f0,-4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	f0.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f0,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,-8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// lfs f0,-12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	f0.f64 = double(temp.f32);
	// addi r10,r10,-16
	ctx.r10.s64 = ctx.r10.s64 + -16;
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,-12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -12, temp.u32);
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// bne cr6,0x825e2bd4
	if (!cr6.eq) goto loc_825E2BD4;
loc_825E2C28:
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// bge cr6,0x825e2c58
	if (!cr6.lt) goto loc_825E2C58;
	// subf r9,r7,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r7.s64;
loc_825E2C34:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lfs f13,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x825e2c34
	if (!cr6.eq) goto loc_825E2C34;
loc_825E2C58:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x825e2c7c
	if (cr6.lt) goto loc_825E2C7C;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// stw r28,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r28.u32);
	// b 0x825e2cbc
	goto loc_825E2CBC;
loc_825E2C7C:
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// add r4,r3,r6
	ctx.r4.u64 = ctx.r3.u64 + ctx.r6.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239d800
	sub_8239D800(ctx, base);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r11,r30,r28
	r11.s64 = r28.s64 - r30.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
loc_825E2CBC:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x825e2ccc
	if (cr6.eq) goto loc_825E2CCC;
	// stw r30,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r30.u32);
loc_825E2CCC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

