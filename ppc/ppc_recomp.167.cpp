#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_826370A8"))) PPC_WEAK_FUNC(sub_826370A8);
PPC_FUNC_IMPL(__imp__sub_826370A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcd4
	// rlwinm r20,r10,1,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// rlwinm r19,r10,1,0,30
	r19.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,52(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// li r21,4
	r21.s64 = 4;
	// add r25,r20,r7
	r25.u64 = r20.u64 + ctx.r7.u64;
	// add r22,r11,r10
	r22.u64 = r11.u64 + ctx.r10.u64;
loc_826370D8:
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// subf r27,r25,r7
	r27.s64 = ctx.r7.s64 - r25.s64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r31,r24
	r31.u64 = r24.u64;
	// mr r30,r23
	r30.u64 = r23.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// addi r11,r25,2
	r11.s64 = r25.s64 + 2;
	// subf r26,r9,r8
	r26.s64 = ctx.r8.s64 - ctx.r9.s64;
	// li r7,4
	ctx.r7.s64 = 4;
loc_82637100:
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r29,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r29.u8);
	// lbz r29,-2(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// stb r29,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r29.u8);
	// lwz r29,48(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbzx r28,r27,r11
	r28.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// add r4,r29,r4
	ctx.r4.u64 = r29.u64 + ctx.r4.u64;
	// stb r28,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r28.u8);
	// lbz r29,0(r11)
	r29.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stb r29,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r29.u8);
	// lbzx r28,r26,r10
	r28.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// lwz r29,48(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// add r4,r29,r4
	ctx.r4.u64 = r29.u64 + ctx.r4.u64;
	// stb r28,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r28.u8);
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stb r28,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r28.u8);
	// lwz r28,56(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r31,r28,r31
	r31.u64 = r28.u64 + r31.u64;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
	// bne cr6,0x82637100
	if (!cr6.eq) goto loc_82637100;
	// lwz r10,52(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// add r7,r20,r25
	ctx.r7.u64 = r20.u64 + r25.u64;
	// lwz r6,60(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// add r11,r10,r22
	r11.u64 = ctx.r10.u64 + r22.u64;
	// add r8,r19,r8
	ctx.r8.u64 = r19.u64 + ctx.r8.u64;
	// add r9,r19,r9
	ctx.r9.u64 = r19.u64 + ctx.r9.u64;
	// add r25,r20,r7
	r25.u64 = r20.u64 + ctx.r7.u64;
	// add r24,r6,r24
	r24.u64 = ctx.r6.u64 + r24.u64;
	// add r23,r6,r23
	r23.u64 = ctx.r6.u64 + r23.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// add r22,r10,r11
	r22.u64 = ctx.r10.u64 + r11.u64;
	// bne cr6,0x826370d8
	if (!cr6.eq) goto loc_826370D8;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_826371A4"))) PPC_WEAK_FUNC(sub_826371A4);
PPC_FUNC_IMPL(__imp__sub_826371A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826371A8"))) PPC_WEAK_FUNC(sub_826371A8);
PPC_FUNC_IMPL(__imp__sub_826371A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bccc
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// lwz r18,84(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// add r23,r7,r10
	r23.u64 = ctx.r7.u64 + ctx.r10.u64;
	// li r19,8
	r19.s64 = 8;
	// add r20,r4,r11
	r20.u64 = ctx.r4.u64 + r11.u64;
loc_826371D0:
	// addi r6,r7,3
	ctx.r6.s64 = ctx.r7.s64 + 3;
	// subf r25,r23,r7
	r25.s64 = ctx.r7.s64 - r23.s64;
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// mr r28,r21
	r28.u64 = r21.u64;
	// addi r4,r8,2
	ctx.r4.s64 = ctx.r8.s64 + 2;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// addi r11,r23,1
	r11.s64 = r23.s64 + 1;
	// subf r24,r9,r8
	r24.s64 = ctx.r8.s64 - ctx.r9.s64;
	// li r7,2
	ctx.r7.s64 = 2;
loc_826371F8:
	// lbz r27,-3(r6)
	r27.u64 = PPC_LOAD_U8(ctx.r6.u32 + -3);
	// stb r27,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r27.u8);
	// lbz r27,-1(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbzx r26,r25,r11
	r26.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r26.u8);
	// lbz r27,0(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lbzx r26,r5,r24
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + r24.u32);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
	// lbz r26,0(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// stb r26,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r26.u8);
	// lbz r17,-1(r6)
	r17.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
	// lwz r26,56(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r29,r26,r29
	r29.u64 = r26.u64 + r29.u64;
	// add r28,r26,r28
	r28.u64 = r26.u64 + r28.u64;
	// stb r17,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r17.u8);
	// lbz r27,1(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbz r26,0(r6)
	r26.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r26.u8);
	// lbz r27,2(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lbz r26,-1(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + -1);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
	// lbz r26,1(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// stb r26,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r26.u8);
	// lbz r17,1(r6)
	r17.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lwz r26,56(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r29,r26,r29
	r29.u64 = r26.u64 + r29.u64;
	// add r28,r26,r28
	r28.u64 = r26.u64 + r28.u64;
	// stb r17,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r17.u8);
	// lbz r27,3(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbz r26,2(r6)
	r26.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r26.u8);
	// lbz r27,4(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lbz r26,0(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
	// lbz r26,2(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// stb r26,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r26.u8);
	// lbz r17,3(r6)
	r17.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lwz r26,56(r3)
	r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r29,r26,r29
	r29.u64 = r26.u64 + r29.u64;
	// add r28,r26,r28
	r28.u64 = r26.u64 + r28.u64;
	// stb r17,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r17.u8);
	// lbz r27,5(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbz r26,4(r6)
	r26.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r26.u8);
	// lbz r27,6(r11)
	r27.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// stb r27,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r27.u8);
	// lbz r26,1(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// lwz r27,48(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r31,r27,r31
	r31.u64 = r27.u64 + r31.u64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
	// lbz r26,3(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 3);
	// stb r26,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r26.u8);
	// lwz r27,56(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// add r29,r27,r29
	r29.u64 = r27.u64 + r29.u64;
	// add r28,r27,r28
	r28.u64 = r27.u64 + r28.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x826371f8
	if (!cr6.eq) goto loc_826371F8;
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// add r7,r23,r10
	ctx.r7.u64 = r23.u64 + ctx.r10.u64;
	// lwz r6,60(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// add r31,r11,r20
	r31.u64 = r11.u64 + r20.u64;
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + r18.u64;
	// add r9,r9,r18
	ctx.r9.u64 = ctx.r9.u64 + r18.u64;
	// add r23,r7,r10
	r23.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r22,r6,r22
	r22.u64 = ctx.r6.u64 + r22.u64;
	// add r21,r6,r21
	r21.u64 = ctx.r6.u64 + r21.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// add r20,r11,r31
	r20.u64 = r11.u64 + r31.u64;
	// bne cr6,0x826371d0
	if (!cr6.eq) goto loc_826371D0;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_8263739C"))) PPC_WEAK_FUNC(sub_8263739C);
PPC_FUNC_IMPL(__imp__sub_8263739C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826373A0"))) PPC_WEAK_FUNC(sub_826373A0);
PPC_FUNC_IMPL(__imp__sub_826373A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lwz r29,52(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// stw r8,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, ctx.r8.u32);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// lwz r8,60(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// rlwinm r5,r29,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r28,48(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// lwz r10,56(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// stw r6,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r6.u32);
	// li r6,8
	ctx.r6.s64 = 8;
	// add r14,r7,r4
	r14.u64 = ctx.r7.u64 + ctx.r4.u64;
	// rlwinm r23,r10,1,0,30
	r23.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,76(r1)
	PPC_STORE_U32(ctx.r1.u32 + 76, ctx.r4.u32);
	// rlwinm r27,r28,1,0,30
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r5,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r5.u32);
	// rlwinm r5,r8,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r6.u32);
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// subf r17,r8,r10
	r17.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r16,r8,r6
	r16.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r26,r29,r28
	r26.u64 = r29.u64 + r28.u64;
	// stw r5,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r5.u32);
	// rlwinm r5,r29,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r15,r10,r6
	r15.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r5,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r5.u32);
loc_82637418:
	// lwz r6,-168(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// subf r21,r14,r7
	r21.s64 = ctx.r7.s64 - r14.s64;
	// lwz r7,60(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// add r4,r6,r22
	ctx.r4.u64 = ctx.r6.u64 + r22.u64;
	// lwz r6,-176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// subf r20,r9,r7
	r20.s64 = ctx.r7.s64 - ctx.r9.s64;
	// mr r30,r22
	r30.u64 = r22.u64;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// add r3,r10,r22
	ctx.r3.u64 = ctx.r10.u64 + r22.u64;
	// addi r5,r14,1
	ctx.r5.s64 = r14.s64 + 1;
	// li r25,8
	r25.s64 = 8;
	// add r7,r4,r8
	ctx.r7.u64 = ctx.r4.u64 + ctx.r8.u64;
	// subf r19,r4,r22
	r19.s64 = r22.s64 - ctx.r4.s64;
	// subf r18,r22,r4
	r18.s64 = ctx.r4.s64 - r22.s64;
loc_82637454:
	// lbz r4,0(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// addi r24,r24,2
	r24.s64 = r24.s64 + 2;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// stbx r4,r11,r26
	PPC_STORE_U8(r11.u32 + r26.u32, ctx.r4.u8);
	// stbx r4,r11,r29
	PPC_STORE_U8(r11.u32 + r29.u32, ctx.r4.u8);
	// stbx r4,r11,r28
	PPC_STORE_U8(r11.u32 + r28.u32, ctx.r4.u8);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// lbz r4,-1(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + -1);
	// stbx r4,r6,r26
	PPC_STORE_U8(ctx.r6.u32 + r26.u32, ctx.r4.u8);
	// stbx r4,r6,r29
	PPC_STORE_U8(ctx.r6.u32 + r29.u32, ctx.r4.u8);
	// stbx r4,r6,r28
	PPC_STORE_U8(ctx.r6.u32 + r28.u32, ctx.r4.u8);
	// stb r4,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r4.u8);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lbzx r4,r21,r5
	ctx.r4.u64 = PPC_LOAD_U8(r21.u32 + ctx.r5.u32);
	// stbx r4,r11,r26
	PPC_STORE_U8(r11.u32 + r26.u32, ctx.r4.u8);
	// stbx r4,r11,r29
	PPC_STORE_U8(r11.u32 + r29.u32, ctx.r4.u8);
	// stbx r4,r11,r28
	PPC_STORE_U8(r11.u32 + r28.u32, ctx.r4.u8);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// lbz r4,0(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// stbx r4,r6,r26
	PPC_STORE_U8(ctx.r6.u32 + r26.u32, ctx.r4.u8);
	// stbx r4,r6,r29
	PPC_STORE_U8(ctx.r6.u32 + r29.u32, ctx.r4.u8);
	// stbx r4,r6,r28
	PPC_STORE_U8(ctx.r6.u32 + r28.u32, ctx.r4.u8);
	// stb r4,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r4.u8);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// lbzx r4,r20,r31
	ctx.r4.u64 = PPC_LOAD_U8(r20.u32 + r31.u32);
	// stbx r4,r15,r3
	PPC_STORE_U8(r15.u32 + ctx.r3.u32, ctx.r4.u8);
	// stbx r4,r19,r7
	PPC_STORE_U8(r19.u32 + ctx.r7.u32, ctx.r4.u8);
	// stb r4,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r4.u8);
	// add r3,r3,r23
	ctx.r3.u64 = ctx.r3.u64 + r23.u64;
	// stb r4,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r4.u8);
	// lbz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stbx r4,r16,r7
	PPC_STORE_U8(r16.u32 + ctx.r7.u32, ctx.r4.u8);
	// stb r4,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r4.u8);
	// stbx r4,r17,r7
	PPC_STORE_U8(r17.u32 + ctx.r7.u32, ctx.r4.u8);
	// add r7,r7,r23
	ctx.r7.u64 = ctx.r7.u64 + r23.u64;
	// stbx r4,r18,r30
	PPC_STORE_U8(r18.u32 + r30.u32, ctx.r4.u8);
	// add r30,r30,r23
	r30.u64 = r30.u64 + r23.u64;
	// bne cr6,0x82637454
	if (!cr6.eq) goto loc_82637454;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,60(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 60);
	// lwz r3,-164(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// lwz r5,-160(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// lwz r6,-172(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// add r22,r5,r22
	r22.u64 = ctx.r5.u64 + r22.u64;
	// lwz r4,76(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 76);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r5,r3,r11
	ctx.r5.u64 = ctx.r3.u64 + r11.u64;
	// stw r31,60(r1)
	PPC_STORE_U32(ctx.r1.u32 + 60, r31.u32);
	// add r7,r14,r4
	ctx.r7.u64 = r14.u64 + ctx.r4.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// add r14,r7,r4
	r14.u64 = ctx.r7.u64 + ctx.r4.u64;
	// stw r6,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r6.u32);
	// stw r5,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r5.u32);
	// bne cr6,0x82637418
	if (!cr6.eq) goto loc_82637418;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82637554"))) PPC_WEAK_FUNC(sub_82637554);
PPC_FUNC_IMPL(__imp__sub_82637554) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82637558"))) PPC_WEAK_FUNC(sub_82637558);
PPC_FUNC_IMPL(__imp__sub_82637558) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263759c
	if (cr6.eq) goto loc_8263759C;
	// lwz r31,324(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r11,15884(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15884);
	// stw r31,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r31.u32);
	// lwz r31,316(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// lwz r31,308(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_8263759C:
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// rlwinm r20,r10,1,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,52(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// add r22,r10,r4
	r22.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r25,r20,r7
	r25.u64 = r20.u64 + ctx.r7.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826376ac
	if (!cr6.gt) goto loc_826376AC;
	// lwz r10,332(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r21,r11
	r21.u64 = r11.u64;
	// lwz r6,308(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// srawi r18,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r18.s64 = ctx.r10.s32 >> 1;
	// rlwinm r19,r6,1,0,30
	r19.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
loc_826375D8:
	// mr r31,r22
	r31.u64 = r22.u64;
	// mr r29,r24
	r29.u64 = r24.u64;
	// mr r28,r23
	r28.u64 = r23.u64;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// ble cr6,0x82637678
	if (!cr6.gt) goto loc_82637678;
	// addi r11,r18,-1
	r11.s64 = r18.s64 + -1;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// rlwinm r5,r11,31,1,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// subf r27,r25,r7
	r27.s64 = ctx.r7.s64 - r25.s64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// addi r11,r25,2
	r11.s64 = r25.s64 + 2;
	// subf r26,r9,r8
	r26.s64 = ctx.r8.s64 - ctx.r9.s64;
	// addi r7,r5,1
	ctx.r7.s64 = ctx.r5.s64 + 1;
loc_8263760C:
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r5,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r5.u8);
	// lbz r5,-2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// stb r5,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r5.u8);
	// lwz r5,48(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbzx r30,r27,r11
	r30.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// add r4,r5,r4
	ctx.r4.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stb r31,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r31.u8);
	// lbzx r30,r26,r10
	r30.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// lwz r31,48(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r4,r31,r4
	ctx.r4.u64 = r31.u64 + ctx.r4.u64;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// stb r30,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r30.u8);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stb r30,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r30.u8);
	// lwz r30,56(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r29,r30,r29
	r29.u64 = r30.u64 + r29.u64;
	// add r28,r30,r28
	r28.u64 = r30.u64 + r28.u64;
	// bne cr6,0x8263760c
	if (!cr6.eq) goto loc_8263760C;
loc_82637678:
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// add r7,r20,r25
	ctx.r7.u64 = r20.u64 + r25.u64;
	// lwz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// add r4,r11,r22
	ctx.r4.u64 = r11.u64 + r22.u64;
	// add r8,r19,r8
	ctx.r8.u64 = r19.u64 + ctx.r8.u64;
	// add r9,r19,r9
	ctx.r9.u64 = r19.u64 + ctx.r9.u64;
	// add r25,r20,r7
	r25.u64 = r20.u64 + ctx.r7.u64;
	// add r24,r10,r24
	r24.u64 = ctx.r10.u64 + r24.u64;
	// add r23,r10,r23
	r23.u64 = ctx.r10.u64 + r23.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// add r22,r11,r4
	r22.u64 = r11.u64 + ctx.r4.u64;
	// bne cr6,0x826375d8
	if (!cr6.eq) goto loc_826375D8;
loc_826376AC:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_826376B4"))) PPC_WEAK_FUNC(sub_826376B4);
PPC_FUNC_IMPL(__imp__sub_826376B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826376B8"))) PPC_WEAK_FUNC(sub_826376B8);
PPC_FUNC_IMPL(__imp__sub_826376B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826376fc
	if (cr6.eq) goto loc_826376FC;
	// lwz r31,324(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r11,15884(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15884);
	// stw r31,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r31.u32);
	// lwz r31,316(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// lwz r31,308(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_826376FC:
	// lwz r11,340(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// lwz r6,52(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// add r24,r7,r10
	r24.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r21,r6,r4
	r21.u64 = ctx.r6.u64 + ctx.r4.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82637800
	if (!cr6.gt) goto loc_82637800;
	// lwz r6,332(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r20,r11
	r20.u64 = r11.u64;
	// lwz r19,308(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// srawi r18,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r18.s64 = ctx.r6.s32 >> 1;
loc_82637734:
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r28,r23
	r28.u64 = r23.u64;
	// mr r27,r22
	r27.u64 = r22.u64;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// ble cr6,0x826377cc
	if (!cr6.gt) goto loc_826377CC;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// subf r26,r24,r7
	r26.s64 = ctx.r7.s64 - r24.s64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// addi r11,r24,1
	r11.s64 = r24.s64 + 1;
	// subf r25,r9,r8
	r25.s64 = ctx.r8.s64 - ctx.r9.s64;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
loc_82637760:
	// lbz r4,0(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r4,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r4.u8);
	// lbz r4,-1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// stb r4,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r4.u8);
	// lwz r4,48(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lbzx r29,r26,r11
	r29.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// stb r29,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r29.u8);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stb r30,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r30.u8);
	// lbzx r29,r25,r6
	r29.u64 = PPC_LOAD_U8(r25.u32 + ctx.r6.u32);
	// lwz r30,48(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// add r30,r30,r4
	r30.u64 = r30.u64 + ctx.r4.u64;
	// stb r29,0(r28)
	PPC_STORE_U8(r28.u32 + 0, r29.u8);
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r29,0(r27)
	PPC_STORE_U8(r27.u32 + 0, r29.u8);
	// lwz r29,56(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r28,r29,r28
	r28.u64 = r29.u64 + r28.u64;
	// add r27,r29,r27
	r27.u64 = r29.u64 + r27.u64;
	// bne cr6,0x82637760
	if (!cr6.eq) goto loc_82637760;
loc_826377CC:
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// add r7,r24,r10
	ctx.r7.u64 = r24.u64 + ctx.r10.u64;
	// lwz r6,60(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
	// add r31,r11,r21
	r31.u64 = r11.u64 + r21.u64;
	// add r8,r8,r19
	ctx.r8.u64 = ctx.r8.u64 + r19.u64;
	// add r9,r9,r19
	ctx.r9.u64 = ctx.r9.u64 + r19.u64;
	// add r24,r7,r10
	r24.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r23,r6,r23
	r23.u64 = ctx.r6.u64 + r23.u64;
	// add r22,r6,r22
	r22.u64 = ctx.r6.u64 + r22.u64;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// add r21,r11,r31
	r21.u64 = r11.u64 + r31.u64;
	// bne cr6,0x82637734
	if (!cr6.eq) goto loc_82637734;
loc_82637800:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_82637808"))) PPC_WEAK_FUNC(sub_82637808);
PPC_FUNC_IMPL(__imp__sub_82637808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r18,r8
	r18.u64 = ctx.r8.u64;
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r18,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r18.u32);
	// stw r26,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r26.u32);
	// beq cr6,0x8263786c
	if (cr6.eq) goto loc_8263786C;
	// lwz r9,380(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r10,388(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// lwz r11,15884(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15884);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// lwz r8,372(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
loc_8263786C:
	// lwz r10,404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// lwz r30,52(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// srawi r31,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r31.s64 = ctx.r10.s32 >> 1;
	// lwz r29,48(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r14,56(r3)
	r14.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// add r9,r7,r26
	ctx.r9.u64 = ctx.r7.u64 + r26.u64;
	// lwz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// rlwinm r3,r30,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r29,1,0,30
	r25.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r3,r4
	ctx.r4.u64 = ctx.r3.u64 + ctx.r4.u64;
	// rlwinm r23,r14,1,0,30
	r23.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// add r24,r30,r29
	r24.u64 = r30.u64 + r29.u64;
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// ble cr6,0x82637a20
	if (!cr6.gt) goto loc_82637A20;
	// lwz r4,396(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// srawi r27,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	r27.s64 = ctx.r4.s32 >> 1;
	// stw r6,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r6.u32);
	// stw r27,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r27.u32);
	// b 0x826378d0
	goto loc_826378D0;
loc_826378CC:
	// lwz r18,348(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
loc_826378D0:
	// lwz r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r28,r15
	r28.u64 = r15.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x826379d8
	if (!cr6.gt) goto loc_826379D8;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// add r3,r14,r15
	ctx.r3.u64 = r14.u64 + r15.u64;
	// add r22,r11,r15
	r22.u64 = r11.u64 + r15.u64;
	// add r11,r10,r14
	r11.u64 = ctx.r10.u64 + r14.u64;
	// subf r16,r22,r15
	r16.s64 = r15.s64 - r22.s64;
	// subf r20,r10,r11
	r20.s64 = r11.s64 - ctx.r10.s64;
	// subf r17,r14,r11
	r17.s64 = r11.s64 - r14.s64;
	// add r11,r22,r10
	r11.u64 = r22.u64 + ctx.r10.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// addi r4,r9,1
	ctx.r4.s64 = ctx.r9.s64 + 1;
	// subf r21,r9,r7
	r21.s64 = ctx.r7.s64 - ctx.r9.s64;
	// subf r19,r10,r14
	r19.s64 = r14.s64 - ctx.r10.s64;
	// subf r18,r8,r18
	r18.s64 = r18.s64 - ctx.r8.s64;
	// subf r22,r15,r22
	r22.s64 = r22.s64 - r15.s64;
loc_82637920:
	// lbz r7,0(r26)
	ctx.r7.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// addi r26,r26,2
	r26.s64 = r26.s64 + 2;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stbx r7,r5,r24
	PPC_STORE_U8(ctx.r5.u32 + r24.u32, ctx.r7.u8);
	// stbx r7,r5,r30
	PPC_STORE_U8(ctx.r5.u32 + r30.u32, ctx.r7.u8);
	// stbx r7,r5,r29
	PPC_STORE_U8(ctx.r5.u32 + r29.u32, ctx.r7.u8);
	// stb r7,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r7.u8);
	// add r7,r5,r25
	ctx.r7.u64 = ctx.r5.u64 + r25.u64;
	// lbz r5,-1(r4)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r4.u32 + -1);
	// stbx r5,r6,r24
	PPC_STORE_U8(ctx.r6.u32 + r24.u32, ctx.r5.u8);
	// stbx r5,r6,r30
	PPC_STORE_U8(ctx.r6.u32 + r30.u32, ctx.r5.u8);
	// stbx r5,r6,r29
	PPC_STORE_U8(ctx.r6.u32 + r29.u32, ctx.r5.u8);
	// stb r5,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r5.u8);
	// add r6,r6,r25
	ctx.r6.u64 = ctx.r6.u64 + r25.u64;
	// lbzx r5,r21,r4
	ctx.r5.u64 = PPC_LOAD_U8(r21.u32 + ctx.r4.u32);
	// stbx r5,r7,r24
	PPC_STORE_U8(ctx.r7.u32 + r24.u32, ctx.r5.u8);
	// stbx r5,r7,r30
	PPC_STORE_U8(ctx.r7.u32 + r30.u32, ctx.r5.u8);
	// stbx r5,r7,r29
	PPC_STORE_U8(ctx.r7.u32 + r29.u32, ctx.r5.u8);
	// stb r5,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r5.u8);
	// add r5,r7,r25
	ctx.r5.u64 = ctx.r7.u64 + r25.u64;
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// stbx r7,r6,r24
	PPC_STORE_U8(ctx.r6.u32 + r24.u32, ctx.r7.u8);
	// stbx r7,r6,r30
	PPC_STORE_U8(ctx.r6.u32 + r30.u32, ctx.r7.u8);
	// stbx r7,r6,r29
	PPC_STORE_U8(ctx.r6.u32 + r29.u32, ctx.r7.u8);
	// stb r7,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r7.u8);
	// add r6,r6,r25
	ctx.r6.u64 = ctx.r6.u64 + r25.u64;
	// lbzx r7,r18,r31
	ctx.r7.u64 = PPC_LOAD_U8(r18.u32 + r31.u32);
	// stbx r7,r17,r3
	PPC_STORE_U8(r17.u32 + ctx.r3.u32, ctx.r7.u8);
	// stbx r7,r16,r11
	PPC_STORE_U8(r16.u32 + r11.u32, ctx.r7.u8);
	// stb r7,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r7.u8);
	// add r3,r3,r23
	ctx.r3.u64 = ctx.r3.u64 + r23.u64;
	// stb r7,0(r28)
	PPC_STORE_U8(r28.u32 + 0, ctx.r7.u8);
	// lbz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stbx r7,r20,r11
	PPC_STORE_U8(r20.u32 + r11.u32, ctx.r7.u8);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// stbx r7,r19,r11
	PPC_STORE_U8(r19.u32 + r11.u32, ctx.r7.u8);
	// add r11,r11,r23
	r11.u64 = r11.u64 + r23.u64;
	// stbx r7,r22,r28
	PPC_STORE_U8(r22.u32 + r28.u32, ctx.r7.u8);
	// add r28,r28,r23
	r28.u64 = r28.u64 + r23.u64;
	// bne cr6,0x82637920
	if (!cr6.eq) goto loc_82637920;
	// lwz r26,364(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r18,348(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// lwz r27,124(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
loc_826379D8:
	// add r7,r9,r26
	ctx.r7.u64 = ctx.r9.u64 + r26.u64;
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// rlwinm r6,r30,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r9,-1
	ctx.r5.s64 = ctx.r9.s64 + -1;
	// lwz r9,372(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// add r11,r6,r11
	r11.u64 = ctx.r6.u64 + r11.u64;
	// add r4,r18,r9
	ctx.r4.u64 = r18.u64 + ctx.r9.u64;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r15,r9,r15
	r15.u64 = ctx.r9.u64 + r15.u64;
	// stw r4,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, ctx.r4.u32);
	// add r9,r7,r26
	ctx.r9.u64 = ctx.r7.u64 + r26.u64;
	// stw r6,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r6.u32);
	// bne cr6,0x826378cc
	if (!cr6.eq) goto loc_826378CC;
loc_82637A20:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82637A28"))) PPC_WEAK_FUNC(sub_82637A28);
PPC_FUNC_IMPL(__imp__sub_82637A28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// lwz r10,340(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// rlwinm r24,r11,0,0,30
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// srawi r22,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r22.s64 = r24.s32 >> 1;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// srawi r25,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r25.s64 = ctx.r10.s32 >> 1;
	// beq cr6,0x82637aa8
	if (cr6.eq) goto loc_82637AA8;
	// lwz r10,324(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r9,316(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r8,308(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r11,15884(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15884);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
loc_82637AA8:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82637b28
	if (!cr6.gt) goto loc_82637B28;
	// lwz r21,324(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r19,308(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
loc_82637ABC:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r31,r31,r23
	r31.u64 = r31.u64 + r23.u64;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// add r31,r31,r23
	r31.u64 = r31.u64 + r23.u64;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r29,r29,r19
	r29.u64 = r29.u64 + r19.u64;
	// add r28,r28,r21
	r28.u64 = r28.u64 + r21.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// add r27,r27,r19
	r27.u64 = r27.u64 + r19.u64;
	// add r26,r26,r21
	r26.u64 = r26.u64 + r21.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x82637abc
	if (!cr6.eq) goto loc_82637ABC;
loc_82637B28:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_82637B30"))) PPC_WEAK_FUNC(sub_82637B30);
PPC_FUNC_IMPL(__imp__sub_82637B30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r28,284(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r21,r29,r31
	r21.u64 = r29.u64 + r31.u64;
	// add r20,r30,r28
	r20.u64 = r30.u64 + r28.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637B9C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637b9c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637B9C;
	// lwz r30,276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r11,r24
	r11.u64 = r24.u64;
	// lwz r29,292(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r25,r30
	r23.u64 = r25.u64 + r30.u64;
	// add r22,r27,r29
	r22.u64 = r27.u64 + r29.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637BD0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637bd0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637BD0;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// add r25,r24,r30
	r25.u64 = r24.u64 + r30.u64;
	// add r24,r26,r29
	r24.u64 = r26.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r21,r31
	r27.u64 = r21.u64 + r31.u64;
	// add r26,r20,r28
	r26.u64 = r20.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637C2C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637c2c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637C2C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637C58:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637c58
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637C58;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637CB4:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637cb4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637CB4;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637CE0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637ce0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637CE0;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637D3C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637d3c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637D3C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637D68:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637d68
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637D68;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637DC4:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637dc4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637DC4;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637DF0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637df0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637DF0;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637E4C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637e4c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637E4C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637E78:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637e78
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637E78;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r21,r24,r29
	r21.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637ED4:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637ed4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637ED4;
	// add r24,r23,r30
	r24.u64 = r23.u64 + r30.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82637EFC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82637efc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637EFC;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r30,r25,r30
	r30.u64 = r25.u64 + r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r27,r31
	ctx.r4.u64 = r27.u64 + r31.u64;
	// add r3,r26,r28
	ctx.r3.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r11,r22,r29
	r11.u64 = r22.u64 + r29.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82637F44:
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82637f44
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637F44;
	// add r11,r21,r29
	r11.u64 = r21.u64 + r29.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82637F64:
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82637f64
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637F64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_82637F80"))) PPC_WEAK_FUNC(sub_82637F80);
PPC_FUNC_IMPL(__imp__sub_82637F80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r28,284(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// add r29,r29,r31
	r29.u64 = r29.u64 + r31.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r21,r29,r31
	r21.u64 = r29.u64 + r31.u64;
	// add r20,r30,r28
	r20.u64 = r30.u64 + r28.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82637FEC:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82637fec
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82637FEC;
	// lwz r30,276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r11,r24
	r11.u64 = r24.u64;
	// lwz r29,292(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r25,r30
	r23.u64 = r25.u64 + r30.u64;
	// add r22,r27,r29
	r22.u64 = r27.u64 + r29.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82638020:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82638020
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638020;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// add r25,r24,r30
	r25.u64 = r24.u64 + r30.u64;
	// add r24,r26,r29
	r24.u64 = r26.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r21,r31
	r27.u64 = r21.u64 + r31.u64;
	// add r26,r20,r28
	r26.u64 = r20.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_8263807C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x8263807c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263807C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826380A8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x826380a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826380A8;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638104:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638104
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638104;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82638130:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82638130
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638130;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_8263818C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x8263818c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263818C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826381B8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x826381b8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826381B8;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638214:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638214
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638214;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82638240:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82638240
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638240;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r24,r24,r29
	r24.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_8263829C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x8263829c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263829C;
	// add r23,r23,r30
	r23.u64 = r23.u64 + r30.u64;
	// add r22,r22,r29
	r22.u64 = r22.u64 + r29.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826382C8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x826382c8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826382C8;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r25,r25,r30
	r25.u64 = r25.u64 + r30.u64;
	// add r21,r24,r29
	r21.u64 = r24.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r27,r27,r31
	r27.u64 = r27.u64 + r31.u64;
	// add r26,r26,r28
	r26.u64 = r26.u64 + r28.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638324:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638324
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638324;
	// add r24,r23,r30
	r24.u64 = r23.u64 + r30.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8263834C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x8263834c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263834C;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// add r30,r25,r30
	r30.u64 = r25.u64 + r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r4,r27,r31
	ctx.r4.u64 = r27.u64 + r31.u64;
	// add r3,r26,r28
	ctx.r3.u64 = r26.u64 + r28.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r11,r22,r29
	r11.u64 = r22.u64 + r29.u64;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82638394:
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82638394
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638394;
	// add r11,r21,r29
	r11.u64 = r21.u64 + r29.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_826383B4:
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826383b4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826383B4;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_826383D0"))) PPC_WEAK_FUNC(sub_826383D0);
PPC_FUNC_IMPL(__imp__sub_826383D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r30,r30,r11
	r30.u64 = r30.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r23,r30,r11
	r23.u64 = r30.u64 + r11.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638444:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638444
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638444;
	// lwz r30,260(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// lwz r10,21460(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// add r22,r26,r30
	r22.u64 = r26.u64 + r30.u64;
	// li r11,8
	r11.s64 = 8;
	// add r21,r27,r10
	r21.u64 = r27.u64 + ctx.r10.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638478:
	// lbz r11,0(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r11.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82638478
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638478;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// add r26,r24,r30
	r26.u64 = r24.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r23,r11
	r27.u64 = r23.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_826384E0:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x826384e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826384E0;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r22,r30
	r24.u64 = r22.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r21,r11
	r23.u64 = r21.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638510:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638510
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638510;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638578:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638578
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638578;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r24,r30
	r24.u64 = r24.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r23,r11
	r23.u64 = r23.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_826385A8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x826385a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826385A8;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638610:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638610
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638610;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r24,r30
	r24.u64 = r24.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r23,r11
	r23.u64 = r23.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638640:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638640
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638640;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_826386A8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x826386a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826386A8;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r24,r30
	r24.u64 = r24.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r23,r11
	r23.u64 = r23.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_826386D8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x826386d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826386D8;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638740:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638740
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638740;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r24,r30
	r24.u64 = r24.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r23,r11
	r23.u64 = r23.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638770:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638770
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638770;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r26,r26,r30
	r26.u64 = r26.u64 + r30.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// add r27,r27,r11
	r27.u64 = r27.u64 + r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_826387D8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x826387d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826387D8;
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// add r24,r24,r30
	r24.u64 = r24.u64 + r30.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// add r23,r23,r11
	r23.u64 = r23.u64 + r11.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638808:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82638808
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638808;
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r30,r26,r30
	r30.u64 = r26.u64 + r30.u64;
	// add r26,r11,r25
	r26.u64 = r11.u64 + r25.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r4,r28,r29
	ctx.r4.u64 = r28.u64 + r29.u64;
	// li r5,16
	ctx.r5.s64 = 16;
	// add r3,r27,r11
	ctx.r3.u64 = r27.u64 + r11.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_8263885C:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x8263885c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263885C;
	// li r11,8
	r11.s64 = 8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82638878:
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// stb r11,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r11.u8);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// bdnz 0x82638878
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82638878;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82638894"))) PPC_WEAK_FUNC(sub_82638894);
PPC_FUNC_IMPL(__imp__sub_82638894) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82638898"))) PPC_WEAK_FUNC(sub_82638898);
PPC_FUNC_IMPL(__imp__sub_82638898) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// rlwinm r23,r11,0,0,30
	r23.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// srawi r21,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r21.s64 = r23.s32 >> 1;
	// srawi r24,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r24.s64 = r11.s32 >> 1;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// ble cr6,0x82638960
	if (!cr6.gt) goto loc_82638960;
	// lwz r20,276(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
loc_826388E4:
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// add r30,r30,r22
	r30.u64 = r30.u64 + r22.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21456(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21456);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// add r30,r30,r22
	r30.u64 = r30.u64 + r22.u64;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21460);
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// add r28,r28,r20
	r28.u64 = r28.u64 + r20.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,21464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21464);
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// add r26,r26,r20
	r26.u64 = r26.u64 + r20.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x826388e4
	if (!cr6.eq) goto loc_826388E4;
loc_82638960:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_82638968"))) PPC_WEAK_FUNC(sub_82638968);
PPC_FUNC_IMPL(__imp__sub_82638968) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// srawi r11,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	r11.s64 = ctx.r7.s32 >> 4;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82638a9c
	if (!cr6.gt) goto loc_82638A9C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82638980:
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r8,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r8.u8);
	// lbz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// stb r8,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r8.u8);
	// lbz r8,1(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// stb r8,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r8.u8);
	// lbz r8,0(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// stb r8,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r8.u8);
	// lbz r8,2(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// stb r8,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r8.u8);
	// lbz r8,1(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// stb r8,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r8.u8);
	// lbz r8,3(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// stb r8,6(r3)
	PPC_STORE_U8(ctx.r3.u32 + 6, ctx.r8.u8);
	// lbz r8,1(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// stb r8,7(r3)
	PPC_STORE_U8(ctx.r3.u32 + 7, ctx.r8.u8);
	// lbz r8,4(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// stb r8,8(r3)
	PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r8.u8);
	// lbz r8,2(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 2);
	// stb r8,9(r3)
	PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r8.u8);
	// lbz r8,5(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// stb r8,10(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10, ctx.r8.u8);
	// lbz r8,2(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// stb r8,11(r3)
	PPC_STORE_U8(ctx.r3.u32 + 11, ctx.r8.u8);
	// lbz r8,6(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// stb r8,12(r3)
	PPC_STORE_U8(ctx.r3.u32 + 12, ctx.r8.u8);
	// lbz r8,3(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 3);
	// stb r8,13(r3)
	PPC_STORE_U8(ctx.r3.u32 + 13, ctx.r8.u8);
	// lbz r8,7(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 7);
	// stb r8,14(r3)
	PPC_STORE_U8(ctx.r3.u32 + 14, ctx.r8.u8);
	// lbz r8,3(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// stb r8,15(r3)
	PPC_STORE_U8(ctx.r3.u32 + 15, ctx.r8.u8);
	// lbz r8,8(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 8);
	// stb r8,16(r3)
	PPC_STORE_U8(ctx.r3.u32 + 16, ctx.r8.u8);
	// lbz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 4);
	// stb r8,17(r3)
	PPC_STORE_U8(ctx.r3.u32 + 17, ctx.r8.u8);
	// lbz r8,9(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 9);
	// stb r8,18(r3)
	PPC_STORE_U8(ctx.r3.u32 + 18, ctx.r8.u8);
	// lbz r8,4(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// stb r8,19(r3)
	PPC_STORE_U8(ctx.r3.u32 + 19, ctx.r8.u8);
	// lbz r8,10(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 10);
	// stb r8,20(r3)
	PPC_STORE_U8(ctx.r3.u32 + 20, ctx.r8.u8);
	// lbz r8,5(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 5);
	// stb r8,21(r3)
	PPC_STORE_U8(ctx.r3.u32 + 21, ctx.r8.u8);
	// lbz r8,11(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 11);
	// stb r8,22(r3)
	PPC_STORE_U8(ctx.r3.u32 + 22, ctx.r8.u8);
	// lbz r8,5(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// stb r8,23(r3)
	PPC_STORE_U8(ctx.r3.u32 + 23, ctx.r8.u8);
	// lbz r8,12(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 12);
	// stb r8,24(r3)
	PPC_STORE_U8(ctx.r3.u32 + 24, ctx.r8.u8);
	// lbz r8,6(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 6);
	// stb r8,25(r3)
	PPC_STORE_U8(ctx.r3.u32 + 25, ctx.r8.u8);
	// lbz r8,13(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 13);
	// stb r8,26(r3)
	PPC_STORE_U8(ctx.r3.u32 + 26, ctx.r8.u8);
	// lbz r8,6(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// stb r8,27(r3)
	PPC_STORE_U8(ctx.r3.u32 + 27, ctx.r8.u8);
	// lbz r8,14(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 14);
	// stb r8,28(r3)
	PPC_STORE_U8(ctx.r3.u32 + 28, ctx.r8.u8);
	// lbz r8,7(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 7);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// stb r8,29(r3)
	PPC_STORE_U8(ctx.r3.u32 + 29, ctx.r8.u8);
	// lbz r8,15(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 15);
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// stb r8,30(r3)
	PPC_STORE_U8(ctx.r3.u32 + 30, ctx.r8.u8);
	// lbz r8,7(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// stb r8,31(r3)
	PPC_STORE_U8(ctx.r3.u32 + 31, ctx.r8.u8);
	// addi r3,r3,32
	ctx.r3.s64 = ctx.r3.s64 + 32;
	// bne cr6,0x82638980
	if (!cr6.eq) goto loc_82638980;
loc_82638A9C:
	// rlwinm r11,r10,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x82638ae8
	if (!cr6.lt) goto loc_82638AE8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
loc_82638AAC:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// lbz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r10,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r10.u8);
	// lbz r10,1(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// stb r10,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r10.u8);
	// lbz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// stb r10,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r10.u8);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne cr6,0x82638aac
	if (!cr6.eq) goto loc_82638AAC;
loc_82638AE8:
	// clrlwi r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// lbz r11,0(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r11.u8);
	// lbz r11,0(r6)
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// stb r11,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82638B10"))) PPC_WEAK_FUNC(sub_82638B10);
PPC_FUNC_IMPL(__imp__sub_82638B10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lwz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// clrlwi r8,r11,28
	ctx.r8.u64 = r11.u32 & 0xF;
	// addi r11,r11,15
	r11.s64 = r11.s64 + 15;
	// addi r9,r10,15
	ctx.r9.s64 = ctx.r10.s64 + 15;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// clrlwi r7,r10,28
	ctx.r7.u64 = ctx.r10.u32 & 0xF;
	// addze r21,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r21.s64 = temp.s64;
	// srawi r11,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	r11.s64 = ctx.r9.s32 >> 4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addze r17,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r17.s64 = temp.s64;
	// bne cr6,0x82638b64
	if (!cr6.eq) goto loc_82638B64;
	// li r8,16
	ctx.r8.s64 = 16;
loc_82638B64:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x82638b70
	if (!cr6.eq) goto loc_82638B70;
	// li r7,16
	ctx.r7.s64 = 16;
loc_82638B70:
	// lwz r11,3716(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3716);
	// rlwinm r14,r7,0,0,29
	r14.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// mr r19,r4
	r19.u64 = ctx.r4.u64;
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// rlwinm r15,r8,0,0,29
	r15.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// lwz r3,96(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// li r23,0
	r23.s64 = 0;
	// mullw r30,r7,r10
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// lwz r5,220(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 220);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lwz r8,224(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 224);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r3,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 1;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r20,r9,r6
	r20.u64 = ctx.r9.u64 + ctx.r6.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// add r22,r9,r11
	r22.u64 = ctx.r9.u64 + r11.u64;
	// add r18,r10,r11
	r18.u64 = ctx.r10.u64 + r11.u64;
	// beq cr6,0x82638cd0
	if (cr6.eq) goto loc_82638CD0;
loc_82638BEC:
	// mr r28,r20
	r28.u64 = r20.u64;
	// mr r29,r22
	r29.u64 = r22.u64;
	// mr r27,r19
	r27.u64 = r19.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82638ca8
	if (cr6.eq) goto loc_82638CA8;
	// addi r16,r17,-1
	r16.s64 = r17.s64 + -1;
	// addi r26,r21,-1
	r26.s64 = r21.s64 + -1;
	// subf r11,r23,r16
	r11.s64 = r16.s64 - r23.s64;
	// subf r25,r22,r18
	r25.s64 = r18.s64 - r22.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r24,r11,27,31,31
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_82638C1C:
	// cmplw cr6,r30,r26
	cr6.compare<uint32_t>(r30.u32, r26.u32, xer);
	// li r10,16
	ctx.r10.s64 = 16;
	// bne cr6,0x82638c2c
	if (!cr6.eq) goto loc_82638C2C;
	// mr r10,r15
	ctx.r10.u64 = r15.u64;
loc_82638C2C:
	// cmplw cr6,r23,r16
	cr6.compare<uint32_t>(r23.u32, r16.u32, xer);
	// li r11,16
	r11.s64 = 16;
	// bne cr6,0x82638c3c
	if (!cr6.eq) goto loc_82638C3C;
	// mr r11,r14
	r11.u64 = r14.u64;
loc_82638C3C:
	// subf r9,r30,r26
	ctx.r9.s64 = r26.s64 - r30.s64;
	// lwz r5,52(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// lwz r4,48(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// add r7,r25,r29
	ctx.r7.u64 = r25.u64 + r29.u64;
	// cntlzw r6,r9
	ctx.r6.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,108(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 108);
	// rlwinm r6,r6,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// lwz r8,96(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r24.u32);
	// stw r6,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r6.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x82635698
	sub_82635698(ctx, base);
	// lwz r11,15632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15632);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// cmplw cr6,r30,r21
	cr6.compare<uint32_t>(r30.u32, r21.u32, xer);
	// blt cr6,0x82638c1c
	if (cr6.lt) goto loc_82638C1C;
loc_82638CA8:
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// lwz r9,100(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// lwz r10,15644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15644);
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// add r20,r9,r20
	r20.u64 = ctx.r9.u64 + r20.u64;
	// add r19,r10,r19
	r19.u64 = ctx.r10.u64 + r19.u64;
	// add r18,r11,r18
	r18.u64 = r11.u64 + r18.u64;
	// cmplw cr6,r23,r17
	cr6.compare<uint32_t>(r23.u32, r17.u32, xer);
	// blt cr6,0x82638bec
	if (cr6.lt) goto loc_82638BEC;
loc_82638CD0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82638CDC"))) PPC_WEAK_FUNC(sub_82638CDC);
PPC_FUNC_IMPL(__imp__sub_82638CDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82638CE0"))) PPC_WEAK_FUNC(sub_82638CE0);
PPC_FUNC_IMPL(__imp__sub_82638CE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,15568(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15568);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638d00
	if (cr6.eq) goto loc_82638D00;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638cfc
	if (cr6.eq) goto loc_82638CFC;
	// b 0x82636678
	sub_82636678(ctx, base);
	return;
loc_82638CFC:
	// b 0x826368f0
	sub_826368F0(ctx, base);
	return;
loc_82638D00:
	// lwz r11,3924(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638d10
	if (cr6.eq) goto loc_82638D10;
	// b 0x826364f0
	sub_826364F0(ctx, base);
	return;
loc_82638D10:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638d30
	if (cr6.eq) goto loc_82638D30;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x82638d2c
	if (!cr6.eq) goto loc_82638D2C;
	// b 0x82638b10
	sub_82638B10(ctx, base);
	return;
loc_82638D2C:
	// b 0x826354c0
	sub_826354C0(ctx, base);
	return;
loc_82638D30:
	// b 0x82636288
	sub_82636288(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82638D34"))) PPC_WEAK_FUNC(sub_82638D34);
PPC_FUNC_IMPL(__imp__sub_82638D34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82638D38"))) PPC_WEAK_FUNC(sub_82638D38);
PPC_FUNC_IMPL(__imp__sub_82638D38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-640(r1)
	ea = -640 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// rlwinm r27,r10,0,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638d6c
	if (cr6.eq) goto loc_82638D6C;
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// bl 0x82636cf8
	sub_82636CF8(ctx, base);
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// b 0x8239bd44
	return;
loc_82638D6C:
	// li r10,32
	ctx.r10.s64 = 32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82636cf8
	sub_82636CF8(ctx, base);
	// lwz r30,724(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 724);
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82638db4
	if (!cr6.gt) goto loc_82638DB4;
	// rlwinm r27,r27,1,0,30
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
loc_82638D8C:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,15620(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82638d8c
	if (!cr6.eq) goto loc_82638D8C;
loc_82638DB4:
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82638DBC"))) PPC_WEAK_FUNC(sub_82638DBC);
PPC_FUNC_IMPL(__imp__sub_82638DBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82638DC0"))) PPC_WEAK_FUNC(sub_82638DC0);
PPC_FUNC_IMPL(__imp__sub_82638DC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-640(r1)
	ea = -640 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// rlwinm r27,r10,0,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82638df4
	if (cr6.eq) goto loc_82638DF4;
	// lwz r10,15620(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// bl 0x82636c60
	sub_82636C60(ctx, base);
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// b 0x8239bd44
	return;
loc_82638DF4:
	// li r10,32
	ctx.r10.s64 = 32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82636c60
	sub_82636C60(ctx, base);
	// lwz r30,724(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 724);
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82638e3c
	if (!cr6.gt) goto loc_82638E3C;
	// rlwinm r27,r27,1,0,30
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
loc_82638E14:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,15620(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15620);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82638e14
	if (!cr6.eq) goto loc_82638E14;
loc_82638E3C:
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82638E44"))) PPC_WEAK_FUNC(sub_82638E44);
PPC_FUNC_IMPL(__imp__sub_82638E44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82638E48"))) PPC_WEAK_FUNC(sub_82638E48);
PPC_FUNC_IMPL(__imp__sub_82638E48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,15488(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 15488);
	// stw r5,15568(r11)
	PPC_STORE_U32(r11.u32 + 15568, ctx.r5.u32);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stw r5,15892(r11)
	PPC_STORE_U32(r11.u32 + 15892, ctx.r5.u32);
	// stw r5,15896(r11)
	PPC_STORE_U32(r11.u32 + 15896, ctx.r5.u32);
	// bne cr6,0x82638e74
	if (!cr6.eq) goto loc_82638E74;
loc_82638E68:
	// li r3,5
	ctx.r3.s64 = 5;
	// stw r5,15572(r11)
	PPC_STORE_U32(r11.u32 + 15572, ctx.r5.u32);
	// blr 
	return;
loc_82638E74:
	// cmplwi cr6,r6,3
	cr6.compare<uint32_t>(ctx.r6.u32, 3, xer);
	// beq cr6,0x82638e68
	if (cr6.eq) goto loc_82638E68;
	// lis r10,12889
	ctx.r10.s64 = 844693504;
	// lis r9,12849
	ctx.r9.s64 = 842072064;
	// ori r10,r10,21849
	ctx.r10.u64 = ctx.r10.u64 | 21849;
	// ori r4,r9,22105
	ctx.r4.u64 = ctx.r9.u64 | 22105;
	// cmplw cr6,r6,r10
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r10.u32, xer);
	// bne cr6,0x82638f00
	if (!cr6.eq) goto loc_82638F00;
	// lwz r10,3924(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 3924);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82638ec4
	if (cr6.eq) goto loc_82638EC4;
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r9,r9,27744
	ctx.r9.s64 = ctx.r9.s64 + 27744;
	// addi r10,r10,-29248
	ctx.r10.s64 = ctx.r10.s64 + -29248;
	// stw r8,15572(r11)
	PPC_STORE_U32(r11.u32 + 15572, ctx.r8.u32);
	// stw r9,15872(r11)
	PPC_STORE_U32(r11.u32 + 15872, ctx.r9.u32);
	// stw r10,15876(r11)
	PPC_STORE_U32(r11.u32 + 15876, ctx.r10.u32);
	// b 0x82639044
	goto loc_82639044;
loc_82638EC4:
	// lis r7,-32157
	ctx.r7.s64 = -2107441152;
	// lis r8,-32156
	ctx.r8.s64 = -2107375616;
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r7,r7,27896
	ctx.r7.s64 = ctx.r7.s64 + 27896;
	// addi r8,r8,-29384
	ctx.r8.s64 = ctx.r8.s64 + -29384;
	// addi r9,r9,28504
	ctx.r9.s64 = ctx.r9.s64 + 28504;
	// addi r10,r10,-30360
	ctx.r10.s64 = ctx.r10.s64 + -30360;
	// stw r3,15572(r11)
	PPC_STORE_U32(r11.u32 + 15572, ctx.r3.u32);
	// stw r7,15872(r11)
	PPC_STORE_U32(r11.u32 + 15872, ctx.r7.u32);
	// stw r8,15876(r11)
	PPC_STORE_U32(r11.u32 + 15876, ctx.r8.u32);
	// stw r9,15880(r11)
	PPC_STORE_U32(r11.u32 + 15880, ctx.r9.u32);
	// stw r10,15892(r11)
	PPC_STORE_U32(r11.u32 + 15892, ctx.r10.u32);
	// b 0x82639044
	goto loc_82639044;
loc_82638F00:
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r9,r10,13385
	ctx.r9.u64 = ctx.r10.u64 | 13385;
	// cmplw cr6,r6,r9
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r9.u32, xer);
	// beq cr6,0x82638f30
	if (cr6.eq) goto loc_82638F30;
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmplw cr6,r6,r10
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r10.u32, xer);
	// beq cr6,0x82638f30
	if (cr6.eq) goto loc_82638F30;
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// beq cr6,0x82638f30
	if (cr6.eq) goto loc_82638F30;
loc_82638F28:
	// li r3,5
	ctx.r3.s64 = 5;
	// blr 
	return;
loc_82638F30:
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r8,24(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r7,8
	ctx.r7.s64 = 8;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r10,15572(r11)
	PPC_STORE_U32(r11.u32 + 15572, ctx.r10.u32);
	// stw r10,15568(r11)
	PPC_STORE_U32(r11.u32 + 15568, ctx.r10.u32);
	// sth r7,15492(r11)
	PPC_STORE_U16(r11.u32 + 15492, ctx.r7.u16);
	// beq cr6,0x82638fc8
	if (cr6.eq) goto loc_82638FC8;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// beq cr6,0x82638fac
	if (cr6.eq) goto loc_82638FAC;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x82638f90
	if (cr6.eq) goto loc_82638F90;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// beq cr6,0x82638f74
	if (cr6.eq) goto loc_82638F74;
	// li r3,12
	ctx.r3.s64 = 12;
	// blr 
	return;
loc_82638F74:
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r9,r9,29600
	ctx.r9.s64 = ctx.r9.s64 + 29600;
	// addi r10,r10,30728
	ctx.r10.s64 = ctx.r10.s64 + 30728;
	// stw r9,15884(r11)
	PPC_STORE_U32(r11.u32 + 15884, ctx.r9.u32);
	// stw r10,15888(r11)
	PPC_STORE_U32(r11.u32 + 15888, ctx.r10.u32);
	// b 0x82639044
	goto loc_82639044;
loc_82638F90:
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r9,r9,29096
	ctx.r9.s64 = ctx.r9.s64 + 29096;
	// addi r10,r10,30392
	ctx.r10.s64 = ctx.r10.s64 + 30392;
	// stw r9,15884(r11)
	PPC_STORE_U32(r11.u32 + 15884, ctx.r9.u32);
	// stw r10,15888(r11)
	PPC_STORE_U32(r11.u32 + 15888, ctx.r10.u32);
	// b 0x82639044
	goto loc_82639044;
loc_82638FAC:
	// lis r9,-32157
	ctx.r9.s64 = -2107441152;
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r9,r9,28840
	ctx.r9.s64 = ctx.r9.s64 + 28840;
	// addi r10,r10,30040
	ctx.r10.s64 = ctx.r10.s64 + 30040;
	// stw r9,15884(r11)
	PPC_STORE_U32(r11.u32 + 15884, ctx.r9.u32);
	// stw r10,15888(r11)
	PPC_STORE_U32(r11.u32 + 15888, ctx.r10.u32);
	// b 0x82639044
	goto loc_82639044;
loc_82638FC8:
	// lwz r10,152(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 152);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82638ff8
	if (cr6.eq) goto loc_82638FF8;
	// lwz r10,21440(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21440);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x82638fec
	if (!cr6.eq) goto loc_82638FEC;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// addi r10,r10,-31792
	ctx.r10.s64 = ctx.r10.s64 + -31792;
	// b 0x82639000
	goto loc_82639000;
loc_82638FEC:
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r10,r10,32640
	ctx.r10.s64 = ctx.r10.s64 + 32640;
	// b 0x82639000
	goto loc_82639000;
loc_82638FF8:
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r10,r10,31536
	ctx.r10.s64 = ctx.r10.s64 + 31536;
loc_82639000:
	// stw r10,15884(r11)
	PPC_STORE_U32(r11.u32 + 15884, ctx.r10.u32);
	// lwz r10,21440(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 21440);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8263901c
	if (!cr6.eq) goto loc_8263901C;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// addi r10,r10,-30568
	ctx.r10.s64 = ctx.r10.s64 + -30568;
	// b 0x82639024
	goto loc_82639024;
loc_8263901C:
	// lis r10,-32157
	ctx.r10.s64 = -2107441152;
	// addi r10,r10,31272
	ctx.r10.s64 = ctx.r10.s64 + 31272;
loc_82639024:
	// stw r10,15888(r11)
	PPC_STORE_U32(r11.u32 + 15888, ctx.r10.u32);
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// beq cr6,0x82639038
	if (cr6.eq) goto loc_82639038;
	// cmplw cr6,r6,r9
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r9.u32, xer);
	// bne cr6,0x82639044
	if (!cr6.eq) goto loc_82639044;
loc_82639038:
	// lis r10,-32153
	ctx.r10.s64 = -2107179008;
	// addi r10,r10,4152
	ctx.r10.s64 = ctx.r10.s64 + 4152;
	// stw r10,15896(r11)
	PPC_STORE_U32(r11.u32 + 15896, ctx.r10.u32);
loc_82639044:
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82639178
	if (cr6.eq) goto loc_82639178;
	// lwz r6,28(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmpwi cr6,r6,3
	cr6.compare<int32_t>(ctx.r6.s32, 3, xer);
	// bne cr6,0x826390b4
	if (!cr6.eq) goto loc_826390B4;
	// lhz r10,15492(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 15492);
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// beq cr6,0x82639078
	if (cr6.eq) goto loc_82639078;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// beq cr6,0x82639078
	if (cr6.eq) goto loc_82639078;
	// cmplwi cr6,r10,15
	cr6.compare<uint32_t>(ctx.r10.u32, 15, xer);
	// bne cr6,0x82639080
	if (!cr6.eq) goto loc_82639080;
loc_82639078:
	// li r10,16
	ctx.r10.s64 = 16;
	// sth r10,15492(r11)
	PPC_STORE_U16(r11.u32 + 15492, ctx.r10.u16);
loc_82639080:
	// lwz r8,15496(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 15496);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,15620(r11)
	PPC_STORE_U32(r11.u32 + 15620, ctx.r8.u32);
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r7,15632(r11)
	PPC_STORE_U32(r11.u32 + 15632, ctx.r7.u32);
	// stw r8,15644(r11)
	PPC_STORE_U32(r11.u32 + 15644, ctx.r8.u32);
	// b 0x82639108
	goto loc_82639108;
loc_826390B4:
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// stw r5,15628(r11)
	PPC_STORE_U32(r11.u32 + 15628, ctx.r5.u32);
	// mullw r8,r10,r6
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// rlwinm r5,r8,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// mullw r7,r9,r6
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// stw r5,15632(r11)
	PPC_STORE_U32(r11.u32 + 15632, ctx.r5.u32);
	// rlwinm r5,r7,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r5,15644(r11)
	PPC_STORE_U32(r11.u32 + 15644, ctx.r5.u32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// stw r8,15636(r11)
	PPC_STORE_U32(r11.u32 + 15636, ctx.r8.u32);
	// stw r7,15648(r11)
	PPC_STORE_U32(r11.u32 + 15648, ctx.r7.u32);
loc_82639108:
	// lwz r8,15568(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 15568);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// bne cr6,0x82639280
	if (!cr6.eq) goto loc_82639280;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// srawi r7,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// xor r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 ^ ctx.r7.u64;
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// bne cr6,0x82639134
	if (!cr6.eq) goto loc_82639134;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// b 0x82639138
	goto loc_82639138;
loc_82639134:
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
loc_82639138:
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r9,60(r11)
	PPC_STORE_U32(r11.u32 + 60, ctx.r9.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// mullw r9,r9,r6
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// stw r10,15640(r11)
	PPC_STORE_U32(r11.u32 + 15640, ctx.r10.u32);
	// stw r9,15652(r11)
	PPC_STORE_U32(r11.u32 + 15652, ctx.r9.u32);
	// blr 
	return;
loc_82639178:
	// lhz r10,15492(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 15492);
	// cmplwi cr6,r10,12
	cr6.compare<uint32_t>(ctx.r10.u32, 12, xer);
	// beq cr6,0x82639194
	if (cr6.eq) goto loc_82639194;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// beq cr6,0x82639194
	if (cr6.eq) goto loc_82639194;
	// cmplwi cr6,r10,15
	cr6.compare<uint32_t>(ctx.r10.u32, 15, xer);
	// bne cr6,0x8263919c
	if (!cr6.eq) goto loc_8263919C;
loc_82639194:
	// li r10,16
	ctx.r10.s64 = 16;
	// sth r10,15492(r11)
	PPC_STORE_U16(r11.u32 + 15492, ctx.r10.u16);
loc_8263919C:
	// lwz r8,15568(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 15568);
	// lhz r10,15492(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 15492);
	// lwz r9,15496(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 15496);
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// stw r5,15628(r11)
	PPC_STORE_U32(r11.u32 + 15628, ctx.r5.u32);
	// rotlwi r7,r10,4
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 4);
	// mullw r8,r9,r10
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addi r8,r8,31
	ctx.r8.s64 = ctx.r8.s64 + 31;
	// rotlwi r5,r10,3
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// rlwinm r10,r8,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r8,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 3;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// srawi r7,r5,3
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 3;
	// stw r10,15620(r11)
	PPC_STORE_U32(r11.u32 + 15620, ctx.r10.u32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// stw r8,15632(r11)
	PPC_STORE_U32(r11.u32 + 15632, ctx.r8.u32);
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r7,15636(r11)
	PPC_STORE_U32(r11.u32 + 15636, ctx.r7.u32);
	// rlwinm r7,r10,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,15648(r11)
	PPC_STORE_U32(r11.u32 + 15648, ctx.r8.u32);
	// stw r7,15644(r11)
	PPC_STORE_U32(r11.u32 + 15644, ctx.r7.u32);
	// bne cr6,0x82639280
	if (!cr6.eq) goto loc_82639280;
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82638f28
	if (!cr6.eq) goto loc_82638F28;
	// lwz r8,92(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// clrlwi r5,r8,31
	ctx.r5.u64 = ctx.r8.u32 & 0x1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x82638f28
	if (!cr6.eq) goto loc_82638F28;
	// srawi r5,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// stw r5,15624(r11)
	PPC_STORE_U32(r11.u32 + 15624, ctx.r5.u32);
	// beq cr6,0x8263925c
	if (cr6.eq) goto loc_8263925C;
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,15576(r11)
	PPC_STORE_U32(r11.u32 + 15576, ctx.r10.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// srawi r10,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 2;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// stw r9,15580(r11)
	PPC_STORE_U32(r11.u32 + 15580, ctx.r9.u32);
	// stw r10,15652(r11)
	PPC_STORE_U32(r11.u32 + 15652, ctx.r10.u32);
	// blr 
	return;
loc_8263925C:
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,15580(r11)
	PPC_STORE_U32(r11.u32 + 15580, ctx.r10.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 2;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// stw r9,15576(r11)
	PPC_STORE_U32(r11.u32 + 15576, ctx.r9.u32);
	// stw r10,15652(r11)
	PPC_STORE_U32(r11.u32 + 15652, ctx.r10.u32);
loc_82639280:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82639288"))) PPC_WEAK_FUNC(sub_82639288);
PPC_FUNC_IMPL(__imp__sub_82639288) {
	PPC_FUNC_PROLOGUE();
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// stb r5,11(r10)
	PPC_STORE_U8(ctx.r10.u32 + 11, ctx.r5.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82639294"))) PPC_WEAK_FUNC(sub_82639294);
PPC_FUNC_IMPL(__imp__sub_82639294) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82639298"))) PPC_WEAK_FUNC(sub_82639298);
PPC_FUNC_IMPL(__imp__sub_82639298) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82639320
	if (!cr6.lt) goto loc_82639320;
loc_826392C8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639320
	if (cr6.eq) goto loc_82639320;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82639310
	if (!cr0.lt) goto loc_82639310;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639310:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826392c8
	if (cr6.gt) goto loc_826392C8;
loc_82639320:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263935c
	if (!cr0.lt) goto loc_8263935C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263935C:
	// cmpwi cr6,r30,7
	cr6.compare<int32_t>(r30.s32, 7, xer);
	// bne cr6,0x8263941c
	if (!cr6.eq) goto loc_8263941C;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,5
	r30.s64 = 5;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bge cr6,0x826393d8
	if (!cr6.lt) goto loc_826393D8;
loc_82639380:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826393d8
	if (cr6.eq) goto loc_826393D8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826393c8
	if (!cr0.lt) goto loc_826393C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826393C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639380
	if (cr6.gt) goto loc_82639380;
loc_826393D8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82639414
	if (!cr0.lt) goto loc_82639414;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639414:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// b 0x82639428
	goto loc_82639428;
loc_8263941C:
	// lwz r11,248(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 248);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
loc_82639428:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82639438
	if (!cr6.eq) goto loc_82639438;
	// rlwinm r11,r3,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
loc_82639438:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82639440"))) PPC_WEAK_FUNC(sub_82639440);
PPC_FUNC_IMPL(__imp__sub_82639440) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// lwz r11,248(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 248);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r26)
	PPC_STORE_U8(r26.u32 + 4, r11.u8);
	// lwz r11,448(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 448);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826394f8
	if (cr6.eq) goto loc_826394F8;
	// lwz r11,432(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 432);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826394c4
	if (!cr6.eq) goto loc_826394C4;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826394a8
	if (!cr0.lt) goto loc_826394A8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826394A8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r11,r31,31,0,0
	r11.u64 = (__builtin_rotateleft32(r31.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82639b04
	if (!cr6.eq) goto loc_82639B04;
loc_826394C4:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639504
	if (cr6.eq) goto loc_82639504;
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r26)
	PPC_STORE_U32(r26.u32 + 12, r11.u32);
	// sth r11,16(r26)
	PPC_STORE_U16(r26.u32 + 16, r11.u16);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_826394F8:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi r11,r11,1
	r11.u64 = r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_82639504:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,440(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 440);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// beq cr6,0x8263964c
	if (cr6.eq) goto loc_8263964C;
	// lwz r11,2140(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 2140);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x82639604
	if (cr6.lt) goto loc_82639604;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826395fc
	if (!cr6.lt) goto loc_826395FC;
loc_82639564:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82639590
	if (cr6.lt) goto loc_82639590;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82639564
	if (cr6.eq) goto loc_82639564;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x82639704
	goto loc_82639704;
loc_82639590:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_826395FC:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x82639704
	goto loc_82639704;
loc_82639604:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r30,r11,32768
	r30.u64 = r11.u64 | 32768;
loc_82639614:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x82639614
	if (cr6.lt) goto loc_82639614;
	// b 0x82639704
	goto loc_82639704;
loc_8263964C:
	// lbz r4,2136(r27)
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + 2136);
	// lwz r28,2128(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 2128);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826396c0
	if (cr6.lt) goto loc_826396C0;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x826395fc
	if (!cr6.lt) goto loc_826395FC;
loc_82639694:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82639590
	if (cr6.lt) goto loc_82639590;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82639694
	if (cr6.eq) goto loc_82639694;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x82639704
	goto loc_82639704;
loc_826396C0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r30,r11,32768
	r30.u64 = r11.u64 | 32768;
loc_826396D0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x826396d0
	if (cr6.lt) goto loc_826396D0;
loc_82639704:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82639b04
	if (!cr6.eq) goto loc_82639B04;
	// cmplwi cr6,r29,127
	cr6.compare<uint32_t>(r29.u32, 127, xer);
	// bgt cr6,0x82639b04
	if (cr6.gt) goto loc_82639B04;
	// rlwinm r11,r29,0,25,25
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x40;
	// li r28,1
	r28.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// beq cr6,0x826397a8
	if (cr6.eq) goto loc_826397A8;
	// xori r29,r29,64
	r29.u64 = r29.u64 ^ 64;
	// oris r10,r11,2
	ctx.r10.u64 = r11.u64 | 131072;
	// cntlzw r9,r29
	ctx.r9.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwimi r11,r9,30,1,1
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 30) & 0x40000000) | (r11.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r10,324(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 324);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263979c
	if (cr6.eq) goto loc_8263979C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639788
	if (!cr0.lt) goto loc_82639788;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639788:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// rlwimi r10,r11,18,12,13
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 18) & 0xC0000) | (ctx.r10.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// b 0x826398fc
	goto loc_826398FC;
loc_8263979C:
	// rlwimi r11,r28,19,12,13
	r11.u64 = (__builtin_rotateleft32(r28.u32, 19) & 0xC0000) | (r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x826398fc
	goto loc_826398FC;
loc_826397A8:
	// rlwinm r11,r11,0,15,13
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826397dc
	if (!cr0.lt) goto loc_826397DC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826397DC:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// lwz r11,400(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 400);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826398ec
	if (cr6.eq) goto loc_826398EC;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639824
	if (!cr0.lt) goto loc_82639824;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639824:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x82639838
	if (!cr6.eq) goto loc_82639838;
	// lbz r11,18(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 18);
	// andi. r11,r11,221
	r11.u64 = r11.u64 & 221;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// b 0x826398c8
	goto loc_826398C8;
loc_82639838:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639864
	if (!cr0.lt) goto loc_82639864;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639864:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8263987c
	if (!cr6.eq) goto loc_8263987C;
	// lbz r11,18(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 18);
	// andi. r11,r11,221
	r11.u64 = r11.u64 & 221;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// b 0x826398c8
	goto loc_826398C8;
loc_8263987C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826398a8
	if (!cr0.lt) goto loc_826398A8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826398A8:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826398bc
	if (cr6.eq) goto loc_826398BC;
	// lbz r11,18(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 18);
	// ori r11,r11,34
	r11.u64 = r11.u64 | 34;
	// b 0x826398c8
	goto loc_826398C8;
loc_826398BC:
	// lbz r11,18(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 18);
	// andi. r11,r11,221
	r11.u64 = r11.u64 & 221;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
loc_826398C8:
	// stb r11,18(r26)
	PPC_STORE_U8(r26.u32 + 18, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// andi. r11,r11,179
	r11.u64 = r11.u64 & 179;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// rlwinm r10,r11,1,25,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x40;
	// stb r11,18(r26)
	PPC_STORE_U8(r26.u32 + 18, r11.u8);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stb r11,18(r26)
	PPC_STORE_U8(r26.u32 + 18, r11.u8);
loc_826398EC:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82639b04
	if (!cr6.eq) goto loc_82639B04;
loc_826398FC:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,0,10,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF3FFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r11,392(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639990
	if (cr6.eq) goto loc_82639990;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82639990
	if (cr6.eq) goto loc_82639990;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639948
	if (!cr0.lt) goto loc_82639948;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639948:
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x82639984
	if (cr6.eq) goto loc_82639984;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639980
	if (!cr0.lt) goto loc_82639980;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639980:
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
loc_82639984:
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r10,r11,22,8,9
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r10.u64 & 0xFFFFFFFFFF3FFFFF);
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
loc_82639990:
	// clrlwi r10,r29,31
	ctx.r10.u64 = r29.u32 & 0x1;
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// stb r10,12(r26)
	PPC_STORE_U8(r26.u32 + 12, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,13(r26)
	PPC_STORE_U8(r26.u32 + 13, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,14(r26)
	PPC_STORE_U8(r26.u32 + 14, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,15(r26)
	PPC_STORE_U8(r26.u32 + 15, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// stb r10,16(r26)
	PPC_STORE_U8(r26.u32 + 16, ctx.r10.u8);
	// stb r11,17(r26)
	PPC_STORE_U8(r26.u32 + 17, r11.u8);
	// lwz r11,436(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 436);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639af8
	if (cr6.eq) goto loc_82639AF8;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,0,4,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// lwz r10,328(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 328);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82639af8
	if (cr6.eq) goto loc_82639AF8;
	// rlwinm r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82639af8
	if (!cr6.eq) goto loc_82639AF8;
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82639af8
	if (!cr6.eq) goto loc_82639AF8;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639a40
	if (!cr0.lt) goto loc_82639A40;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639A40:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwimi r11,r31,28,3,3
	r11.u64 = (__builtin_rotateleft32(r31.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// rlwinm r10,r11,0,3,3
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// bne cr6,0x82639af8
	if (!cr6.eq) goto loc_82639AF8;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639a84
	if (!cr0.lt) goto loc_82639A84;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639A84:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82639aa4
	if (!cr6.eq) goto loc_82639AA4;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,0,8,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFF8FFFFFF;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639AA4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639ad0
	if (!cr0.lt) goto loc_82639AD0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639AD0:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82639af0
	if (!cr6.eq) goto loc_82639AF0;
	// rlwimi r11,r28,24,5,7
	r11.u64 = (__builtin_rotateleft32(r28.u32, 24) & 0x7000000) | (r11.u64 & 0xFFFFFFFFF8FFFFFF);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639AF0:
	// rlwimi r11,r28,25,5,7
	r11.u64 = (__builtin_rotateleft32(r28.u32, 25) & 0x7000000) | (r11.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_82639AF8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639B04:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82639B10"))) PPC_WEAK_FUNC(sub_82639B10);
PPC_FUNC_IMPL(__imp__sub_82639B10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r30,9
	r30.s64 = 9;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,9
	cr6.compare<uint32_t>(r11.u32, 9, xer);
	// bge cr6,0x82639b9c
	if (!cr6.lt) goto loc_82639B9C;
loc_82639B44:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639b9c
	if (cr6.eq) goto loc_82639B9C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82639b8c
	if (!cr0.lt) goto loc_82639B8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639B8C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639b44
	if (cr6.gt) goto loc_82639B44;
loc_82639B9C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82639bd8
	if (!cr0.lt) goto loc_82639BD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639BD8:
	// lwz r11,19976(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639c18
	if (cr6.eq) goto loc_82639C18;
	// lwz r11,19980(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 19980);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639c18
	if (cr6.eq) goto loc_82639C18;
	// lwz r11,21000(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21000);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82639c18
	if (!cr6.eq) goto loc_82639C18;
	// lwz r11,140(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 140);
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// beq cr6,0x82639c20
	if (cr6.eq) goto loc_82639C20;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639C18:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// bne cr6,0x82639e90
	if (!cr6.eq) goto loc_82639E90;
loc_82639C20:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r28,1
	r28.s64 = 1;
	// mr r29,r26
	r29.u64 = r26.u64;
	// mr r30,r28
	r30.u64 = r28.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82639c98
	if (!cr6.lt) goto loc_82639C98;
loc_82639C40:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639c98
	if (cr6.eq) goto loc_82639C98;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82639c88
	if (!cr0.lt) goto loc_82639C88;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639C88:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639c40
	if (cr6.gt) goto loc_82639C40;
loc_82639C98:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82639cd4
	if (!cr0.lt) goto loc_82639CD4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639CD4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82639ce8
	if (!cr6.eq) goto loc_82639CE8;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639CE8:
	// lwz r11,21160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21160);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639e9c
	if (cr6.eq) goto loc_82639E9C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82639d68
	if (!cr6.lt) goto loc_82639D68;
loc_82639D10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639d68
	if (cr6.eq) goto loc_82639D68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82639d58
	if (!cr0.lt) goto loc_82639D58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639D58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639d10
	if (cr6.gt) goto loc_82639D10;
loc_82639D68:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82639da4
	if (!cr0.lt) goto loc_82639DA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639DA4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82639e9c
	if (cr6.eq) goto loc_82639E9C;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82639e20
	if (!cr6.lt) goto loc_82639E20;
loc_82639DC8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639e20
	if (cr6.eq) goto loc_82639E20;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82639e10
	if (!cr0.lt) goto loc_82639E10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639E10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639dc8
	if (cr6.gt) goto loc_82639DC8;
loc_82639E20:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82639e5c
	if (!cr0.lt) goto loc_82639E5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639E5C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82639ec4
	if (cr6.eq) goto loc_82639EC4;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r28,19976(r27)
	PPC_STORE_U32(r27.u32 + 19976, r28.u32);
	// stw r28,19980(r27)
	PPC_STORE_U32(r27.u32 + 19980, r28.u32);
	// bl 0x82620780
	sub_82620780(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82639e94
	if (!cr6.eq) goto loc_82639E94;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x826322d0
	sub_826322D0(ctx, base);
loc_82639E84:
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x82639e94
	if (!cr6.eq) goto loc_82639E94;
loc_82639E90:
	// li r3,4
	ctx.r3.s64 = 4;
loc_82639E94:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639E9C:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r26,19976(r27)
	PPC_STORE_U32(r27.u32 + 19976, r26.u32);
	// stw r26,19984(r27)
	PPC_STORE_U32(r27.u32 + 19984, r26.u32);
	// bl 0x825f60b0
	sub_825F60B0(ctx, base);
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x82639e84
	if (!cr6.eq) goto loc_82639E84;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_82639EC4:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r28,19976(r27)
	PPC_STORE_U32(r27.u32 + 19976, r28.u32);
	// stw r26,19984(r27)
	PPC_STORE_U32(r27.u32 + 19984, r26.u32);
	// bl 0x8261e1c8
	sub_8261E1C8(ctx, base);
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x82639e84
	if (!cr6.eq) goto loc_82639E84;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_82639EEC"))) PPC_WEAK_FUNC(sub_82639EEC);
PPC_FUNC_IMPL(__imp__sub_82639EEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82639EF0"))) PPC_WEAK_FUNC(sub_82639EF0);
PPC_FUNC_IMPL(__imp__sub_82639EF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8263a198
	if (cr6.eq) goto loc_8263A198;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82639f34
	if (!cr0.lt) goto loc_82639F34;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639F34:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8263a198
	if (!cr6.eq) goto loc_8263A198;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82639f54
	if (cr6.eq) goto loc_82639F54;
	// li r11,0
	r11.s64 = 0;
	// b 0x82639f5c
	goto loc_82639F5C;
loc_82639F54:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
loc_82639F5C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrlwi r30,r11,29
	r30.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x82639fe0
	if (cr6.eq) goto loc_82639FE0;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82639fb8
	if (!cr6.gt) goto loc_82639FB8;
loc_82639F78:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82639fb8
	if (cr6.eq) goto loc_82639FB8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x82639fa8
	if (!cr0.lt) goto loc_82639FA8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639FA8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639f78
	if (cr6.gt) goto loc_82639F78;
loc_82639FB8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x82639fe0
	if (!cr0.lt) goto loc_82639FE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82639FE0:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x8263a054
	if (!cr6.lt) goto loc_8263A054;
loc_82639FFC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a054
	if (cr6.eq) goto loc_8263A054;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263a044
	if (!cr0.lt) goto loc_8263A044;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A044:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82639ffc
	if (cr6.gt) goto loc_82639FFC;
loc_8263A054:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263a090
	if (!cr0.lt) goto loc_8263A090;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A090:
	// cmpwi cr6,r30,170
	cr6.compare<int32_t>(r30.s32, 170, xer);
	// bne cr6,0x8263a1a4
	if (!cr6.eq) goto loc_8263A1A4;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x8263a0f0
	if (!cr6.lt) goto loc_8263A0F0;
loc_8263A0B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a0f0
	if (cr6.eq) goto loc_8263A0F0;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a0e0
	if (!cr0.lt) goto loc_8263A0E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A0E0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a0b0
	if (cr6.gt) goto loc_8263A0B0;
loc_8263A0F0:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8263a118
	if (!cr0.lt) goto loc_8263A118;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A118:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x8263a170
	if (!cr6.lt) goto loc_8263A170;
loc_8263A130:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a170
	if (cr6.eq) goto loc_8263A170;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a160
	if (!cr0.lt) goto loc_8263A160;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A160:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a130
	if (cr6.gt) goto loc_8263A130;
loc_8263A170:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x8263a198
	if (!cr0.lt) goto loc_8263A198;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A198:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8263A1A4:
	// cmpwi cr6,r30,171
	cr6.compare<int32_t>(r30.s32, 171, xer);
	// bne cr6,0x8263a388
	if (!cr6.eq) goto loc_8263A388;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x8263a204
	if (!cr6.lt) goto loc_8263A204;
loc_8263A1C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a204
	if (cr6.eq) goto loc_8263A204;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a1f4
	if (!cr0.lt) goto loc_8263A1F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A1F4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a1c4
	if (cr6.gt) goto loc_8263A1C4;
loc_8263A204:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8263a22c
	if (!cr0.lt) goto loc_8263A22C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A22C:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x8263a284
	if (!cr6.lt) goto loc_8263A284;
loc_8263A244:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a284
	if (cr6.eq) goto loc_8263A284;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a274
	if (!cr0.lt) goto loc_8263A274;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A274:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a244
	if (cr6.gt) goto loc_8263A244;
loc_8263A284:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8263a2ac
	if (!cr0.lt) goto loc_8263A2AC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A2AC:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,24
	r30.s64 = 24;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,24
	cr6.compare<uint32_t>(r11.u32, 24, xer);
	// bge cr6,0x8263a304
	if (!cr6.lt) goto loc_8263A304;
loc_8263A2C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a304
	if (cr6.eq) goto loc_8263A304;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a2f4
	if (!cr0.lt) goto loc_8263A2F4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A2F4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a2c4
	if (cr6.gt) goto loc_8263A2C4;
loc_8263A304:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf. r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge 0x8263a32c
	if (!cr0.lt) goto loc_8263A32C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A32C:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x8263a170
	if (!cr6.lt) goto loc_8263A170;
loc_8263A344:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a170
	if (cr6.eq) goto loc_8263A170;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x8263a374
	if (!cr0.lt) goto loc_8263A374;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A374:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a344
	if (cr6.gt) goto loc_8263A344;
	// b 0x8263a170
	goto loc_8263A170;
loc_8263A388:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8263A394"))) PPC_WEAK_FUNC(sub_8263A394);
PPC_FUNC_IMPL(__imp__sub_8263A394) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263A398"))) PPC_WEAK_FUNC(sub_8263A398);
PPC_FUNC_IMPL(__imp__sub_8263A398) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcec
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8263a3bc
	if (cr6.eq) goto loc_8263A3BC;
	// stb r10,-79(r1)
	PPC_STORE_U8(ctx.r1.u32 + -79, ctx.r10.u8);
	// stb r10,-80(r1)
	PPC_STORE_U8(ctx.r1.u32 + -80, ctx.r10.u8);
	// lhz r27,-80(r1)
	r27.u64 = PPC_LOAD_U16(ctx.r1.u32 + -80);
	// b 0x8263a3c4
	goto loc_8263A3C4;
loc_8263A3BC:
	// lhz r27,-2(r5)
	r27.u64 = PPC_LOAD_U16(ctx.r5.u32 + -2);
	// sth r27,-80(r1)
	PPC_STORE_U16(ctx.r1.u32 + -80, r27.u16);
loc_8263A3C4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8263a3d4
	if (cr6.eq) goto loc_8263A3D4;
	// sth r27,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, r27.u16);
	// b 0x8239bd3c
	return;
loc_8263A3D4:
	// lwz r11,340(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// lhz r28,0(r11)
	r28.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r28,-76(r1)
	PPC_STORE_U16(ctx.r1.u32 + -76, r28.u16);
	// beq cr6,0x8263a3fc
	if (cr6.eq) goto loc_8263A3FC;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// b 0x8263a40c
	goto loc_8263A40C;
loc_8263A3FC:
	// lhz r11,2(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// sth r11,-78(r1)
	PPC_STORE_U16(ctx.r1.u32 + -78, r11.u16);
	// lbz r5,-77(r1)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + -77);
	// lbz r7,-78(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + -78);
loc_8263A40C:
	// lbz r11,-76(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + -76);
	// extsb r7,r7
	ctx.r7.s64 = ctx.r7.s8;
	// lbz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -80);
	// extsb r5,r5
	ctx.r5.s64 = ctx.r5.s8;
	// lbz r9,-75(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -75);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// lbz r8,-79(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + -79);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// subf r30,r7,r11
	r30.s64 = r11.s64 - ctx.r7.s64;
	// subf r29,r10,r7
	r29.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r31,r8,r9
	r31.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r26,r5,r9
	r26.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r25,r8,r5
	r25.s64 = ctx.r5.s64 - ctx.r8.s64;
	// xor r30,r30,r3
	r30.u64 = r30.u64 ^ ctx.r3.u64;
	// xor r29,r29,r3
	r29.u64 = r29.u64 ^ ctx.r3.u64;
	// xor r26,r26,r31
	r26.u64 = r26.u64 ^ r31.u64;
	// srawi r3,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r30.s32 >> 31;
	// xor r25,r25,r31
	r25.u64 = r25.u64 ^ r31.u64;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// srawi r30,r26,31
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r26.s32 >> 31;
	// srawi r29,r25,31
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r25.s32 >> 31;
	// or r26,r3,r31
	r26.u64 = ctx.r3.u64 | r31.u64;
	// or r25,r30,r29
	r25.u64 = r30.u64 | r29.u64;
	// and r10,r31,r10
	ctx.r10.u64 = r31.u64 & ctx.r10.u64;
	// andc r7,r7,r26
	ctx.r7.u64 = ctx.r7.u64 & ~r26.u64;
	// andc r5,r5,r25
	ctx.r5.u64 = ctx.r5.u64 & ~r25.u64;
	// and r9,r30,r9
	ctx.r9.u64 = r30.u64 & ctx.r9.u64;
	// and r11,r3,r11
	r11.u64 = ctx.r3.u64 & r11.u64;
	// or r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 | ctx.r10.u64;
	// or r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 | ctx.r9.u64;
	// and r8,r29,r8
	ctx.r8.u64 = r29.u64 & ctx.r8.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// stb r10,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r10.u8);
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r11,r11,14,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 14) & 0x3;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// beq cr6,0x8263a4c8
	if (cr6.eq) goto loc_8263A4C8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// beq cr6,0x8263a4c4
	if (cr6.eq) goto loc_8263A4C4;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_8263A4C4:
	// sth r11,0(r4)
	PPC_STORE_U16(ctx.r4.u32 + 0, r11.u16);
loc_8263A4C8:
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8263A4CC"))) PPC_WEAK_FUNC(sub_8263A4CC);
PPC_FUNC_IMPL(__imp__sub_8263A4CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263A4D0"))) PPC_WEAK_FUNC(sub_8263A4D0);
PPC_FUNC_IMPL(__imp__sub_8263A4D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// bl 0x8263a398
	sub_8263A398(ctx, base);
	// lbz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 8);
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r10,r11,32
	ctx.r10.u64 = r11.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8263a5f8
	if (cr6.lt) goto loc_8263A5F8;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x8263a5f0
	if (!cr6.lt) goto loc_8263A5F0;
loc_8263A558:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8263a584
	if (cr6.lt) goto loc_8263A584;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8263a558
	if (cr6.eq) goto loc_8263A558;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8263a63c
	goto loc_8263A63C;
loc_8263A584:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8263A5F0:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8263a63c
	goto loc_8263A63C;
loc_8263A5F8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_8263A608:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8263a608
	if (cr6.lt) goto loc_8263A608;
loc_8263A63C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263a658
	if (cr6.eq) goto loc_8263A658;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
loc_8263A658:
	// cmpwi cr6,r30,1099
	cr6.compare<int32_t>(r30.s32, 1099, xer);
	// beq cr6,0x8263a670
	if (cr6.eq) goto loc_8263A670;
	// lwz r11,244(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lbzx r28,r30,r27
	r28.u64 = PPC_LOAD_U8(r30.u32 + r27.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// b 0x8263a7d0
	goto loc_8263A7D0;
loc_8263A670:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,6
	r30.s64 = 6;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8263a6e0
	if (!cr6.lt) goto loc_8263A6E0;
loc_8263A688:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a6e0
	if (cr6.eq) goto loc_8263A6E0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263a6d0
	if (!cr0.lt) goto loc_8263A6D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A6D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a688
	if (cr6.gt) goto loc_8263A688;
loc_8263A6E0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263a71c
	if (!cr0.lt) goto loc_8263A71C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A71C:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r30,6
	r30.s64 = 6;
	// li r29,0
	r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8263a794
	if (!cr6.lt) goto loc_8263A794;
loc_8263A73C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a794
	if (cr6.eq) goto loc_8263A794;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263a784
	if (!cr0.lt) goto loc_8263A784;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A784:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a73c
	if (cr6.gt) goto loc_8263A73C;
loc_8263A794:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263a7d0
	if (!cr0.lt) goto loc_8263A7D0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A7D0:
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// lwz r10,244(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 244);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8263a7f4
	if (!cr6.gt) goto loc_8263A7F4;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x8263a804
	goto loc_8263A804;
loc_8263A7F4:
	// lwz r10,240(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 240);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263a804
	if (!cr6.lt) goto loc_8263A804;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
loc_8263A804:
	// stb r11,0(r25)
	PPC_STORE_U8(r25.u32 + 0, r11.u8);
	// lbz r11,81(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// lwz r10,244(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 244);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8263a82c
	if (!cr6.gt) goto loc_8263A82C;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// b 0x8263a83c
	goto loc_8263A83C;
loc_8263A82C:
	// lwz r10,240(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 240);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263a83c
	if (!cr6.lt) goto loc_8263A83C;
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
loc_8263A83C:
	// stb r11,1(r25)
	PPC_STORE_U8(r25.u32 + 1, r11.u8);
	// lwz r11,452(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 452);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// li r11,0
	r11.s64 = 0;
	// stw r11,332(r26)
	PPC_STORE_U32(r26.u32 + 332, r11.u32);
	// bne cr6,0x8263a928
	if (!cr6.eq) goto loc_8263A928;
	// lbz r11,0(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263a874
	if (!cr6.eq) goto loc_8263A874;
	// lbz r11,1(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 1);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263a928
	if (cr6.eq) goto loc_8263A928;
loc_8263A874:
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263a8e8
	if (!cr6.lt) goto loc_8263A8E8;
loc_8263A890:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263a8e8
	if (cr6.eq) goto loc_8263A8E8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263a8d8
	if (!cr0.lt) goto loc_8263A8D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A8D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263a890
	if (cr6.gt) goto loc_8263A890;
loc_8263A8E8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263a924
	if (!cr0.lt) goto loc_8263A924;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263A924:
	// stw r30,332(r26)
	PPC_STORE_U32(r26.u32 + 332, r30.u32);
loc_8263A928:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_8263A934"))) PPC_WEAK_FUNC(sub_8263A934);
PPC_FUNC_IMPL(__imp__sub_8263A934) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263A938"))) PPC_WEAK_FUNC(sub_8263A938);
PPC_FUNC_IMPL(__imp__sub_8263A938) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,444(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 444);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263a9d4
	if (cr6.eq) goto loc_8263A9D4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8263a9d4
	if (cr6.eq) goto loc_8263A9D4;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x8263a9d4
	if (cr6.eq) goto loc_8263A9D4;
	// lwz r11,452(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 452);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263a9d4
	if (!cr6.eq) goto loc_8263A9D4;
	// lwz r11,340(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// lhz r10,-2(r6)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r6.u32 + -2);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// sth r10,-16(r1)
	PPC_STORE_U16(ctx.r1.u32 + -16, ctx.r10.u16);
	// lbz r10,-15(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + -15);
	// lbz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + -16);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// sth r11,-16(r1)
	PPC_STORE_U16(ctx.r1.u32 + -16, r11.u16);
	// lbz r11,-15(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + -15);
	// lbz r8,-16(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + -16);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// extsb r8,r8
	ctx.r8.s64 = ctx.r8.s8;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// srawi r9,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 31;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// xor r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r8.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8263a9c8
	if (!cr6.gt) goto loc_8263A9C8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8263A9C8:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// bgelr cr6
	if (!cr6.lt) return;
loc_8263A9D4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263A9DC"))) PPC_WEAK_FUNC(sub_8263A9DC);
PPC_FUNC_IMPL(__imp__sub_8263A9DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263A9E0"))) PPC_WEAK_FUNC(sub_8263A9E0);
PPC_FUNC_IMPL(__imp__sub_8263A9E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
loc_8263A9E0:
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x8263a9e0
	if (!cr6.lt) goto loc_8263A9E0;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263A9F4"))) PPC_WEAK_FUNC(sub_8263A9F4);
PPC_FUNC_IMPL(__imp__sub_8263A9F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263A9F8"))) PPC_WEAK_FUNC(sub_8263A9F8);
PPC_FUNC_IMPL(__imp__sub_8263A9F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
loc_8263A9FC:
	// subf r3,r4,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r4.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge cr6,0x8263a9fc
	if (!cr6.lt) goto loc_8263A9FC;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263AA14"))) PPC_WEAK_FUNC(sub_8263AA14);
PPC_FUNC_IMPL(__imp__sub_8263AA14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263AA18"))) PPC_WEAK_FUNC(sub_8263AA18);
PPC_FUNC_IMPL(__imp__sub_8263AA18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r11,3976(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263aa80
	if (cr6.eq) goto loc_8263AA80;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r10,r10,20,12,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFFFF;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// clrlwi r11,r11,28
	r11.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263aa64
	if (cr6.eq) goto loc_8263AA64;
	// lwz r11,3984(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3984);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r27)
	PPC_STORE_U8(r27.u32 + 4, r11.u8);
	// b 0x8263ab84
	goto loc_8263AB84;
loc_8263AA64:
	// lwz r11,248(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 248);
	// lwz r10,252(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r27)
	PPC_STORE_U8(r27.u32 + 4, r11.u8);
	// b 0x8263ab84
	goto loc_8263AB84;
loc_8263AA80:
	// lwz r11,472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 472);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263ab74
	if (cr6.eq) goto loc_8263AB74;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263ab00
	if (!cr6.lt) goto loc_8263AB00;
loc_8263AAA8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263ab00
	if (cr6.eq) goto loc_8263AB00;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263aaf0
	if (!cr0.lt) goto loc_8263AAF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AAF0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263aaa8
	if (cr6.gt) goto loc_8263AAA8;
loc_8263AB00:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263ab3c
	if (!cr0.lt) goto loc_8263AB3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AB3C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8263ab58
	if (cr6.eq) goto loc_8263AB58;
	// lwz r11,3984(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 3984);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r11,4(r27)
	PPC_STORE_U8(r27.u32 + 4, r11.u8);
	// b 0x8263ab84
	goto loc_8263AB84;
loc_8263AB58:
	// lwz r11,248(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 248);
	// lwz r10,252(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r11,4(r27)
	PPC_STORE_U8(r27.u32 + 4, r11.u8);
	// b 0x8263ab84
	goto loc_8263AB84;
loc_8263AB74:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82639298
	sub_82639298(ctx, base);
	// stb r3,4(r27)
	PPC_STORE_U8(r27.u32 + 4, ctx.r3.u8);
loc_8263AB84:
	// lbz r11,4(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 4);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x8263ab9c
	if (cr6.lt) goto loc_8263AB9C;
	// cmplwi cr6,r11,62
	cr6.compare<uint32_t>(r11.u32, 62, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// ble cr6,0x8263aba0
	if (!cr6.gt) goto loc_8263ABA0;
loc_8263AB9C:
	// li r3,4
	ctx.r3.s64 = 4;
loc_8263ABA0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8263ABA8"))) PPC_WEAK_FUNC(sub_8263ABA8);
PPC_FUNC_IMPL(__imp__sub_8263ABA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r15,r4
	r15.u64 = ctx.r4.u64;
	// mr r17,r5
	r17.u64 = ctx.r5.u64;
	// mr r14,r6
	r14.u64 = ctx.r6.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r11,248(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lwz r10,252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// mr r19,r30
	r19.u64 = r30.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r21,352(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwz r24,84(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// mr r27,r30
	r27.u64 = r30.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r17,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r17.u32);
	// stw r14,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, r14.u32);
	// mr r26,r30
	r26.u64 = r30.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stw r21,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r21.u32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// stb r11,4(r15)
	PPC_STORE_U8(r15.u32 + 4, r11.u8);
	// blt cr6,0x8263b12c
	if (cr6.lt) goto loc_8263B12C;
	// cmplwi cr6,r10,62
	cr6.compare<uint32_t>(ctx.r10.u32, 62, xer);
	// bgt cr6,0x8263b12c
	if (cr6.gt) goto loc_8263B12C;
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r11,r11,0,10,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF3FFFFF;
	// rlwinm r11,r11,0,4,2
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// oris r11,r11,2
	r11.u64 = r11.u64 | 131072;
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
	// lwz r11,348(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263ac7c
	if (!cr6.eq) goto loc_8263AC7C;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263ac64
	if (!cr0.lt) goto loc_8263AC64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AC64:
	// cntlzw r11,r29
	r11.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// rlwimi r10,r11,8,21,23
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 8) & 0x700) | (ctx.r10.u64 & 0xFFFFFFFFFFFFF8FF);
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
loc_8263AC7C:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// lwz r10,344(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 344);
	// rlwinm r20,r11,24,29,31
	r20.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r20.u32);
	// bne cr6,0x8263accc
	if (!cr6.eq) goto loc_8263ACCC;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263acc0
	if (!cr0.lt) goto loc_8263ACC0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263ACC0:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwimi r11,r29,31,0,0
	r11.u64 = (__builtin_rotateleft32(r29.u32, 31) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
loc_8263ACCC:
	// lwz r11,21632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21632);
	// li r16,5
	r16.s64 = 5;
	// li r18,3
	r18.s64 = 3;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263ad78
	if (cr6.eq) goto loc_8263AD78;
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,256
	cr6.compare<uint32_t>(r11.u32, 256, xer);
	// bne cr6,0x8263ad78
	if (!cr6.eq) goto loc_8263AD78;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263ad1c
	if (!cr0.lt) goto loc_8263AD1C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AD1C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x8263ad6c
	if (!cr6.eq) goto loc_8263AD6C;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263ad50
	if (!cr0.lt) goto loc_8263AD50;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AD50:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8263ad64
	if (cr6.eq) goto loc_8263AD64;
	// rlwimi r11,r16,8,21,23
	r11.u64 = (__builtin_rotateleft32(r16.u32, 8) & 0x700) | (r11.u64 & 0xFFFFFFFFFFFFF8FF);
	// b 0x8263ad68
	goto loc_8263AD68;
loc_8263AD64:
	// rlwimi r11,r18,9,21,23
	r11.u64 = (__builtin_rotateleft32(r18.u32, 9) & 0x700) | (r11.u64 & 0xFFFFFFFFFFFFF8FF);
loc_8263AD68:
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
loc_8263AD6C:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263AD78:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263b0fc
	if (cr6.eq) goto loc_8263B0FC;
	// stw r30,12(r15)
	PPC_STORE_U32(r15.u32 + 12, r30.u32);
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// sth r30,16(r15)
	PPC_STORE_U16(r15.u32 + 16, r30.u16);
	// bne cr6,0x8263ae14
	if (!cr6.eq) goto loc_8263AE14;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stw r30,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r30.u32);
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,1
	ctx.r6.s64 = 1;
	// rlwinm r5,r14,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r17,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263ae04
	if (cr6.eq) goto loc_8263AE04;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263adec
	if (!cr0.lt) goto loc_8263ADEC;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263ADEC:
	// extsb r11,r30
	r11.s64 = r30.s8;
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwimi r11,r10,0,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// b 0x8263bdd8
	goto loc_8263BDD8;
loc_8263AE04:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// b 0x8263bdd8
	goto loc_8263BDD8;
loc_8263AE14:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r23,r17,1,0,30
	r23.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,24,29,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0x7;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bne cr6,0x8263af20
	if (!cr6.eq) goto loc_8263AF20;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// mr r29,r30
	r29.u64 = r30.u64;
	// rlwinm r22,r14,1,0,30
	r22.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
loc_8263AE44:
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r10,r29,31
	ctx.r10.u64 = r29.u32 & 0x1;
	// add r27,r11,r22
	r27.u64 = r11.u64 + r22.u64;
	// add r28,r10,r23
	r28.u64 = ctx.r10.u64 + r23.u64;
	// mullw r11,r9,r27
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r27.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// add r26,r11,r28
	r26.u64 = r11.u64 + r28.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263aecc
	if (cr6.eq) goto loc_8263AECC;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r25,r11,0
	r25.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263aeb4
	if (!cr0.lt) goto loc_8263AEB4;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AEB4:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// extsb r10,r25
	ctx.r10.s64 = r25.s8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r9,r30,r11
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// rlwimi r10,r9,0,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// stwx r10,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, ctx.r10.u32);
loc_8263AECC:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// add r6,r30,r11
	ctx.r6.u64 = r30.u64 + r11.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r11,r26,1,0,30
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// cmpwi cr6,r30,16
	cr6.compare<int32_t>(r30.s32, 16, xer);
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// sth r9,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r9.u16);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r10,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r10.u16);
	// blt cr6,0x8263ae44
	if (cr6.lt) goto loc_8263AE44;
	// b 0x8263bdd0
	goto loc_8263BDD0;
loc_8263AF20:
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bne cr6,0x8263b038
	if (!cr6.eq) goto loc_8263B038;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// mr r29,r30
	r29.u64 = r30.u64;
	// rlwinm r22,r14,1,0,30
	r22.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
loc_8263AF44:
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r10,r29,31
	ctx.r10.u64 = r29.u32 & 0x1;
	// add r26,r11,r22
	r26.u64 = r11.u64 + r22.u64;
	// add r27,r10,r23
	r27.u64 = ctx.r10.u64 + r23.u64;
	// mullw r11,r9,r26
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r26.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263afcc
	if (cr6.eq) goto loc_8263AFCC;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r25,r11,0
	r25.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263afb4
	if (!cr0.lt) goto loc_8263AFB4;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263AFB4:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// extsb r10,r25
	ctx.r10.s64 = r25.s8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r9,r30,r11
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + r11.u32);
	// rlwimi r10,r9,0,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// stwx r10,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, ctx.r10.u32);
loc_8263AFCC:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// add r6,r30,r11
	ctx.r6.u64 = r30.u64 + r11.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// lhzx r8,r11,r10
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// sthx r8,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r8.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,1776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// lhzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// blt cr6,0x8263af44
	if (cr6.lt) goto loc_8263AF44;
	// b 0x8263bdd0
	goto loc_8263BDD0;
loc_8263B038:
	// stw r30,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r30.u32);
	// rlwinm r25,r14,1,0,30
	r25.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r30.u32);
	// mr r29,r21
	r29.u64 = r21.u64;
	// stw r30,8(r21)
	PPC_STORE_U32(r21.u32 + 8, r30.u32);
	// stw r30,12(r21)
	PPC_STORE_U32(r21.u32 + 12, r30.u32);
loc_8263B050:
	// clrlwi r10,r30,31
	ctx.r10.u64 = r30.u32 & 0x1;
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r28,r10,r23
	r28.u64 = ctx.r10.u64 + r23.u64;
	// add r27,r11,r25
	r27.u64 = r11.u64 + r25.u64;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b0c4
	if (cr6.eq) goto loc_8263B0C4;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r26,r11,0
	r26.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b0b0
	if (!cr0.lt) goto loc_8263B0B0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B0B0:
	// extsb r11,r26
	r11.s64 = r26.s8;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwimi r11,r10,0,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// b 0x8263b0cc
	goto loc_8263B0CC;
loc_8263B0C4:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
loc_8263B0CC:
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// blt cr6,0x8263b050
	if (cr6.lt) goto loc_8263B050;
	// b 0x8263bdd0
	goto loc_8263BDD0;
loc_8263B0FC:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x8263b2cc
	if (!cr6.eq) goto loc_8263B2CC;
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,2376(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 2376);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b138
	if (cr6.eq) goto loc_8263B138;
	// lwz r5,352(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// bl 0x825ffc80
	sub_825FFC80(ctx, base);
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b144
	if (cr6.eq) goto loc_8263B144;
loc_8263B12C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_8263B138:
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// bl 0x82654158
	sub_82654158(ctx, base);
loc_8263B144:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// li r6,1
	ctx.r6.s64 = 1;
	// rlwinm r5,r14,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r17,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b1c4
	if (cr6.eq) goto loc_8263B1C4;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263b1c4
	if (!cr6.eq) goto loc_8263B1C4;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b1b0
	if (!cr0.lt) goto loc_8263B1B0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B1B0:
	// extsb r11,r29
	r11.s64 = r29.s8;
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwimi r11,r10,0,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
loc_8263B1C4:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263b260
	if (!cr6.eq) goto loc_8263B260;
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b210
	if (cr6.eq) goto loc_8263B210;
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263b210
	if (cr6.eq) goto loc_8263B210;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263B210:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263bd8c
	if (cr6.eq) goto loc_8263BD8C;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b24c
	if (!cr0.lt) goto loc_8263B24C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B24C:
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
	// b 0x8263bd8c
	goto loc_8263BD8C;
loc_8263B260:
	// lwz r10,328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// lwz r27,392(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263b280
	if (cr6.eq) goto loc_8263B280;
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// li r26,1
	r26.s64 = 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8263b284
	if (cr6.eq) goto loc_8263B284;
loc_8263B280:
	// mr r26,r30
	r26.u64 = r30.u64;
loc_8263B284:
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263b2cc
	if (cr6.eq) goto loc_8263B2CC;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b2bc
	if (!cr0.lt) goto loc_8263B2BC;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B2BC:
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
loc_8263B2CC:
	// lwz r11,2140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2140);
	// ld r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r17,r11
	r17.s64 = r11.s16;
	// lis r11,0
	r11.s64 = 0;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ori r14,r11,32768
	r14.u64 = r11.u64 | 32768;
	// blt cr6,0x8263b3c4
	if (cr6.lt) goto loc_8263B3C4;
	// clrlwi r11,r17,28
	r11.u64 = r17.u32 & 0xF;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8263b3bc
	if (!cr6.lt) goto loc_8263B3BC;
loc_8263B324:
	// lwz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r11,12(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8263b350
	if (cr6.lt) goto loc_8263B350;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8263b324
	if (cr6.eq) goto loc_8263B324;
	// srawi r17,r17,4
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0xF) != 0);
	r17.s64 = r17.s32 >> 4;
	// b 0x8263b400
	goto loc_8263B400;
loc_8263B350:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// ld r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r24)
	PPC_STORE_U32(r24.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r24)
	PPC_STORE_U64(r24.u32 + 0, r11.u64);
loc_8263B3BC:
	// srawi r17,r17,4
	xer.ca = (r17.s32 < 0) & ((r17.u32 & 0xF) != 0);
	r17.s64 = r17.s32 >> 4;
	// b 0x8263b400
	goto loc_8263B400;
loc_8263B3C4:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8263B3CC:
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r28,r11,r17
	r28.u64 = r11.u64 + r17.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r28,r14
	r11.u64 = r28.u64 + r14.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r17,r11
	r17.s64 = r11.s16;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// blt cr6,0x8263b3cc
	if (cr6.lt) goto loc_8263B3CC;
loc_8263B400:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// mr r19,r17
	r19.u64 = r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b438
	if (cr6.eq) goto loc_8263B438;
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x8263b438
	if (!cr6.eq) goto loc_8263B438;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263B438:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// cmplwi cr6,r20,1
	cr6.compare<uint32_t>(r20.u32, 1, xer);
	// rlwinm r11,r11,0,2,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
	// bne cr6,0x8263b7bc
	if (!cr6.eq) goto loc_8263B7BC;
	// lwz r29,88(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r21,r17,0,26,27
	r21.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x30;
	// lwz r16,300(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mr r22,r30
	r22.u64 = r30.u64;
	// lwz r19,292(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r20,r30
	r20.u64 = r30.u64;
	// mr r18,r30
	r18.u64 = r30.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// mr r23,r30
	r23.u64 = r30.u64;
loc_8263B470:
	// li r11,1
	r11.s64 = 1;
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b4c4
	if (cr6.eq) goto loc_8263B4C4;
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,2376(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 2376);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b4b4
	if (cr6.eq) goto loc_8263B4B4;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// add r5,r23,r11
	ctx.r5.u64 = r23.u64 + r11.u64;
	// bl 0x825ffc80
	sub_825FFC80(ctx, base);
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
	// b 0x8263b4c8
	goto loc_8263B4C8;
loc_8263B4B4:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// bl 0x82654158
	sub_82654158(ctx, base);
	// b 0x8263b4c8
	goto loc_8263B4C8;
loc_8263B4C4:
	// stw r30,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r30.u32);
loc_8263B4C8:
	// clrlwi r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r10,r19,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r16,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r9.s64 = r28.s32 >> 1;
	// add r26,r11,r10
	r26.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r7,0,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// add r27,r9,r8
	r27.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b560
	if (cr6.eq) goto loc_8263B560;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263b560
	if (!cr6.eq) goto loc_8263B560;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r25,r11,0
	r25.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b54c
	if (!cr0.lt) goto loc_8263B54C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B54C:
	// extsb r11,r25
	r11.s64 = r25.s8;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwimi r11,r10,0,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_8263B560:
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x8263b598
	if (cr6.eq) goto loc_8263B598;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8262a978
	sub_8262A978(ctx, base);
	// or r18,r3,r18
	r18.u64 = ctx.r3.u64 | r18.u64;
loc_8263B598:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// slw r10,r25,r28
	ctx.r10.u64 = r28.u8 & 0x20 ? 0 : (r25.u32 << (r28.u8 & 0x3F));
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// rlwinm r11,r11,29,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1;
	// add r20,r25,r20
	r20.u64 = r25.u64 + r20.u64;
	// or r22,r10,r22
	r22.u64 = ctx.r10.u64 | r22.u64;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpwi cr6,r23,16
	cr6.compare<int32_t>(r23.s32, 16, xer);
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// or r21,r11,r21
	r21.u64 = r11.u64 | r21.u64;
	// blt cr6,0x8263b470
	if (cr6.lt) goto loc_8263B470;
	// cmpwi cr6,r20,3
	cr6.compare<int32_t>(r20.s32, 3, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// blt cr6,0x8263b5d8
	if (cr6.lt) goto loc_8263B5D8;
	// li r11,48
	r11.s64 = 48;
loc_8263B5D8:
	// lwz r10,392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// or r11,r11,r22
	r11.u64 = r11.u64 | r22.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263b5f4
	if (cr6.eq) goto loc_8263B5F4;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// li r27,1
	r27.s64 = 1;
	// bne cr6,0x8263b5f8
	if (!cr6.eq) goto loc_8263B5F8;
loc_8263B5F4:
	// mr r27,r30
	r27.u64 = r30.u64;
loc_8263B5F8:
	// lwz r10,328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263b614
	if (cr6.eq) goto loc_8263B614;
	// andc r11,r21,r11
	r11.u64 = r21.u64 & ~r11.u64;
	// li r26,1
	r26.s64 = 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b618
	if (!cr6.eq) goto loc_8263B618;
loc_8263B614:
	// mr r26,r30
	r26.u64 = r30.u64;
loc_8263B618:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8263b62c
	if (!cr6.eq) goto loc_8263B62C;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// mr r29,r30
	r29.u64 = r30.u64;
	// beq cr6,0x8263b630
	if (cr6.eq) goto loc_8263B630;
loc_8263B62C:
	// li r29,1
	r29.s64 = 1;
loc_8263B630:
	// cmpwi cr6,r20,2
	cr6.compare<int32_t>(r20.s32, 2, xer);
	// ble cr6,0x8263b64c
	if (!cr6.gt) goto loc_8263B64C;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8262aa08
	sub_8262AA08(ctx, base);
	// or r18,r3,r18
	r18.u64 = ctx.r3.u64 | r18.u64;
loc_8263B64C:
	// lwz r11,280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// mr r19,r21
	r19.u64 = r21.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b678
	if (cr6.eq) goto loc_8263B678;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8263b678
	if (cr6.eq) goto loc_8263B678;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263B678:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// beq cr6,0x8263b6bc
	if (cr6.eq) goto loc_8263B6BC;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b6ac
	if (!cr0.lt) goto loc_8263B6AC;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B6AC:
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
loc_8263B6BC:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x8263b738
	if (cr6.eq) goto loc_8263B738;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b6f0
	if (!cr0.lt) goto loc_8263B6F0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B6F0:
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8263b72c
	if (cr6.eq) goto loc_8263B72C;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b728
	if (!cr0.lt) goto loc_8263B728;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B728:
	// add r11,r29,r28
	r11.u64 = r29.u64 + r28.u64;
loc_8263B72C:
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwimi r10,r11,22,8,9
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r10.u64 & 0xFFFFFFFFFF3FFFFF);
loc_8263B734:
	// stw r10,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r10.u32);
loc_8263B738:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x8263bd7c
	if (cr6.eq) goto loc_8263BD7C;
	// lwz r11,2516(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2516);
	// ld r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8263bce4
	if (cr6.lt) goto loc_8263BCE4;
	// clrlwi r11,r29,28
	r11.u64 = r29.u32 & 0xF;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8263bcdc
	if (!cr6.lt) goto loc_8263BCDC;
loc_8263B790:
	// lwz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r11,12(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8263bc70
	if (cr6.lt) goto loc_8263BC70;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8263b790
	if (cr6.eq) goto loc_8263B790;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8263bd20
	goto loc_8263BD20;
loc_8263B7BC:
	// rlwinm r10,r11,0,21,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r10,1280
	cr6.compare<uint32_t>(ctx.r10.u32, 1280, xer);
	// bne cr6,0x8263ba2c
	if (!cr6.eq) goto loc_8263BA2C;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// andi. r19,r17,58
	r19.u64 = r17.u64 & 58;
	cr0.compare<int32_t>(r19.s32, 0, xer);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r20,r30
	r20.u64 = r30.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// rlwinm r22,r10,1,0,30
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mr r29,r30
	r29.u64 = r30.u64;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// rlwinm r21,r10,1,0,30
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// li r16,1
	r16.s64 = 1;
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
loc_8263B800:
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r10,r28,31
	ctx.r10.u64 = r28.u32 & 0x1;
	// add r26,r11,r21
	r26.u64 = r11.u64 + r21.u64;
	// add r27,r10,r22
	r27.u64 = ctx.r10.u64 + r22.u64;
	// slw r11,r16,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r16.u32 << (r28.u8 & 0x3F));
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mullw r11,r26,r9
	r11.s64 = int64_t(r26.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r23,r11,r27
	r23.u64 = r11.u64 + r27.u64;
	// beq cr6,0x8263b86c
	if (cr6.eq) goto loc_8263B86C;
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,2376(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 2376);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// beq cr6,0x8263b854
	if (cr6.eq) goto loc_8263B854;
	// add r5,r29,r11
	ctx.r5.u64 = r29.u64 + r11.u64;
	// bl 0x825ffc80
	sub_825FFC80(ctx, base);
	// b 0x8263b860
	goto loc_8263B860;
loc_8263B854:
	// li r5,8
	ctx.r5.s64 = 8;
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// bl 0x82654158
	sub_82654158(ctx, base);
loc_8263B860:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263B86C:
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b8e8
	if (cr6.eq) goto loc_8263B8E8;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263b8e8
	if (!cr6.eq) goto loc_8263B8E8;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r25,r11,0
	r25.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263b8d0
	if (!cr0.lt) goto loc_8263B8D0;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263B8D0:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// extsb r10,r25
	ctx.r10.s64 = r25.s8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r9,r29,r11
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwimi r10,r9,0,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// stwx r10,r29,r11
	PPC_STORE_U32(r29.u32 + r11.u32, ctx.r10.u32);
loc_8263B8E8:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r11,r23,1,0,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// sth r9,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r9.u16);
	// lwz r10,1776(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// sth r10,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r10.u16);
	// beq cr6,0x8263b94c
	if (cr6.eq) goto loc_8263B94C;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// slw r10,r18,r28
	ctx.r10.u64 = r28.u8 & 0x20 ? 0 : (r18.u32 << (r28.u8 & 0x3F));
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// or r20,r10,r20
	r20.u64 = ctx.r10.u64 | r20.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// ori r10,r10,4
	ctx.r10.u64 = ctx.r10.u64 | 4;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8263B94C:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// rlwinm r11,r11,29,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1;
	// cmpwi cr6,r29,16
	cr6.compare<int32_t>(r29.s32, 16, xer);
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// or r19,r11,r19
	r19.u64 = r11.u64 | r19.u64;
	// blt cr6,0x8263b800
	if (cr6.lt) goto loc_8263B800;
	// cmpwi cr6,r20,15
	cr6.compare<int32_t>(r20.s32, 15, xer);
	// bne cr6,0x8263b97c
	if (!cr6.eq) goto loc_8263B97C;
	// li r20,63
	r20.s64 = 63;
loc_8263B97C:
	// lwz r11,328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b998
	if (cr6.eq) goto loc_8263B998;
	// andc r11,r19,r20
	r11.u64 = r19.u64 & ~r20.u64;
	// mr r26,r16
	r26.u64 = r16.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b99c
	if (!cr6.eq) goto loc_8263B99C;
loc_8263B998:
	// mr r26,r30
	r26.u64 = r30.u64;
loc_8263B99C:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x8263b9b0
	if (!cr6.eq) goto loc_8263B9B0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// beq cr6,0x8263b9b4
	if (cr6.eq) goto loc_8263B9B4;
loc_8263B9B0:
	// mr r11,r16
	r11.u64 = r16.u64;
loc_8263B9B4:
	// lwz r10,280(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// li r3,-100
	ctx.r3.s64 = -100;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_8263B9E8:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8263b738
	if (cr6.eq) goto loc_8263B738;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263ba1c
	if (!cr0.lt) goto loc_8263BA1C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263BA1C:
	// lwz r10,0(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// b 0x8263b734
	goto loc_8263B734;
loc_8263BA2C:
	// rlwinm r11,r11,0,21,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x700;
	// cmplwi cr6,r11,1536
	cr6.compare<uint32_t>(r11.u32, 1536, xer);
	// bne cr6,0x8263b6bc
	if (!cr6.eq) goto loc_8263B6BC;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// rlwinm r19,r17,0,26,29
	r19.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 0) & 0x3C;
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r20,r30
	r20.u64 = r30.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// rlwinm r22,r10,1,0,30
	r22.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mr r29,r30
	r29.u64 = r30.u64;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// rlwinm r21,r10,1,0,30
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// li r18,1
	r18.s64 = 1;
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
loc_8263BA70:
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r10,r28,31
	ctx.r10.u64 = r28.u32 & 0x1;
	// add r25,r11,r21
	r25.u64 = r11.u64 + r21.u64;
	// add r26,r10,r22
	r26.u64 = ctx.r10.u64 + r22.u64;
	// slw r11,r18,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r18.u32 << (r28.u8 & 0x3F));
	// and r11,r11,r17
	r11.u64 = r11.u64 & r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mullw r11,r25,r9
	r11.s64 = int64_t(r25.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r27,r11,r26
	r27.u64 = r11.u64 + r26.u64;
	// beq cr6,0x8263badc
	if (cr6.eq) goto loc_8263BADC;
	// lwz r11,21596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21596);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,2376(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 2376);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// beq cr6,0x8263bac4
	if (cr6.eq) goto loc_8263BAC4;
	// add r5,r29,r11
	ctx.r5.u64 = r29.u64 + r11.u64;
	// bl 0x825ffc80
	sub_825FFC80(ctx, base);
	// b 0x8263bad0
	goto loc_8263BAD0;
loc_8263BAC4:
	// li r5,8
	ctx.r5.s64 = 8;
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// bl 0x82654158
	sub_82654158(ctx, base);
loc_8263BAD0:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
loc_8263BADC:
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82654740
	sub_82654740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263bb58
	if (cr6.eq) goto loc_8263BB58;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263bb58
	if (!cr6.eq) goto loc_8263BB58;
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r23,r11,0
	r23.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r24)
	PPC_STORE_U64(r24.u32 + 0, ctx.r9.u64);
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// bge 0x8263bb40
	if (!cr0.lt) goto loc_8263BB40;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263BB40:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// extsb r10,r23
	ctx.r10.s64 = r23.s8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r9,r29,r11
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwimi r10,r9,0,0,29
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFC) | (ctx.r10.u64 & 0xFFFFFFFF00000003);
	// stwx r10,r29,r11
	PPC_STORE_U32(r29.u32 + r11.u32, ctx.r10.u32);
loc_8263BB58:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r10,r27,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// lhzx r8,r11,r10
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r8,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r8.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r11,1776(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// lhzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// beq cr6,0x8263bbd4
	if (cr6.eq) goto loc_8263BBD4;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// slw r10,r16,r28
	ctx.r10.u64 = r28.u8 & 0x20 ? 0 : (r16.u32 << (r28.u8 & 0x3F));
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// or r20,r10,r20
	r20.u64 = ctx.r10.u64 | r20.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// ori r10,r10,4
	ctx.r10.u64 = ctx.r10.u64 | 4;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_8263BBD4:
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// rlwinm r11,r11,29,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// slw r11,r11,r28
	r11.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// or r19,r11,r19
	r19.u64 = r11.u64 | r19.u64;
	// blt cr6,0x8263ba70
	if (cr6.lt) goto loc_8263BA70;
	// cmpwi cr6,r20,15
	cr6.compare<int32_t>(r20.s32, 15, xer);
	// bne cr6,0x8263bc04
	if (!cr6.eq) goto loc_8263BC04;
	// li r20,63
	r20.s64 = 63;
loc_8263BC04:
	// lwz r11,328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263bc20
	if (cr6.eq) goto loc_8263BC20;
	// andc r11,r19,r20
	r11.u64 = r19.u64 & ~r20.u64;
	// mr r26,r18
	r26.u64 = r18.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263bc24
	if (!cr6.eq) goto loc_8263BC24;
loc_8263BC20:
	// mr r26,r30
	r26.u64 = r30.u64;
loc_8263BC24:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bne cr6,0x8263bc38
	if (!cr6.eq) goto loc_8263BC38;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// beq cr6,0x8263bc3c
	if (cr6.eq) goto loc_8263BC3C;
loc_8263BC38:
	// mr r11,r18
	r11.u64 = r18.u64;
loc_8263BC3C:
	// lwz r10,280(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 280);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8263b9e8
	if (cr6.eq) goto loc_8263B9E8;
	// li r3,-100
	ctx.r3.s64 = -100;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_8263BC70:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// ld r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r24)
	PPC_STORE_U32(r24.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r24)
	PPC_STORE_U64(r24.u32 + 0, r11.u64);
loc_8263BCDC:
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// b 0x8263bd20
	goto loc_8263BD20;
loc_8263BCE4:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8263BCEC:
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r14
	r11.u64 = r29.u64 + r14.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r29,r11
	r29.s64 = r11.s16;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8263bcec
	if (cr6.lt) goto loc_8263BCEC;
loc_8263BD20:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263b12c
	if (!cr6.eq) goto loc_8263B12C;
	// cmpwi cr6,r29,8
	cr6.compare<int32_t>(r29.s32, 8, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x8263bd3c
	if (cr6.lt) goto loc_8263BD3C;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_8263BD3C:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwimi r11,r9,28,3,3
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 28) & 0x10000000) | (r11.u64 & 0xFFFFFFFFEFFFFFFF);
	// lis r9,-32138
	ctx.r9.s64 = -2106195968;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// addi r9,r9,15752
	ctx.r9.s64 = ctx.r9.s64 + 15752;
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
	// addi r8,r9,64
	ctx.r8.s64 = ctx.r9.s64 + 64;
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwimi r7,r11,24,5,7
	ctx.r7.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0x7000000) | (ctx.r7.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r7,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r7.u32);
	// lwzx r11,r10,r8
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// rlwimi r10,r11,20,10,11
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 20) & 0x300000) | (ctx.r10.u64 & 0xFFFFFFFFFFCFFFFF);
	// rlwinm r11,r10,0,5,3
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
loc_8263BD7C:
	// lwz r17,292(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r14,300(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r20,92(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8263BD8C:
	// clrlwi r10,r19,31
	ctx.r10.u64 = r19.u32 & 0x1;
	// srawi r11,r19,1
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x1) != 0);
	r11.s64 = r19.s32 >> 1;
	// stb r10,12(r15)
	PPC_STORE_U8(r15.u32 + 12, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,13(r15)
	PPC_STORE_U8(r15.u32 + 13, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,14(r15)
	PPC_STORE_U8(r15.u32 + 14, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stb r10,15(r15)
	PPC_STORE_U8(r15.u32 + 15, ctx.r10.u8);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// stb r10,16(r15)
	PPC_STORE_U8(r15.u32 + 16, ctx.r10.u8);
	// stb r11,17(r15)
	PPC_STORE_U8(r15.u32 + 17, r11.u8);
loc_8263BDD0:
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x8263be48
	if (!cr6.eq) goto loc_8263BE48;
loc_8263BDD8:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// rlwinm r5,r14,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r14.u32 | (r14.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r17,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82628ff0
	sub_82628FF0(ctx, base);
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,1776(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1776);
	// mullw r9,r11,r14
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(r14.s32);
	// add r7,r9,r17
	ctx.r7.u64 = ctx.r9.u64 + r17.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r7,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r11,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r11.u16);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r10,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r10.u16);
	// sth r11,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, r11.u16);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// sth r11,-2(r7)
	PPC_STORE_U16(ctx.r7.u32 + -2, r11.u16);
	// sth r10,-2(r9)
	PPC_STORE_U16(ctx.r9.u32 + -2, ctx.r10.u16);
loc_8263BE48:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 20);
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r3,r11,0,29,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8263BE60"))) PPC_WEAK_FUNC(sub_8263BE60);
PPC_FUNC_IMPL(__imp__sub_8263BE60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lbz r4,2124(r25)
	ctx.r4.u64 = PPC_LOAD_U8(r25.u32 + 2124);
	// lwz r29,2116(r25)
	r29.u64 = PPC_LOAD_U32(r25.u32 + 2116);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8263c3dc
	if (cr6.lt) goto loc_8263C3DC;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x8263bf60
	if (!cr6.lt) goto loc_8263BF60;
loc_8263BECC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8263bef4
	if (cr6.lt) goto loc_8263BEF4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8263becc
	if (cr6.eq) goto loc_8263BECC;
	// b 0x8263bf60
	goto loc_8263BF60;
loc_8263BEF4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8263BF60:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8263c424
	if (cr6.lt) goto loc_8263C424;
loc_8263BF6C:
	// cmpwi cr6,r30,63
	cr6.compare<int32_t>(r30.s32, 63, xer);
	// bgt cr6,0x8263c424
	if (cr6.gt) goto loc_8263C424;
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263c424
	if (!cr6.eq) goto loc_8263C424;
	// srawi r11,r30,5
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1F) != 0);
	r11.s64 = r30.s32 >> 5;
	// srawi r10,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	ctx.r10.s64 = r30.s32 >> 4;
	// srawi r9,r30,3
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7) != 0);
	ctx.r9.s64 = r30.s32 >> 3;
	// srawi r8,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	ctx.r8.s64 = r30.s32 >> 2;
	// srawi r7,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r7.s64 = r30.s32 >> 1;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// clrlwi r8,r8,31
	ctx.r8.u64 = ctx.r8.u32 & 0x1;
	// clrlwi r7,r7,31
	ctx.r7.u64 = ctx.r7.u32 & 0x1;
	// clrlwi r31,r30,31
	r31.u64 = r30.u32 & 0x1;
	// stb r11,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r11.u8);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// stb r10,13(r24)
	PPC_STORE_U8(r24.u32 + 13, ctx.r10.u8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stb r9,14(r24)
	PPC_STORE_U8(r24.u32 + 14, ctx.r9.u8);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// stb r8,15(r24)
	PPC_STORE_U8(r24.u32 + 15, ctx.r8.u8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stb r7,16(r24)
	PPC_STORE_U8(r24.u32 + 16, ctx.r7.u8);
	// stb r31,17(r24)
	PPC_STORE_U8(r24.u32 + 17, r31.u8);
	// bl 0x82655a60
	sub_82655A60(ctx, base);
	// lwz r11,15472(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 15472);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x8263bff4
	if (!cr6.eq) goto loc_8263BFF4;
	// lwz r11,20004(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 20004);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263c0b4
	if (!cr6.eq) goto loc_8263C0B4;
loc_8263BFF4:
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263c068
	if (!cr6.lt) goto loc_8263C068;
loc_8263C010:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c068
	if (cr6.eq) goto loc_8263C068;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263c058
	if (!cr0.lt) goto loc_8263C058;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C058:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263c010
	if (cr6.gt) goto loc_8263C010;
loc_8263C068:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263c0a4
	if (!cr0.lt) goto loc_8263C0A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C0A4:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
loc_8263C0B4:
	// lwz r11,84(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263c424
	if (!cr6.eq) goto loc_8263C424;
	// lis r12,32573
	r12.s64 = 2134704128;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// lbz r11,12(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 12);
	// lbz r10,13(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 13);
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// lbz r9,14(r24)
	ctx.r9.u64 = PPC_LOAD_U8(r24.u32 + 14);
	// lbz r8,15(r24)
	ctx.r8.u64 = PPC_LOAD_U8(r24.u32 + 15);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// lbz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U8(r24.u32 + 16);
	// lbz r7,17(r24)
	ctx.r7.u64 = PPC_LOAD_U8(r24.u32 + 17);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,392(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 392);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// beq cr6,0x8263c2a8
	if (cr6.eq) goto loc_8263C2A8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263c2a8
	if (cr6.eq) goto loc_8263C2A8;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263c1a0
	if (!cr6.lt) goto loc_8263C1A0;
loc_8263C148:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c1a0
	if (cr6.eq) goto loc_8263C1A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263c190
	if (!cr0.lt) goto loc_8263C190;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C190:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263c148
	if (cr6.gt) goto loc_8263C148;
loc_8263C1A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263c1dc
	if (!cr0.lt) goto loc_8263C1DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C1DC:
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8263c29c
	if (cr6.eq) goto loc_8263C29C;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263c25c
	if (!cr6.lt) goto loc_8263C25C;
loc_8263C204:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c25c
	if (cr6.eq) goto loc_8263C25C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263c24c
	if (!cr0.lt) goto loc_8263C24C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C24C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263c204
	if (cr6.gt) goto loc_8263C204;
loc_8263C25C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263c298
	if (!cr0.lt) goto loc_8263C298;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C298:
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
loc_8263C29C:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwimi r10,r11,22,8,9
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 22) & 0xC00000) | (ctx.r10.u64 & 0xFFFFFFFFFF3FFFFF);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
loc_8263C2A8:
	// lwz r11,2968(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 2968);
	// rlwinm r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263c38c
	if (cr6.eq) goto loc_8263C38C;
	// lwz r11,20940(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 20940);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8263c38c
	if (!cr6.eq) goto loc_8263C38C;
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x8263c338
	if (!cr6.lt) goto loc_8263C338;
loc_8263C2E0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c338
	if (cr6.eq) goto loc_8263C338;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8263c328
	if (!cr0.lt) goto loc_8263C328;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C328:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8263c2e0
	if (cr6.gt) goto loc_8263C2E0;
loc_8263C338:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8263c374
	if (!cr0.lt) goto loc_8263C374;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8263C374:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// rlwimi r10,r11,11,20,20
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 11) & 0x800) | (ctx.r10.u64 & 0xFFFFFFFFFFFFF7FF);
	// stw r10,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r10.u32);
loc_8263C38C:
	// lwz r11,20056(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 20056);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263c3d0
	if (cr6.eq) goto loc_8263C3D0;
	// lwz r11,248(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 248);
	// lwz r10,252(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 252);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// stb r11,4(r24)
	PPC_STORE_U8(r24.u32 + 4, r11.u8);
	// lwz r11,280(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8263c3d0
	if (cr6.eq) goto loc_8263C3D0;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8263aa18
	sub_8263AA18(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8263c424
	if (!cr6.eq) goto loc_8263C424;
loc_8263C3D0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8263C3DC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_8263C3EC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8263c3ec
	if (cr6.lt) goto loc_8263C3EC;
	// b 0x8263bf6c
	goto loc_8263BF6C;
loc_8263C424:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8263C430"))) PPC_WEAK_FUNC(sub_8263C430);
PPC_FUNC_IMPL(__imp__sub_8263C430) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8263c494
	if (cr6.eq) goto loc_8263C494;
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8263c468
	if (cr6.eq) goto loc_8263C468;
loc_8263C454:
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x82604090
	sub_82604090(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8263c454
	if (!cr6.eq) goto loc_8263C454;
loc_8263C468:
	// li r29,0
	r29.s64 = 0;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// beq cr6,0x8263c490
	if (cr6.eq) goto loc_8263C490;
loc_8263C47C:
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x82604090
	sub_82604090(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x8263c47c
	if (!cr6.eq) goto loc_8263C47C;
loc_8263C490:
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
loc_8263C494:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8263C49C"))) PPC_WEAK_FUNC(sub_8263C49C);
PPC_FUNC_IMPL(__imp__sub_8263C49C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263C4A0"))) PPC_WEAK_FUNC(sub_8263C4A0);
PPC_FUNC_IMPL(__imp__sub_8263C4A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8263c4b4
	if (!cr6.eq) goto loc_8263C4B4;
loc_8263C4AC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8263C4B4:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// beq cr6,0x8263c4ac
	if (cr6.eq) goto loc_8263C4AC;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263c4ac
	if (!cr6.lt) goto loc_8263C4AC;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8263c4f4
	if (!cr6.eq) goto loc_8263C4F4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bne cr6,0x8263c558
	if (!cr6.eq) goto loc_8263C558;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// b 0x8263c558
	goto loc_8263C558;
loc_8263C4F4:
	// cmpwi cr6,r5,-1
	cr6.compare<int32_t>(ctx.r5.s32, -1, xer);
	// bne cr6,0x8263c510
	if (!cr6.eq) goto loc_8263C510;
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x8263c510
	if (!cr6.eq) goto loc_8263C510;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x8263c558
	goto loc_8263C558;
loc_8263C510:
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// cmpwi cr6,r5,-1
	cr6.compare<int32_t>(ctx.r5.s32, -1, xer);
	// bne cr6,0x8263c520
	if (!cr6.eq) goto loc_8263C520;
	// addi r10,r9,-1
	ctx.r10.s64 = ctx.r9.s64 + -1;
loc_8263C520:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8263c540
	if (!cr6.gt) goto loc_8263C540;
loc_8263C530:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263c530
	if (!cr6.eq) goto loc_8263C530;
loc_8263C540:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r5,-1
	cr6.compare<int32_t>(ctx.r5.s32, -1, xer);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// bne cr6,0x8263c558
	if (!cr6.eq) goto loc_8263C558;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
loc_8263C558:
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8263c57c
	if (!cr6.eq) goto loc_8263C57C;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
loc_8263C57C:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263C590"))) PPC_WEAK_FUNC(sub_8263C590);
PPC_FUNC_IMPL(__imp__sub_8263C590) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c6b0
	if (cr6.eq) goto loc_8263C6B0;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8263c6b0
	if (!cr6.gt) goto loc_8263C6B0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bne cr6,0x8263c5d4
	if (!cr6.eq) goto loc_8263C5D4;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
loc_8263C5D4:
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r31,4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8263c5f4
	if (!cr6.eq) goto loc_8263C5F4;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
loc_8263C5F4:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// beq cr6,0x8263c6b0
	if (cr6.eq) goto loc_8263C6B0;
loc_8263C608:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8263c61c
	if (cr6.eq) goto loc_8263C61C;
	// bl 0x826040a0
	sub_826040A0(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
loc_8263C61C:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8263c630
	if (cr6.eq) goto loc_8263C630;
	// bl 0x826040a0
	sub_826040A0(ctx, base);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
loc_8263C630:
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8263c644
	if (cr6.eq) goto loc_8263C644;
	// bl 0x826040a0
	sub_826040A0(ctx, base);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
loc_8263C644:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c6b0
	if (cr6.eq) goto loc_8263C6B0;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8263c6b0
	if (!cr6.gt) goto loc_8263C6B0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bne cr6,0x8263c67c
	if (!cr6.eq) goto loc_8263C67C;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
loc_8263C67C:
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r31,4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8263c69c
	if (!cr6.eq) goto loc_8263C69C;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
loc_8263C69C:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// bne cr6,0x8263c608
	if (!cr6.eq) goto loc_8263C608;
loc_8263C6B0:
	// lwz r3,12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// bl 0x8263c430
	sub_8263C430(ctx, base);
	// lwz r3,12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8263c6cc
	if (cr6.eq) goto loc_8263C6CC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r30.u32);
loc_8263C6CC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8263C6D4"))) PPC_WEAK_FUNC(sub_8263C6D4);
PPC_FUNC_IMPL(__imp__sub_8263C6D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263C6D8"))) PPC_WEAK_FUNC(sub_8263C6D8);
PPC_FUNC_IMPL(__imp__sub_8263C6D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263c71c
	if (cr6.eq) goto loc_8263C71C;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r5,r9
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r9.s32, xer);
	// bgt cr6,0x8263c71c
	if (cr6.gt) goto loc_8263C71C;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x8263c70c
	if (!cr6.lt) goto loc_8263C70C;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bne cr6,0x8263c718
	if (!cr6.eq) goto loc_8263C718;
loc_8263C70C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_8263C718:
	// b 0x8263c4a0
	sub_8263C4A0(ctx, base);
	return;
loc_8263C71C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263C728"))) PPC_WEAK_FUNC(sub_8263C728);
PPC_FUNC_IMPL(__imp__sub_8263C728) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lwz r11,12(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c7a8
	if (cr6.eq) goto loc_8263C7A8;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x8263c7a8
	if (cr6.eq) goto loc_8263C7A8;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x8263c7a8
	if (cr6.lt) goto loc_8263C7A8;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// bne cr6,0x8263c768
	if (!cr6.eq) goto loc_8263C768;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r9.u32);
loc_8263C768:
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8263c788
	if (!cr6.eq) goto loc_8263C788;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8263C788:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// blr 
	return;
loc_8263C7A8:
	// li r3,-100
	ctx.r3.s64 = -100;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263C7B0"))) PPC_WEAK_FUNC(sub_8263C7B0);
PPC_FUNC_IMPL(__imp__sub_8263C7B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8263c7d8
	if (!cr6.eq) goto loc_8263C7D8;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
loc_8263C7D8:
	// li r31,0
	r31.s64 = 0;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// li r10,6
	ctx.r10.s64 = 6;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_8263C7EC:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x8263c7ec
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263C7EC;
	// addi r29,r28,8
	r29.s64 = r28.s64 + 8;
	// stw r27,20(r28)
	PPC_STORE_U32(r28.u32 + 20, r27.u32);
	// mr r30,r31
	r30.u64 = r31.u64;
	// stw r31,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r31.u32);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x8263c860
	if (!cr6.gt) goto loc_8263C860;
loc_8263C810:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c84c
	if (cr6.eq) goto loc_8263C84C;
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// stw r31,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r31.u32);
	// stw r31,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r31.u32);
	// cmpw cr6,r30,r27
	cr6.compare<int32_t>(r30.s32, r27.s32, xer);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// mr r29,r11
	r29.u64 = r11.u64;
	// blt cr6,0x8263c810
	if (cr6.lt) goto loc_8263C810;
	// b 0x8263c864
	goto loc_8263C864;
loc_8263C84C:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8263c430
	sub_8263C430(ctx, base);
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
loc_8263C860:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_8263C864:
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
	// stw r31,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r31.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_8263C880"))) PPC_WEAK_FUNC(sub_8263C880);
PPC_FUNC_IMPL(__imp__sub_8263C880) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// stw r23,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r23.u32);
	// stw r23,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r23.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263c8e4
	if (!cr6.eq) goto loc_8263C8E4;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,24
	ctx.r3.s64 = 24;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,12(r28)
	PPC_STORE_U32(r28.u32 + 12, ctx.r3.u32);
	// beq cr6,0x8263c998
	if (cr6.eq) goto loc_8263C998;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8263c7b0
	sub_8263C7B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8263c99c
	if (!cr6.eq) goto loc_8263C99C;
loc_8263C8E4:
	// mr r29,r23
	r29.u64 = r23.u64;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x8263c98c
	if (!cr6.gt) goto loc_8263C98C;
loc_8263C8F0:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,636
	ctx.r3.s64 = 636;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8263c998
	if (cr6.eq) goto loc_8263C998;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82606928
	sub_82606928(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x8263c99c
	if (!cr6.eq) goto loc_8263C99C;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8263c998
	if (cr6.eq) goto loc_8263C998;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x8263c998
	if (cr6.lt) goto loc_8263C998;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// bne cr6,0x8263c954
	if (!cr6.eq) goto loc_8263C954;
	// stw r23,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r23.u32);
loc_8263C954:
	// stw r30,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r30.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8263c974
	if (!cr6.eq) goto loc_8263C974;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8263C974:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r29,r27
	cr6.compare<int32_t>(r29.s32, r27.s32, xer);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// blt cr6,0x8263c8f0
	if (cr6.lt) goto loc_8263C8F0;
loc_8263C98C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_8263C998:
	// li r31,2
	r31.s64 = 2;
loc_8263C99C:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8263c590
	sub_8263C590(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8263C9B0"))) PPC_WEAK_FUNC(sub_8263C9B0);
PPC_FUNC_IMPL(__imp__sub_8263C9B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stfd f29,-176(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -176, f29.u64);
	// stfd f30,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// fmr f29,f1
	f29.f64 = ctx.f1.f64;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// bne cr6,0x8263c9f4
	if (!cr6.eq) goto loc_8263C9F4;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lfd f29,-176(r1)
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lfd f30,-168(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8239bd10
	return;
loc_8263C9F4:
	// lwz r11,15344(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15344);
	// fneg f0,f29
	ctx.fpscr.disableFlushMode();
	f0.u64 = f29.u64 ^ 0x8000000000000000;
	// lwz r10,20(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,15348(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15348);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f31,-31368(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// srawi r29,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r29.s64 = r11.s32 >> 1;
	// stw r29,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r29.u32);
	// bge cr6,0x8263ca34
	if (!cr6.lt) goto loc_8263CA34;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263CA34:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,92
	r11.s64 = ctx.r1.s64 + 92;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// lwz r10,15328(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r10,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r10.u64);
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x8263ca7c
	if (!cr6.lt) goto loc_8263CA7C;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263CA7C:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// lwz r10,15328(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// std r10,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r10.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 - f29.f64;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x8263cac4
	if (cr6.lt) goto loc_8263CAC4;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_8263CAC4:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f1,f0
	ctx.f1.f64 = double(f0.s64);
	// fcmpu cr6,f1,f31
	cr6.compare(ctx.f1.f64, f31.f64);
	// bge cr6,0x8263caf8
	if (!cr6.lt) goto loc_8263CAF8;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
loc_8263CAF8:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// fneg f0,f30
	ctx.fpscr.disableFlushMode();
	f0.u64 = f30.u64 ^ 0x8000000000000000;
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// fctiwz f13,f1
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bge cr6,0x8263cb18
	if (!cr6.lt) goto loc_8263CB18;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263CB18:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// lwz r11,15332(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15332);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x8263cb60
	if (cr6.lt) goto loc_8263CB60;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263CB60:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15332(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15332);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// lwz r11,15324(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15324);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f30
	ctx.f13.f64 = ctx.f13.f64 - f30.f64;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x8263cba8
	if (cr6.lt) goto loc_8263CBA8;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_8263CBA8:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r11.u64);
	// lfd f0,160(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bge cr6,0x8263cbdc
	if (!cr6.lt) goto loc_8263CBDC;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263CBDC:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r31,r11,0,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r19,r10,0,0,30
	r19.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r30,r11,0,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// stw r19,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r19.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// rlwinm r3,r11,0,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
	// bge cr6,0x8263cc38
	if (!cr6.lt) goto loc_8263CC38;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
loc_8263CC38:
	// addi r11,r30,-2
	r11.s64 = r30.s64 + -2;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// bge cr6,0x8263cc50
	if (!cr6.lt) goto loc_8263CC50;
	// stw r6,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r6.u32);
loc_8263CC50:
	// srawi r8,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r8.s64 = r31.s32 >> 1;
	// srawi r9,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r9.s64 = r30.s32 >> 1;
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// stw r8,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r8.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// bge cr6,0x8263cc74
	if (!cr6.lt) goto loc_8263CC74;
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
loc_8263CC74:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r5,15356(r21)
	ctx.r5.u64 = PPC_LOAD_U32(r21.u32 + 15356);
	// rlwinm r10,r19,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r7,15360(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 15360);
	// rlwinm r4,r31,7,0,24
	ctx.r4.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r14,15352(r21)
	r14.u64 = PPC_LOAD_U32(r21.u32 + 15352);
	// lfd f0,29136(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 29136);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// fmul f13,f30,f0
	ctx.f13.f64 = f30.f64 * f0.f64;
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// fmul f0,f29,f0
	f0.f64 = f29.f64 * f0.f64;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stfiwx f0,0,r28
	PPC_STORE_U32(r28.u32, f0.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// bge cr6,0x8263ccd4
	if (!cr6.lt) goto loc_8263CCD4;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
loc_8263CCD4:
	// srawi r4,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 7;
	// clrlwi r18,r11,25
	r18.u64 = r11.u32 & 0x7F;
	// clrlwi r17,r10,25
	r17.u64 = ctx.r10.u32 & 0x7F;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// stw r4,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r4.u32);
	// srawi r4,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	ctx.r4.s64 = r11.s32 >> 7;
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// srawi r4,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	ctx.r4.s64 = r11.s32 >> 8;
	// stw r4,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r4.u32);
	// srawi r4,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r4.s64 = r11.s32 >> 1;
	// srawi r28,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	r28.s64 = ctx.r10.s32 >> 8;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// clrlwi r23,r4,25
	r23.u64 = ctx.r4.u32 & 0x7F;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// li r4,128
	ctx.r4.s64 = 128;
	// stw r28,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r28.u32);
	// srawi r28,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r28.s64 = ctx.r10.s32 >> 1;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// clrlwi r22,r28,25
	r22.u64 = r28.u32 & 0x7F;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r16,r10,-1
	r16.s64 = ctx.r10.s64 + -1;
	// subfic r11,r22,128
	xer.ca = r22.u32 <= 128;
	r11.s64 = 128 - r22.s64;
	// subfic r10,r18,128
	xer.ca = r18.u32 <= 128;
	ctx.r10.s64 = 128 - r18.s64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mullw r15,r22,r23
	r15.s64 = int64_t(r22.s32) * int64_t(r23.s32);
	// subf r24,r23,r11
	r24.s64 = r11.s64 - r23.s64;
	// subf r20,r17,r10
	r20.s64 = ctx.r10.s64 - r17.s64;
	// ble cr6,0x8263cdf4
	if (!cr6.gt) goto loc_8263CDF4;
loc_8263CD44:
	// lwz r9,15328(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// mr r10,r14
	ctx.r10.u64 = r14.u64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8263cd70
	if (!cr6.gt) goto loc_8263CD70;
loc_8263CD58:
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263cd58
	if (cr6.lt) goto loc_8263CD58;
loc_8263CD70:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8263cd9c
	if (!cr6.gt) goto loc_8263CD9C;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// subf r9,r7,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_8263CD84:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r4,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r4.u8);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263cd84
	if (!cr6.eq) goto loc_8263CD84;
loc_8263CD9C:
	// lwz r10,15328(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// add r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 + r29.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + r29.u64;
	// add r9,r10,r14
	ctx.r9.u64 = ctx.r10.u64 + r14.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x8263cdd8
	if (!cr6.gt) goto loc_8263CDD8;
loc_8263CDC0:
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,15328(r21)
	r31.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// blt cr6,0x8263cdc0
	if (cr6.lt) goto loc_8263CDC0;
loc_8263CDD8:
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r14,r11,r9
	r14.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r19
	cr6.compare<int32_t>(ctx.r8.s32, r19.s32, xer);
	// blt cr6,0x8263cd44
	if (cr6.lt) goto loc_8263CD44;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
loc_8263CDF4:
	// addi r11,r3,-2
	r11.s64 = ctx.r3.s64 + -2;
	// cmpw cr6,r19,r11
	cr6.compare<int32_t>(r19.s32, r11.s32, xer);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// bge cr6,0x8263d510
	if (!cr6.lt) goto loc_8263D510;
loc_8263CE04:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mr r31,r14
	r31.u64 = r14.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// subf r29,r11,r19
	r29.s64 = r19.s64 - r11.s64;
	// lwz r11,20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// lwz r10,15340(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15340);
	// add r9,r29,r9
	ctx.r9.u64 = r29.u64 + ctx.r9.u64;
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x8263ce60
	if (!cr6.gt) goto loc_8263CE60;
	// mr r11,r14
	r11.u64 = r14.u64;
	// li r10,16
	ctx.r10.s64 = 16;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x8263ce5c
	if (cr6.eq) goto loc_8263CE5C;
	// mtctr r27
	ctr.u64 = r27.u64;
loc_8263CE50:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8263ce50
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263CE50;
loc_8263CE5C:
	// add r31,r14,r27
	r31.u64 = r14.u64 + r27.u64;
loc_8263CE60:
	// lwz r10,20(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// bne cr6,0x8263cea0
	if (!cr6.eq) goto loc_8263CEA0;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263cee8
	if (!cr6.eq) goto loc_8263CEE8;
	// lwz r28,80(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8263cf98
	if (!cr6.gt) goto loc_8263CF98;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// b 0x8263cf98
	goto loc_8263CF98;
loc_8263CEA0:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263cf2c
	if (!cr6.eq) goto loc_8263CF2C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263cf98
	if (cr6.eq) goto loc_8263CF98;
loc_8263CEB4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r8,r9,r20
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r20.s32);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r9,r7,r17
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(r17.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stb r9,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r9.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263ceb4
	if (!cr6.eq) goto loc_8263CEB4;
	// b 0x8263cf98
	goto loc_8263CF98;
loc_8263CEE8:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8263cf98
	if (cr6.eq) goto loc_8263CF98;
loc_8263CEF4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r8,r8,r20
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r20.s32);
	// mullw r7,r7,r18
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263cef4
	if (!cr6.eq) goto loc_8263CEF4;
	// b 0x8263cf98
	goto loc_8263CF98;
loc_8263CF2C:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8263cf98
	if (cr6.eq) goto loc_8263CF98;
loc_8263CF38:
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mullw r4,r8,r20
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(r20.s32);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r3,r7,r5
	ctx.r3.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mullw r5,r7,r18
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// subf r3,r6,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mullw r7,r6,r17
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// add r6,r3,r8
	ctx.r6.u64 = ctx.r3.u64 + ctx.r8.u64;
	// mullw r8,r18,r17
	ctx.r8.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263cf38
	if (!cr6.eq) goto loc_8263CF38;
loc_8263CF98:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// bgt cr6,0x8263cfcc
	if (cr6.gt) goto loc_8263CFCC;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263cfcc
	if (!cr6.lt) goto loc_8263CFCC;
	// subf r10,r27,r30
	ctx.r10.s64 = r30.s64 - r27.s64;
loc_8263CFB4:
	// lbzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// blt cr6,0x8263cfb4
	if (cr6.lt) goto loc_8263CFB4;
loc_8263CFCC:
	// lwz r10,15328(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263cff8
	if (!cr6.lt) goto loc_8263CFF8;
	// li r10,16
	ctx.r10.s64 = 16;
loc_8263CFE0:
	// stb r10,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263cfe0
	if (cr6.lt) goto loc_8263CFE0;
loc_8263CFF8:
	// lwz r10,152(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// lwz r9,108(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r31,96(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// mullw r11,r11,r9
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// lwz r30,100(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r25,r11,r9
	r25.u64 = r11.u64 + ctx.r9.u64;
	// ble cr6,0x8263d04c
	if (!cr6.gt) goto loc_8263D04C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// li r10,128
	ctx.r10.s64 = 128;
loc_8263D030:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r10,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r10.u8);
	// stb r10,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r10.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263d030
	if (!cr6.eq) goto loc_8263D030;
loc_8263D04C:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// add r4,r25,r11
	ctx.r4.u64 = r25.u64 + r11.u64;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r26,r25,r11
	r26.u64 = r25.u64 + r11.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// bne cr6,0x8263d0a4
	if (!cr6.eq) goto loc_8263D0A4;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8263d110
	if (!cr6.eq) goto loc_8263D110;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x8263d250
	if (!cr6.gt) goto loc_8263D250;
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r31,r31,r16
	r31.u64 = r31.u64 + r16.u64;
	// add r30,r30,r16
	r30.u64 = r30.u64 + r16.u64;
	// b 0x8263d250
	goto loc_8263D250;
loc_8263D0A4:
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// bne cr6,0x8263d190
	if (!cr6.eq) goto loc_8263D190;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x8263d250
	if (!cr6.gt) goto loc_8263D250;
	// mr r9,r16
	ctx.r9.u64 = r16.u64;
loc_8263D0B8:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mullw r7,r7,r24
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r24.s32);
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r5,r5,r22
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r22.s32);
	// add r7,r5,r7
	ctx.r7.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mullw r8,r8,r24
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r24.s32);
	// srawi r7,r7,7
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 7;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stb r7,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r7.u8);
	// mullw r7,r6,r22
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(r22.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stb r8,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r8.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// bne cr6,0x8263d0b8
	if (!cr6.eq) goto loc_8263D0B8;
	// b 0x8263d250
	goto loc_8263D250;
loc_8263D110:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x8263d250
	if (!cr6.gt) goto loc_8263D250;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r7,108(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// subf r9,r8,r11
	ctx.r9.s64 = r11.s64 - ctx.r8.s64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lwz r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// subf r28,r8,r7
	r28.s64 = ctx.r7.s64 - ctx.r8.s64;
loc_8263D138:
	// lbzx r8,r4,r10
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r10.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// mullw r3,r8,r24
	ctx.r3.s64 = int64_t(ctx.r8.s32) * int64_t(r24.s32);
	// lbzx r5,r11,r28
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + r28.u32);
	// mullw r29,r7,r23
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(r23.s32);
	// mullw r8,r6,r24
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// mullw r7,r5,r23
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(r23.s32);
	// add r6,r3,r29
	ctx.r6.u64 = ctx.r3.u64 + r29.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r7,r6,7
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7F) != 0);
	ctx.r7.s64 = ctx.r6.s32 >> 7;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r16
	cr6.compare<int32_t>(ctx.r10.s32, r16.s32, xer);
	// stb r7,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r7.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stb r8,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r8.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// blt cr6,0x8263d138
	if (cr6.lt) goto loc_8263D138;
	// b 0x8263d250
	goto loc_8263D250;
loc_8263D190:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// ble cr6,0x8263d250
	if (!cr6.gt) goto loc_8263D250;
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// subfic r5,r8,1
	xer.ca = ctx.r8.u32 <= 1;
	ctx.r5.s64 = 1 - ctx.r8.s64;
loc_8263D1AC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r3,1(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// mullw r29,r8,r23
	r29.s64 = int64_t(ctx.r8.s32) * int64_t(r23.s32);
	// lbzx r7,r5,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// subf r8,r8,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r8.s64;
	// lbzx r6,r4,r9
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r9.u32);
	// mullw r27,r6,r24
	r27.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// mullw r28,r7,r22
	r28.s64 = int64_t(ctx.r7.s32) * int64_t(r22.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r8,r8,r15
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r15.s32);
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + r27.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r3,1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// mullw r29,r8,r23
	r29.s64 = int64_t(ctx.r8.s32) * int64_t(r23.s32);
	// lbzx r7,r5,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r10.u32);
	// lbzx r6,r26,r9
	ctx.r6.u64 = PPC_LOAD_U8(r26.u32 + ctx.r9.u32);
	// subf r8,r8,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r8.s64;
	// mullw r3,r6,r24
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(r24.s32);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// mullw r7,r7,r22
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r22.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mullw r8,r8,r15
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r15.s32);
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// cmpw cr6,r9,r16
	cr6.compare<int32_t>(ctx.r9.s32, r16.s32, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stb r8,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r8.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// blt cr6,0x8263d1ac
	if (cr6.lt) goto loc_8263D1AC;
loc_8263D250:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r7,140(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x8263d2b0
	if (cr6.gt) goto loc_8263D2B0;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// bge cr6,0x8263d2b0
	if (!cr6.lt) goto loc_8263D2B0;
	// subf r11,r11,r25
	r11.s64 = r25.s64 - r11.s64;
	// lwz r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
loc_8263D288:
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r8,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r8.u8);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// bne cr6,0x8263d288
	if (!cr6.eq) goto loc_8263D288;
loc_8263D2B0:
	// lwz r9,148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263d2e0
	if (!cr6.lt) goto loc_8263D2E0;
	// subf r11,r7,r9
	r11.s64 = ctx.r9.s64 - ctx.r7.s64;
	// li r10,128
	ctx.r10.s64 = 128;
loc_8263D2C4:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r10,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r10.u8);
	// stb r10,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r10.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263d2c4
	if (!cr6.eq) goto loc_8263D2C4;
loc_8263D2E0:
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r28,r19,1
	r28.s64 = r19.s64 + 1;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// add r29,r11,r14
	r29.u64 = r11.u64 + r14.u64;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r8,20(r21)
	ctx.r8.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r26,92(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r31,r29
	r31.u64 = r29.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,15340(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15340);
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x8263d360
	if (!cr6.gt) goto loc_8263D360;
	// mr r11,r29
	r11.u64 = r29.u64;
	// li r10,16
	ctx.r10.s64 = 16;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x8263d35c
	if (cr6.eq) goto loc_8263D35C;
	// mtctr r26
	ctr.u64 = r26.u64;
loc_8263D350:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8263d350
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263D350;
loc_8263D35C:
	// add r31,r29,r26
	r31.u64 = r29.u64 + r26.u64;
loc_8263D360:
	// lwz r10,20(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// bne cr6,0x8263d3a0
	if (!cr6.eq) goto loc_8263D3A0;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263d3e8
	if (!cr6.eq) goto loc_8263D3E8;
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x8263d498
	if (!cr6.gt) goto loc_8263D498;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// b 0x8263d498
	goto loc_8263D498;
loc_8263D3A0:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263d42c
	if (!cr6.eq) goto loc_8263D42C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8263d498
	if (cr6.eq) goto loc_8263D498;
loc_8263D3B4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mullw r8,r9,r20
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r20.s32);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// mullw r9,r7,r17
	ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(r17.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// stb r9,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r9.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263d3b4
	if (!cr6.eq) goto loc_8263D3B4;
	// b 0x8263d498
	goto loc_8263D498;
loc_8263D3E8:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8263d498
	if (cr6.eq) goto loc_8263D498;
loc_8263D3F4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mullw r8,r8,r20
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r20.s32);
	// mullw r7,r7,r18
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263d3f4
	if (!cr6.eq) goto loc_8263D3F4;
	// b 0x8263d498
	goto loc_8263D498;
loc_8263D42C:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8263d498
	if (cr6.eq) goto loc_8263D498;
loc_8263D438:
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mullw r4,r8,r20
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(r20.s32);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r3,r7,r5
	ctx.r3.s64 = ctx.r5.s64 - ctx.r7.s64;
	// mullw r5,r7,r18
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// subf r3,r6,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mullw r7,r6,r17
	ctx.r7.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// add r6,r3,r8
	ctx.r6.u64 = ctx.r3.u64 + ctx.r8.u64;
	// mullw r8,r18,r17
	ctx.r8.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// mullw r8,r6,r8
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bne cr6,0x8263d438
	if (!cr6.eq) goto loc_8263D438;
loc_8263D498:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// bgt cr6,0x8263d4cc
	if (cr6.gt) goto loc_8263D4CC;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263d4cc
	if (!cr6.lt) goto loc_8263D4CC;
	// subf r10,r26,r30
	ctx.r10.s64 = r30.s64 - r26.s64;
loc_8263D4B4:
	// lbzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// blt cr6,0x8263d4b4
	if (cr6.lt) goto loc_8263D4B4;
loc_8263D4CC:
	// lwz r10,15328(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263d4f8
	if (!cr6.lt) goto loc_8263D4F8;
	// li r10,16
	ctx.r10.s64 = 16;
loc_8263D4E0:
	// stb r10,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263d4e0
	if (cr6.lt) goto loc_8263D4E0;
loc_8263D4F8:
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r19,r28,1
	r19.s64 = r28.s64 + 1;
	// add r14,r11,r29
	r14.u64 = r11.u64 + r29.u64;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmpw cr6,r19,r11
	cr6.compare<int32_t>(r19.s32, r11.s32, xer);
	// blt cr6,0x8263ce04
	if (cr6.lt) goto loc_8263CE04;
loc_8263D510:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263d728
	if (cr6.lt) goto loc_8263D728;
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r10,15332(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15332);
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263d728
	if (!cr6.lt) goto loc_8263D728;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// lwz r29,148(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// li r18,128
	r18.s64 = 128;
	// lwz r26,108(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// lwz r20,112(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// lwz r22,116(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// li r17,16
	r17.s64 = 16;
	// lwz r23,92(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r25,128(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r28,96(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r30,132(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r19,152(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// lwz r24,160(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
loc_8263D578:
	// lwz r11,20(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// lwz r10,15340(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15340);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// lwz r8,15328(r21)
	ctx.r8.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// ble cr6,0x8263d5f0
	if (!cr6.gt) goto loc_8263D5F0;
	// subf r11,r23,r30
	r11.s64 = r30.s64 - r23.s64;
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	// subf r8,r30,r23
	ctx.r8.s64 = r23.s64 - r30.s64;
loc_8263D5A8:
	// lwz r7,15324(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 15324);
	// cmpw cr6,r3,r7
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r7.s32, xer);
	// bge cr6,0x8263d5d4
	if (!cr6.lt) goto loc_8263D5D4;
	// lwz r7,20(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x8263d5d4
	if (!cr6.lt) goto loc_8263D5D4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x8263d5d4
	if (cr6.lt) goto loc_8263D5D4;
	// lbzx r7,r10,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// b 0x8263d5d8
	goto loc_8263D5D8;
loc_8263D5D4:
	// stb r17,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r17.u8);
loc_8263D5D8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,15328(r21)
	ctx.r7.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + r11.u64;
	// cmpw cr6,r6,r7
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r7.s32, xer);
	// blt cr6,0x8263d5a8
	if (cr6.lt) goto loc_8263D5A8;
loc_8263D5F0:
	// srawi r9,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 1;
	// mr r11,r28
	r11.u64 = r28.u64;
	// add r6,r9,r19
	ctx.r6.u64 = ctx.r9.u64 + r19.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mullw r8,r6,r26
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r26.s32);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// add r7,r8,r24
	ctx.r7.u64 = ctx.r8.u64 + r24.u64;
	// ble cr6,0x8263d678
	if (!cr6.gt) goto loc_8263D678;
	// subf r5,r25,r24
	ctx.r5.s64 = r24.s64 - r25.s64;
loc_8263D618:
	// lwz r8,15324(r21)
	ctx.r8.u64 = PPC_LOAD_U32(r21.u32 + 15324);
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// bge cr6,0x8263d65c
	if (!cr6.lt) goto loc_8263D65C;
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r26
	cr6.compare<int32_t>(ctx.r8.s32, r26.s32, xer);
	// bge cr6,0x8263d65c
	if (!cr6.lt) goto loc_8263D65C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8263d65c
	if (cr6.lt) goto loc_8263D65C;
	// subf r8,r25,r7
	ctx.r8.s64 = ctx.r7.s64 - r25.s64;
	// add r16,r8,r9
	r16.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lbzx r16,r16,r22
	r16.u64 = PPC_LOAD_U8(r16.u32 + r22.u32);
	// stb r16,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r16.u8);
	// lbzx r8,r8,r20
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r20.u32);
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// b 0x8263d664
	goto loc_8263D664;
loc_8263D65C:
	// stb r18,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r18.u8);
	// stb r18,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r18.u8);
loc_8263D664:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// blt cr6,0x8263d618
	if (cr6.lt) goto loc_8263D618;
loc_8263D678:
	// lwz r10,20(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// addi r8,r3,1
	ctx.r8.s64 = ctx.r3.s64 + 1;
	// lwz r9,15340(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + 15340);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// mullw r10,r8,r10
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r7,r11,r14
	ctx.r7.u64 = r11.u64 + r14.u64;
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + r30.u64;
	// add r27,r27,r29
	r27.u64 = r27.u64 + r29.u64;
	// addi r6,r31,1
	ctx.r6.s64 = r31.s64 + 1;
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// ble cr6,0x8263d708
	if (!cr6.gt) goto loc_8263D708;
	// subf r11,r23,r30
	r11.s64 = r30.s64 - r23.s64;
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// subf r4,r30,r23
	ctx.r4.s64 = r23.s64 - r30.s64;
loc_8263D6C0:
	// lwz r3,15324(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 15324);
	// cmpw cr6,r8,r3
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r3.s32, xer);
	// bge cr6,0x8263d6ec
	if (!cr6.lt) goto loc_8263D6EC;
	// lwz r3,20(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 20);
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// bge cr6,0x8263d6ec
	if (!cr6.lt) goto loc_8263D6EC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x8263d6ec
	if (cr6.lt) goto loc_8263D6EC;
	// lbzx r3,r11,r9
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + ctx.r9.u32);
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// b 0x8263d6f0
	goto loc_8263D6F0;
loc_8263D6EC:
	// stb r17,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r17.u8);
loc_8263D6F0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r3,15328(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r31,r11,r4
	r31.u64 = r11.u64 + ctx.r4.u64;
	// cmpw cr6,r31,r3
	cr6.compare<int32_t>(r31.s32, ctx.r3.s32, xer);
	// blt cr6,0x8263d6c0
	if (cr6.lt) goto loc_8263D6C0;
loc_8263D708:
	// lwz r11,15328(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 15328);
	// addi r31,r6,1
	r31.s64 = ctx.r6.s64 + 1;
	// lwz r10,15332(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 15332);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// add r14,r11,r7
	r14.u64 = r11.u64 + ctx.r7.u64;
	// addi r3,r8,1
	ctx.r3.s64 = ctx.r8.s64 + 1;
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263d578
	if (cr6.lt) goto loc_8263D578;
loc_8263D728:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lfd f29,-176(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lfd f30,-168(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8263D740"))) PPC_WEAK_FUNC(sub_8263D740);
PPC_FUNC_IMPL(__imp__sub_8263D740) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5e8
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = ctx.f1.f64;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// fmr f28,f3
	f28.f64 = ctx.f3.f64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x8263d780
	if (!cr6.eq) goto loc_8263D780;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d634
	// b 0x8239bd10
	return;
loc_8263D780:
	// lwz r11,15344(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15344);
	// fneg f0,f29
	ctx.fpscr.disableFlushMode();
	f0.u64 = f29.u64 ^ 0x8000000000000000;
	// lwz r10,20(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// lwz r11,15348(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15348);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f31,-31368(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// srawi r30,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r30.s64 = r11.s32 >> 1;
	// stw r30,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r30.u32);
	// bge cr6,0x8263d7c0
	if (!cr6.lt) goto loc_8263D7C0;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263D7C0:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// lwz r10,15328(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r10,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r10.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f13,144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x8263d808
	if (!cr6.lt) goto loc_8263D808;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263D808:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r10,15328(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// lwz r11,20(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// std r10,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r10.u64);
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// fsub f13,f13,f29
	ctx.f13.f64 = ctx.f13.f64 - f29.f64;
	// lfd f0,144(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x8263d850
	if (cr6.lt) goto loc_8263D850;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_8263D850:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f1,f0
	ctx.f1.f64 = double(f0.s64);
	// fcmpu cr6,f1,f31
	cr6.compare(ctx.f1.f64, f31.f64);
	// bge cr6,0x8263d884
	if (!cr6.lt) goto loc_8263D884;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
loc_8263D884:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// fneg f0,f30
	ctx.fpscr.disableFlushMode();
	f0.u64 = f30.u64 ^ 0x8000000000000000;
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// fctiwz f13,f1
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bge cr6,0x8263d8a4
	if (!cr6.lt) goto loc_8263D8A4;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263D8A4:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// lwz r11,15332(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15332);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f13,144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f1,f13
	ctx.f1.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x8263d8ec
	if (cr6.lt) goto loc_8263D8EC;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263D8EC:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15332(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15332);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// lwz r11,15324(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15324);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f13,144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f30
	ctx.f13.f64 = ctx.f13.f64 - f30.f64;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x8263d934
	if (cr6.lt) goto loc_8263D934;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_8263D934:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r11.u64);
	// lfd f0,152(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// bge cr6,0x8263d968
	if (!cr6.lt) goto loc_8263D968;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263D968:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r31,r11,0,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// rlwinm r20,r10,0,0,30
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r15,r11,0,0,30
	r15.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r20,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r20.u32);
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// rlwinm r3,r11,0,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
	// bge cr6,0x8263d9c0
	if (!cr6.lt) goto loc_8263D9C0;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
loc_8263D9C0:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r6,15356(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 15356);
	// srawi r8,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r8.s64 = r31.s32 >> 1;
	// lwz r7,15360(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 15360);
	// rlwinm r10,r20,11,0,20
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 11) & 0xFFFFF800;
	// lwz r14,15352(r22)
	r14.u64 = PPC_LOAD_U32(r22.u32 + 15352);
	// rlwinm r9,r31,11,0,20
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 11) & 0xFFFFF800;
	// li r5,0
	ctx.r5.s64 = 0;
	// lfd f0,30680(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30680);
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// fmul f13,f30,f0
	ctx.f13.f64 = f30.f64 * f0.f64;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// srawi r8,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	ctx.r8.s64 = r15.s32 >> 1;
	// fmul f0,f29,f0
	f0.f64 = f29.f64 * f0.f64;
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// bge cr6,0x8263da34
	if (!cr6.lt) goto loc_8263DA34;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_8263DA34:
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// clrlwi r18,r11,21
	r18.u64 = r11.u32 & 0x7FF;
	// clrlwi r17,r10,21
	r17.u64 = ctx.r10.u32 & 0x7FF;
	// lfd f0,-31360(r9)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31360);
	// srawi r9,r10,11
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 11;
	// fcmpu cr6,f28,f0
	cr6.compare(f28.f64, f0.f64);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// srawi r9,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	ctx.r9.s64 = r11.s32 >> 11;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// srawi r9,r11,12
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFF) != 0);
	ctx.r9.s64 = r11.s32 >> 12;
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// srawi r8,r10,12
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 12;
	// clrlwi r24,r9,21
	r24.u64 = ctx.r9.u32 & 0x7FF;
	// stw r8,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r8.u32);
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// clrlwi r23,r8,21
	r23.u64 = ctx.r8.u32 & 0x7FF;
	// subfic r11,r23,2048
	xer.ca = r23.u32 <= 2048;
	r11.s64 = 2048 - r23.s64;
	// subfic r10,r18,2048
	xer.ca = r18.u32 <= 2048;
	ctx.r10.s64 = 2048 - r18.s64;
	// mullw r16,r23,r24
	r16.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// subf r25,r24,r11
	r25.s64 = r11.s64 - r24.s64;
	// subf r21,r17,r10
	r21.s64 = ctx.r10.s64 - r17.s64;
	// ble cr6,0x8263da98
	if (!cr6.gt) goto loc_8263DA98;
	// fmr f28,f0
	f28.f64 = f0.f64;
	// b 0x8263daa4
	goto loc_8263DAA4;
loc_8263DA98:
	// fcmpu cr6,f28,f31
	ctx.fpscr.disableFlushMode();
	cr6.compare(f28.f64, f31.f64);
	// bge cr6,0x8263daa4
	if (!cr6.lt) goto loc_8263DAA4;
	// fmr f28,f31
	f28.f64 = f31.f64;
loc_8263DAA4:
	// lis r11,-32248
	r11.s64 = -2113404928;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// li r4,128
	ctx.r4.s64 = 128;
	// lfd f0,-26736(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -26736);
	// addi r11,r1,152
	r11.s64 = ctx.r1.s64 + 152;
	// fmul f0,f28,f0
	f0.f64 = f28.f64 * f0.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// ble cr6,0x8263db7c
	if (!cr6.gt) goto loc_8263DB7C;
loc_8263DACC:
	// lwz r9,15328(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// mr r10,r14
	ctx.r10.u64 = r14.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8263daf8
	if (!cr6.gt) goto loc_8263DAF8;
loc_8263DAE0:
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263dae0
	if (cr6.lt) goto loc_8263DAE0;
loc_8263DAF8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8263db24
	if (!cr6.gt) goto loc_8263DB24;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// subf r9,r7,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r7.s64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_8263DB0C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r4,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r4.u8);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263db0c
	if (!cr6.eq) goto loc_8263DB0C;
loc_8263DB24:
	// lwz r10,15328(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// add r9,r10,r14
	ctx.r9.u64 = ctx.r10.u64 + r14.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x8263db60
	if (!cr6.gt) goto loc_8263DB60;
loc_8263DB48:
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,15328(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r31
	cr6.compare<int32_t>(r11.s32, r31.s32, xer);
	// blt cr6,0x8263db48
	if (cr6.lt) goto loc_8263DB48;
loc_8263DB60:
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r14,r11,r9
	r14.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r20
	cr6.compare<int32_t>(ctx.r8.s32, r20.s32, xer);
	// blt cr6,0x8263dacc
	if (cr6.lt) goto loc_8263DACC;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
loc_8263DB7C:
	// addi r11,r3,-2
	r11.s64 = ctx.r3.s64 + -2;
	// lwz r4,152(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// cmpw cr6,r20,r11
	cr6.compare<int32_t>(r20.s32, r11.s32, xer);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// bge cr6,0x8263e2d4
	if (!cr6.lt) goto loc_8263E2D4;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r19,128(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
loc_8263DBA0:
	// lwz r10,20(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// mr r11,r14
	r11.u64 = r14.u64;
	// lwz r9,15340(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15340);
	// mullw r10,r19,r10
	ctx.r10.s64 = int64_t(r19.s32) * int64_t(ctx.r10.s32);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ble cr6,0x8263dbe8
	if (!cr6.gt) goto loc_8263DBE8;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8263dbe4
	if (cr6.eq) goto loc_8263DBE4;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8263DBD8:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8263dbd8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263DBD8;
loc_8263DBE4:
	// add r11,r14,r8
	r11.u64 = r14.u64 + ctx.r8.u64;
loc_8263DBE8:
	// lwz r9,20(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// bne cr6,0x8263dc34
	if (!cr6.eq) goto loc_8263DC34;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263dc84
	if (!cr6.eq) goto loc_8263DC84;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263dd40
	if (!cr6.lt) goto loc_8263DD40;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
loc_8263DC10:
	// lbzx r10,r8,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r9.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmpw cr6,r9,r15
	cr6.compare<int32_t>(ctx.r9.s32, r15.s32, xer);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// blt cr6,0x8263dc10
	if (cr6.lt) goto loc_8263DC10;
	// b 0x8263dd40
	goto loc_8263DD40;
loc_8263DC34:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263dccc
	if (!cr6.eq) goto loc_8263DCCC;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263dd40
	if (!cr6.lt) goto loc_8263DD40;
	// subf r9,r8,r15
	ctx.r9.s64 = r15.s64 - ctx.r8.s64;
loc_8263DC48:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mullw r7,r8,r21
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(r21.s32);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r8,r6,r17
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r8,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 11;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// rlwinm r8,r8,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFFFFFF;
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8263dc48
	if (!cr6.eq) goto loc_8263DC48;
	// b 0x8263dd40
	goto loc_8263DD40;
loc_8263DC84:
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263dd40
	if (!cr6.lt) goto loc_8263DD40;
	// subf r6,r8,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r8.s64;
loc_8263DC90:
	// lbzx r10,r6,r8
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mullw r10,r10,r21
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r21.s32);
	// mullw r7,r7,r18
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// srawi r10,r10,11
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 11;
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// blt cr6,0x8263dc90
	if (cr6.lt) goto loc_8263DC90;
	// b 0x8263dd40
	goto loc_8263DD40;
loc_8263DCCC:
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263dd40
	if (!cr6.lt) goto loc_8263DD40;
	// subf r8,r8,r15
	ctx.r8.s64 = r15.s64 - ctx.r8.s64;
loc_8263DCD8:
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// mullw r31,r7,r21
	r31.s64 = int64_t(ctx.r7.s32) * int64_t(r21.s32);
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r30,r6,r3
	r30.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mullw r3,r6,r18
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(r18.s32);
	// subf r30,r5,r30
	r30.s64 = r30.s64 - ctx.r5.s64;
	// mullw r6,r5,r17
	ctx.r6.s64 = int64_t(ctx.r5.s32) * int64_t(r17.s32);
	// add r5,r30,r7
	ctx.r5.u64 = r30.u64 + ctx.r7.u64;
	// mullw r7,r18,r17
	ctx.r7.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// mullw r7,r5,r7
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// rlwinm r7,r7,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8263dcd8
	if (!cr6.eq) goto loc_8263DCD8;
loc_8263DD40:
	// lwz r9,15328(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// mr r10,r15
	ctx.r10.u64 = r15.u64;
	// cmpw cr6,r15,r9
	cr6.compare<int32_t>(r15.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263dd6c
	if (!cr6.lt) goto loc_8263DD6C;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8263DD54:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,15328(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x8263dd54
	if (cr6.lt) goto loc_8263DD54;
loc_8263DD6C:
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// srawi r11,r20,1
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x1) != 0);
	r11.s64 = r20.s32 >> 1;
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// lwz r8,136(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// lwz r8,144(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// ble cr6,0x8263ddc8
	if (!cr6.gt) goto loc_8263DDC8;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// li r7,128
	ctx.r7.s64 = 128;
loc_8263DDAC:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263ddac
	if (!cr6.eq) goto loc_8263DDAC;
loc_8263DDC8:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// lwz r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 + ctx.r7.u64;
	// bne cr6,0x8263de5c
	if (!cr6.eq) goto loc_8263DE5C;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x8263def4
	if (!cr6.eq) goto loc_8263DEF4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// bge cr6,0x8263e090
	if (!cr6.lt) goto loc_8263E090;
	// rotlwi r8,r7,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// subf r5,r8,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lwz r8,92(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
loc_8263DE10:
	// lbzx r7,r5,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,-128
	ctx.r7.s64 = ctx.r7.s64 + -128;
	// addi r6,r6,-128
	ctx.r6.s64 = ctx.r6.s64 + -128;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// mullw r3,r6,r4
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// rlwinm r6,r7,24,8,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r7,r3,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 24) & 0xFFFFFF;
	// addi r6,r6,128
	ctx.r6.s64 = ctx.r6.s64 + 128;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8263de10
	if (!cr6.eq) goto loc_8263DE10;
	// b 0x8263e090
	goto loc_8263E090;
loc_8263DE5C:
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x8263df9c
	if (!cr6.eq) goto loc_8263DF9C;
	// cmpw cr6,r5,r8
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r8.s32, xer);
	// bge cr6,0x8263e090
	if (!cr6.lt) goto loc_8263E090;
	// lwz r8,92(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
loc_8263DE7C:
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbz r7,0(r6)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// mullw r5,r5,r25
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r25.s32);
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// mullw r31,r31,r23
	r31.s64 = int64_t(r31.s32) * int64_t(r23.s32);
	// add r5,r31,r5
	ctx.r5.u64 = r31.u64 + ctx.r5.u64;
	// mullw r7,r7,r25
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r25.s32);
	// srawi r5,r5,11
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 11;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r5,r5,-128
	ctx.r5.s64 = ctx.r5.s64 + -128;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// rlwinm r5,r5,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFFFFFF;
	// addi r31,r5,128
	r31.s64 = ctx.r5.s64 + 128;
	// mullw r5,r3,r23
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(r23.s32);
	// stb r31,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r31.u8);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// addi r7,r7,-128
	ctx.r7.s64 = ctx.r7.s64 + -128;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// rlwinm r7,r7,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
	// addi r7,r7,128
	ctx.r7.s64 = ctx.r7.s64 + 128;
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8263de7c
	if (!cr6.eq) goto loc_8263DE7C;
	// b 0x8263e090
	goto loc_8263E090;
loc_8263DEF4:
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r5,r8
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r8.s32, xer);
	// bge cr6,0x8263e090
	if (!cr6.lt) goto loc_8263E090;
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// subf r27,r11,r6
	r27.s64 = ctx.r6.s64 - r11.s64;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// subf r28,r7,r5
	r28.s64 = ctx.r5.s64 - ctx.r7.s64;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// subf r7,r7,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r7.s64;
loc_8263DF24:
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbzx r3,r11,r27
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// mullw r30,r6,r25
	r30.s64 = int64_t(ctx.r6.s32) * int64_t(r25.s32);
	// lbzx r31,r28,r8
	r31.u64 = PPC_LOAD_U8(r28.u32 + ctx.r8.u32);
	// mullw r29,r5,r24
	r29.s64 = int64_t(ctx.r5.s32) * int64_t(r24.s32);
	// mullw r6,r3,r25
	ctx.r6.s64 = int64_t(ctx.r3.s32) * int64_t(r25.s32);
	// mullw r5,r31,r24
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(r24.s32);
	// add r3,r30,r29
	ctx.r3.u64 = r30.u64 + r29.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// srawi r5,r3,11
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 11;
	// srawi r6,r6,11
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7FF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 11;
	// addi r5,r5,-128
	ctx.r5.s64 = ctx.r5.s64 + -128;
	// addi r6,r6,-128
	ctx.r6.s64 = ctx.r6.s64 + -128;
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// mullw r6,r6,r4
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// rlwinm r5,r5,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFFFFFF;
	// rlwinm r6,r6,24,8,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFFFFFF;
	// addi r5,r5,128
	ctx.r5.s64 = ctx.r5.s64 + 128;
	// addi r6,r6,128
	ctx.r6.s64 = ctx.r6.s64 + 128;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r6,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r6.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8263df24
	if (!cr6.eq) goto loc_8263DF24;
	// b 0x8263e090
	goto loc_8263E090;
loc_8263DF9C:
	// cmpw cr6,r5,r8
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r8.s32, xer);
	// bge cr6,0x8263e090
	if (!cr6.lt) goto loc_8263E090;
	// lwz r5,108(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// li r28,1
	r28.s64 = 1;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r7,r6,r5
	ctx.r7.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
	// subfic r29,r5,1
	xer.ca = ctx.r5.u32 <= 1;
	r29.s64 = 1 - ctx.r5.s64;
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// subf r5,r5,r3
	ctx.r5.s64 = ctx.r3.s64 - ctx.r5.s64;
loc_8263DFC4:
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbzx r27,r28,r8
	r27.u64 = PPC_LOAD_U8(r28.u32 + ctx.r8.u32);
	// mullw r26,r3,r24
	r26.s64 = int64_t(ctx.r3.s32) * int64_t(r24.s32);
	// lbzx r31,r29,r8
	r31.u64 = PPC_LOAD_U8(r29.u32 + ctx.r8.u32);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// subf r3,r3,r27
	ctx.r3.s64 = r27.s64 - ctx.r3.s64;
	// mullw r27,r30,r25
	r27.s64 = int64_t(r30.s32) * int64_t(r25.s32);
	// subf r3,r31,r3
	ctx.r3.s64 = ctx.r3.s64 - r31.s64;
	// mullw r31,r31,r23
	r31.s64 = int64_t(r31.s32) * int64_t(r23.s32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mullw r3,r3,r16
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r16.s32);
	// srawi r3,r3,11
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 11;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// srawi r3,r3,11
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 11;
	// addi r3,r3,-128
	ctx.r3.s64 = ctx.r3.s64 + -128;
	// mullw r3,r3,r4
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// rlwinm r3,r3,24,8,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 24) & 0xFFFFFF;
	// addi r3,r3,128
	ctx.r3.s64 = ctx.r3.s64 + 128;
	// stb r3,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r3.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbzx r27,r28,r7
	r27.u64 = PPC_LOAD_U8(r28.u32 + ctx.r7.u32);
	// mullw r26,r3,r24
	r26.s64 = int64_t(ctx.r3.s32) * int64_t(r24.s32);
	// lbzx r31,r29,r7
	r31.u64 = PPC_LOAD_U8(r29.u32 + ctx.r7.u32);
	// lbz r30,0(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// subf r3,r3,r27
	ctx.r3.s64 = r27.s64 - ctx.r3.s64;
	// mullw r27,r30,r25
	r27.s64 = int64_t(r30.s32) * int64_t(r25.s32);
	// subf r3,r31,r3
	ctx.r3.s64 = ctx.r3.s64 - r31.s64;
	// mullw r31,r31,r23
	r31.s64 = int64_t(r31.s32) * int64_t(r23.s32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// mullw r3,r3,r16
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(r16.s32);
	// srawi r3,r3,11
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 11;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// add r3,r3,r26
	ctx.r3.u64 = ctx.r3.u64 + r26.u64;
	// srawi r3,r3,11
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 11;
	// addi r3,r3,-128
	ctx.r3.s64 = ctx.r3.s64 + -128;
	// mullw r3,r3,r4
	ctx.r3.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// rlwinm r3,r3,24,8,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 24) & 0xFFFFFF;
	// addi r3,r3,128
	ctx.r3.s64 = ctx.r3.s64 + 128;
	// stb r3,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r3.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8263dfc4
	if (!cr6.eq) goto loc_8263DFC4;
loc_8263E090:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r8,132(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x8263e0c4
	if (!cr6.lt) goto loc_8263E0C4;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// li r7,128
	ctx.r7.s64 = 128;
loc_8263E0A8:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r7,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263e0a8
	if (!cr6.eq) goto loc_8263E0A8;
loc_8263E0C4:
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r28,r19,1
	r28.s64 = r19.s64 + 1;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r27,r20,1
	r27.s64 = r20.s64 + 1;
	// add r29,r11,r14
	r29.u64 = r11.u64 + r14.u64;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r9,20(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// mullw r11,r28,r9
	r11.s64 = int64_t(r28.s32) * int64_t(ctx.r9.s32);
	// lwz r10,15340(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15340);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ble cr6,0x8263e134
	if (!cr6.gt) goto loc_8263E134;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x8263e130
	if (cr6.eq) goto loc_8263E130;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_8263E124:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8263e124
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263E124;
loc_8263E130:
	// add r11,r29,r8
	r11.u64 = r29.u64 + ctx.r8.u64;
loc_8263E134:
	// lwz r9,20(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// bne cr6,0x8263e180
	if (!cr6.eq) goto loc_8263E180;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263e1d0
	if (!cr6.eq) goto loc_8263E1D0;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263e28c
	if (!cr6.lt) goto loc_8263E28C;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
loc_8263E15C:
	// lbzx r10,r8,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r9.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// cmpw cr6,r9,r15
	cr6.compare<int32_t>(ctx.r9.s32, r15.s32, xer);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// blt cr6,0x8263e15c
	if (cr6.lt) goto loc_8263E15C;
	// b 0x8263e28c
	goto loc_8263E28C;
loc_8263E180:
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// bne cr6,0x8263e218
	if (!cr6.eq) goto loc_8263E218;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263e28c
	if (!cr6.lt) goto loc_8263E28C;
	// subf r9,r8,r15
	ctx.r9.s64 = r15.s64 - ctx.r8.s64;
loc_8263E194:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mullw r7,r8,r21
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(r21.s32);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r8,r6,r17
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// srawi r8,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 11;
	// mullw r8,r8,r4
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// rlwinm r8,r8,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFFFFFF;
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8263e194
	if (!cr6.eq) goto loc_8263E194;
	// b 0x8263e28c
	goto loc_8263E28C;
loc_8263E1D0:
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263e28c
	if (!cr6.lt) goto loc_8263E28C;
	// subf r6,r8,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r8.s64;
loc_8263E1DC:
	// lbzx r10,r6,r8
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mullw r10,r10,r21
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r21.s32);
	// mullw r7,r7,r18
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r18.s32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// srawi r10,r10,11
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 11;
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// blt cr6,0x8263e1dc
	if (cr6.lt) goto loc_8263E1DC;
	// b 0x8263e28c
	goto loc_8263E28C;
loc_8263E218:
	// cmpw cr6,r8,r15
	cr6.compare<int32_t>(ctx.r8.s32, r15.s32, xer);
	// bge cr6,0x8263e28c
	if (!cr6.lt) goto loc_8263E28C;
	// subf r8,r8,r15
	ctx.r8.s64 = r15.s64 - ctx.r8.s64;
loc_8263E224:
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// mullw r31,r7,r21
	r31.s64 = int64_t(ctx.r7.s32) * int64_t(r21.s32);
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r30,r6,r3
	r30.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mullw r3,r6,r18
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(r18.s32);
	// subf r30,r5,r30
	r30.s64 = r30.s64 - ctx.r5.s64;
	// mullw r6,r5,r17
	ctx.r6.s64 = int64_t(ctx.r5.s32) * int64_t(r17.s32);
	// add r5,r30,r7
	ctx.r5.u64 = r30.u64 + ctx.r7.u64;
	// mullw r7,r18,r17
	ctx.r7.s64 = int64_t(r18.s32) * int64_t(r17.s32);
	// mullw r7,r5,r7
	ctx.r7.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// rlwinm r7,r7,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8263e224
	if (!cr6.eq) goto loc_8263E224;
loc_8263E28C:
	// lwz r9,15328(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// mr r10,r15
	ctx.r10.u64 = r15.u64;
	// cmpw cr6,r15,r9
	cr6.compare<int32_t>(r15.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263e2b8
	if (!cr6.lt) goto loc_8263E2B8;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8263E2A0:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r8,15328(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x8263e2a0
	if (cr6.lt) goto loc_8263E2A0;
loc_8263E2B8:
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r20,r27,1
	r20.s64 = r27.s64 + 1;
	// addi r19,r28,1
	r19.s64 = r28.s64 + 1;
	// add r14,r11,r29
	r14.u64 = r11.u64 + r29.u64;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmpw cr6,r20,r11
	cr6.compare<int32_t>(r20.s32, r11.s32, xer);
	// blt cr6,0x8263dba0
	if (cr6.lt) goto loc_8263DBA0;
loc_8263E2D4:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263e524
	if (cr6.lt) goto loc_8263E524;
	// lwz r30,124(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,15332(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15332);
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263e524
	if (!cr6.lt) goto loc_8263E524;
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r25,0
	r25.s64 = 0;
	// lwz r29,132(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r19,108(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// lwz r20,112(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r16,128
	r16.s64 = 128;
	// lwz r21,116(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r24,92(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r28,96(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r27,100(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r26,120(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r17,136(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r18,144(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_8263E33C:
	// lwz r9,20(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// mr r11,r14
	r11.u64 = r14.u64;
	// lwz r8,15340(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 15340);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mullw r9,r9,r31
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r31.s32);
	// lwz r7,15328(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// add r8,r9,r26
	ctx.r8.u64 = ctx.r9.u64 + r26.u64;
	// ble cr6,0x8263e3bc
	if (!cr6.gt) goto loc_8263E3BC;
	// subf r7,r23,r26
	ctx.r7.s64 = r26.s64 - r23.s64;
loc_8263E368:
	// lwz r9,15324(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15324);
	// cmpw cr6,r31,r9
	cr6.compare<int32_t>(r31.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263e3a4
	if (!cr6.lt) goto loc_8263E3A4;
	// lwz r6,20(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// add r9,r10,r7
	ctx.r9.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// bge cr6,0x8263e3a4
	if (!cr6.lt) goto loc_8263E3A4;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x8263e3a4
	if (cr6.lt) goto loc_8263E3A4;
	// subf r9,r23,r8
	ctx.r9.s64 = ctx.r8.s64 - r23.s64;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// b 0x8263e3a8
	goto loc_8263E3A8;
loc_8263E3A4:
	// stb r25,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r25.u8);
loc_8263E3A8:
	// lwz r9,15328(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263e368
	if (cr6.lt) goto loc_8263E368;
loc_8263E3BC:
	// srawi r9,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 1;
	// mr r11,r28
	r11.u64 = r28.u64;
	// add r6,r9,r17
	ctx.r6.u64 = ctx.r9.u64 + r17.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mullw r9,r6,r19
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(r19.s32);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// add r7,r9,r18
	ctx.r7.u64 = ctx.r9.u64 + r18.u64;
	// ble cr6,0x8263e46c
	if (!cr6.gt) goto loc_8263E46C;
	// subf r5,r24,r18
	ctx.r5.s64 = r18.s64 - r24.s64;
loc_8263E3E4:
	// lwz r9,15324(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15324);
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// cmpw cr6,r6,r9
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263e450
	if (!cr6.lt) goto loc_8263E450;
	// add r9,r8,r5
	ctx.r9.u64 = ctx.r8.u64 + ctx.r5.u64;
	// cmpw cr6,r9,r19
	cr6.compare<int32_t>(ctx.r9.s32, r19.s32, xer);
	// bge cr6,0x8263e450
	if (!cr6.lt) goto loc_8263E450;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x8263e450
	if (cr6.lt) goto loc_8263E450;
	// subf r9,r24,r7
	ctx.r9.s64 = ctx.r7.s64 - r24.s64;
	// add r15,r9,r8
	r15.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// lbzx r9,r15,r21
	ctx.r9.u64 = PPC_LOAD_U8(r15.u32 + r21.u32);
	// lwz r15,152(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// lbzx r9,r15,r20
	ctx.r9.u64 = PPC_LOAD_U8(r15.u32 + r20.u32);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// mullw r9,r9,r4
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// b 0x8263e458
	goto loc_8263E458;
loc_8263E450:
	// stb r16,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r16.u8);
	// stb r16,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r16.u8);
loc_8263E458:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r8,r29
	cr6.compare<int32_t>(ctx.r8.s32, r29.s32, xer);
	// blt cr6,0x8263e3e4
	if (cr6.lt) goto loc_8263E3E4;
loc_8263E46C:
	// lwz r10,20(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// addi r7,r31,1
	ctx.r7.s64 = r31.s64 + 1;
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// lwz r9,15340(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 15340);
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// add r6,r11,r14
	ctx.r6.u64 = r11.u64 + r14.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// add r27,r27,r29
	r27.u64 = r27.u64 + r29.u64;
	// addi r5,r30,1
	ctx.r5.s64 = r30.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// add r9,r10,r26
	ctx.r9.u64 = ctx.r10.u64 + r26.u64;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// ble cr6,0x8263e504
	if (!cr6.gt) goto loc_8263E504;
	// subf r31,r23,r26
	r31.s64 = r26.s64 - r23.s64;
loc_8263E4B0:
	// lwz r10,15324(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15324);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263e4ec
	if (!cr6.lt) goto loc_8263E4EC;
	// lwz r30,20(r22)
	r30.u64 = PPC_LOAD_U32(r22.u32 + 20);
	// add r10,r8,r31
	ctx.r10.u64 = ctx.r8.u64 + r31.u64;
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// bge cr6,0x8263e4ec
	if (!cr6.lt) goto loc_8263E4EC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x8263e4ec
	if (cr6.lt) goto loc_8263E4EC;
	// subf r10,r23,r9
	ctx.r10.s64 = ctx.r9.s64 - r23.s64;
	// lbzx r10,r10,r8
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r8.u32);
	// mullw r10,r10,r4
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// b 0x8263e4f0
	goto loc_8263E4F0;
loc_8263E4EC:
	// stb r25,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r25.u8);
loc_8263E4F0:
	// lwz r10,15328(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263e4b0
	if (cr6.lt) goto loc_8263E4B0;
loc_8263E504:
	// lwz r11,15328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15328);
	// addi r30,r5,1
	r30.s64 = ctx.r5.s64 + 1;
	// lwz r10,15332(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 15332);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// add r14,r11,r6
	r14.u64 = r11.u64 + ctx.r6.u64;
	// addi r31,r7,1
	r31.s64 = ctx.r7.s64 + 1;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263e33c
	if (cr6.lt) goto loc_8263E33C;
loc_8263E524:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d634
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8263E538"))) PPC_WEAK_FUNC(sub_8263E538);
PPC_FUNC_IMPL(__imp__sub_8263E538) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// addi r11,r4,16
	r11.s64 = ctx.r4.s64 + 16;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r4,48
	ctx.r9.s64 = ctx.r4.s64 + 48;
	// vcfsx v0,v0,11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r4,32
	ctx.r10.s64 = ctx.r4.s64 + 32;
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,16
	ctx.r8.s64 = ctx.r5.s64 + 16;
	// vcfsx v10,v10,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// addi r7,r5,32
	ctx.r7.s64 = ctx.r5.s64 + 32;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v13,v13,11
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v12,v12,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// addi r10,r5,64
	ctx.r10.s64 = ctx.r5.s64 + 64;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,64
	r11.s64 = ctx.r4.s64 + 64;
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// addi r9,r4,80
	ctx.r9.s64 = ctx.r4.s64 + 80;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,80
	ctx.r8.s64 = ctx.r5.s64 + 80;
	// vmulfp128 v0,v0,v1
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r7,r4,160
	ctx.r7.s64 = ctx.r4.s64 + 160;
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,112
	ctx.r10.s64 = ctx.r4.s64 + 112;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,96
	r11.s64 = ctx.r4.s64 + 96;
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v4,v4,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// vmulfp128 v13,v13,v1
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r8,r5,112
	ctx.r8.s64 = ctx.r5.s64 + 112;
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v29,r0,r7
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v6,v6,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r7,r4,208
	ctx.r7.s64 = ctx.r4.s64 + 208;
	// vcfsx v2,v2,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vcfsx v28,v5,0
	_mm_store_ps(v28.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmsum4fp128 v0,v0,v10
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32), 0xFF));
	// vmsum4fp128 v10,v13,v9
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32), 0xFF));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v3,v11,v7
	_mm_store_ps(ctx.v3.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v7.f32), 0xFF));
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v13,v13,11
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r4,128
	r11.s64 = ctx.r4.s64 + 128;
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v9,r0,r9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v12,v12,v8
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32), 0xFF));
	// vcfsx v27,v9,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,144
	ctx.r10.s64 = ctx.r4.s64 + 144;
	// vcfsx v26,v8,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// addi r9,r5,128
	ctx.r9.s64 = ctx.r5.s64 + 128;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v8,v6,v1
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)));
	// vcfsx v9,v7,11
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,144
	ctx.r8.s64 = ctx.r5.s64 + 144;
	// vmulfp128 v7,v4,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r11,r4,176
	r11.s64 = ctx.r4.s64 + 176;
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,160
	ctx.r10.s64 = ctx.r5.s64 + 160;
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v6,v31,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v31.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r5,176
	ctx.r9.s64 = ctx.r5.s64 + 176;
	// lvx128 v30,r0,r8
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r4,192
	ctx.r8.s64 = ctx.r4.s64 + 192;
	// vmulfp128 v13,v13,v1
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v8,v8,v28
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(v28.f32), 0xFF));
	// vmsum4fp128 v7,v7,v2
	_mm_store_ps(ctx.v7.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v2.f32), 0xFF));
	// vmsum4fp128 v4,v13,v27
	_mm_store_ps(ctx.v4.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v27.f32), 0xFF));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v2,v11,v26
	_mm_store_ps(ctx.v2.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(v26.f32), 0xFF));
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v29,v29,11
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v31,r0,r9
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v28,v9,v1
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v22,v5,0
	_mm_store_ps(v22.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// addi r9,r4,224
	ctx.r9.s64 = ctx.r4.s64 + 224;
	// vmulfp128 v27,v6,v1
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v20,v11,0
	_mm_store_ps(v20.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// addi r7,r4,240
	ctx.r7.s64 = ctx.r4.s64 + 240;
	// vcfsx v21,v30,0
	_mm_store_ps(v21.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)));
	// addi r11,r5,192
	r11.s64 = ctx.r5.s64 + 192;
	// vcfsx v13,v13,11
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,224
	ctx.r8.s64 = ctx.r5.s64 + 224;
	// vcfsx v11,v9,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v26,r0,r9
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v9,v6,11
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,208
	ctx.r10.s64 = ctx.r5.s64 + 208;
	// vcfsx v19,v31,0
	_mm_store_ps(v19.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v31.u32)));
	// lvx128 v24,r0,r7
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r5,240
	ctx.r6.s64 = ctx.r5.s64 + 240;
	// lvx128 v25,r0,r8
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctuxs v12,v12,0
	// vmulfp128 v29,v29,v1
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v1.f32)));
	// li r11,16
	r11.s64 = 16;
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v25,v25,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)));
	// vmsum4fp128 v6,v28,v22
	_mm_store_ps(ctx.v6.f32, _mm_dp_ps(_mm_load_ps(v28.f32), _mm_load_ps(v22.f32), 0xFF));
	// vcfsx v22,v5,0
	_mm_store_ps(v22.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// lvx128 v23,r0,r6
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v30,v30,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)));
	// vctuxs v0,v0,0
	// vmsum4fp128 v31,v27,v21
	_mm_store_ps(v31.f32, _mm_dp_ps(_mm_load_ps(v27.f32), _mm_load_ps(v21.f32), 0xFF));
	// vcfsx v27,v26,11
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmulfp128 v13,v13,v1
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)));
	// vcfsx v26,v24,11
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmulfp128 v24,v11,v1
	_mm_store_ps(v24.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v21,v10,0
	// vmulfp128 v5,v9,v1
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v11,v4,0
	// vmsum4fp128 v29,v29,v20
	_mm_store_ps(v29.f32, _mm_dp_ps(_mm_load_ps(v29.f32), _mm_load_ps(v20.f32), 0xFF));
	// vctuxs v20,v7,0
	// vmsum4fp128 v28,v13,v19
	_mm_store_ps(v28.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v19.f32), 0xFF));
	// vctuxs v10,v6,0
	// vctuxs v13,v8,0
	// vmsum4fp128 v6,v24,v22
	_mm_store_ps(ctx.v6.f32, _mm_dp_ps(_mm_load_ps(v24.f32), _mm_load_ps(v22.f32), 0xFF));
	// vmulfp128 v8,v27,v1
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v7,v26,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v1.f32)));
	// vrlimi128 v0,v21,4,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(v21.f32), 228), 4));
	// vmsum4fp128 v5,v5,v30
	_mm_store_ps(ctx.v5.f32, _mm_dp_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(v30.f32), 0xFF));
	// vctuxs v31,v31,0
	// vctuxs v9,v29,0
	// vcfsx v29,v23,0
	_mm_store_ps(v29.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v23.u32)));
	// vrlimi128 v13,v20,4,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(v20.f32), 228), 4));
	// vmsum4fp128 v4,v8,v25
	_mm_store_ps(ctx.v4.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(v25.f32), 0xFF));
	// vrlimi128 v10,v31,4,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(v31.f32), 228), 4));
	// vctuxs v8,v6,0
	// vctuxs v6,v3,0
	// vctuxs v3,v2,0
	// vctuxs v5,v5,0
	// vmsum4fp128 v1,v7,v29
	_mm_store_ps(ctx.v1.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(v29.f32), 0xFF));
	// vctuxs v7,v4,0
	// vrlimi128 v12,v6,1,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 228), 1));
	// vctuxs v4,v28,0
	// vrlimi128 v11,v3,1,0
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 228), 1));
	// vrlimi128 v8,v5,4,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 4));
	// vrlimi128 v0,v12,3,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 228), 3));
	// vrlimi128 v13,v11,3,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 228), 3));
	// vctuxs v6,v1,0
	// vpkswus v0,v0,v13
	// vrlimi128 v9,v4,1,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v4.f32), 228), 1));
	// vrlimi128 v10,v9,3,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 228), 3));
	// vrlimi128 v7,v6,1,0
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 228), 1));
	// vrlimi128 v8,v7,3,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 228), 3));
	// vpkswus v13,v10,v8
	// vpkuhus v0,v0,v13
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r3,r11
	ea = ctx.r3.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263E7BC"))) PPC_WEAK_FUNC(sub_8263E7BC);
PPC_FUNC_IMPL(__imp__sub_8263E7BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263E7C0"))) PPC_WEAK_FUNC(sub_8263E7C0);
PPC_FUNC_IMPL(__imp__sub_8263E7C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// addi r11,r4,16
	r11.s64 = ctx.r4.s64 + 16;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v0,v0,11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r4,32
	ctx.r10.s64 = ctx.r4.s64 + 32;
	// addi r9,r4,48
	ctx.r9.s64 = ctx.r4.s64 + 48;
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,16
	ctx.r8.s64 = ctx.r5.s64 + 16;
	// vcfsx v10,v10,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// addi r7,r5,32
	ctx.r7.s64 = ctx.r5.s64 + 32;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// vcfsx v13,v13,11
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v12,v12,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// addi r10,r5,64
	ctx.r10.s64 = ctx.r5.s64 + 64;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,64
	r11.s64 = ctx.r4.s64 + 64;
	// addi r9,r4,80
	ctx.r9.s64 = ctx.r4.s64 + 80;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// addi r8,r5,80
	ctx.r8.s64 = ctx.r5.s64 + 80;
	// vmulfp128 v0,v0,v2
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r7,r4,160
	ctx.r7.s64 = ctx.r4.s64 + 160;
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r4,112
	ctx.r10.s64 = ctx.r4.s64 + 112;
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,96
	r11.s64 = ctx.r4.s64 + 96;
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v4,v4,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// vmulfp128 v13,v13,v2
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v2.f32)));
	// lvx128 v3,r0,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v12,v12,v2
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r8,r5,112
	ctx.r8.s64 = ctx.r5.s64 + 112;
	// vmulfp128 v11,v11,v2
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v2.f32)));
	// lvx128 v28,r0,r7
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v31,v6,11
	_mm_store_ps(v31.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r7,r4,208
	ctx.r7.s64 = ctx.r4.s64 + 208;
	// vcfsx v3,v3,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// vcfsx v27,v5,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmsum4fp128 v0,v0,v10
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32), 0xFF));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v10,v10,11
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r4,128
	r11.s64 = ctx.r4.s64 + 128;
	// vmsum4fp128 v13,v13,v9
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v9.f32), 0xFF));
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v9,v9,11
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v12,v12,v8
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32), 0xFF));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v11,v11,v7
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v7.f32), 0xFF));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v26,v8,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v8,v6,11
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v25,v7,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// addi r10,r4,144
	ctx.r10.s64 = ctx.r4.s64 + 144;
	// vmulfp128 v6,v4,v2
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r9,r5,128
	ctx.r9.s64 = ctx.r5.s64 + 128;
	// vmulfp128 v7,v31,v2
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r8,r5,144
	ctx.r8.s64 = ctx.r5.s64 + 144;
	// vmulfp128 v10,v10,v2
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r11,r4,176
	r11.s64 = ctx.r4.s64 + 176;
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,160
	ctx.r10.s64 = ctx.r5.s64 + 160;
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v4,v30,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v29,r0,r8
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,176
	ctx.r9.s64 = ctx.r5.s64 + 176;
	// vmulfp128 v9,v9,v2
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v2.f32)));
	// addi r8,r4,192
	ctx.r8.s64 = ctx.r4.s64 + 192;
	// lvx128 v31,r0,r10
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v6,v6,v3
	_mm_store_ps(ctx.v6.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v3.f32), 0xFF));
	// lvx128 v3,r0,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v7,v7,v27
	_mm_store_ps(ctx.v7.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(v27.f32), 0xFF));
	// vmsum4fp128 v10,v10,v26
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v26.f32), 0xFF));
	// vmsum4fp128 v9,v9,v25
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(v25.f32), 0xFF));
	// vmulfp128 v27,v8,v2
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v2.f32)));
	// lvx128 v8,r0,r8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v21,v5,0
	_mm_store_ps(v21.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// lvx128 v30,r0,r9
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v26,v4,v2
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v2.f32)));
	// lvx128 v4,r0,r7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v20,v29,0
	_mm_store_ps(v20.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// addi r9,r4,224
	ctx.r9.s64 = ctx.r4.s64 + 224;
	// vcfsx v8,v8,11
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r7,r4,240
	ctx.r7.s64 = ctx.r4.s64 + 240;
	// vcfsx v4,v4,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r5,192
	r11.s64 = ctx.r5.s64 + 192;
	// addi r10,r5,208
	ctx.r10.s64 = ctx.r5.s64 + 208;
	// vcfsx v19,v31,0
	_mm_store_ps(v19.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v31.u32)));
	// vcfsx v28,v28,11
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,224
	ctx.r8.s64 = ctx.r5.s64 + 224;
	// lvx128 v25,r0,r9
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v18,v30,0
	_mm_store_ps(v18.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)));
	// lvx128 v23,r0,r7
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v3,v3,11
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r5,240
	ctx.r6.s64 = ctx.r5.s64 + 240;
	// lvx128 v29,r0,r10
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v24,r0,r8
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v31,v27,v21
	_mm_store_ps(v31.f32, _mm_dp_ps(_mm_load_ps(v27.f32), _mm_load_ps(v21.f32), 0xFF));
	// vcfsx v27,v25,11
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v25,v5,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmsum4fp128 v30,v26,v20
	_mm_store_ps(v30.f32, _mm_dp_ps(_mm_load_ps(v26.f32), _mm_load_ps(v20.f32), 0xFF));
	// vcfsx v26,v23,11
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v23.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmulfp128 v8,v8,v2
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v2.f32)));
	// lvx128 v22,r0,r6
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v4,v4,v2
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v2.f32)));
	// vcfsx v23,v29,0
	_mm_store_ps(v23.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// vmulfp128 v28,v28,v2
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v29,v9,v1
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v1.f32)));
	// vcfsx v24,v24,0
	_mm_store_ps(v24.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)));
	// vmulfp128 v3,v3,v2
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v5,v13,v1
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v0,v0,v1
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v9,v27,v2
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v2.f32)));
	// vcfsx v22,v22,0
	_mm_store_ps(v22.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v22.u32)));
	// vmulfp128 v2,v26,v2
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v2.f32)));
	// vmsum4fp128 v8,v8,v25
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(v25.f32), 0xFF));
	// vctuxs v13,v12,0
	// vctuxs v26,v11,0
	// vmsum4fp128 v4,v4,v23
	_mm_store_ps(ctx.v4.f32, _mm_dp_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(v23.f32), 0xFF));
	// vmulfp128 v7,v7,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v28,v28,v19
	_mm_store_ps(v28.f32, _mm_dp_ps(_mm_load_ps(v28.f32), _mm_load_ps(v19.f32), 0xFF));
	// vmulfp128 v10,v10,v1
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v6,v6,v1
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v3,v3,v18
	_mm_store_ps(ctx.v3.f32, _mm_dp_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v18.f32), 0xFF));
	// vctuxs v5,v5,0
	// vctuxs v0,v0,0
	// vmsum4fp128 v12,v9,v24
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(v24.f32), 0xFF));
	// vmsum4fp128 v9,v2,v22
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(v22.f32), 0xFF));
	// vmulfp128 v2,v31,v1
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v31,v30,v1
	_mm_store_ps(v31.f32, _mm_mul_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v1.f32)));
	// vrlimi128 v13,v26,1,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(v26.f32), 228), 1));
	// vctuxs v11,v10,0
	// vmulfp128 v8,v8,v1
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v27,v6,0
	// vmulfp128 v4,v4,v1
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v30,v28,v1
	_mm_store_ps(v30.f32, _mm_mul_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v1.f32)));
	// vrlimi128 v0,v5,4,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 4));
	// vor v5,v13,v13
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vctuxs v13,v2,0
	// vmulfp128 v28,v12,v1
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// vrlimi128 v0,v5,3,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 3));
	// vctuxs v12,v7,0
	// vmulfp128 v7,v3,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v10,v8,0
	// vctuxs v3,v31,0
	// vctuxs v4,v4,0
	// vmulfp128 v6,v9,v1
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v9,v30,0
	// vctuxs v8,v28,0
	// vrlimi128 v12,v27,4,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(v27.f32), 228), 4));
	// vrlimi128 v13,v3,4,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 228), 4));
	// vrlimi128 v10,v4,4,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v4.f32), 228), 4));
	// vctuxs v7,v7,0
	// li r11,16
	r11.s64 = 16;
	// vctuxs v5,v29,0
	// vctuxs v6,v6,0
	// vrlimi128 v9,v7,1,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 228), 1));
	// vrlimi128 v11,v5,1,0
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 1));
	// vrlimi128 v8,v6,1,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 228), 1));
	// vrlimi128 v13,v9,3,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 228), 3));
	// vrlimi128 v12,v11,3,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 228), 3));
	// vrlimi128 v10,v8,3,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 228), 3));
	// vpkswus v0,v0,v12
	// vpkswus v13,v13,v10
	// vpkuhus v0,v0,v13
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r3,r11
	ea = ctx.r3.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263EA88"))) PPC_WEAK_FUNC(sub_8263EA88);
PPC_FUNC_IMPL(__imp__sub_8263EA88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	uint32_t ea{};
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,48
	ctx.r9.s64 = ctx.r5.s64 + 48;
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,32
	ctx.r10.s64 = ctx.r5.s64 + 32;
	// vcfsx v3,v0,11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r6,16
	ctx.r8.s64 = ctx.r6.s64 + 16;
	// vcfsx v0,v13,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// addi r7,r5,64
	ctx.r7.s64 = ctx.r5.s64 + 64;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,80
	r11.s64 = ctx.r5.s64 + 80;
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v12,v12,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v10,v10,11
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// vcfsx v13,v9,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// addi r8,r5,112
	ctx.r8.s64 = ctx.r5.s64 + 112;
	// lvx128 v8,r0,r7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r6,32
	ctx.r10.s64 = ctx.r6.s64 + 32;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v8,v8,11
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r6,48
	r11.s64 = ctx.r6.s64 + 48;
	// vcfsx v7,v7,11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r7,r6,112
	ctx.r7.s64 = ctx.r6.s64 + 112;
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v9,v3,v1
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v4,r0,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v5,v5,11
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v4,v4,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,128
	ctx.r10.s64 = ctx.r5.s64 + 128;
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r8,r6,64
	ctx.r8.s64 = ctx.r6.s64 + 64;
	// vmulfp128 v10,v10,v1
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v9,v9,v0
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vmsum4fp128 v3,v12,v0
	_mm_store_ps(ctx.v3.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v31,v10,v13
	_mm_store_ps(v31.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v28,v12,11
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v2,v11,v13
	_mm_store_ps(ctx.v2.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v12,v10,0
	_mm_store_ps(ctx.v12.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v10,v8,v1
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r11,r5,160
	r11.s64 = ctx.r5.s64 + 160;
	// vmulfp128 v8,v7,v1
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v7,v5,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r10,r5,176
	ctx.r10.s64 = ctx.r5.s64 + 176;
	// vcfsx v13,v13,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// addi r9,r6,80
	ctx.r9.s64 = ctx.r6.s64 + 80;
	// vmulfp128 v5,v4,v1
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r8,r5,224
	ctx.r8.s64 = ctx.r5.s64 + 224;
	// vcfsx v0,v6,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r5,192
	r11.s64 = ctx.r5.s64 + 192;
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,208
	ctx.r10.s64 = ctx.r5.s64 + 208;
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r6,96
	ctx.r9.s64 = ctx.r6.s64 + 96;
	// vcfsx v30,v30,11
	_mm_store_ps(v30.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmulfp128 v28,v28,v1
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v27,v7,v13
	_mm_store_ps(v27.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v7,v6,11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v26,v5,v13
	_mm_store_ps(v26.f32, _mm_dp_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,240
	r11.s64 = ctx.r5.s64 + 240;
	// vmsum4fp128 v10,v10,v0
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vmsum4fp128 v8,v8,v0
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vmulfp128 v25,v11,v1
	_mm_store_ps(v25.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v0,v29,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// lvx128 v6,r0,r9
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r8
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v24,v13,11
	_mm_store_ps(v24.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v29,r0,r7
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v13,v6,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// addi r11,r5,256
	r11.s64 = ctx.r5.s64 + 256;
	// vcfsx v6,v5,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,272
	ctx.r10.s64 = ctx.r5.s64 + 272;
	// vcfsx v23,v11,11
	_mm_store_ps(v23.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r5,288
	ctx.r9.s64 = ctx.r5.s64 + 288;
	// vcfsx v5,v4,11
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,304
	ctx.r8.s64 = ctx.r5.s64 + 304;
	// vmulfp128 v7,v7,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r7,r6,128
	ctx.r7.s64 = ctx.r6.s64 + 128;
	// vcfsx v11,v29,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// vmsum4fp128 v29,v25,v12
	_mm_store_ps(v29.f32, _mm_dp_ps(_mm_load_ps(v25.f32), _mm_load_ps(ctx.v12.f32), 0xFF));
	// vmulfp128 v4,v30,v1
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v30,v28,v12
	_mm_store_ps(v30.f32, _mm_dp_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v12.f32), 0xFF));
	// vmulfp128 v12,v24,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(v24.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v24,r0,r8
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v24,v24,11
	_mm_store_ps(v24.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,368
	ctx.r8.s64 = ctx.r5.s64 + 368;
	// vmulfp128 v21,v6,v1
	_mm_store_ps(v21.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v22,v23,v1
	_mm_store_ps(v22.f32, _mm_mul_ps(_mm_load_ps(v23.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v23,r0,r7
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v5,v5,v1
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r7,r6,160
	ctx.r7.s64 = ctx.r6.s64 + 160;
	// vmsum4fp128 v28,v7,v0
	_mm_store_ps(v28.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// lvx128 v7,r0,r10
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v7,v7,11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,336
	ctx.r10.s64 = ctx.r5.s64 + 336;
	// vmsum4fp128 v25,v4,v0
	_mm_store_ps(v25.f32, _mm_dp_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r9
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r6,144
	r11.s64 = ctx.r6.s64 + 144;
	// vcfsx v4,v4,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r5,352
	ctx.r9.s64 = ctx.r5.s64 + 352;
	// lvx128 v19,r0,r8
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,432
	ctx.r8.s64 = ctx.r5.s64 + 432;
	// lvx128 v18,r0,r7
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r6,192
	ctx.r7.s64 = ctx.r6.s64 + 192;
	// vmsum4fp128 v20,v12,v13
	_mm_store_ps(v20.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v12,v0,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v0,v23,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v23.u32)));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,320
	r11.s64 = ctx.r5.s64 + 320;
	// vmsum4fp128 v23,v22,v13
	_mm_store_ps(v23.f32, _mm_dp_ps(_mm_load_ps(v22.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v13,v6,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmsum4fp128 v22,v21,v11
	_mm_store_ps(v22.f32, _mm_dp_ps(_mm_load_ps(v21.f32), _mm_load_ps(ctx.v11.f32), 0xFF));
	// vmulfp128 v6,v24,v1
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(v24.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v21,v5,v11
	_mm_store_ps(v21.f32, _mm_dp_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v11.f32), 0xFF));
	// lvx128 v24,r0,r9
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v11,v7,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v1.f32)));
	// addi r9,r5,416
	ctx.r9.s64 = ctx.r5.s64 + 416;
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r6,176
	r11.s64 = ctx.r6.s64 + 176;
	// vmulfp128 v7,v4,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,400
	ctx.r10.s64 = ctx.r5.s64 + 400;
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v17,r0,r11
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,384
	r11.s64 = ctx.r5.s64 + 384;
	// vmsum4fp128 v63,v6,v13
	_mm_store_ps(v63.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v6,v19,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v19.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v15,v11,v0
	_mm_store_ps(v15.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vcfsx v11,v4,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v14,v7,v13
	_mm_store_ps(v14.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vcfsx v7,v24,11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v13,v17,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v17.u32)));
	// vmsum4fp128 v16,v12,v0
	_mm_store_ps(v16.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vcfsx v12,v5,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v0,v18,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v18.u32)));
	// vmulfp128 v6,v6,v1
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v7,v7,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// vmsum4fp128 v17,v6,v13
	_mm_store_ps(v17.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vmsum4fp128 v19,v11,v0
	_mm_store_ps(v19.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vmsum4fp128 v18,v7,v13
	_mm_store_ps(v18.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v24,v12,v0
	_mm_store_ps(v24.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r6,208
	r11.s64 = ctx.r6.s64 + 208;
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v60,v0,11
	_mm_store_ps(v60.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v11,r0,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v12,v12,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v11,v11,11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v6,r0,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v59,v13,11
	_mm_store_ps(v59.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r5,448
	r11.s64 = ctx.r5.s64 + 448;
	// vcfsx v13,v6,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// addi r10,r5,464
	ctx.r10.s64 = ctx.r5.s64 + 464;
	// addi r8,r5,480
	ctx.r8.s64 = ctx.r5.s64 + 480;
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v0,v7,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// addi r9,r6,224
	ctx.r9.s64 = ctx.r6.s64 + 224;
	// vctuxs v2,v2,0
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,496
	r11.s64 = ctx.r5.s64 + 496;
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v56,v5,11
	_mm_store_ps(v56.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v61,r0,r8
	_mm_store_si128((__m128i*)v61.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v55,v4,11
	_mm_store_ps(v55.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcsxwfp128 v61,v61,11
	_mm_store_ps(v61.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v61.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r6,240
	ctx.r10.s64 = ctx.r6.s64 + 240;
	// vmulfp128 v7,v60,v1
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(v60.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v62,r0,r9
	_mm_store_si128((__m128i*)v62.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v12,v12,v1
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v11,v11,v1
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v6,v59,v1
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(v59.f32), _mm_load_ps(ctx.v1.f32)));
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vctuxs v28,v28,0
	// vctuxs v25,v25,0
	// vmsum4fp128 v60,v7,v0
	_mm_store_ps(v60.f32, _mm_dp_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vctuxs v7,v23,0
	// vmsum4fp128 v58,v12,v13
	_mm_store_ps(v58.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vctuxs v12,v10,0
	// vmsum4fp128 v57,v11,v13
	_mm_store_ps(v57.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// vctuxs v13,v3,0
	// vcfsx v3,v5,11
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum4fp128 v59,v6,v0
	_mm_store_ps(v59.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// vctuxs v10,v30,0
	// vcfsx v5,v4,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// vcsxwfp128 v6,v62,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v62.u32)));
	// vctuxs v0,v9,0
	// vmulfp128 v30,v55,v1
	_mm_store_ps(v30.f32, _mm_mul_ps(_mm_load_ps(v55.f32), _mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v4,v56,v1
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(v56.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v9,v29,0
	// vmulfp128 v29,v61,v1
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(v61.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v11,v8,0
	// vctuxs v8,v20,0
	// vmulfp128 v3,v3,v1
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v1.f32)));
	// vctuxs v1,v27,0
	// vrlimi128 v10,v28,4,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(v28.f32), 228), 4));
	// vctuxs v27,v31,0
	// vrlimi128 v0,v2,4,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 228), 4));
	// vctuxs v2,v24,0
	// vmsum4fp128 v30,v30,v6
	_mm_store_ps(v30.f32, _mm_dp_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v6.f32), 0xFF));
	// vmsum4fp128 v23,v4,v6
	_mm_store_ps(v23.f32, _mm_dp_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v6.f32), 0xFF));
	// vctuxs v6,v16,0
	// vctuxs v16,v14,0
	// vrlimi128 v9,v25,4,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(v25.f32), 228), 4));
	// vmsum4fp128 v29,v29,v5
	_mm_store_ps(v29.f32, _mm_dp_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v5.f32), 0xFF));
	// vcfpuxws128 v4,v60,0
	// vcfpuxws128 v14,v58,0
	// vmsum4fp128 v20,v3,v5
	_mm_store_ps(v20.f32, _mm_dp_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v5.f32), 0xFF));
	// vctuxs v5,v15,0
	// vcfpuxws128 v15,v63,0
	// vrlimi128 v12,v1,1,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v1.f32), 228), 1));
	// vcfpuxws128 v3,v59,0
	// vrlimi128 v13,v27,4,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(v27.f32), 228), 4));
	// vcfpuxws128 v63,v57,0
	// vctuxs v1,v19,0
	// vor v24,v12,v12
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vrlimi128 v6,v16,4,0
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(v16.f32), 228), 4));
	// vctuxs v12,v30,0
	// vrlimi128 v4,v14,4,0
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(v14.f32), 228), 4));
	// vctuxs v31,v23,0
	// vrlimi128 v0,v24,3,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(v24.f32), 228), 3));
	// vctuxs v30,v26,0
	// vrlimi128 v5,v15,4,0
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(v15.f32), 228), 4));
	// vrlimi128 v3,v63,4,0
	_mm_store_ps(ctx.v3.f32, _mm_blend_ps(_mm_load_ps(ctx.v3.f32), _mm_permute_ps(_mm_load_ps(v63.f32), 228), 4));
	// vrlimi128 v11,v30,1,0
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(v30.f32), 228), 1));
	// vctuxs v30,v29,0
	// vctuxs v28,v18,0
	// li r11,16
	r11.s64 = 16;
	// vctuxs v27,v22,0
	// vctuxs v29,v17,0
	// vrlimi128 v13,v11,3,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 228), 3));
	// vctuxs v11,v20,0
	// vctuxs v26,v21,0
	// vrlimi128 v31,v30,1,0
	_mm_store_ps(v31.f32, _mm_blend_ps(_mm_load_ps(v31.f32), _mm_permute_ps(_mm_load_ps(v30.f32), 228), 1));
	// vrlimi128 v2,v28,1,0
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(v28.f32), 228), 1));
	// vrlimi128 v8,v27,1,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(v27.f32), 228), 1));
	// vrlimi128 v1,v29,1,0
	_mm_store_ps(ctx.v1.f32, _mm_blend_ps(_mm_load_ps(ctx.v1.f32), _mm_permute_ps(_mm_load_ps(v29.f32), 228), 1));
	// vrlimi128 v12,v11,1,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 228), 1));
	// vrlimi128 v6,v2,3,0
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 228), 3));
	// vrlimi128 v10,v8,3,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 228), 3));
	// vrlimi128 v4,v31,3,0
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(v31.f32), 228), 3));
	// vrlimi128 v7,v26,1,0
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(v26.f32), 228), 1));
	// vrlimi128 v3,v12,3,0
	_mm_store_ps(ctx.v3.f32, _mm_blend_ps(_mm_load_ps(ctx.v3.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 228), 3));
	// vpkswus v0,v0,v10
	// vrlimi128 v5,v1,3,0
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(ctx.v1.f32), 228), 3));
	// vpkswus v12,v6,v4
	// vrlimi128 v9,v7,3,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 228), 3));
	// vpkuhus v0,v0,v12
	// vpkswus v13,v13,v9
	// vpkswus v12,v5,v3
	// vpkuhus v13,v13,v12
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,r3,r11
	ea = ctx.r3.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stvlx v13,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r4,r11
	ea = ctx.r4.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263EED4"))) PPC_WEAK_FUNC(sub_8263EED4);
PPC_FUNC_IMPL(__imp__sub_8263EED4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263EED8"))) PPC_WEAK_FUNC(sub_8263EED8);
PPC_FUNC_IMPL(__imp__sub_8263EED8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r31{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r10,r5,64
	ctx.r10.s64 = ctx.r5.s64 + 64;
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v13,r0,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r6,32
	ctx.r9.s64 = ctx.r6.s64 + 32;
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// vcfsx v0,v0,11
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,32
	ctx.r8.s64 = ctx.r5.s64 + 32;
	// addi r7,r6,16
	ctx.r7.s64 = ctx.r6.s64 + 16;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,80
	ctx.r10.s64 = ctx.r5.s64 + 80;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v30,v11,11
	_mm_store_ps(v30.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v12,v12,11
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v11,v10,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// addi r11,r5,48
	r11.s64 = ctx.r5.s64 + 48;
	// lvx128 v9,r0,r8
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,96
	ctx.r9.s64 = ctx.r5.s64 + 96;
	// lvx128 v6,r0,r10
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r6,48
	ctx.r8.s64 = ctx.r6.s64 + 48;
	// vcfsx v10,v6,11
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r6,64
	ctx.r10.s64 = ctx.r6.s64 + 64;
	// addi r31,r5,112
	r31.s64 = ctx.r5.s64 + 112;
	// vcfsx v23,v9,11
	_mm_store_ps(v23.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v8,r0,r11
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,128
	r11.s64 = ctx.r5.s64 + 128;
	// lvx128 v7,r0,r7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v22,v8,11
	_mm_store_ps(v22.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v5,r0,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,144
	ctx.r9.s64 = ctx.r5.s64 + 144;
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,160
	ctx.r8.s64 = ctx.r5.s64 + 160;
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r5,176
	ctx.r7.s64 = ctx.r5.s64 + 176;
	// vmsum3fp128 v13,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r11,r6,80
	r11.s64 = ctx.r6.s64 + 80;
	// vmsum3fp128 v12,v30,v11
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// lvx128 v4,r0,r31
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v2,v7,0
	_mm_store_ps(ctx.v2.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// lvx128 v7,r0,r9
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v21,v5,11
	_mm_store_ps(v21.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v10,v11
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// vcfsx v10,v9,11
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v9,v8,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v20,v4,11
	_mm_store_ps(v20.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r5,208
	r31.s64 = ctx.r5.s64 + 208;
	// vcfsx v31,v3,0
	_mm_store_ps(v31.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)));
	// addi r10,r5,192
	ctx.r10.s64 = ctx.r5.s64 + 192;
	// vcfsx v18,v5,11
	_mm_store_ps(v18.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r9,r6,96
	ctx.r9.s64 = ctx.r6.s64 + 96;
	// vcfsx v8,v7,11
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,224
	ctx.r8.s64 = ctx.r5.s64 + 224;
	// vcfsx v19,v6,11
	_mm_store_ps(v19.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r7,r5,240
	ctx.r7.s64 = ctx.r5.s64 + 240;
	// vcfsx v30,v4,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)));
	// addi r11,r6,112
	r11.s64 = ctx.r6.s64 + 112;
	// lvx128 v29,r0,r31
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v5,v29,11
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v28,r0,r9
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v6,v3,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v27,r0,r8
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v7,v28,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)));
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v17,v27,11
	_mm_store_ps(v17.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v27.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v25,r0,r11
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v16,v26,11
	_mm_store_ps(v16.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v29,v25,0
	_mm_store_ps(v29.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)));
	// vmsum3fp128 v10,v10,v9
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r11,r5,256
	r11.s64 = ctx.r5.s64 + 256;
	// addi r10,r5,272
	ctx.r10.s64 = ctx.r5.s64 + 272;
	// addi r9,r6,128
	ctx.r9.s64 = ctx.r6.s64 + 128;
	// addi r8,r5,288
	ctx.r8.s64 = ctx.r5.s64 + 288;
	// addi r7,r5,304
	ctx.r7.s64 = ctx.r5.s64 + 304;
	// addi r31,r6,144
	r31.s64 = ctx.r6.s64 + 144;
	// lvx128 v4,r0,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,320
	r11.s64 = ctx.r5.s64 + 320;
	// lvx128 v28,r0,r9
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r6,160
	ctx.r9.s64 = ctx.r6.s64 + 160;
	// lvx128 v3,r0,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,336
	ctx.r10.s64 = ctx.r5.s64 + 336;
	// vmsum3fp128 v9,v8,v9
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32), 0xEF));
	// lvx128 v27,r0,r8
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v8,v6,v7
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v7.f32), 0xEF));
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v5,v7
	_mm_store_ps(ctx.v7.f32, _mm_dp_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v7.f32), 0xEF));
	// lvx128 v24,r0,r11
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v14,r0,r9
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v6,v4,11
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v4.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vcfsx v5,v28,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)));
	// lvx128 v15,r0,r10
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v4,v3,11
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v3.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r6,176
	r11.s64 = ctx.r6.s64 + 176;
	// vcsxwfp128 v60,v27,11
	_mm_store_ps(v60.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v27.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r8,r5,352
	ctx.r8.s64 = ctx.r5.s64 + 352;
	// vcfsx v27,v24,11
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,400
	ctx.r10.s64 = ctx.r5.s64 + 400;
	// vcfsx v3,v14,0
	_mm_store_ps(ctx.v3.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v14.u32)));
	// addi r9,r6,192
	ctx.r9.s64 = ctx.r6.s64 + 192;
	// vcsxwfp128 v59,v26,11
	_mm_store_ps(v59.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v26.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v25,r0,r31
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v26,v15,11
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v15.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v61,r0,r11
	_mm_store_si128((__m128i*)v61.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,384
	r11.s64 = ctx.r5.s64 + 384;
	// lvx128 v63,r0,r8
	_mm_store_si128((__m128i*)v63.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r5,368
	ctx.r7.s64 = ctx.r5.s64 + 368;
	// vcfsx v28,v25,0
	_mm_store_ps(v28.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)));
	// lvx128 v24,r0,r10
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v15,v63,11
	_mm_store_ps(v15.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v63.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v63,r0,r9
	_mm_store_si128((__m128i*)v63.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,464
	ctx.r10.s64 = ctx.r5.s64 + 464;
	// addi r8,r5,416
	ctx.r8.s64 = ctx.r5.s64 + 416;
	// vcsxwfp128 v50,v24,11
	_mm_store_ps(v50.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v24.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v6,v6,v5
	_mm_store_ps(ctx.v6.f32, _mm_dp_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v5.f32), 0xEF));
	// lvx128 v25,r0,r11
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v4,v5
	_mm_store_ps(ctx.v5.f32, _mm_dp_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v5.f32), 0xEF));
	// addi r11,r5,448
	r11.s64 = ctx.r5.s64 + 448;
	// lvx128 v62,r0,r7
	_mm_store_si128((__m128i*)v62.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r6,208
	r31.s64 = ctx.r6.s64 + 208;
	// addi r7,r5,432
	ctx.r7.s64 = ctx.r5.s64 + 432;
	// lvx128 v55,r0,r10
	_mm_store_si128((__m128i*)v55.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v4,v27,v3
	_mm_store_ps(ctx.v4.f32, _mm_dp_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v3.f32), 0xEF));
	// vcsxwfp128 v27,v61,0
	_mm_store_ps(v27.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v61.u32)));
	// vcsxwfp128 v61,v25,11
	_mm_store_ps(v61.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v25.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r10,r5,480
	ctx.r10.s64 = ctx.r5.s64 + 480;
	// vmsum3fp128 v3,v26,v3
	_mm_store_ps(ctx.v3.f32, _mm_dp_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v3.f32), 0xEF));
	// vcsxwfp128 v26,v63,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v63.u32)));
	// lvx128 v56,r0,r11
	_mm_store_si128((__m128i*)v56.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v14,v62,11
	_mm_store_ps(v14.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v62.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// addi r11,r6,224
	r11.s64 = ctx.r6.s64 + 224;
	// lvx128 v62,r0,r8
	_mm_store_si128((__m128i*)v62.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,496
	ctx.r9.s64 = ctx.r5.s64 + 496;
	// lvx128 v58,r0,r7
	_mm_store_si128((__m128i*)v58.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r6,240
	ctx.r8.s64 = ctx.r6.s64 + 240;
	// lvx128 v57,r0,r31
	_mm_store_si128((__m128i*)v57.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v63,v62,11
	_mm_store_ps(v63.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v62.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v19,v19,v30
	_mm_store_ps(v19.f32, _mm_dp_ps(_mm_load_ps(v19.f32), _mm_load_ps(v30.f32), 0xEF));
	// vcsxwfp128 v62,v58,11
	_mm_store_ps(v62.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v58.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// lvx128 v53,r0,r10
	_mm_store_si128((__m128i*)v53.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v25,v57,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v57.u32)));
	// lvx128 v54,r0,r11
	_mm_store_si128((__m128i*)v54.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcsxwfp128 v58,v56,11
	_mm_store_ps(v58.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v56.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v30,v18,v30
	_mm_store_ps(v30.f32, _mm_dp_ps(_mm_load_ps(v18.f32), _mm_load_ps(v30.f32), 0xEF));
	// vcsxwfp128 v57,v55,11
	_mm_store_ps(v57.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v55.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v56,v23,v2
	_mm_store_ps(v56.f32, _mm_dp_ps(_mm_load_ps(v23.f32), _mm_load_ps(ctx.v2.f32), 0xEF));
	// vmsum3fp128 v55,v22,v2
	_mm_store_ps(v55.f32, _mm_dp_ps(_mm_load_ps(v22.f32), _mm_load_ps(ctx.v2.f32), 0xEF));
	// lvx128 v52,r0,r9
	_mm_store_si128((__m128i*)v52.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v21,v21,v31
	_mm_store_ps(v21.f32, _mm_dp_ps(_mm_load_ps(v21.f32), _mm_load_ps(v31.f32), 0xEF));
	// lvx128 v51,r0,r8
	_mm_store_si128((__m128i*)v51.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v20,v20,v31
	_mm_store_ps(v20.f32, _mm_dp_ps(_mm_load_ps(v20.f32), _mm_load_ps(v31.f32), 0xEF));
	// vcsxwfp128 v24,v54,0
	_mm_store_ps(v24.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v54.u32)));
	// vmsum3fp128 v18,v17,v29
	_mm_store_ps(v18.f32, _mm_dp_ps(_mm_load_ps(v17.f32), _mm_load_ps(v29.f32), 0xEF));
	// vcsxwfp128 v23,v53,11
	_mm_store_ps(v23.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v53.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vmsum3fp128 v29,v16,v29
	_mm_store_ps(v29.f32, _mm_dp_ps(_mm_load_ps(v16.f32), _mm_load_ps(v29.f32), 0xEF));
	// vmsum3fp128 v2,v61,v26
	_mm_store_ps(ctx.v2.f32, _mm_dp_ps(_mm_load_ps(v61.f32), _mm_load_ps(v26.f32), 0xEF));
	// vmsum3fp128 v31,v50,v26
	_mm_store_ps(v31.f32, _mm_dp_ps(_mm_load_ps(v50.f32), _mm_load_ps(v26.f32), 0xEF));
	// vrlimi128 v9,v30,4,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(v30.f32), 228), 4));
	// vmsum3fp128 v30,v58,v24
	_mm_store_ps(v30.f32, _mm_dp_ps(_mm_load_ps(v58.f32), _mm_load_ps(v24.f32), 0xEF));
	// vrlimi128 v7,v29,1,0
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(v29.f32), 228), 1));
	// vmsum3fp128 v29,v57,v24
	_mm_store_ps(v29.f32, _mm_dp_ps(_mm_load_ps(v57.f32), _mm_load_ps(v24.f32), 0xEF));
	// vrlimi128 v11,v20,1,0
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(v20.f32), 228), 1));
	// vmsum3fp128 v24,v60,v28
	_mm_store_ps(v24.f32, _mm_dp_ps(_mm_load_ps(v60.f32), _mm_load_ps(v28.f32), 0xEF));
	// vmsum3fp128 v20,v59,v28
	_mm_store_ps(v20.f32, _mm_dp_ps(_mm_load_ps(v59.f32), _mm_load_ps(v28.f32), 0xEF));
	// lis r11,-32138
	r11.s64 = -2106195968;
	// vmsum3fp128 v17,v63,v25
	_mm_store_ps(v17.f32, _mm_dp_ps(_mm_load_ps(v63.f32), _mm_load_ps(v25.f32), 0xEF));
	// vrlimi128 v0,v56,4,0
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(v56.f32), 228), 4));
	// vcsxwfp128 v26,v51,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v51.u32)));
	// addi r11,r11,15888
	r11.s64 = r11.s64 + 15888;
	// vcsxwfp128 v22,v52,11
	_mm_store_ps(v22.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)v52.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3A000000)))));
	// vrlimi128 v10,v19,4,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(v19.f32), 228), 4));
	// vrlimi128 v12,v21,1,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(v21.f32), 228), 1));
	// vmsum3fp128 v16,v62,v25
	_mm_store_ps(v16.f32, _mm_dp_ps(_mm_load_ps(v62.f32), _mm_load_ps(v25.f32), 0xEF));
	// vor v21,v0,v0
	_mm_store_si128((__m128i*)v21.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmsum3fp128 v19,v15,v27
	_mm_store_ps(v19.f32, _mm_dp_ps(_mm_load_ps(v15.f32), _mm_load_ps(v27.f32), 0xEF));
	// vrlimi128 v8,v18,1,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(v18.f32), 228), 1));
	// vmsum3fp128 v18,v14,v27
	_mm_store_ps(v18.f32, _mm_dp_ps(_mm_load_ps(v14.f32), _mm_load_ps(v27.f32), 0xEF));
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v13,v55,4,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(v55.f32), 228), 4));
	// vsubfp v25,v11,v0
	_mm_store_ps(v25.f32, _mm_sub_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// li r11,16
	r11.s64 = 16;
	// vsubfp v11,v10,v0
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v10,v9,v0
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v28,v21,v0
	_mm_store_ps(v28.f32, _mm_sub_ps(_mm_load_ps(v21.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v27,v13,v0
	_mm_store_ps(v27.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v23,v23,v26
	_mm_store_ps(v23.f32, _mm_dp_ps(_mm_load_ps(v23.f32), _mm_load_ps(v26.f32), 0xEF));
	// vrlimi128 v6,v24,4,0
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(v24.f32), 228), 4));
	// vmsum3fp128 v22,v22,v26
	_mm_store_ps(v22.f32, _mm_dp_ps(_mm_load_ps(v22.f32), _mm_load_ps(v26.f32), 0xEF));
	// vrlimi128 v5,v20,4,0
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(v20.f32), 228), 4));
	// vsubfp v26,v12,v0
	_mm_store_ps(v26.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v2,v17,4,0
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(v17.f32), 228), 4));
	// vsubfp v24,v7,v0
	_mm_store_ps(v24.f32, _mm_sub_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v9,v6,v0
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v6,v5,v0
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v31,v16,4,0
	_mm_store_ps(v31.f32, _mm_blend_ps(_mm_load_ps(v31.f32), _mm_permute_ps(_mm_load_ps(v16.f32), 228), 4));
	// vsubfp v5,v2,v0
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v4,v19,1,0
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(v19.f32), 228), 1));
	// vrlimi128 v3,v18,1,0
	_mm_store_ps(ctx.v3.f32, _mm_blend_ps(_mm_load_ps(ctx.v3.f32), _mm_permute_ps(_mm_load_ps(v18.f32), 228), 1));
	// vmaddfp v10,v10,v1,v0
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v13,v28,v1,v0
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v2,v31,v0
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v31,v8,v0
	_mm_store_ps(v31.f32, _mm_sub_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v3,v3,v0
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v4,v4,v0
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v11,v11,v1,v0
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v12,v27,v1,v0
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v30,v23,1,0
	_mm_store_ps(v30.f32, _mm_blend_ps(_mm_load_ps(v30.f32), _mm_permute_ps(_mm_load_ps(v23.f32), 228), 1));
	// vrlimi128 v29,v22,1,0
	_mm_store_ps(v29.f32, _mm_blend_ps(_mm_load_ps(v29.f32), _mm_permute_ps(_mm_load_ps(v22.f32), 228), 1));
	// vmaddfp v9,v9,v1,v0
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v8,v6,v1,v0
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v7,v5,v1,v0
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v5,v26,v1,v0
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v30,v30,v0
	_mm_store_ps(v30.f32, _mm_sub_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v29,v29,v0
	_mm_store_ps(v29.f32, _mm_sub_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v6,v2,v1,v0
	_mm_store_ps(ctx.v6.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v2,v31,v1,v0
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v31.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v31,v25,v1,v0
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v25.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v4,v4,v1,v0
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v13,v5,3,0
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 3));
	// vmaddfp v5,v3,v1,v0
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v11,v2,3,0
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 228), 3));
	// vrlimi128 v12,v31,3,0
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(v31.f32), 228), 3));
	// vrlimi128 v9,v4,3,0
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v4.f32), 228), 3));
	// vrlimi128 v8,v5,3,0
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 3));
	// vmaddfp v5,v24,v1,v0
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v24.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vrlimi128 v10,v5,3,0
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 3));
	// vmaddfp v5,v30,v1,v0
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v0,v29,v1,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vctuxs v10,v10,0
	// vrlimi128 v7,v5,3,0
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 228), 3));
	// vctuxs v5,v11,0
	// vrlimi128 v6,v0,3,0
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 228), 3));
	// vctuxs v0,v13,0
	// vctuxs v13,v12,0
	// vctuxs v12,v9,0
	// vctuxs v11,v8,0
	// vctuxs v9,v7,0
	// vpkswus v0,v0,v5
	// vctuxs v8,v6,0
	// vpkswus v12,v12,v9
	// vpkswus v13,v13,v10
	// li r10,16
	ctx.r10.s64 = 16;
	// vpkuhus v0,v0,v12
	// stvlx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// vpkswus v11,v11,v8
	// stvrx v0,r3,r11
	ea = ctx.r3.u32 + r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// vpkuhus v13,v13,v11
	// stvlx v13,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,r4,r10
	ea = ctx.r4.u32 + ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8263F2DC"))) PPC_WEAK_FUNC(sub_8263F2DC);
PPC_FUNC_IMPL(__imp__sub_8263F2DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8263F2E0"))) PPC_WEAK_FUNC(sub_8263F2E0);
PPC_FUNC_IMPL(__imp__sub_8263F2E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCVRegister v18{};
	PPCVRegister v54{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5d8
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// fmr f26,f1
	ctx.fpscr.disableFlushMode();
	f26.f64 = ctx.f1.f64;
	// fmr f24,f2
	f24.f64 = ctx.f2.f64;
	// fmr f27,f3
	f27.f64 = ctx.f3.f64;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// fmr f25,f4
	f25.f64 = ctx.f4.f64;
	// stw r15,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, r15.u32);
	// bne cr6,0x8263f328
	if (!cr6.eq) goto loc_8263F328;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d624
	// b 0x8239bd10
	return;
loc_8263F328:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r23,20(r15)
	r23.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// lwz r21,15344(r15)
	r21.u64 = PPC_LOAD_U32(r15.u32 + 15344);
	// srawi r29,r23,1
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1) != 0);
	r29.s64 = r23.s32 >> 1;
	// lwz r20,15348(r15)
	r20.u64 = PPC_LOAD_U32(r15.u32 + 15348);
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stw r21,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r21.u32);
	// stw r20,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r20.u32);
	// lfd f28,-31368(r11)
	f28.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// fcmpu cr6,f26,f28
	cr6.compare(f26.f64, f28.f64);
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// vspltw v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// srawi r27,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r27.s64 = r11.s32 >> 1;
	// srawi r26,r23,5
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x1F) != 0);
	r26.s64 = r23.s32 >> 5;
	// srawi r24,r23,6
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x3F) != 0);
	r24.s64 = r23.s32 >> 6;
	// stw r27,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r27.u32);
	// stvx v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r26,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r26.u32);
	// stw r24,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r24.u32);
	// beq cr6,0x82640778
	if (cr6.eq) goto loc_82640778;
	// fcmpu cr6,f27,f28
	cr6.compare(f27.f64, f28.f64);
	// beq cr6,0x82640778
	if (cr6.eq) goto loc_82640778;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// rotlwi r10,r23,0
	ctx.r10.u64 = __builtin_rotateleft32(r23.u32, 0);
	// lfd f29,-30984(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -30984);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// fdiv f0,f29,f26
	f0.f64 = f29.f64 / f26.f64;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// fmul f0,f0,f24
	f0.f64 = f0.f64 * f24.f64;
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f24
	ctx.f13.f64 = ctx.f13.f64 - f24.f64;
	// fdiv f31,f13,f26
	f31.f64 = ctx.f13.f64 / f26.f64;
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// ble cr6,0x8263f3d4
	if (!cr6.gt) goto loc_8263F3D4;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// fmr f0,f31
	f0.f64 = f31.f64;
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
loc_8263F3D4:
	// fcmpu cr6,f0,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x8263f3e0
	if (!cr6.lt) goto loc_8263F3E0;
	// fmr f0,f28
	f0.f64 = f28.f64;
loc_8263F3E0:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x8263f408
	if (!cr6.lt) goto loc_8263F408;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263F408:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fctiwz f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lfd f0,30704(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30704);
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// fadd f0,f31,f0
	f0.f64 = f31.f64 + f0.f64;
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x8263f444
	if (!cr6.lt) goto loc_8263F444;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_8263F444:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = ctx.f1.f64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// blt cr6,0x8263f470
	if (cr6.lt) goto loc_8263F470;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263F470:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f28.f64);
	// bge cr6,0x8263f484
	if (!cr6.lt) goto loc_8263F484;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
loc_8263F484:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,124
	r11.s64 = ctx.r1.s64 + 124;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// fcmpu cr6,f30,f28
	cr6.compare(f30.f64, f28.f64);
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// bge cr6,0x8263f4a4
	if (!cr6.lt) goto loc_8263F4A4;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// b 0x8263f4a8
	goto loc_8263F4A8;
loc_8263F4A4:
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
loc_8263F4A8:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,140
	r11.s64 = ctx.r1.s64 + 140;
	// fctiwz f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// fdiv f0,f29,f27
	f0.f64 = f29.f64 / f27.f64;
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,15324(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15324);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmul f0,f0,f25
	f0.f64 = f0.f64 * f25.f64;
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f25
	ctx.f13.f64 = ctx.f13.f64 - f25.f64;
	// fdiv f12,f13,f27
	ctx.f12.f64 = ctx.f13.f64 / f27.f64;
	// lfd f13,-31360(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fadd f31,f12,f13
	f31.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// ble cr6,0x8263f4fc
	if (!cr6.gt) goto loc_8263F4FC;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// fmr f0,f31
	f0.f64 = f31.f64;
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
loc_8263F4FC:
	// fcmpu cr6,f0,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x8263f508
	if (!cr6.lt) goto loc_8263F508;
	// fmr f0,f28
	f0.f64 = f28.f64;
loc_8263F508:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// lwz r11,15332(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15332);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x8263f530
	if (!cr6.lt) goto loc_8263F530;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_8263F530:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15332(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15332);
	// addi r10,r1,180
	ctx.r10.s64 = ctx.r1.s64 + 180;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// blt cr6,0x8263f560
	if (cr6.lt) goto loc_8263F560;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_8263F560:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f28.f64);
	// bge cr6,0x8263f574
	if (!cr6.lt) goto loc_8263F574;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
loc_8263F574:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r31,r11,0,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// rlwinm r14,r10,0,0,30
	r14.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r30,r11,0,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// rlwinm r28,r11,0,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r31,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r31.u32);
	// stw r14,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r14.u32);
	// stw r30,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r30.u32);
	// stw r28,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r28.u32);
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rlwinm r25,r11,0,0,30
	r25.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// cmpwi cr6,r25,2
	cr6.compare<int32_t>(r25.s32, 2, xer);
	// stw r25,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r25.u32);
	// bge cr6,0x8263f5d8
	if (!cr6.lt) goto loc_8263F5D8;
	// li r25,2
	r25.s64 = 2;
	// stw r25,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r25.u32);
loc_8263F5D8:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r7,15352(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 15352);
	// addi r10,r1,156
	ctx.r10.s64 = ctx.r1.s64 + 156;
	// lwz r5,15356(r15)
	ctx.r5.u64 = PPC_LOAD_U32(r15.u32 + 15356);
	// addi r9,r1,148
	ctx.r9.s64 = ctx.r1.s64 + 148;
	// lwz r6,15360(r15)
	ctx.r6.u64 = PPC_LOAD_U32(r15.u32 + 15360);
	// addi r8,r1,152
	ctx.r8.s64 = ctx.r1.s64 + 152;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfd f0,30696(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30696);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f13,f26,f0
	ctx.f13.f64 = f26.f64 * f0.f64;
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// fmul f12,f27,f0
	ctx.f12.f64 = f27.f64 * f0.f64;
	// stw r5,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r5.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// lfd f0,30680(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30680);
	// srawi r11,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r11.s64 = r31.s32 >> 1;
	// fmul f11,f25,f0
	ctx.f11.f64 = f25.f64 * f0.f64;
	// fmul f0,f24,f0
	f0.f64 = f24.f64 * f0.f64;
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r11.u32);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r11.u32);
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// fctiwz f10,f0
	ctx.f10.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stw r11,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f0,264(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r22,156(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r11,r11,r14
	r11.s64 = int64_t(r11.s32) * int64_t(r14.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mullw r9,r22,r31
	ctx.r9.s64 = int64_t(r22.s32) * int64_t(r31.s32);
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// subf r31,r10,r11
	r31.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bge cr6,0x8263f6ac
	if (!cr6.lt) goto loc_8263F6AC;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
loc_8263F6AC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8263f6b8
	if (!cr6.lt) goto loc_8263F6B8;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
loc_8263F6B8:
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// li r3,128
	ctx.r3.s64 = 128;
	// cmpwi cr6,r14,0
	cr6.compare<int32_t>(r14.s32, 0, xer);
	// ble cr6,0x8263f77c
	if (!cr6.gt) goto loc_8263F77C;
loc_8263F6C8:
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8263f6f4
	if (!cr6.gt) goto loc_8263F6F4;
loc_8263F6DC:
	// stb r4,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8263f6dc
	if (cr6.lt) goto loc_8263F6DC;
loc_8263F6F4:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x8263f720
	if (!cr6.gt) goto loc_8263F720;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// subf r9,r6,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r6.s64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_8263F708:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r3,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r3.u8);
	// stb r3,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r3.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263f708
	if (!cr6.eq) goto loc_8263F708;
loc_8263F720:
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// add r5,r5,r27
	ctx.r5.u64 = ctx.r5.u64 + r27.u64;
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// add r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x8263f75c
	if (!cr6.gt) goto loc_8263F75C;
loc_8263F744:
	// stb r4,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,15328(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x8263f744
	if (cr6.lt) goto loc_8263F744;
loc_8263F75C:
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r14
	cr6.compare<int32_t>(ctx.r8.s32, r14.s32, xer);
	// blt cr6,0x8263f6c8
	if (cr6.lt) goto loc_8263F6C8;
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// stw r5,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r5.u32);
loc_8263F77C:
	// lwz r10,20(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// srawi r9,r31,11
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FF) != 0);
	ctx.r9.s64 = r31.s32 >> 11;
	// lwz r8,15340(r15)
	ctx.r8.u64 = PPC_LOAD_U32(r15.u32 + 15340);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ble cr6,0x8263f7bc
	if (!cr6.gt) goto loc_8263F7BC;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_8263F7A4:
	// dcbt r11,r9
	// dcbt r11,r8
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263f7a4
	if (!cr6.eq) goto loc_8263F7A4;
loc_8263F7BC:
	// srawi r11,r31,12
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFFF) != 0);
	r11.s64 = r31.s32 >> 12;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// mullw r11,r11,r29
	r11.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// ble cr6,0x8263f7e8
	if (!cr6.gt) goto loc_8263F7E8;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_8263F7D0:
	// dcbt r11,r21
	// dcbt r11,r20
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263f7d0
	if (!cr6.eq) goto loc_8263F7D0;
loc_8263F7E8:
	// addi r4,r25,-2
	ctx.r4.s64 = r25.s64 + -2;
	// lwz r20,152(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// stw r14,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r14.u32);
	// cmpw cr6,r14,r4
	cr6.compare<int32_t>(r14.s32, ctx.r4.s32, xer);
	// bge cr6,0x826403f8
	if (!cr6.lt) goto loc_826403F8;
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lfs f0,30688(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 30688);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-24976
	r11.s64 = r11.s64 + -24976;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
loc_8263F830:
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r11,20(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// clrlwi r24,r28,21
	r24.u64 = r28.u32 & 0x7FF;
	// lwz r9,15340(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15340);
	// srawi r10,r28,11
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FF) != 0);
	ctx.r10.s64 = r28.s32 >> 11;
	// lwz r8,196(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// extsw r7,r24
	ctx.r7.s64 = r24.s32;
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// std r7,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r7.u64);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lfd f13,200(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 200);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// ble cr6,0x8263f890
	if (!cr6.gt) goto loc_8263F890;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_8263F87C:
	// dcbt r11,r30
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263f87c
	if (!cr6.eq) goto loc_8263F87C;
loc_8263F890:
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8263f8c0
	if (!cr6.gt) goto loc_8263F8C0;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8263f8bc
	if (cr6.eq) goto loc_8263F8BC;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8263F8B0:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x8263f8b0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8263F8B0;
loc_8263F8BC:
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
loc_8263F8C0:
	// lwz r18,112(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r11,16
	r11.s64 = 16;
	// addi r10,r18,1280
	ctx.r10.s64 = r18.s64 + 1280;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_8263F8D0:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r24,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r24.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263f8d0
	if (!cr6.eq) goto loc_8263F8D0;
	// lwz r10,152(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// subfic r27,r24,2048
	xer.ca = r24.u32 <= 2048;
	r27.s64 = 2048 - r24.s64;
	// stfs f13,172(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// srawi r11,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r11.s64 = ctx.r10.s32 >> 4;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r19,r9,r10
	r19.s64 = ctx.r10.s64 - ctx.r9.s64;
	// ble cr6,0x8263fab0
	if (!cr6.gt) goto loc_8263FAB0;
	// mr r21,r11
	r21.u64 = r11.u64;
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r28,r23,1
	r28.s64 = r23.s64 + 1;
	// lvx128 v18,r0,r11
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8263F914:
	// addi r10,r18,1280
	ctx.r10.s64 = r18.s64 + 1280;
	// addi r11,r18,8
	r11.s64 = r18.s64 + 8;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// li r6,4
	ctx.r6.s64 = 4;
loc_8263F924:
	// srawi r9,r31,11
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FF) != 0);
	ctx.r9.s64 = r31.s32 >> 11;
	// clrlwi r8,r31,21
	ctx.r8.u64 = r31.u32 & 0x7FF;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// subf r17,r8,r27
	r17.s64 = r27.s64 - ctx.r8.s64;
	// add r7,r31,r22
	ctx.r7.u64 = r31.u64 + r22.u64;
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r17,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, r17.u32);
	// rotlwi r17,r16,0
	r17.u64 = __builtin_rotateleft32(r16.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r16,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r16.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r4.u32);
	// stw r31,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r31.u32);
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// stw r17,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r17.u32);
	// subf r17,r8,r27
	r17.s64 = r27.s64 - ctx.r8.s64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// stw r26,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r26.u32);
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r17,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r17.u32);
	// rotlwi r17,r16,0
	r17.u64 = __builtin_rotateleft32(r16.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r16,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r16.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r4.u32);
	// stw r31,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r31.u32);
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// stw r17,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r17.u32);
	// subf r17,r8,r27
	r17.s64 = r27.s64 - ctx.r8.s64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// stw r26,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r26.u32);
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r17,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r17.u32);
	// rotlwi r17,r16,0
	r17.u64 = __builtin_rotateleft32(r16.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r16,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, r16.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r4.u32);
	// stw r31,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r31.u32);
	// subf r25,r8,r27
	r25.s64 = r27.s64 - ctx.r8.s64;
	// stw r17,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, r17.u32);
	// add r31,r7,r22
	r31.u64 = ctx.r7.u64 + r22.u64;
	// stw r26,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r26.u32);
	// lbz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r26,r9,r23
	r26.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r9,r28,r9
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// stw r25,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r25.u32);
	// stw r5,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r5.u32);
	// stw r4,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r4.u32);
	// stw r8,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r8.u32);
	// stw r8,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r8.u32);
	// subf r9,r26,r9
	ctx.r9.s64 = ctx.r9.s64 - r26.s64;
	// stw r26,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r26.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// subf r9,r4,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r4.s64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bne cr6,0x8263f924
	if (!cr6.eq) goto loc_8263F924;
	// vor v1,v18,v18
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v18.u8));
	// addi r5,r18,1280
	ctx.r5.s64 = r18.s64 + 1280;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// bl 0x8263e538
	sub_8263E538(ctx, base);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// bne cr6,0x8263f914
	if (!cr6.eq) goto loc_8263F914;
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
loc_8263FAB0:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x8263fb30
	if (!cr6.gt) goto loc_8263FB30;
	// addi r4,r23,1
	ctx.r4.s64 = r23.s64 + 1;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
loc_8263FAC0:
	// srawi r11,r31,11
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FF) != 0);
	r11.s64 = r31.s32 >> 11;
	// clrlwi r10,r31,21
	ctx.r10.u64 = r31.u32 & 0x7FF;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r31,r31,r22
	r31.u64 = r31.u64 + r22.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbzx r8,r11,r23
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + r23.u32);
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbzx r11,r4,r11
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// mullw r8,r8,r24
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r24.s32);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// subfic r10,r10,2048
	xer.ca = ctx.r10.u32 <= 2048;
	ctx.r10.s64 = 2048 - ctx.r10.s64;
	// subf r10,r24,r10
	ctx.r10.s64 = ctx.r10.s64 - r24.s64;
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x8263fac0
	if (!cr6.eq) goto loc_8263FAC0;
loc_8263FB30:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8263fb68
	if (!cr6.lt) goto loc_8263FB68;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_8263FB44:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r9,r9,11
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 11;
	// add r31,r31,r22
	r31.u64 = r31.u64 + r22.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r9,r9,r30
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r30.u32);
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x8263fb44
	if (!cr6.eq) goto loc_8263FB44;
loc_8263FB68:
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x8263fb94
	if (!cr6.lt) goto loc_8263FB94;
loc_8263FB78:
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8263fb78
	if (cr6.lt) goto loc_8263FB78;
loc_8263FB94:
	// srawi r11,r28,12
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xFFF) != 0);
	r11.s64 = r28.s32 >> 12;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// srawi r10,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	ctx.r10.s64 = r28.s32 >> 1;
	// lwz r3,128(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mullw r16,r11,r29
	r16.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// lwz r4,108(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// clrlwi r19,r10,21
	r19.u64 = ctx.r10.u32 & 0x7FF;
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// add r11,r16,r29
	r11.u64 = r16.u64 + r29.u64;
	// extsw r9,r19
	ctx.r9.s64 = r19.s32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// std r9,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, ctx.r9.u64);
	// lfd f13,216(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// ble cr6,0x8263fbf8
	if (!cr6.gt) goto loc_8263FBF8;
	// lwz r8,136(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
loc_8263FBE0:
	// dcbt r11,r9
	// dcbt r11,r8
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8263fbe0
	if (!cr6.eq) goto loc_8263FBE0;
loc_8263FBF8:
	// lwz r11,176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8263fc24
	if (!cr6.gt) goto loc_8263FC24;
	// li r10,128
	ctx.r10.s64 = 128;
loc_8263FC08:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263fc08
	if (!cr6.eq) goto loc_8263FC08;
loc_8263FC24:
	// addi r10,r18,1280
	ctx.r10.s64 = r18.s64 + 1280;
	// li r11,16
	r11.s64 = 16;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_8263FC30:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r19,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r19.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8263fc30
	if (!cr6.eq) goto loc_8263FC30;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// subfic r26,r19,2048
	xer.ca = r19.u32 <= 2048;
	r26.s64 = 2048 - r19.s64;
	// stfs f13,172(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// srawi r11,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r11.s64 = ctx.r10.s32 >> 4;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r17,r9,r10
	r17.s64 = ctx.r10.s64 - ctx.r9.s64;
	// ble cr6,0x8263fed4
	if (!cr6.gt) goto loc_8263FED4;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r18,r11
	r18.u64 = r11.u64;
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r28,r16,r10
	r28.u64 = r16.u64 + ctx.r10.u64;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r31,r29,1
	r31.s64 = r29.s64 + 1;
	// add r27,r16,r10
	r27.u64 = r16.u64 + ctx.r10.u64;
	// lvx128 v54,r0,r11
	_mm_store_si128((__m128i*)v54.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8263FC88:
	// addi r11,r5,1280
	r11.s64 = ctx.r5.s64 + 1280;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
loc_8263FC98:
	// srawi r5,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r5.s64 = r30.s32 >> 12;
	// srawi r9,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r9.s64 = r30.s32 >> 1;
	// add r8,r30,r20
	ctx.r8.u64 = r30.u64 + r20.u64;
	// clrlwi r7,r9,21
	ctx.r7.u64 = ctx.r9.u32 & 0x7FF;
	// add r9,r28,r5
	ctx.r9.u64 = r28.u64 + ctx.r5.u64;
	// subf r14,r7,r26
	r14.s64 = r26.s64 - ctx.r7.s64;
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r21,r30,r21
	r21.s64 = r21.s64 - r30.s64;
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r27,r5
	ctx.r9.u64 = r27.u64 + ctx.r5.u64;
	// stw r7,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, ctx.r7.u32);
	// subf r21,r25,r21
	r21.s64 = r21.s64 - r25.s64;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// stw r14,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, r14.u32);
	// srawi r5,r8,12
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 12;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r25,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r25.u32);
	// srawi r14,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r14.s64 = ctx.r8.s32 >> 1;
	// stw r24,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r24.u32);
	// clrlwi r7,r14,21
	ctx.r7.u64 = r14.u32 & 0x7FF;
	// stw r21,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r21.u32);
	// subf r14,r7,r26
	r14.s64 = r26.s64 - ctx.r7.s64;
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r21,r30,r21
	r21.s64 = r21.s64 - r30.s64;
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r28,r5
	ctx.r9.u64 = r28.u64 + ctx.r5.u64;
	// subf r21,r25,r21
	r21.s64 = r21.s64 - r25.s64;
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// stw r25,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r25.u32);
	// stw r24,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r24.u32);
	// stw r21,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r21.u32);
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r27,r5
	ctx.r9.u64 = r27.u64 + ctx.r5.u64;
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r7,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r7.u32);
	// subf r7,r30,r21
	ctx.r7.s64 = r21.s64 - r30.s64;
	// stw r14,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r14.u32);
	// subf r7,r25,r7
	ctx.r7.s64 = ctx.r7.s64 - r25.s64;
	// stw r30,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r30.u32);
	// stw r25,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r25.u32);
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// stw r24,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r24.u32);
	// stw r7,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r7.u32);
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r7,1(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r5,r9,r29
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r9,r31,r9
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// stw r30,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r30.u32);
	// stw r7,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r7.u32);
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// stw r5,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r5.u32);
	// add r9,r8,r20
	ctx.r9.u64 = ctx.r8.u64 + r20.u64;
	// add r8,r7,r30
	ctx.r8.u64 = ctx.r7.u64 + r30.u64;
	// srawi r5,r9,12
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFF) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 12;
	// stw r8,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r8.u32);
	// add r8,r9,r20
	ctx.r8.u64 = ctx.r9.u64 + r20.u64;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// clrlwi r7,r9,21
	ctx.r7.u64 = ctx.r9.u32 & 0x7FF;
	// add r9,r28,r5
	ctx.r9.u64 = r28.u64 + ctx.r5.u64;
	// subf r14,r7,r26
	r14.s64 = r26.s64 - ctx.r7.s64;
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r27,r5
	ctx.r9.u64 = r27.u64 + ctx.r5.u64;
	// srawi r5,r8,12
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 12;
	// stw r14,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r14.u32);
	// srawi r14,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r14.s64 = ctx.r8.s32 >> 1;
	// stw r7,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r7.u32);
	// stw r7,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, ctx.r7.u32);
	// clrlwi r7,r14,21
	ctx.r7.u64 = r14.u32 & 0x7FF;
	// subf r21,r30,r21
	r21.s64 = r21.s64 - r30.s64;
	// stw r30,64(r11)
	PPC_STORE_U32(r11.u32 + 64, r30.u32);
	// stw r25,60(r11)
	PPC_STORE_U32(r11.u32 + 60, r25.u32);
	// subf r14,r7,r26
	r14.s64 = r26.s64 - ctx.r7.s64;
	// subf r21,r25,r21
	r21.s64 = r21.s64 - r25.s64;
	// stw r24,56(r11)
	PPC_STORE_U32(r11.u32 + 56, r24.u32);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stw r21,68(r11)
	PPC_STORE_U32(r11.u32 + 68, r21.u32);
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r21,r30,r21
	r21.s64 = r21.s64 - r30.s64;
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r28,r5
	ctx.r9.u64 = r28.u64 + ctx.r5.u64;
	// subf r21,r25,r21
	r21.s64 = r21.s64 - r25.s64;
	// stw r30,80(r11)
	PPC_STORE_U32(r11.u32 + 80, r30.u32);
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// stw r25,76(r11)
	PPC_STORE_U32(r11.u32 + 76, r25.u32);
	// stw r24,72(r11)
	PPC_STORE_U32(r11.u32 + 72, r24.u32);
	// stw r21,84(r11)
	PPC_STORE_U32(r11.u32 + 84, r21.u32);
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbzx r21,r31,r9
	r21.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// lbz r25,1(r9)
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbz r24,0(r9)
	r24.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r27,r5
	ctx.r9.u64 = r27.u64 + ctx.r5.u64;
	// stw r7,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r7.u32);
	// stw r7,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r7.u32);
	// subf r7,r30,r21
	ctx.r7.s64 = r21.s64 - r30.s64;
	// stw r30,96(r11)
	PPC_STORE_U32(r11.u32 + 96, r30.u32);
	// subf r7,r25,r7
	ctx.r7.s64 = ctx.r7.s64 - r25.s64;
	// stw r14,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r14.u32);
	// stw r25,92(r11)
	PPC_STORE_U32(r11.u32 + 92, r25.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// add r7,r7,r24
	ctx.r7.u64 = ctx.r7.u64 + r24.u64;
	// stw r24,88(r11)
	PPC_STORE_U32(r11.u32 + 88, r24.u32);
	// stw r7,100(r11)
	PPC_STORE_U32(r11.u32 + 100, ctx.r7.u32);
	// lbzx r30,r9,r29
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + r29.u32);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r9,r31,r9
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + ctx.r9.u32);
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// stw r30,112(r11)
	PPC_STORE_U32(r11.u32 + 112, r30.u32);
	// stw r7,104(r11)
	PPC_STORE_U32(r11.u32 + 104, ctx.r7.u32);
	// add r30,r8,r20
	r30.u64 = ctx.r8.u64 + r20.u64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// stw r5,108(r11)
	PPC_STORE_U32(r11.u32 + 108, ctx.r5.u32);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r9,116(r11)
	PPC_STORE_U32(r11.u32 + 116, ctx.r9.u32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// bne cr6,0x8263fc98
	if (!cr6.eq) goto loc_8263FC98;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// vor128 v1,v54,v54
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v54.u8));
	// addi r6,r5,1280
	ctx.r6.s64 = ctx.r5.s64 + 1280;
	// bl 0x8263ea88
	sub_8263EA88(ctx, base);
	// addi r18,r18,-1
	r18.s64 = r18.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// bne cr6,0x8263fc88
	if (!cr6.eq) goto loc_8263FC88;
	// lwz r14,156(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
loc_8263FED4:
	// lwz r21,188(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// li r18,128
	r18.s64 = 128;
	// lwz r24,144(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// ble cr6,0x8263ffc4
	if (!cr6.gt) goto loc_8263FFC4;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r28,r29,1
	r28.s64 = r29.s64 + 1;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// add r26,r16,r11
	r26.u64 = r16.u64 + r11.u64;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r25,r16,r11
	r25.u64 = r16.u64 + r11.u64;
loc_8263FF00:
	// srawi r8,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r8.s64 = r30.s32 >> 12;
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r10,r26,r8
	ctx.r10.u64 = r26.u64 + ctx.r8.u64;
	// clrlwi r11,r11,21
	r11.u64 = r11.u32 & 0x7FF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// subfic r7,r11,2048
	xer.ca = r11.u32 <= 2048;
	ctx.r7.s64 = 2048 - r11.s64;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// lbzx r6,r10,r29
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + r29.u32);
	// subf r7,r19,r7
	ctx.r7.s64 = ctx.r7.s64 - r19.s64;
	// lbzx r27,r28,r10
	r27.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r25,r8
	ctx.r10.u64 = r25.u64 + ctx.r8.u64;
	// subf r8,r6,r27
	ctx.r8.s64 = r27.s64 - ctx.r6.s64;
	// mullw r27,r5,r11
	r27.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// mullw r5,r6,r19
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(r19.s32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// mullw r6,r7,r31
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(r31.s32);
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// mullw r8,r8,r19
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r19.s32);
	// srawi r8,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 11;
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + r27.u64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// srawi r8,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 11;
	// stb r8,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r8.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbzx r8,r10,r29
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r29.u32);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r31,r6,r11
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mullw r8,r8,r19
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r19.s32);
	// subf r10,r6,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r6.s64;
	// add r6,r10,r5
	ctx.r6.u64 = ctx.r10.u64 + ctx.r5.u64;
	// mullw r10,r7,r5
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r5.s32);
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// mullw r11,r11,r19
	r11.s64 = int64_t(r11.s32) * int64_t(r19.s32);
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x8263ff00
	if (!cr6.eq) goto loc_8263FF00;
loc_8263FFC4:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmpw cr6,r11,r21
	cr6.compare<int32_t>(r11.s32, r21.s32, xer);
	// bge cr6,0x82640010
	if (!cr6.lt) goto loc_82640010;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// subf r11,r11,r21
	r11.s64 = r21.s64 - r11.s64;
	// add r9,r16,r10
	ctx.r9.u64 = r16.u64 + ctx.r10.u64;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r8,r16,r10
	ctx.r8.u64 = r16.u64 + ctx.r10.u64;
loc_8263FFE4:
	// srawi r10,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r10.s64 = r30.s32 >> 12;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r7,r9,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// stb r7,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r7.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lbzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x8263ffe4
	if (!cr6.eq) goto loc_8263FFE4;
loc_82640010:
	// cmpw cr6,r21,r24
	cr6.compare<int32_t>(r21.s32, r24.s32, xer);
	// bge cr6,0x82640038
	if (!cr6.lt) goto loc_82640038;
	// subf r11,r21,r24
	r11.s64 = r24.s64 - r21.s64;
loc_8264001C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r18,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r18.u8);
	// stb r18,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r18.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264001c
	if (!cr6.eq) goto loc_8264001C;
loc_82640038:
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// addi r16,r14,1
	r16.s64 = r14.s64 + 1;
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// add r18,r9,r10
	r18.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r10,r10,r24
	ctx.r10.u64 = ctx.r10.u64 + r24.u64;
	// cmpwi cr6,r18,0
	cr6.compare<int32_t>(r18.s32, 0, xer);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r17,r11,r10
	r17.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r11,r11,r24
	r11.u64 = r11.u64 + r24.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// bge cr6,0x8264007c
	if (!cr6.lt) goto loc_8264007C;
	// li r18,0
	r18.s64 = 0;
loc_8264007C:
	// lwz r11,20(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// srawi r8,r18,11
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7FF) != 0);
	ctx.r8.s64 = r18.s32 >> 11;
	// lwz r10,15340(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15340);
	// clrlwi r24,r18,21
	r24.u64 = r18.u32 & 0x7FF;
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// extsw r11,r24
	r11.s64 = r24.s32;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// std r11,208(r1)
	PPC_STORE_U64(ctx.r1.u32 + 208, r11.u64);
	// lfd f13,208(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// ble cr6,0x826400e0
	if (!cr6.gt) goto loc_826400E0;
	// mr r11,r17
	r11.u64 = r17.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826400dc
	if (cr6.eq) goto loc_826400DC;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_826400D0:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826400d0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826400D0;
loc_826400DC:
	// add r3,r17,r9
	ctx.r3.u64 = r17.u64 + ctx.r9.u64;
loc_826400E0:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r10,r11,1280
	ctx.r10.s64 = r11.s64 + 1280;
	// li r11,16
	r11.s64 = 16;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_826400F0:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r24,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r24.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826400f0
	if (!cr6.eq) goto loc_826400F0;
	// lwz r10,152(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// subfic r27,r24,2048
	xer.ca = r24.u32 <= 2048;
	r27.s64 = 2048 - r24.s64;
	// stfs f13,172(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// srawi r11,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r11.s64 = ctx.r10.s32 >> 4;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r19,r9,r10
	r19.s64 = ctx.r10.s64 - ctx.r9.s64;
	// ble cr6,0x826402d4
	if (!cr6.gt) goto loc_826402D4;
	// mr r21,r11
	r21.u64 = r11.u64;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r28,r23,1
	r28.s64 = r23.s64 + 1;
	// lvx128 v18,r0,r11
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82640138:
	// addi r10,r4,1280
	ctx.r10.s64 = ctx.r4.s64 + 1280;
	// addi r11,r4,8
	r11.s64 = ctx.r4.s64 + 8;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// li r6,4
	ctx.r6.s64 = 4;
loc_82640148:
	// srawi r9,r31,11
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FF) != 0);
	ctx.r9.s64 = r31.s32 >> 11;
	// clrlwi r8,r31,21
	ctx.r8.u64 = r31.u32 & 0x7FF;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// subf r15,r8,r27
	r15.s64 = r27.s64 - ctx.r8.s64;
	// add r7,r31,r22
	ctx.r7.u64 = r31.u64 + r22.u64;
	// mr r14,r8
	r14.u64 = ctx.r8.u64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r15,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, r15.u32);
	// rotlwi r15,r14,0
	r15.u64 = __builtin_rotateleft32(r14.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r14,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r14.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r4.u32);
	// stw r31,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r31.u32);
	// mr r14,r8
	r14.u64 = ctx.r8.u64;
	// stw r15,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r15.u32);
	// subf r15,r8,r27
	r15.s64 = r27.s64 - ctx.r8.s64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// stw r26,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r26.u32);
	// add r7,r7,r22
	ctx.r7.u64 = ctx.r7.u64 + r22.u64;
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r15,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r15.u32);
	// rotlwi r15,r14,0
	r15.u64 = __builtin_rotateleft32(r14.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r14,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r14.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r4.u32);
	// stw r31,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r31.u32);
	// mr r14,r8
	r14.u64 = ctx.r8.u64;
	// stw r15,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r15.u32);
	// subf r15,r8,r27
	r15.s64 = r27.s64 - ctx.r8.s64;
	// srawi r25,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r7.s32 >> 11;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// clrlwi r8,r7,21
	ctx.r8.u64 = ctx.r7.u32 & 0x7FF;
	// stw r26,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r26.u32);
	// lbzx r5,r9,r23
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r26,r28,r9
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// subf r26,r5,r26
	r26.s64 = r26.s64 - ctx.r5.s64;
	// lbz r31,0(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r15,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r15.u32);
	// rotlwi r15,r14,0
	r15.u64 = __builtin_rotateleft32(r14.u32, 0);
	// subf r26,r4,r26
	r26.s64 = r26.s64 - ctx.r4.s64;
	// stw r14,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, r14.u32);
	// add r9,r25,r30
	ctx.r9.u64 = r25.u64 + r30.u64;
	// stw r5,32(r11)
	PPC_STORE_U32(r11.u32 + 32, ctx.r5.u32);
	// add r26,r26,r31
	r26.u64 = r26.u64 + r31.u64;
	// stw r4,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r4.u32);
	// stw r31,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r31.u32);
	// subf r25,r8,r27
	r25.s64 = r27.s64 - ctx.r8.s64;
	// stw r15,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, r15.u32);
	// add r31,r7,r22
	r31.u64 = ctx.r7.u64 + r22.u64;
	// stw r26,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r26.u32);
	// lbz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r26,r9,r23
	r26.u64 = PPC_LOAD_U8(ctx.r9.u32 + r23.u32);
	// lbzx r9,r28,r9
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// stw r25,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r25.u32);
	// stw r5,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r5.u32);
	// stw r4,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r4.u32);
	// stw r8,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r8.u32);
	// stw r8,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r8.u32);
	// subf r9,r26,r9
	ctx.r9.s64 = ctx.r9.s64 - r26.s64;
	// stw r26,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r26.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// subf r9,r4,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r4.s64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bne cr6,0x82640148
	if (!cr6.eq) goto loc_82640148;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// vor v1,v18,v18
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v18.u8));
	// addi r5,r4,1280
	ctx.r5.s64 = ctx.r4.s64 + 1280;
	// bl 0x8263e538
	sub_8263E538(ctx, base);
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// bne cr6,0x82640138
	if (!cr6.eq) goto loc_82640138;
	// lwz r15,468(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
loc_826402D4:
	// lwz r28,140(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x82640358
	if (!cr6.gt) goto loc_82640358;
	// addi r4,r23,1
	ctx.r4.s64 = r23.s64 + 1;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
loc_826402E8:
	// srawi r11,r31,11
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FF) != 0);
	r11.s64 = r31.s32 >> 11;
	// clrlwi r10,r31,21
	ctx.r10.u64 = r31.u32 & 0x7FF;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r31,r31,r22
	r31.u64 = r31.u64 + r22.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbzx r8,r11,r23
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + r23.u32);
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbzx r11,r4,r11
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// mullw r8,r8,r24
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r24.s32);
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// subfic r10,r10,2048
	xer.ca = ctx.r10.u32 <= 2048;
	ctx.r10.s64 = 2048 - ctx.r10.s64;
	// subf r10,r24,r10
	ctx.r10.s64 = ctx.r10.s64 - r24.s64;
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x826402e8
	if (!cr6.eq) goto loc_826402E8;
loc_82640358:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bge cr6,0x8264038c
	if (!cr6.lt) goto loc_8264038C;
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
loc_82640368:
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r10,r10,11
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 11;
	// add r31,r31,r22
	r31.u64 = r31.u64 + r22.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r30.u32);
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x82640368
	if (!cr6.eq) goto loc_82640368;
loc_8264038C:
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// bge cr6,0x826403b8
	if (!cr6.lt) goto loc_826403B8;
loc_8264039C:
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8264039c
	if (cr6.lt) goto loc_8264039C;
loc_826403B8:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// add r10,r18,r11
	ctx.r10.u64 = r18.u64 + r11.u64;
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// add r11,r11,r17
	r11.u64 = r11.u64 + r17.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// bge cr6,0x826403e0
	if (!cr6.lt) goto loc_826403E0;
	// li r11,0
	r11.s64 = 0;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
loc_826403E0:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r14,r16,1
	r14.s64 = r16.s64 + 1;
	// addi r4,r11,-2
	ctx.r4.s64 = r11.s64 + -2;
	// cmpw cr6,r14,r4
	cr6.compare<int32_t>(r14.s32, ctx.r4.s32, xer);
	// stw r14,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r14.u32);
	// blt cr6,0x8263f830
	if (cr6.lt) goto loc_8263F830;
loc_826403F8:
	// lwz r30,104(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r11,15324(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15324);
	// lwz r18,80(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// srawi r6,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r6.s64 = r30.s32 >> 11;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// blt cr6,0x82640418
	if (cr6.lt) goto loc_82640418;
	// addi r6,r11,-1
	ctx.r6.s64 = r11.s64 + -1;
loc_82640418:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r14,88(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r14,r11
	cr6.compare<int32_t>(r14.s32, r11.s32, xer);
	// blt cr6,0x826406a4
	if (cr6.lt) goto loc_826406A4;
	// cmpw cr6,r4,r14
	cr6.compare<int32_t>(ctx.r4.s32, r14.s32, xer);
	// bge cr6,0x826406a4
	if (!cr6.lt) goto loc_826406A4;
	// lwz r28,124(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// li r23,0
	r23.s64 = 0;
	// lwz r31,120(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// li r17,128
	r17.s64 = 128;
	// lwz r26,144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r16,132(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r25,176(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// lwz r21,184(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// lwz r24,128(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r27,108(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r19,148(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
loc_82640464:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82640490
	if (!cr6.gt) goto loc_82640490;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264048c
	if (cr6.eq) goto loc_8264048C;
	// mtctr r31
	ctr.u64 = r31.u64;
loc_82640480:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82640480
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82640480;
loc_8264048C:
	// add r11,r3,r31
	r11.u64 = ctx.r3.u64 + r31.u64;
loc_82640490:
	// lwz r10,20(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// lwz r9,15340(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15340);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x826404d0
	if (!cr6.lt) goto loc_826404D0;
	// subf r10,r31,r28
	ctx.r10.s64 = r28.s64 - r31.s64;
loc_826404AC:
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// add r8,r8,r22
	ctx.r8.u64 = ctx.r8.u64 + r22.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r7,r7,r9
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x826404ac
	if (!cr6.eq) goto loc_826404AC;
loc_826404D0:
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// bge cr6,0x826404f8
	if (!cr6.lt) goto loc_826404F8;
loc_826404E0:
	// stb r23,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r23.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x826404e0
	if (cr6.lt) goto loc_826404E0;
loc_826404F8:
	// mr r11,r24
	r11.u64 = r24.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82640528
	if (!cr6.gt) goto loc_82640528;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
loc_8264050C:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r17,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r17.u8);
	// stb r17,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r17.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8264050c
	if (!cr6.eq) goto loc_8264050C;
loc_82640528:
	// lwz r8,15324(r15)
	ctx.r8.u64 = PPC_LOAD_U32(r15.u32 + 15324);
	// srawi r9,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r9.s64 = r30.s32 >> 12;
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x82640544
	if (cr6.lt) goto loc_82640544;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
loc_82640544:
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// cmpw cr6,r25,r21
	cr6.compare<int32_t>(r25.s32, r21.s32, xer);
	// bge cr6,0x82640590
	if (!cr6.lt) goto loc_82640590;
	// lwz r8,136(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r6,r9,r16
	ctx.r6.u64 = ctx.r9.u64 + r16.u64;
	// add r5,r9,r8
	ctx.r5.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r25,r21
	ctx.r9.s64 = r21.s64 - r25.s64;
loc_82640560:
	// srawi r8,r7,12
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r7,r7,r20
	ctx.r7.u64 = ctx.r7.u64 + r20.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbzx r14,r6,r8
	r14.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r8.u32);
	// stb r14,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r14.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r8,r5,r8
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r8.u32);
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bne cr6,0x82640560
	if (!cr6.eq) goto loc_82640560;
	// lwz r14,88(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82640590:
	// cmpw cr6,r21,r26
	cr6.compare<int32_t>(r21.s32, r26.s32, xer);
	// bge cr6,0x826405b8
	if (!cr6.lt) goto loc_826405B8;
	// subf r9,r21,r26
	ctx.r9.s64 = r26.s64 - r21.s64;
loc_8264059C:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r17,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r17.u8);
	// stb r17,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r17.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8264059c
	if (!cr6.eq) goto loc_8264059C;
loc_826405B8:
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// add r30,r30,r19
	r30.u64 = r30.u64 + r19.u64;
	// add r24,r24,r26
	r24.u64 = r24.u64 + r26.u64;
	// add r7,r11,r3
	ctx.r7.u64 = r11.u64 + ctx.r3.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// addi r5,r4,1
	ctx.r5.s64 = ctx.r4.s64 + 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// bge cr6,0x826405e0
	if (!cr6.lt) goto loc_826405E0;
	// mr r30,r23
	r30.u64 = r23.u64;
loc_826405E0:
	// lwz r10,15324(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15324);
	// srawi r6,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r6.s64 = r30.s32 >> 11;
	// mr r8,r18
	ctx.r8.u64 = r18.u64;
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// blt cr6,0x826405f8
	if (cr6.lt) goto loc_826405F8;
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
loc_826405F8:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82640624
	if (!cr6.gt) goto loc_82640624;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82640620
	if (cr6.eq) goto loc_82640620;
	// mtctr r31
	ctr.u64 = r31.u64;
loc_82640614:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82640614
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82640614;
loc_82640620:
	// add r11,r7,r31
	r11.u64 = ctx.r7.u64 + r31.u64;
loc_82640624:
	// lwz r10,20(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 20);
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// lwz r9,15340(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15340);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82640664
	if (!cr6.lt) goto loc_82640664;
	// subf r10,r31,r28
	ctx.r10.s64 = r28.s64 - r31.s64;
loc_82640640:
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r4,r4,11
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7FF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 11;
	// add r8,r8,r22
	ctx.r8.u64 = ctx.r8.u64 + r22.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r4,r4,r9
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r9.u32);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82640640
	if (!cr6.eq) goto loc_82640640;
loc_82640664:
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// bge cr6,0x8264068c
	if (!cr6.lt) goto loc_8264068C;
loc_82640674:
	// stb r23,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r23.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82640674
	if (cr6.lt) goto loc_82640674;
loc_8264068C:
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r4,r5,1
	ctx.r4.s64 = ctx.r5.s64 + 1;
	// add r3,r11,r7
	ctx.r3.u64 = r11.u64 + ctx.r7.u64;
	// cmpw cr6,r4,r14
	cr6.compare<int32_t>(ctx.r4.s32, r14.s32, xer);
	// blt cr6,0x82640464
	if (cr6.lt) goto loc_82640464;
	// b 0x826406bc
	goto loc_826406BC;
loc_826406A4:
	// lwz r26,144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// li r23,0
	r23.s64 = 0;
	// lwz r24,128(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// li r17,128
	r17.s64 = 128;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r27,108(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
loc_826406BC:
	// lwz r11,15332(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15332);
	// mr r8,r14
	ctx.r8.u64 = r14.u64;
	// cmpw cr6,r14,r11
	cr6.compare<int32_t>(r14.s32, r11.s32, xer);
	// bge cr6,0x82640778
	if (!cr6.lt) goto loc_82640778;
loc_826406CC:
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r11,r23
	r11.u64 = r23.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x826406f8
	if (!cr6.gt) goto loc_826406F8;
loc_826406E0:
	// stb r23,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r23.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r15)
	ctx.r9.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x826406e0
	if (cr6.lt) goto loc_826406E0;
loc_826406F8:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82640724
	if (!cr6.gt) goto loc_82640724;
	// mr r11,r27
	r11.u64 = r27.u64;
	// subf r9,r27,r24
	ctx.r9.s64 = r24.s64 - r27.s64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_8264070C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r17,r11,r9
	PPC_STORE_U8(r11.u32 + ctx.r9.u32, r17.u8);
	// stb r17,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r17.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8264070c
	if (!cr6.eq) goto loc_8264070C;
loc_82640724:
	// lwz r10,15328(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// add r24,r24,r26
	r24.u64 = r24.u64 + r26.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r9,r3,r10
	ctx.r9.u64 = ctx.r3.u64 + ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r23
	r11.u64 = r23.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x82640760
	if (!cr6.gt) goto loc_82640760;
loc_82640748:
	// stb r23,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r23.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,15328(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82640748
	if (cr6.lt) goto loc_82640748;
loc_82640760:
	// lwz r11,15328(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lwz r10,15332(r15)
	ctx.r10.u64 = PPC_LOAD_U32(r15.u32 + 15332);
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x826406cc
	if (cr6.lt) goto loc_826406CC;
loc_82640778:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d624
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264078C"))) PPC_WEAK_FUNC(sub_8264078C);
PPC_FUNC_IMPL(__imp__sub_8264078C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82640790"))) PPC_WEAK_FUNC(sub_82640790);
PPC_FUNC_IMPL(__imp__sub_82640790) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCVRegister v17{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d5d4
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f26,f1
	ctx.fpscr.disableFlushMode();
	f26.f64 = ctx.f1.f64;
	// fmr f24,f2
	f24.f64 = ctx.f2.f64;
	// fmr f27,f3
	f27.f64 = ctx.f3.f64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// fmr f25,f4
	f25.f64 = ctx.f4.f64;
	// fmr f23,f5
	f23.f64 = ctx.f5.f64;
	// bne cr6,0x826407d8
	if (!cr6.eq) goto loc_826407D8;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d620
	// b 0x8239bd10
	return;
loc_826407D8:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r21,15344(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 15344);
	// lwz r20,15348(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 15348);
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stfs f0,208(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// frsp f0,f23
	f0.f64 = double(float(f23.f64));
	// stfs f0,192(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stw r21,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r21.u32);
	// stw r20,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r20.u32);
	// lfd f28,-31368(r11)
	f28.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// fcmpu cr6,f26,f28
	cr6.compare(f26.f64, f28.f64);
	// srawi r24,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r24.s64 = r11.s32 >> 1;
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// vspltw v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// srawi r27,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r27.s64 = ctx.r10.s32 >> 1;
	// srawi r26,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r26.s64 = r11.s32 >> 5;
	// srawi r23,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r23.s64 = r11.s32 >> 6;
	// stw r27,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r27.u32);
	// stvx v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r26,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r26.u32);
	// stw r23,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r23.u32);
	// beq cr6,0x82641c0c
	if (cr6.eq) goto loc_82641C0C;
	// fcmpu cr6,f27,f28
	cr6.compare(f27.f64, f28.f64);
	// beq cr6,0x82641c0c
	if (cr6.eq) goto loc_82641C0C;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f29,-30984(r11)
	f29.u64 = PPC_LOAD_U64(r11.u32 + -30984);
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// fdiv f0,f29,f26
	f0.f64 = f29.f64 / f26.f64;
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// fmul f0,f0,f24
	f0.f64 = f0.f64 * f24.f64;
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f24
	ctx.f13.f64 = ctx.f13.f64 - f24.f64;
	// fdiv f31,f13,f26
	f31.f64 = ctx.f13.f64 / f26.f64;
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// ble cr6,0x8264088c
	if (!cr6.gt) goto loc_8264088C;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// fmr f0,f31
	f0.f64 = f31.f64;
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
loc_8264088C:
	// fcmpu cr6,f0,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x82640898
	if (!cr6.lt) goto loc_82640898;
	// fmr f0,f28
	f0.f64 = f28.f64;
loc_82640898:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// lfd f0,176(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x826408c0
	if (!cr6.lt) goto loc_826408C0;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_826408C0:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fctiwz f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lfd f0,30704(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30704);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// std r10,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r10.u64);
	// fadd f0,f31,f0
	f0.f64 = f31.f64 + f0.f64;
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x826408fc
	if (!cr6.lt) goto loc_826408FC;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_826408FC:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = ctx.f1.f64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// lfd f0,176(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// blt cr6,0x82640928
	if (cr6.lt) goto loc_82640928;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_82640928:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f28.f64);
	// bge cr6,0x8264093c
	if (!cr6.lt) goto loc_8264093C;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
loc_8264093C:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// fcmpu cr6,f30,f28
	cr6.compare(f30.f64, f28.f64);
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// bge cr6,0x8264095c
	if (!cr6.lt) goto loc_8264095C;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// b 0x82640960
	goto loc_82640960;
loc_8264095C:
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
loc_82640960:
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,116
	r11.s64 = ctx.r1.s64 + 116;
	// fctiwz f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// fdiv f0,f29,f27
	f0.f64 = f29.f64 / f27.f64;
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r11,15324(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15324);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmul f0,f0,f25
	f0.f64 = f0.f64 * f25.f64;
	// lfd f30,-31360(r11)
	f30.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fsub f13,f13,f25
	ctx.f13.f64 = ctx.f13.f64 - f25.f64;
	// fdiv f13,f13,f27
	ctx.f13.f64 = ctx.f13.f64 / f27.f64;
	// fadd f31,f13,f30
	f31.f64 = ctx.f13.f64 + f30.f64;
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// ble cr6,0x826409b4
	if (!cr6.gt) goto loc_826409B4;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// fmr f0,f31
	f0.f64 = f31.f64;
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
loc_826409B4:
	// fcmpu cr6,f0,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x826409c0
	if (!cr6.lt) goto loc_826409C0;
	// fmr f0,f28
	f0.f64 = f28.f64;
loc_826409C0:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// lwz r11,15332(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15332);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// lfd f0,176(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x826409e8
	if (!cr6.lt) goto loc_826409E8;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
loc_826409E8:
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// lwz r11,15332(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15332);
	// addi r10,r1,132
	ctx.r10.s64 = ctx.r1.s64 + 132;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// std r11,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r11.u64);
	// lfd f0,176(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// blt cr6,0x82640a18
	if (cr6.lt) goto loc_82640A18;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_82640A18:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// bl 0x8239da30
	sub_8239DA30(ctx, base);
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f28.f64);
	// bge cr6,0x82640a2c
	if (!cr6.lt) goto loc_82640A2C;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
loc_82640A2C:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r30,r11,0,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r17,r10,0,0,30
	r17.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r29,r11,0,0,30
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// rlwinm r28,r11,0,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// stw r17,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r17.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r28,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r28.u32);
	// bl 0x8239e180
	sub_8239E180(ctx, base);
	// addi r11,r1,124
	r11.s64 = ctx.r1.s64 + 124;
	// fctiwz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// rlwinm r3,r11,0,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// bge cr6,0x82640a90
	if (!cr6.lt) goto loc_82640A90;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
loc_82640A90:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r25,15352(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 15352);
	// addi r10,r1,152
	ctx.r10.s64 = ctx.r1.s64 + 152;
	// lwz r6,15356(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15356);
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// lwz r7,15360(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15360);
	// addi r8,r1,148
	ctx.r8.s64 = ctx.r1.s64 + 148;
	// li r5,0
	ctx.r5.s64 = 0;
	// lfd f0,30696(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30696);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f13,f26,f0
	ctx.f13.f64 = f26.f64 * f0.f64;
	// stw r25,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r25.u32);
	// fmul f12,f27,f0
	ctx.f12.f64 = f27.f64 * f0.f64;
	// stw r6,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r6.u32);
	// stw r7,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r7.u32);
	// lfd f0,30680(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30680);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// fmul f11,f25,f0
	ctx.f11.f64 = f25.f64 * f0.f64;
	// fmul f0,f24,f0
	f0.f64 = f24.f64 * f0.f64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// srawi r11,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r11.s64 = r28.s32 >> 1;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// fctiwz f10,f0
	ctx.f10.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r11.u32);
	// lis r11,-32251
	r11.s64 = -2113601536;
	// lfd f0,264(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 264);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r29,152(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// mullw r11,r11,r17
	r11.s64 = int64_t(r11.s32) * int64_t(r17.s32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// addi r10,r1,100
	ctx.r10.s64 = ctx.r1.s64 + 100;
	// mullw r9,r29,r30
	ctx.r9.s64 = int64_t(r29.s32) * int64_t(r30.s32);
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// subf r22,r10,r11
	r22.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// bge cr6,0x82640b64
	if (!cr6.lt) goto loc_82640B64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
loc_82640B64:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82640b70
	if (!cr6.lt) goto loc_82640B70;
	// stw r5,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
loc_82640B70:
	// fcmpu cr6,f23,f30
	ctx.fpscr.disableFlushMode();
	cr6.compare(f23.f64, f30.f64);
	// ble cr6,0x82640b80
	if (!cr6.gt) goto loc_82640B80;
	// fmr f23,f30
	f23.f64 = f30.f64;
	// b 0x82640b8c
	goto loc_82640B8C;
loc_82640B80:
	// fcmpu cr6,f23,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f23.f64, f28.f64);
	// bge cr6,0x82640b8c
	if (!cr6.lt) goto loc_82640B8C;
	// fmr f23,f28
	f23.f64 = f28.f64;
loc_82640B8C:
	// lis r11,-32248
	r11.s64 = -2113404928;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// cmpwi cr6,r17,0
	cr6.compare<int32_t>(r17.s32, 0, xer);
	// li r4,128
	ctx.r4.s64 = 128;
	// lfd f0,-26736(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -26736);
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// fmul f0,f23,f0
	f0.f64 = f23.f64 * f0.f64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,152
	r11.s64 = ctx.r1.s64 + 152;
	// vspltw v1,v0,0
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// ble cr6,0x82640c74
	if (!cr6.gt) goto loc_82640C74;
loc_82640BC0:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82640bec
	if (!cr6.gt) goto loc_82640BEC;
loc_82640BD4:
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82640bd4
	if (cr6.lt) goto loc_82640BD4;
loc_82640BEC:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// ble cr6,0x82640c18
	if (!cr6.gt) goto loc_82640C18;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// subf r9,r7,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r7.s64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_82640C00:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r4,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r4.u8);
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82640c00
	if (!cr6.eq) goto loc_82640C00;
loc_82640C18:
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + r27.u64;
	// add r9,r10,r25
	ctx.r9.u64 = ctx.r10.u64 + r25.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x82640c54
	if (!cr6.gt) goto loc_82640C54;
loc_82640C3C:
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r30,15328(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// blt cr6,0x82640c3c
	if (cr6.lt) goto loc_82640C3C;
loc_82640C54:
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r25,r11,r9
	r25.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r17
	cr6.compare<int32_t>(ctx.r8.s32, r17.s32, xer);
	// blt cr6,0x82640bc0
	if (cr6.lt) goto loc_82640BC0;
	// stw r25,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r25.u32);
	// stw r7,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r7.u32);
	// stw r6,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r6.u32);
loc_82640C74:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// srawi r9,r22,11
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FF) != 0);
	ctx.r9.s64 = r22.s32 >> 11;
	// lwz r8,15340(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15340);
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ble cr6,0x82640cb4
	if (!cr6.gt) goto loc_82640CB4;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_82640C9C:
	// dcbt r11,r9
	// dcbt r11,r8
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82640c9c
	if (!cr6.eq) goto loc_82640C9C;
loc_82640CB4:
	// srawi r11,r22,12
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFFF) != 0);
	r11.s64 = r22.s32 >> 12;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// mullw r11,r11,r24
	r11.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// ble cr6,0x82640ce0
	if (!cr6.gt) goto loc_82640CE0;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_82640CC8:
	// dcbt r11,r21
	// dcbt r11,r20
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82640cc8
	if (!cr6.eq) goto loc_82640CC8;
loc_82640CE0:
	// addi r3,r3,-2
	ctx.r3.s64 = ctx.r3.s64 + -2;
	// lwz r20,148(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r16,152(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// cmpw cr6,r17,r3
	cr6.compare<int32_t>(r17.s32, ctx.r3.s32, xer);
	// bge cr6,0x82641864
	if (!cr6.lt) goto loc_82641864;
	// lwz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r11,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r14,r11,r10
	r14.s64 = ctx.r10.s64 - r11.s64;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// addi r15,r11,-24464
	r15.s64 = r11.s64 + -24464;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// stw r14,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r14.u32);
	// stw r15,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r15.u32);
	// lfs f13,30688(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 30688);
	ctx.f13.f64 = double(temp.f32);
loc_82640D28:
	// clrlwi r23,r22,21
	r23.u64 = r22.u32 & 0x7FF;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// srawi r10,r22,11
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FF) != 0);
	ctx.r10.s64 = r22.s32 >> 11;
	// lwz r9,15340(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15340);
	// extsw r7,r23
	ctx.r7.s64 = r23.s32;
	// lwz r8,156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r30,100(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// std r7,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r7.u64);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r28,r10,r9
	r28.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lfd f0,184(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// ble cr6,0x82640d88
	if (!cr6.gt) goto loc_82640D88;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_82640D74:
	// dcbt r11,r28
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82640d74
	if (!cr6.eq) goto loc_82640D74;
loc_82640D88:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82640db8
	if (!cr6.gt) goto loc_82640DB8;
	// mr r11,r25
	r11.u64 = r25.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82640db4
	if (cr6.eq) goto loc_82640DB4;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82640DA8:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82640da8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82640DA8;
loc_82640DB4:
	// add r3,r25,r9
	ctx.r3.u64 = r25.u64 + ctx.r9.u64;
loc_82640DB8:
	// li r11,16
	r11.s64 = 16;
	// addi r10,r15,8
	ctx.r10.s64 = r15.s64 + 8;
loc_82640DC0:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r23,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r23.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82640dc0
	if (!cr6.eq) goto loc_82640DC0;
	// subfic r27,r23,2048
	xer.ca = r23.u32 <= 2048;
	r27.s64 = 2048 - r23.s64;
	// stfs f0,220(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// srawi r11,r14,4
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0xF) != 0);
	r11.s64 = r14.s32 >> 4;
	// rlwinm r10,r11,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r21,r10,r14
	r21.s64 = r14.s64 - ctx.r10.s64;
	// ble cr6,0x82640fb4
	if (!cr6.gt) goto loc_82640FB4;
	// mr r22,r11
	r22.u64 = r11.u64;
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// lvx128 v17,r0,r11
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82640DFC:
	// addi r11,r15,256
	r11.s64 = r15.s64 + 256;
	// addi r10,r15,12
	ctx.r10.s64 = r15.s64 + 12;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// li r6,4
	ctx.r6.s64 = 4;
loc_82640E0C:
	// srawi r7,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r7.s64 = r30.s32 >> 11;
	// lwz r5,20(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi r9,r30,21
	ctx.r9.u64 = r30.u32 & 0x7FF;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// add r8,r30,r29
	ctx.r8.u64 = r30.u64 + r29.u64;
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mr r19,r9
	r19.u64 = ctx.r9.u64;
	// mr r18,r9
	r18.u64 = ctx.r9.u64;
	// lbz r4,1(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// subf r25,r9,r27
	r25.s64 = r27.s64 - ctx.r9.s64;
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// srawi r7,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 11;
	// lbz r26,0(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r9,r7,r28
	ctx.r9.u64 = ctx.r7.u64 + r28.u64;
	// lbz r5,1(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r19,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r19.u32);
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// stw r18,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r18.u32);
	// subf r5,r26,r5
	ctx.r5.s64 = ctx.r5.s64 - r26.s64;
	// stw r4,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r4.u32);
	// subf r19,r7,r27
	r19.s64 = r27.s64 - ctx.r7.s64;
	// stw r25,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, r25.u32);
	// mr r18,r7
	r18.u64 = ctx.r7.u64;
	// stw r30,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r30.u32);
	// mr r14,r7
	r14.u64 = ctx.r7.u64;
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// subf r4,r7,r27
	ctx.r4.s64 = r27.s64 - ctx.r7.s64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// srawi r25,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r8.s32 >> 11;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stw r4,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r4.u32);
	// stw r5,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r5.u32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r25,r28
	ctx.r9.u64 = r25.u64 + r28.u64;
	// lbz r26,0(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r4,1(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// stw r5,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r5.u32);
	// subf r4,r26,r4
	ctx.r4.s64 = ctx.r4.s64 - r26.s64;
	// stw r19,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r19.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// stw r26,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r26.u32);
	// stw r18,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r18.u32);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// stw r14,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r14.u32);
	// stw r5,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r5.u32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r30,0(r4)
	r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r4,1(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// stw r7,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r7.u32);
	// stw r7,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, ctx.r7.u32);
	// subf r7,r30,r4
	ctx.r7.s64 = ctx.r4.s64 - r30.s64;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// stw r30,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r30.u32);
	// stw r5,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r5.u32);
	// add r30,r8,r29
	r30.u64 = ctx.r8.u64 + r29.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// srawi r7,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 11;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// clrlwi r9,r8,21
	ctx.r9.u64 = ctx.r8.u32 & 0x7FF;
	// subf r26,r9,r27
	r26.s64 = r27.s64 - ctx.r9.s64;
	// lwz r19,160(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stw r19,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r19.u32);
	// lwz r5,20(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r8,r5,r7
	ctx.r8.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lbz r5,1(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// stw r9,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r9.u32);
	// stw r9,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r9.u32);
	// subf r9,r4,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r26,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r26.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// stw r5,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r5.u32);
	// stw r7,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r7.u32);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r4,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r4.u32);
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bne cr6,0x82640e0c
	if (!cr6.eq) goto loc_82640E0C;
	// vor v2,v17,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v17.u8));
	// addi r4,r15,256
	ctx.r4.s64 = r15.s64 + 256;
	// mr r5,r15
	ctx.r5.u64 = r15.u64;
	// bl 0x8263e7c0
	sub_8263E7C0(ctx, base);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82640dfc
	if (!cr6.eq) goto loc_82640DFC;
	// lwz r25,120(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r22,96(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r14,128(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
loc_82640FB4:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82641040
	if (!cr6.gt) goto loc_82641040;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
loc_82640FC0:
	// srawi r10,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r10.s64 = r30.s32 >> 11;
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi r11,r30,21
	r11.u64 = r30.u32 & 0x7FF;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbz r7,1(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mullw r5,r7,r11
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// subf r4,r6,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r6.s64;
	// mullw r8,r6,r23
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r23.s32);
	// subf r7,r7,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r7.s64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r7,r7,r23
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r23.s32);
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// subfic r11,r11,2048
	xer.ca = r11.u32 <= 2048;
	r11.s64 = 2048 - r11.s64;
	// subf r11,r23,r11
	r11.s64 = r11.s64 - r23.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// rlwinm r11,r11,24,8,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x82640fc0
	if (!cr6.eq) goto loc_82640FC0;
loc_82641040:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82641080
	if (!cr6.lt) goto loc_82641080;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_82641054:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r9,r9,11
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 11;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r9,r9,r28
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r28.u32);
	// mullw r9,r9,r16
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r16.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x82641054
	if (!cr6.eq) goto loc_82641054;
loc_82641080:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x826410ac
	if (!cr6.lt) goto loc_826410AC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82641094:
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82641094
	if (cr6.lt) goto loc_82641094;
loc_826410AC:
	// srawi r11,r22,12
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFFF) != 0);
	r11.s64 = r22.s32 >> 12;
	// lwz r30,100(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// srawi r10,r22,1
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x1) != 0);
	ctx.r10.s64 = r22.s32 >> 1;
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mullw r19,r11,r24
	r19.s64 = int64_t(r11.s32) * int64_t(r24.s32);
	// lwz r4,92(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// clrlwi r23,r10,21
	r23.u64 = ctx.r10.u32 & 0x7FF;
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// add r11,r19,r24
	r11.u64 = r19.u64 + r24.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x826410f8
	if (!cr6.gt) goto loc_826410F8;
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
loc_826410E0:
	// dcbt r11,r9
	// dcbt r11,r8
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x826410e0
	if (!cr6.eq) goto loc_826410E0;
loc_826410F8:
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82641124
	if (!cr6.gt) goto loc_82641124;
	// li r10,128
	ctx.r10.s64 = 128;
loc_82641108:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82641108
	if (!cr6.eq) goto loc_82641108;
loc_82641124:
	// li r11,16
	r11.s64 = 16;
	// addi r10,r15,8
	ctx.r10.s64 = r15.s64 + 8;
loc_8264112C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r23,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r23.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264112c
	if (!cr6.eq) goto loc_8264112C;
	// lwz r10,168(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// subfic r26,r23,2048
	xer.ca = r23.u32 <= 2048;
	r26.s64 = 2048 - r23.s64;
	// srawi r11,r10,4
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	r11.s64 = ctx.r10.s32 >> 4;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r21,r9,r10
	r21.s64 = ctx.r10.s64 - ctx.r9.s64;
	// ble cr6,0x82641324
	if (!cr6.gt) goto loc_82641324;
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r22,r11
	r22.u64 = r11.u64;
	// add r28,r19,r10
	r28.u64 = r19.u64 + ctx.r10.u64;
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r27,r19,r10
	r27.u64 = r19.u64 + ctx.r10.u64;
loc_82641170:
	// addi r11,r15,256
	r11.s64 = r15.s64 + 256;
	// addi r10,r15,16
	ctx.r10.s64 = r15.s64 + 16;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// li r7,4
	ctx.r7.s64 = 4;
loc_82641180:
	// srawi r6,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r6.s64 = r30.s32 >> 12;
	// srawi r8,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r8.s64 = r30.s32 >> 1;
	// add r9,r30,r20
	ctx.r9.u64 = r30.u64 + r20.u64;
	// clrlwi r5,r8,21
	ctx.r5.u64 = ctx.r8.u32 & 0x7FF;
	// add r8,r28,r6
	ctx.r8.u64 = r28.u64 + ctx.r6.u64;
	// mr r18,r9
	r18.u64 = ctx.r9.u64;
	// subf r14,r5,r26
	r14.s64 = r26.s64 - ctx.r5.s64;
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// add r9,r9,r20
	ctx.r9.u64 = ctx.r9.u64 + r20.u64;
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbzx r25,r8,r24
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r5,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, ctx.r5.u32);
	// stw r14,-16(r10)
	PPC_STORE_U32(ctx.r10.u32 + -16, r14.u32);
	// stw r30,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r30.u32);
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// add r8,r27,r6
	ctx.r8.u64 = r27.u64 + ctx.r6.u64;
	// srawi r6,r18,12
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0xFFF) != 0);
	ctx.r6.s64 = r18.s32 >> 12;
	// srawi r5,r15,1
	xer.ca = (r15.s32 < 0) & ((r15.u32 & 0x1) != 0);
	ctx.r5.s64 = r15.s32 >> 1;
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// lwz r18,160(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stw r18,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r18.u32);
	// subf r18,r5,r26
	r18.s64 = r26.s64 - ctx.r5.s64;
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbzx r25,r8,r24
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbz r15,0(r8)
	r15.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r28,r6
	ctx.r8.u64 = r28.u64 + ctx.r6.u64;
	// stw r30,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r30.u32);
	// stw r25,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r25.u32);
	// stw r15,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r15.u32);
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbzx r25,r8,r24
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbz r15,0(r8)
	r15.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r27,r6
	ctx.r8.u64 = r27.u64 + ctx.r6.u64;
	// srawi r6,r9,12
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFF) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 12;
	// stw r18,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r18.u32);
	// stw r5,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r5.u32);
	// srawi r5,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 1;
	// stw r30,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r30.u32);
	// add r9,r9,r20
	ctx.r9.u64 = ctx.r9.u64 + r20.u64;
	// stw r25,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r25.u32);
	// clrlwi r5,r5,21
	ctx.r5.u64 = ctx.r5.u32 & 0x7FF;
	// stw r15,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r15.u32);
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// subf r18,r5,r26
	r18.s64 = r26.s64 - ctx.r5.s64;
	// lbzx r25,r8,r24
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbz r15,0(r8)
	r15.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r28,r6
	ctx.r8.u64 = r28.u64 + ctx.r6.u64;
	// stw r30,44(r11)
	PPC_STORE_U32(r11.u32 + 44, r30.u32);
	// stw r25,48(r11)
	PPC_STORE_U32(r11.u32 + 48, r25.u32);
	// stw r15,40(r11)
	PPC_STORE_U32(r11.u32 + 40, r15.u32);
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbzx r25,r8,r24
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbz r15,0(r8)
	r15.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r27,r6
	ctx.r8.u64 = r27.u64 + ctx.r6.u64;
	// stw r18,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r18.u32);
	// stw r5,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r5.u32);
	// stw r30,60(r11)
	PPC_STORE_U32(r11.u32 + 60, r30.u32);
	// add r30,r9,r20
	r30.u64 = ctx.r9.u64 + r20.u64;
	// stw r25,64(r11)
	PPC_STORE_U32(r11.u32 + 64, r25.u32);
	// stw r15,56(r11)
	PPC_STORE_U32(r11.u32 + 56, r15.u32);
	// lbz r6,1(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbzx r8,r8,r24
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// stw r6,76(r11)
	PPC_STORE_U32(r11.u32 + 76, ctx.r6.u32);
	// stw r5,72(r11)
	PPC_STORE_U32(r11.u32 + 72, ctx.r5.u32);
	// stw r8,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r8.u32);
	// srawi r8,r9,12
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 12;
	// srawi r6,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// add r9,r28,r8
	ctx.r9.u64 = r28.u64 + ctx.r8.u64;
	// clrlwi r6,r6,21
	ctx.r6.u64 = ctx.r6.u32 & 0x7FF;
	// subf r18,r6,r26
	r18.s64 = r26.s64 - ctx.r6.s64;
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r25,r9,r24
	r25.u64 = PPC_LOAD_U8(ctx.r9.u32 + r24.u32);
	// lbz r15,0(r9)
	r15.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r27,r8
	ctx.r9.u64 = r27.u64 + ctx.r8.u64;
	// stw r18,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, r18.u32);
	// stw r6,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, ctx.r6.u32);
	// stw r15,88(r11)
	PPC_STORE_U32(r11.u32 + 88, r15.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stw r5,92(r11)
	PPC_STORE_U32(r11.u32 + 92, ctx.r5.u32);
	// stw r25,96(r11)
	PPC_STORE_U32(r11.u32 + 96, r25.u32);
	// lbz r8,1(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r6,r9,r24
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + r24.u32);
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// stw r8,108(r11)
	PPC_STORE_U32(r11.u32 + 108, ctx.r8.u32);
	// stw r6,112(r11)
	PPC_STORE_U32(r11.u32 + 112, ctx.r6.u32);
	// stw r9,104(r11)
	PPC_STORE_U32(r11.u32 + 104, ctx.r9.u32);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// bne cr6,0x82641180
	if (!cr6.eq) goto loc_82641180;
	// lwz r15,148(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// addi r5,r15,256
	ctx.r5.s64 = r15.s64 + 256;
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// bl 0x8263eed8
	sub_8263EED8(ctx, base);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82641170
	if (!cr6.eq) goto loc_82641170;
	// lwz r14,128(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r22,96(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r25,120(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
loc_82641324:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x826413e4
	if (!cr6.gt) goto loc_826413E4;
	// lwz r11,104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// add r5,r19,r11
	ctx.r5.u64 = r19.u64 + r11.u64;
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r28,r19,r11
	r28.u64 = r19.u64 + r11.u64;
loc_82641340:
	// srawi r8,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r8.s64 = r30.s32 >> 12;
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// add r10,r5,r8
	ctx.r10.u64 = ctx.r5.u64 + ctx.r8.u64;
	// clrlwi r11,r11,21
	r11.u64 = r11.u32 & 0x7FF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// subfic r7,r11,2048
	xer.ca = r11.u32 <= 2048;
	ctx.r7.s64 = 2048 - r11.s64;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// lbzx r6,r10,r24
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + r24.u32);
	// subf r7,r23,r7
	ctx.r7.s64 = ctx.r7.s64 - r23.s64;
	// lbz r27,1(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbz r26,0(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r28,r8
	ctx.r10.u64 = r28.u64 + ctx.r8.u64;
	// mullw r8,r6,r23
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r23.s32);
	// mullw r6,r27,r11
	ctx.r6.s64 = int64_t(r27.s32) * int64_t(r11.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mullw r6,r26,r7
	ctx.r6.s64 = int64_t(r26.s32) * int64_t(ctx.r7.s32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// srawi r8,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 11;
	// addi r8,r8,-128
	ctx.r8.s64 = ctx.r8.s64 + -128;
	// mullw r8,r8,r16
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r16.s32);
	// rlwinm r8,r8,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFFFFFF;
	// addi r8,r8,128
	ctx.r8.s64 = ctx.r8.s64 + 128;
	// stb r8,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r8.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbzx r8,r10,r24
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r24.u32);
	// lbz r27,0(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// mullw r11,r6,r11
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(r11.s32);
	// mullw r10,r8,r23
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r23.s32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// mullw r10,r27,r7
	ctx.r10.s64 = int64_t(r27.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// rlwinm r11,r11,24,8,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x82641340
	if (!cr6.eq) goto loc_82641340;
loc_826413E4:
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r6,176(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x82641454
	if (!cr6.lt) goto loc_82641454;
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// subf r11,r11,r6
	r11.s64 = ctx.r6.s64 - r11.s64;
	// add r8,r19,r10
	ctx.r8.u64 = r19.u64 + ctx.r10.u64;
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r7,r19,r10
	ctx.r7.u64 = r19.u64 + ctx.r10.u64;
loc_82641408:
	// srawi r10,r30,12
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xFFF) != 0);
	ctx.r10.s64 = r30.s32 >> 12;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r30,r30,r20
	r30.u64 = r30.u64 + r20.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r9,r8,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r10.u32);
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// mullw r9,r9,r16
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r16.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lbzx r10,r7,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r10.u32);
	// addi r10,r10,-128
	ctx.r10.s64 = ctx.r10.s64 + -128;
	// mullw r10,r10,r16
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r16.s32);
	// rlwinm r10,r10,24,8,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// addi r10,r10,128
	ctx.r10.s64 = ctx.r10.s64 + 128;
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne cr6,0x82641408
	if (!cr6.eq) goto loc_82641408;
loc_82641454:
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// bge cr6,0x82641484
	if (!cr6.lt) goto loc_82641484;
	// subf r11,r6,r10
	r11.s64 = ctx.r10.s64 - ctx.r6.s64;
	// li r9,128
	ctx.r9.s64 = 128;
loc_82641468:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// stb r9,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r9.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82641468
	if (!cr6.eq) goto loc_82641468;
loc_82641484:
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// add r18,r11,r25
	r18.u64 = r11.u64 + r25.u64;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r19,r22,r9
	r19.u64 = r22.u64 + ctx.r9.u64;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// bge cr6,0x826414c0
	if (!cr6.lt) goto loc_826414C0;
	// li r19,0
	r19.s64 = 0;
loc_826414C0:
	// clrlwi r23,r19,21
	r23.u64 = r19.u32 & 0x7FF;
	// lwz r10,15324(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15324);
	// srawi r11,r19,11
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FF) != 0);
	r11.s64 = r19.s32 >> 11;
	// lwz r30,100(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// extsw r9,r23
	ctx.r9.s64 = r23.s32;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// std r9,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r9.u64);
	// lfd f0,192(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// blt cr6,0x826414f4
	if (cr6.lt) goto loc_826414F4;
	// addi r11,r10,-1
	r11.s64 = ctx.r10.s64 + -1;
loc_826414F4:
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,15340(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15340);
	// mullw r11,r8,r11
	r11.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641534
	if (!cr6.gt) goto loc_82641534;
	// mr r11,r18
	r11.u64 = r18.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82641530
	if (cr6.eq) goto loc_82641530;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82641524:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82641524
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82641524;
loc_82641530:
	// add r3,r18,r9
	ctx.r3.u64 = r18.u64 + ctx.r9.u64;
loc_82641534:
	// li r11,16
	r11.s64 = 16;
	// addi r10,r15,8
	ctx.r10.s64 = r15.s64 + 8;
loc_8264153C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r23,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r23.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264153c
	if (!cr6.eq) goto loc_8264153C;
	// subfic r27,r23,2048
	xer.ca = r23.u32 <= 2048;
	r27.s64 = 2048 - r23.s64;
	// stfs f0,220(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// srawi r11,r14,4
	xer.ca = (r14.s32 < 0) & ((r14.u32 & 0xF) != 0);
	r11.s64 = r14.s32 >> 4;
	// rlwinm r10,r11,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// subf r21,r10,r14
	r21.s64 = r14.s64 - ctx.r10.s64;
	// ble cr6,0x82641730
	if (!cr6.gt) goto loc_82641730;
	// mr r22,r11
	r22.u64 = r11.u64;
	// addi r11,r1,208
	r11.s64 = ctx.r1.s64 + 208;
	// lvx128 v17,r0,r11
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82641578:
	// addi r11,r15,256
	r11.s64 = r15.s64 + 256;
	// addi r10,r15,12
	ctx.r10.s64 = r15.s64 + 12;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// li r6,4
	ctx.r6.s64 = 4;
loc_82641588:
	// srawi r7,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r7.s64 = r30.s32 >> 11;
	// lwz r5,20(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi r9,r30,21
	ctx.r9.u64 = r30.u32 & 0x7FF;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// add r8,r30,r29
	ctx.r8.u64 = r30.u64 + r29.u64;
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mr r16,r9
	r16.u64 = ctx.r9.u64;
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// lbz r4,1(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// subf r25,r9,r27
	r25.s64 = r27.s64 - ctx.r9.s64;
	// lbz r30,0(r7)
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// srawi r7,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 11;
	// lbz r26,0(r5)
	r26.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// add r9,r7,r28
	ctx.r9.u64 = ctx.r7.u64 + r28.u64;
	// lbz r5,1(r5)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// stw r16,-8(r10)
	PPC_STORE_U32(ctx.r10.u32 + -8, r16.u32);
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// stw r15,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r15.u32);
	// subf r5,r26,r5
	ctx.r5.s64 = ctx.r5.s64 - r26.s64;
	// stw r4,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r4.u32);
	// subf r16,r7,r27
	r16.s64 = r27.s64 - ctx.r7.s64;
	// stw r25,-12(r10)
	PPC_STORE_U32(ctx.r10.u32 + -12, r25.u32);
	// mr r15,r7
	r15.u64 = ctx.r7.u64;
	// stw r30,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r30.u32);
	// mr r14,r7
	r14.u64 = ctx.r7.u64;
	// stw r26,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r26.u32);
	// clrlwi r7,r8,21
	ctx.r7.u64 = ctx.r8.u32 & 0x7FF;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// subf r4,r7,r27
	ctx.r4.s64 = r27.s64 - ctx.r7.s64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// srawi r25,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	r25.s64 = ctx.r8.s32 >> 11;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stw r4,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r4.u32);
	// stw r5,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r5.u32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r9,r25,r28
	ctx.r9.u64 = r25.u64 + r28.u64;
	// lbz r26,0(r4)
	r26.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r4,1(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// stw r5,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r5.u32);
	// subf r4,r26,r4
	ctx.r4.s64 = ctx.r4.s64 - r26.s64;
	// stw r16,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r16.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// subf r5,r5,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r5.s64;
	// stw r26,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r26.u32);
	// stw r15,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r15.u32);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// stw r14,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r14.u32);
	// stw r5,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r5.u32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lbz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r30,0(r4)
	r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r4,1(r4)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// stw r7,24(r10)
	PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r7.u32);
	// stw r7,32(r10)
	PPC_STORE_U32(ctx.r10.u32 + 32, ctx.r7.u32);
	// subf r7,r30,r4
	ctx.r7.s64 = ctx.r4.s64 - r30.s64;
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// stw r30,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r30.u32);
	// stw r5,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r5.u32);
	// add r30,r8,r29
	r30.u64 = ctx.r8.u64 + r29.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// srawi r7,r8,11
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 11;
	// add r7,r7,r28
	ctx.r7.u64 = ctx.r7.u64 + r28.u64;
	// stw r9,36(r11)
	PPC_STORE_U32(r11.u32 + 36, ctx.r9.u32);
	// clrlwi r9,r8,21
	ctx.r9.u64 = ctx.r8.u32 & 0x7FF;
	// subf r26,r9,r27
	r26.s64 = r27.s64 - ctx.r9.s64;
	// lwz r16,160(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// stw r16,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, r16.u32);
	// lwz r5,20(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r8,r5,r7
	ctx.r8.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lbz r5,1(r7)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// stw r9,40(r10)
	PPC_STORE_U32(ctx.r10.u32 + 40, ctx.r9.u32);
	// stw r9,48(r10)
	PPC_STORE_U32(ctx.r10.u32 + 48, ctx.r9.u32);
	// subf r9,r4,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r26,36(r10)
	PPC_STORE_U32(ctx.r10.u32 + 36, r26.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// subf r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	// stw r5,44(r11)
	PPC_STORE_U32(r11.u32 + 44, ctx.r5.u32);
	// stw r7,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r7.u32);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r4,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r4.u32);
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bne cr6,0x82641588
	if (!cr6.eq) goto loc_82641588;
	// lwz r15,148(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// vor v2,v17,v17
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v17.u8));
	// addi r4,r15,256
	ctx.r4.s64 = r15.s64 + 256;
	// mr r5,r15
	ctx.r5.u64 = r15.u64;
	// bl 0x8263e7c0
	sub_8263E7C0(ctx, base);
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82641578
	if (!cr6.eq) goto loc_82641578;
	// lwz r14,128(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r16,152(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
loc_82641730:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x826417bc
	if (!cr6.gt) goto loc_826417BC;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
loc_8264173C:
	// srawi r10,r30,11
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FF) != 0);
	ctx.r10.s64 = r30.s32 >> 11;
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// clrlwi r11,r30,21
	r11.u64 = r30.u32 & 0x7FF;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbz r7,1(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mullw r5,r7,r11
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// lbz r8,1(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// subf r4,r6,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r6.s64;
	// mullw r8,r6,r23
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(r23.s32);
	// subf r7,r7,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r7.s64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r7,r7,r23
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r23.s32);
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// subfic r11,r11,2048
	xer.ca = r11.u32 <= 2048;
	r11.s64 = 2048 - r11.s64;
	// subf r11,r23,r11
	r11.s64 = r11.s64 - r23.s64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,11
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FF) != 0);
	r11.s64 = r11.s32 >> 11;
	// mullw r11,r11,r16
	r11.s64 = int64_t(r11.s32) * int64_t(r16.s32);
	// rlwinm r11,r11,24,8,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x8264173c
	if (!cr6.eq) goto loc_8264173C;
loc_826417BC:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x826417fc
	if (!cr6.lt) goto loc_826417FC;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_826417D0:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// srawi r9,r9,11
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 11;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lbzx r9,r9,r28
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r28.u32);
	// mullw r9,r9,r16
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r16.s32);
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// bne cr6,0x826417d0
	if (!cr6.eq) goto loc_826417D0;
loc_826417FC:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bge cr6,0x82641828
	if (!cr6.lt) goto loc_82641828;
loc_8264180C:
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r10.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x8264180c
	if (cr6.lt) goto loc_8264180C;
loc_82641828:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// add r22,r19,r11
	r22.u64 = r19.u64 + r11.u64;
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// add r25,r11,r18
	r25.u64 = r11.u64 + r18.u64;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
	// stw r25,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r25.u32);
	// bge cr6,0x82641850
	if (!cr6.lt) goto loc_82641850;
	// li r22,0
	r22.s64 = 0;
	// stw r22,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r22.u32);
loc_82641850:
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// addi r3,r11,-2
	ctx.r3.s64 = r11.s64 + -2;
	// cmpw cr6,r17,r3
	cr6.compare<int32_t>(r17.s32, ctx.r3.s32, xer);
	// blt cr6,0x82640d28
	if (cr6.lt) goto loc_82640D28;
loc_82641864:
	// lwz r11,15324(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15324);
	// srawi r6,r22,11
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FF) != 0);
	ctx.r6.s64 = r22.s32 >> 11;
	// lwz r15,100(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// blt cr6,0x82641880
	if (cr6.lt) goto loc_82641880;
	// addi r6,r11,-1
	ctx.r6.s64 = r11.s64 + -1;
loc_82641880:
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82641b3c
	if (cr6.lt) goto loc_82641B3C;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bge cr6,0x82641b3c
	if (!cr6.lt) goto loc_82641B3C;
	// lwz r23,136(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// li r19,0
	r19.s64 = 0;
	// lwz r18,140(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// li r14,128
	r14.s64 = 128;
	// lwz r17,124(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r28,84(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r26,144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_826418C4:
	// mr r11,r25
	r11.u64 = r25.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826418f0
	if (!cr6.gt) goto loc_826418F0;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826418ec
	if (cr6.eq) goto loc_826418EC;
	// mtctr r30
	ctr.u64 = r30.u64;
loc_826418E0:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x826418e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826418E0;
loc_826418EC:
	// add r11,r25,r30
	r11.u64 = r25.u64 + r30.u64;
loc_826418F0:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// lwz r9,15340(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15340);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82641938
	if (!cr6.lt) goto loc_82641938;
	// subf r10,r30,r28
	ctx.r10.s64 = r28.s64 - r30.s64;
loc_8264190C:
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r7,r7,11
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 11;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r7,r7,r9
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// mullw r7,r7,r16
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r16.s32);
	// rlwinm r7,r7,24,8,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFFFFFF;
	// stb r7,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r7.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8264190c
	if (!cr6.eq) goto loc_8264190C;
loc_82641938:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// bge cr6,0x82641960
	if (!cr6.lt) goto loc_82641960;
loc_82641948:
	// stb r19,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r19.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82641948
	if (cr6.lt) goto loc_82641948;
loc_82641960:
	// mr r11,r21
	r11.u64 = r21.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// ble cr6,0x82641990
	if (!cr6.gt) goto loc_82641990;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
loc_82641974:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r14,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r14.u8);
	// stb r14,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r14.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82641974
	if (!cr6.eq) goto loc_82641974;
loc_82641990:
	// lwz r8,15324(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15324);
	// srawi r9,r22,12
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xFFF) != 0);
	ctx.r9.s64 = r22.s32 >> 12;
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x826419ac
	if (cr6.lt) goto loc_826419AC;
	// addi r9,r8,-1
	ctx.r9.s64 = ctx.r8.s64 + -1;
loc_826419AC:
	// mullw r9,r9,r24
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r24.s32);
	// cmpw cr6,r23,r18
	cr6.compare<int32_t>(r23.s32, r18.s32, xer);
	// bge cr6,0x82641a18
	if (!cr6.lt) goto loc_82641A18;
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// add r5,r9,r8
	ctx.r5.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,108(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// add r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r9,r23,r18
	ctx.r9.s64 = r18.s64 - r23.s64;
loc_826419CC:
	// srawi r8,r7,12
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r7,r7,r20
	ctx.r7.u64 = ctx.r7.u64 + r20.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lbzx r6,r5,r8
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r8.u32);
	// addi r6,r6,-128
	ctx.r6.s64 = ctx.r6.s64 + -128;
	// mullw r6,r6,r16
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(r16.s32);
	// rlwinm r6,r6,24,8,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFFFFFF;
	// addi r6,r6,128
	ctx.r6.s64 = ctx.r6.s64 + 128;
	// stb r6,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r6.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lbzx r8,r4,r8
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r8.u32);
	// addi r8,r8,-128
	ctx.r8.s64 = ctx.r8.s64 + -128;
	// mullw r8,r8,r16
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r16.s32);
	// rlwinm r8,r8,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 24) & 0xFFFFFF;
	// addi r8,r8,128
	ctx.r8.s64 = ctx.r8.s64 + 128;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bne cr6,0x826419cc
	if (!cr6.eq) goto loc_826419CC;
loc_82641A18:
	// cmpw cr6,r18,r26
	cr6.compare<int32_t>(r18.s32, r26.s32, xer);
	// bge cr6,0x82641a40
	if (!cr6.lt) goto loc_82641A40;
	// subf r9,r18,r26
	ctx.r9.s64 = r26.s64 - r18.s64;
loc_82641A24:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r14,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r14.u8);
	// stb r14,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r14.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82641a24
	if (!cr6.eq) goto loc_82641A24;
loc_82641A40:
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// add r22,r22,r17
	r22.u64 = r22.u64 + r17.u64;
	// add r21,r21,r26
	r21.u64 = r21.u64 + r26.u64;
	// add r7,r11,r25
	ctx.r7.u64 = r11.u64 + r25.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// addi r5,r3,1
	ctx.r5.s64 = ctx.r3.s64 + 1;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// bge cr6,0x82641a68
	if (!cr6.lt) goto loc_82641A68;
	// mr r22,r19
	r22.u64 = r19.u64;
loc_82641A68:
	// lwz r10,15324(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15324);
	// srawi r6,r22,11
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FF) != 0);
	ctx.r6.s64 = r22.s32 >> 11;
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// blt cr6,0x82641a80
	if (cr6.lt) goto loc_82641A80;
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
loc_82641A80:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82641aac
	if (!cr6.gt) goto loc_82641AAC;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82641aa8
	if (cr6.eq) goto loc_82641AA8;
	// mtctr r30
	ctr.u64 = r30.u64;
loc_82641A9C:
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82641a9c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82641A9C;
loc_82641AA8:
	// add r11,r7,r30
	r11.u64 = ctx.r7.u64 + r30.u64;
loc_82641AAC:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// lwz r9,15340(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15340);
	// mullw r10,r10,r6
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82641af4
	if (!cr6.lt) goto loc_82641AF4;
	// subf r10,r30,r28
	ctx.r10.s64 = r28.s64 - r30.s64;
loc_82641AC8:
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r4,r4,11
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7FF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 11;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lbzx r4,r4,r9
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r9.u32);
	// mullw r4,r4,r16
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r16.s32);
	// rlwinm r4,r4,24,8,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 24) & 0xFFFFFF;
	// stb r4,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r4.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x82641ac8
	if (!cr6.eq) goto loc_82641AC8;
loc_82641AF4:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// cmpw cr6,r28,r9
	cr6.compare<int32_t>(r28.s32, ctx.r9.s32, xer);
	// bge cr6,0x82641b1c
	if (!cr6.lt) goto loc_82641B1C;
loc_82641B04:
	// stb r19,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r19.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82641b04
	if (cr6.lt) goto loc_82641B04;
loc_82641B1C:
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r3,r5,1
	ctx.r3.s64 = ctx.r5.s64 + 1;
	// add r25,r11,r7
	r25.u64 = r11.u64 + ctx.r7.u64;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// blt cr6,0x826418c4
	if (cr6.lt) goto loc_826418C4;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// b 0x82641b50
	goto loc_82641B50;
loc_82641B3C:
	// lwz r21,88(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r19,0
	r19.s64 = 0;
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// li r14,128
	r14.s64 = 128;
	// lwz r26,144(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
loc_82641B50:
	// lwz r10,15332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15332);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82641c0c
	if (!cr6.lt) goto loc_82641C0C;
loc_82641B60:
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r11,r19
	r11.u64 = r19.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641b8c
	if (!cr6.gt) goto loc_82641B8C;
loc_82641B74:
	// stb r19,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r19.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,15328(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82641b74
	if (cr6.lt) goto loc_82641B74;
loc_82641B8C:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x82641bb8
	if (!cr6.gt) goto loc_82641BB8;
	// mr r11,r27
	r11.u64 = r27.u64;
	// subf r9,r27,r21
	ctx.r9.s64 = r21.s64 - r27.s64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_82641BA0:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stbx r14,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, r14.u8);
	// stb r14,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r14.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82641ba0
	if (!cr6.eq) goto loc_82641BA0;
loc_82641BB8:
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// add r21,r21,r26
	r21.u64 = r21.u64 + r26.u64;
	// add r27,r27,r26
	r27.u64 = r27.u64 + r26.u64;
	// add r9,r10,r25
	ctx.r9.u64 = ctx.r10.u64 + r25.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// mr r11,r19
	r11.u64 = r19.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// ble cr6,0x82641bf4
	if (!cr6.gt) goto loc_82641BF4;
loc_82641BDC:
	// stb r19,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r19.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r7,15328(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82641bdc
	if (cr6.lt) goto loc_82641BDC;
loc_82641BF4:
	// lwz r11,15328(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lwz r10,15332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15332);
	// add r25,r11,r9
	r25.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// blt cr6,0x82641b60
	if (cr6.lt) goto loc_82641B60;
loc_82641C0C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8239d620
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82641C20"))) PPC_WEAK_FUNC(sub_82641C20);
PPC_FUNC_IMPL(__imp__sub_82641C20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lis r11,128
	r11.s64 = 8388608;
	// lwz r10,15332(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15332);
	// lwz r9,15328(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15328);
	// vspltisw v13,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_set1_epi32(int(0x0)));
	// ori r11,r11,128
	r11.u64 = r11.u64 | 128;
	// vspltisb v0,-1
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// mullw r6,r10,r9
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r10,3788(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3788);
	// vmrghb v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// lwz r11,3776(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3776);
	// or r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 | r11.u64;
	// clrlwi r8,r8,28
	ctx.r8.u64 = ctx.r8.u32 & 0xF;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// srawi r9,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 4;
	// vspltw v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// rlwinm r7,r9,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r7,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r7.s64;
	// beq cr6,0x82641ccc
	if (cr6.eq) goto loc_82641CCC;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641cf8
	if (!cr6.gt) goto loc_82641CF8;
loc_82641C80:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,16
	ctx.r5.s64 = 16;
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r31,r11
	r31.u64 = r11.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvrx v10,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lvrx v9,r10,r5
	temp.u32 = ctx.r10.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vaddubs v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvlx v12,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r31,r8
	ea = r31.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x82641c80
	if (!cr6.eq) goto loc_82641C80;
	// b 0x82641cf8
	goto loc_82641CF8;
loc_82641CCC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641cf8
	if (!cr6.gt) goto loc_82641CF8;
loc_82641CD4:
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vaddubs v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82641cd4
	if (!cr6.eq) goto loc_82641CD4;
loc_82641CF8:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82641d30
	if (!cr6.gt) goto loc_82641D30;
loc_82641D00:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82641d18
	if (!cr6.gt) goto loc_82641D18;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82641D18:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82641d00
	if (!cr6.eq) goto loc_82641D00;
loc_82641D30:
	// srawi r9,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 2;
	// lwz r11,3780(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3780);
	// lwz r10,3792(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3792);
	// addze r6,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r6.s64 = temp.s64;
	// or r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 | r11.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// srawi r9,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r9,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r7,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r7.s64;
	// beq cr6,0x82641de0
	if (cr6.eq) goto loc_82641DE0;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641e3c
	if (!cr6.gt) goto loc_82641E3C;
loc_82641D64:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,16
	ctx.r5.s64 = 16;
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r31,r11
	r31.u64 = r11.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvrx v10,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lvrx v9,r10,r5
	temp.u32 = ctx.r10.u32 + ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vupklsb v10,v12
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vupklsb v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v11.s16)));
	// vupkhsb v12,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s8), _mm_load_si128((__m128i*)ctx.v12.s8))));
	// vupkhsb v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s8), _mm_load_si128((__m128i*)ctx.v11.s8))));
	// vand v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v12,v10,v13
	// vsubshs v11,v11,v13
	// vpkshus v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvlx v12,0,r4
	ea = ctx.r4.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r31,r8
	ea = r31.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x82641d64
	if (!cr6.eq) goto loc_82641D64;
	// b 0x82641e3c
	goto loc_82641E3C;
loc_82641DE0:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641e3c
	if (!cr6.gt) goto loc_82641E3C;
loc_82641DE8:
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupklsb v10,v12
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vupklsb v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vupkhsb v12,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s8), _mm_load_si128((__m128i*)ctx.v12.s8))));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vupkhsb v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s8), _mm_load_si128((__m128i*)ctx.v11.s8))));
	// vand v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v12,v10,v13
	// vsubshs v11,v11,v13
	// vpkshus v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82641de8
	if (!cr6.eq) goto loc_82641DE8;
loc_82641E3C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82641e88
	if (!cr6.gt) goto loc_82641E88;
loc_82641E44:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82641e64
	if (!cr6.gt) goto loc_82641E64;
	// li r9,255
	ctx.r9.s64 = 255;
	// b 0x82641e70
	goto loc_82641E70;
loc_82641E64:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82641e70
	if (!cr6.lt) goto loc_82641E70;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82641E70:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82641e44
	if (!cr6.eq) goto loc_82641E44;
loc_82641E88:
	// srawi r9,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 4;
	// lwz r11,3784(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3784);
	// lwz r10,3796(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3796);
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r8,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r8.s64;
	// or r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 | r11.u64;
	// clrlwi r8,r8,28
	ctx.r8.u64 = ctx.r8.u32 & 0xF;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82641f30
	if (cr6.eq) goto loc_82641F30;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641f8c
	if (!cr6.gt) goto loc_82641F8C;
loc_82641EB4:
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r6,16
	ctx.r6.s64 = 16;
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvrx v10,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lvrx v9,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vor v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vupklsb v10,v12
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vupklsb v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v11.s16)));
	// vupkhsb v12,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s8), _mm_load_si128((__m128i*)ctx.v12.s8))));
	// vupkhsb v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s8), _mm_load_si128((__m128i*)ctx.v11.s8))));
	// vand v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v12,v10,v13
	// vsubshs v11,v11,v13
	// vpkshus v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvlx v12,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// stvrx v12,r4,r8
	ea = ctx.r4.u32 + ctx.r8.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// bne cr6,0x82641eb4
	if (!cr6.eq) goto loc_82641EB4;
	// b 0x82641f8c
	goto loc_82641F8C;
loc_82641F30:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82641f8c
	if (!cr6.gt) goto loc_82641F8C;
loc_82641F38:
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lvx128 v11,r0,r10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vupklsb v10,v12
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// vupklsb v9,v11
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v11.s16)));
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// vupkhsb v12,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s8), _mm_load_si128((__m128i*)ctx.v12.s8))));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vupkhsb v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s8), _mm_load_si128((__m128i*)ctx.v11.s8))));
	// vand v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v11,v12,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v12,v10,v13
	// vsubshs v11,v11,v13
	// vpkshus v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// stvx v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne cr6,0x82641f38
	if (!cr6.eq) goto loc_82641F38;
loc_82641F8C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82641fd8
	if (!cr6.gt) goto loc_82641FD8;
loc_82641F94:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r9,r9,-128
	ctx.r9.s64 = ctx.r9.s64 + -128;
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82641fb4
	if (!cr6.gt) goto loc_82641FB4;
	// li r9,255
	ctx.r9.s64 = 255;
	// b 0x82641fc0
	goto loc_82641FC0;
loc_82641FB4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82641fc0
	if (!cr6.lt) goto loc_82641FC0;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82641FC0:
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82641f94
	if (!cr6.eq) goto loc_82641F94;
loc_82641FD8:
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82641FE0"))) PPC_WEAK_FUNC(sub_82641FE0);
PPC_FUNC_IMPL(__imp__sub_82641FE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// srawi r8,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 6;
	// rlwinm r11,r8,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 6) & 0xFFFFFFC0;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r29,r11,r6
	r29.s64 = ctx.r6.s64 - r11.s64;
	// ble cr6,0x826420fc
	if (!cr6.gt) goto loc_826420FC;
	// addi r9,r5,32
	ctx.r9.s64 = ctx.r5.s64 + 32;
	// addi r10,r4,32
	ctx.r10.s64 = ctx.r4.s64 + 32;
	// addi r11,r3,32
	r11.s64 = ctx.r3.s64 + 32;
loc_82642008:
	// li r6,16
	ctx.r6.s64 = 16;
	// lvlx v13,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r28,16
	r28.s64 = 16;
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r26,16
	r26.s64 = 16;
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r25,16
	r25.s64 = 16;
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,-16
	ctx.r7.s64 = r11.s64 + -16;
	// lvrx v0,r3,r6
	temp.u32 = ctx.r3.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v13,r4,r28
	temp.u32 = ctx.r4.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v12,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// li r27,16
	r27.s64 = 16;
	// lvrx v11,r10,r25
	temp.u32 = ctx.r10.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r11,16
	r31.s64 = r11.s64 + 16;
	// li r24,16
	r24.s64 = 16;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r7,r28
	temp.u32 = ctx.r7.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r30,r10,16
	r30.s64 = ctx.r10.s64 + 16;
	// lvlx v9,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vaddubs v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lvrx v8,r6,r27
	temp.u32 = ctx.r6.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvlx v7,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vaddubs v13,v12,v11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v6,r31,r24
	temp.u32 = r31.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v9,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r31,16
	r31.s64 = 16;
	// vor v9,v9,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvrx v7,r30,r26
	temp.u32 = r30.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v6,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r9,-16
	ctx.r7.s64 = ctx.r9.s64 + -16;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// li r30,16
	r30.s64 = 16;
	// vaddubs v12,v10,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// stvlx v0,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// stvrx v0,r5,r31
	ea = ctx.r5.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// vaddubs v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stvlx v12,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// stvrx v12,r7,r30
	ea = ctx.r7.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvlx v13,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// stvrx v13,r27,r26
	ea = r27.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stvlx v11,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// stvrx v11,r6,r31
	ea = ctx.r6.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82642008
	if (!cr6.eq) goto loc_82642008;
loc_826420FC:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8264213c
	if (!cr6.gt) goto loc_8264213C;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_82642108:
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82642120
	if (!cr6.gt) goto loc_82642120;
	// li r11,255
	r11.s64 = 255;
loc_82642120:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r11,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82642108
	if (!cr6.eq) goto loc_82642108;
loc_8264213C:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82642140"))) PPC_WEAK_FUNC(sub_82642140);
PPC_FUNC_IMPL(__imp__sub_82642140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// lis r11,128
	r11.s64 = 8388608;
	// vspltisw v13,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_set1_epi32(int(0x0)));
	// vspltisb v0,-1
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0xFFFFFFFF)));
	// srawi r8,r6,6
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3F) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 6;
	// ori r11,r11,128
	r11.u64 = r11.u64 | 128;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// vmrghb v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// addi r11,r1,-96
	r11.s64 = ctx.r1.s64 + -96;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r11,r8,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 6) & 0xFFFFFFC0;
	// vspltw v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// subf r29,r11,r6
	r29.s64 = ctx.r6.s64 - r11.s64;
	// ble cr6,0x8264234c
	if (!cr6.gt) goto loc_8264234C;
	// addi r9,r5,32
	ctx.r9.s64 = ctx.r5.s64 + 32;
	// addi r10,r4,32
	ctx.r10.s64 = ctx.r4.s64 + 32;
	// addi r11,r3,32
	r11.s64 = ctx.r3.s64 + 32;
loc_8264218C:
	// li r31,16
	r31.s64 = 16;
	// lvlx v11,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r30,16
	r30.s64 = 16;
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r26,16
	r26.s64 = 16;
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r25,16
	r25.s64 = 16;
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,-16
	ctx.r7.s64 = r11.s64 + -16;
	// lvrx v12,r3,r31
	temp.u32 = ctx.r3.u32 + r31.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r28,16
	r28.s64 = 16;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v11,r4,r30
	temp.u32 = ctx.r4.u32 + r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r10,r25
	temp.u32 = ctx.r10.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// li r27,16
	r27.s64 = 16;
	// lvlx v7,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r7,r28
	temp.u32 = ctx.r7.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r31,r11,16
	r31.s64 = r11.s64 + 16;
	// li r24,16
	r24.s64 = 16;
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// addi r30,r10,16
	r30.s64 = ctx.r10.s64 + 16;
	// vupklsb v4,v12
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v12.s16)));
	// lvlx v7,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vupklsb v31,v10
	_mm_store_si128((__m128i*)v31.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v10.s16)));
	// lvrx v6,r6,r27
	temp.u32 = ctx.r6.u32 + r27.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vupklsb v30,v9
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v9.s16)));
	// vor v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v5,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v6,r31,r24
	temp.u32 = r31.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vupkhsb v10,v10
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v10.s8), _mm_load_si128((__m128i*)ctx.v10.s8))));
	// lvrx v3,r30,r26
	temp.u32 = r30.u32 + r26.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v2,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vupkhsb v9,v9
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v9.s8), _mm_load_si128((__m128i*)ctx.v9.s8))));
	// vor v5,v2,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vupklsb v1,v7
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v7.s16)));
	// vupkhsb v3,v12
	_mm_store_si128((__m128i*)ctx.v3.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s8), _mm_load_si128((__m128i*)ctx.v12.s8))));
	// vand v10,v10,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupklsb v2,v8
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v8.s16)));
	// vand v4,v4,v0
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupklsb v12,v11
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v11.s16)));
	// vand v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsb v8,v8
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v8.s8), _mm_load_si128((__m128i*)ctx.v8.s8))));
	// vand v1,v1,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsb v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v7.s8), _mm_load_si128((__m128i*)ctx.v7.s8))));
	// vand v3,v3,v0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsb v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s8), _mm_load_si128((__m128i*)ctx.v11.s8))));
	// vand v2,v2,v0
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupklsb v29,v6
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v6.s16)));
	// vand v12,v12,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupklsb v28,v5
	_mm_store_si128((__m128i*)v28.s32, _mm_cvtepi8_epi16(_mm_load_si128((__m128i*)ctx.v5.s16)));
	// vand v8,v8,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsb v6,v6
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v6.s8), _mm_load_si128((__m128i*)ctx.v6.s8))));
	// vand v7,v7,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vupkhsb v5,v5
	_mm_store_si128((__m128i*)ctx.v5.s16, _mm_cvtepi8_epi16(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v5.s8), _mm_load_si128((__m128i*)ctx.v5.s8))));
	// vand v11,v11,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v31,v31,v0
	_mm_store_si128((__m128i*)v31.u8, _mm_and_si128(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v30,v30,v0
	_mm_store_si128((__m128i*)v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v6,v6,v0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v5,v5,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v7.s16)));
	// vaddshs v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v9.s16)));
	// vaddshs v12,v4,v12
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// vand v29,v29,v0
	_mm_store_si128((__m128i*)v29.u8, _mm_and_si128(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vand v28,v28,v0
	_mm_store_si128((__m128i*)v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vaddshs v11,v3,v11
	_mm_store_si128((__m128i*)ctx.v11.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vaddshs v7,v2,v1
	_mm_store_si128((__m128i*)ctx.v7.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v2.s16), _mm_load_si128((__m128i*)ctx.v1.s16)));
	// vaddshs v9,v31,v30
	_mm_store_si128((__m128i*)ctx.v9.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v31.s16), _mm_load_si128((__m128i*)v30.s16)));
	// vaddshs v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vsubshs v5,v12,v13
	// vsubshs v11,v11,v13
	// vaddshs v12,v29,v28
	_mm_store_si128((__m128i*)ctx.v12.s16, _mm_adds_epi16(_mm_load_si128((__m128i*)v29.s16), _mm_load_si128((__m128i*)v28.s16)));
	// vsubshs v8,v8,v13
	// vsubshs v7,v7,v13
	// vsubshs v10,v10,v13
	// vsubshs v9,v9,v13
	// vsubshs v4,v12,v13
	// vpkshus v12,v11,v5
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// vsubshs v6,v6,v13
	// vpkshus v11,v8,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v8.s16)));
	// vpkshus v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// li r31,16
	r31.s64 = 16;
	// addi r7,r9,-16
	ctx.r7.s64 = ctx.r9.s64 + -16;
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// li r30,16
	r30.s64 = 16;
	// vpkshus v8,v6,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v6.s16)));
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// stvlx v12,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// stvrx v9,r5,r31
	ea = ctx.r5.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v9.u8[i]);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// vor v12,v8,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// stvlx v11,0,r7
	ea = ctx.r7.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v11.u8[15 - i]);
	// stvrx v11,r7,r30
	ea = ctx.r7.u32 + r30.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v11.u8[i]);
	// addi r3,r3,64
	ctx.r3.s64 = ctx.r3.s64 + 64;
	// stvlx v10,0,r28
	ea = r28.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v10.u8[15 - i]);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvrx v10,r27,r26
	ea = r27.u32 + r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v10.u8[i]);
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// stvlx v12,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v12.u8[15 - i]);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// stvrx v12,r6,r31
	ea = ctx.r6.u32 + r31.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v12.u8[i]);
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x8264218c
	if (!cr6.eq) goto loc_8264218C;
loc_8264234C:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x826423a0
	if (!cr6.gt) goto loc_826423A0;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_82642358:
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-128
	r11.s64 = r11.s64 + -128;
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82642378
	if (!cr6.gt) goto loc_82642378;
	// li r11,255
	r11.s64 = 255;
	// b 0x82642384
	goto loc_82642384;
loc_82642378:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82642384
	if (!cr6.lt) goto loc_82642384;
	// li r11,0
	r11.s64 = 0;
loc_82642384:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r11,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r11.u8);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82642358
	if (!cr6.eq) goto loc_82642358;
loc_826423A0:
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826423A4"))) PPC_WEAK_FUNC(sub_826423A4);
PPC_FUNC_IMPL(__imp__sub_826423A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826423A8"))) PPC_WEAK_FUNC(sub_826423A8);
PPC_FUNC_IMPL(__imp__sub_826423A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stb r7,-16(r1)
	PPC_STORE_U8(ctx.r1.u32 + -16, ctx.r7.u8);
	// srawi r10,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 4;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// clrlwi r8,r6,28
	ctx.r8.u64 = ctx.r6.u32 & 0xF;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltb v12,v0,0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_set1_epi8(char(0xF))));
	// ble cr6,0x826423f8
	if (!cr6.gt) goto loc_826423F8;
loc_826423CC:
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// vaddubs v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// vaddubs v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_adds_epu8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// stvx v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// bne cr6,0x826423cc
	if (!cr6.eq) goto loc_826423CC;
loc_826423F8:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// subf r6,r11,r4
	ctx.r6.s64 = ctx.r4.s64 - r11.s64;
	// subf r5,r11,r3
	ctx.r5.s64 = ctx.r3.s64 - r11.s64;
loc_82642408:
	// lbzx r10,r6,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82642424
	if (!cr6.gt) goto loc_82642424;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82642424:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stbx r10,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82642408
	if (!cr6.eq) goto loc_82642408;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8264243C"))) PPC_WEAK_FUNC(sub_8264243C);
PPC_FUNC_IMPL(__imp__sub_8264243C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82642440"))) PPC_WEAK_FUNC(sub_82642440);
PPC_FUNC_IMPL(__imp__sub_82642440) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// sth r7,-16(r1)
	PPC_STORE_U16(ctx.r1.u32 + -16, ctx.r7.u16);
	// vspltish v13,15
	// srawi r10,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 4;
	// vspltisb v0,0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_set1_epi8(char(0x0)));
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// clrlwi r9,r6,28
	ctx.r9.u64 = ctx.r6.u32 & 0xF;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// vslb v13,v13,v13
	ctx.v13.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
	ctx.v13.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
	ctx.v13.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
	ctx.v13.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
	ctx.v13.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
	ctx.v13.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
	ctx.v13.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
	ctx.v13.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
	ctx.v13.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
	ctx.v13.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
	ctx.v13.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
	ctx.v13.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
	ctx.v13.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
	ctx.v13.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
	ctx.v13.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
	ctx.v13.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v12,v12,0
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_set1_epi16(short(0xF0E))));
	// ble cr6,0x826424bc
	if (!cr6.gt) goto loc_826424BC;
	// vsubuhm v11,v12,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
loc_82642474:
	// lvx128 v13,r0,r4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglb v10,v0,v13
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrglb v9,v0,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_unpacklo_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// vmrghb v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// vmrghb v12,v0,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_unpackhi_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// vadduhm v10,v10,v9
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v12.u16)));
	// vadduhm v12,v10,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v13,v13,v11
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vpkshus v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvx v13,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// bne cr6,0x82642474
	if (!cr6.eq) goto loc_82642474;
loc_826424BC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// subf r6,r11,r4
	ctx.r6.s64 = ctx.r4.s64 - r11.s64;
	// subf r5,r11,r3
	ctx.r5.s64 = ctx.r3.s64 - r11.s64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_826424D0:
	// lbzx r10,r6,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r10,r10,-128
	ctx.r10.s64 = ctx.r10.s64 + -128;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x826424f4
	if (!cr6.lt) goto loc_826424F4;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82642500
	goto loc_82642500;
loc_826424F4:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82642500
	if (!cr6.gt) goto loc_82642500;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82642500:
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stbx r10,r5,r11
	PPC_STORE_U8(ctx.r5.u32 + r11.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x826424d0
	if (!cr6.eq) goto loc_826424D0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82642518"))) PPC_WEAK_FUNC(sub_82642518);
PPC_FUNC_IMPL(__imp__sub_82642518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264252c
	if (!cr6.eq) goto loc_8264252C;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_8264252C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bge cr6,0x82642538
	if (!cr6.lt) goto loc_82642538;
	// neg r4,r4
	ctx.r4.s64 = -ctx.r4.s64;
loc_82642538:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x82642544
	if (!cr6.lt) goto loc_82642544;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
loc_82642544:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bge cr6,0x82642550
	if (!cr6.lt) goto loc_82642550;
	// neg r6,r6
	ctx.r6.s64 = -ctx.r6.s64;
loc_82642550:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x8264255c
	if (!cr6.lt) goto loc_8264255C;
	// neg r7,r7
	ctx.r7.s64 = -ctx.r7.s64;
loc_8264255C:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r4,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r4.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r5,15324(r11)
	PPC_STORE_U32(r11.u32 + 15324, ctx.r5.u32);
	// stw r6,15328(r11)
	PPC_STORE_U32(r11.u32 + 15328, ctx.r6.u32);
	// stw r7,15332(r11)
	PPC_STORE_U32(r11.u32 + 15332, ctx.r7.u32);
	// stw r10,15336(r11)
	PPC_STORE_U32(r11.u32 + 15336, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8264257C"))) PPC_WEAK_FUNC(sub_8264257C);
PPC_FUNC_IMPL(__imp__sub_8264257C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82642580"))) PPC_WEAK_FUNC(sub_82642580);
PPC_FUNC_IMPL(__imp__sub_82642580) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82642598
	if (!cr6.eq) goto loc_82642598;
	// li r3,7
	ctx.r3.s64 = 7;
	// b 0x8239bd10
	return;
loc_82642598:
	// fmul f0,f2,f2
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f2.f64 * ctx.f2.f64;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fmul f13,f5,f5
	ctx.f13.f64 = ctx.f5.f64 * ctx.f5.f64;
	// lwz r17,15344(r3)
	r17.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15344);
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// lwz r16,15348(r3)
	r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15348);
	// lwz r27,15352(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15352);
	// lwz r25,15356(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15356);
	// addze r23,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r23.s64 = temp.s64;
	// lwz r24,15360(r3)
	r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15360);
	// fmadd f0,f1,f1,f0
	f0.f64 = ctx.f1.f64 * ctx.f1.f64 + f0.f64;
	// fmadd f13,f4,f4,f13
	ctx.f13.f64 = ctx.f4.f64 * ctx.f4.f64 + ctx.f13.f64;
	// fsqrt f0,f0
	f0.f64 = sqrt(f0.f64);
	// fsqrt f13,f13
	ctx.f13.f64 = sqrt(ctx.f13.f64);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x826425e0
	if (!cr6.lt) goto loc_826425E0;
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// b 0x826425e4
	goto loc_826425E4;
loc_826425E0:
	// fmr f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f13.f64;
loc_826425E4:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// lfd f10,-31360(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fcmpu cr6,f12,f10
	cr6.compare(ctx.f12.f64, ctx.f10.f64);
	// bge cr6,0x82642618
	if (!cr6.lt) goto loc_82642618;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82642604
	if (!cr6.lt) goto loc_82642604;
	// fmr f11,f0
	ctx.f11.f64 = f0.f64;
	// b 0x82642608
	goto loc_82642608;
loc_82642604:
	// fmr f11,f13
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = ctx.f13.f64;
loc_82642608:
	// lis r11,-32253
	r11.s64 = -2113732608;
	// lfd f12,19640(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 19640);
	// fcmpu cr6,f11,f12
	cr6.compare(ctx.f11.f64, ctx.f12.f64);
	// ble cr6,0x8264264c
	if (!cr6.gt) goto loc_8264264C;
loc_82642618:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82642628
	if (!cr6.lt) goto loc_82642628;
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// b 0x8264262c
	goto loc_8264262C;
loc_82642628:
	// fmr f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f13.f64;
loc_8264262C:
	// fcmpu cr6,f12,f10
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f12.f64, ctx.f10.f64);
	// bge cr6,0x82642644
	if (!cr6.lt) goto loc_82642644;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x82642650
	if (cr6.lt) goto loc_82642650;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// b 0x82642650
	goto loc_82642650;
loc_82642644:
	// fmr f0,f10
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f10.f64;
	// b 0x82642650
	goto loc_82642650;
loc_8264264C:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64;
loc_82642650:
	// lis r11,-32248
	r11.s64 = -2113404928;
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// lfd f13,-26736(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -26736);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f12,-28592(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// fmadd f13,f7,f13,f12
	ctx.f13.f64 = ctx.f7.f64 * ctx.f13.f64 + ctx.f12.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r29,-176(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x8264268c
	if (!cr6.gt) goto loc_8264268C;
	// cmpwi cr6,r29,256
	cr6.compare<int32_t>(r29.s32, 256, xer);
	// ble cr6,0x82642690
	if (!cr6.gt) goto loc_82642690;
	// li r29,256
	r29.s64 = 256;
	// b 0x82642690
	goto loc_82642690;
loc_8264268C:
	// li r29,0
	r29.s64 = 0;
loc_82642690:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f13,-18776(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -18776);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x826426b4
	if (!cr6.lt) goto loc_826426B4;
	// li r26,512
	r26.s64 = 512;
	// li r11,9
	r11.s64 = 9;
	// li r19,10
	r19.s64 = 10;
	// li r5,511
	ctx.r5.s64 = 511;
	// b 0x826426e0
	goto loc_826426E0;
loc_826426B4:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f12.f64);
	// bge cr6,0x826426d0
	if (!cr6.lt) goto loc_826426D0;
	// li r26,256
	r26.s64 = 256;
	// li r11,8
	r11.s64 = 8;
	// li r19,9
	r19.s64 = 9;
	// li r5,255
	ctx.r5.s64 = 255;
	// b 0x826426e0
	goto loc_826426E0;
loc_826426D0:
	// li r26,128
	r26.s64 = 128;
	// li r11,7
	r11.s64 = 7;
	// li r19,8
	r19.s64 = 8;
	// li r5,127
	ctx.r5.s64 = 127;
loc_826426E0:
	// extsw r9,r26
	ctx.r9.s64 = r26.s32;
	// lwz r8,15324(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15324);
	// mullw r10,r10,r26
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r26.s32);
	// std r9,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r9.u64);
	// lfd f0,-160(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// stw r10,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r10.u32);
	// addi r7,r1,-192
	ctx.r7.s64 = ctx.r1.s64 + -192;
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// addi r4,r1,-168
	ctx.r4.s64 = ctx.r1.s64 + -168;
	// mullw r10,r8,r26
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// fmul f13,f0,f1
	ctx.f13.f64 = f0.f64 * ctx.f1.f64;
	// fmul f12,f0,f2
	ctx.f12.f64 = f0.f64 * ctx.f2.f64;
	// fmul f11,f0,f3
	ctx.f11.f64 = f0.f64 * ctx.f3.f64;
	// stw r10,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r10.u32);
	// fmul f10,f0,f4
	ctx.f10.f64 = f0.f64 * ctx.f4.f64;
	// lwz r10,15332(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15332);
	// fmul f9,f0,f5
	ctx.f9.f64 = f0.f64 * ctx.f5.f64;
	// fmul f0,f0,f6
	f0.f64 = f0.f64 * ctx.f6.f64;
	// addi r9,r1,-188
	ctx.r9.s64 = ctx.r1.s64 + -188;
	// addi r31,r1,-176
	r31.s64 = ctx.r1.s64 + -176;
	// addi r30,r1,-172
	r30.s64 = ctx.r1.s64 + -172;
	// li r18,0
	r18.s64 = 0;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f13.u32);
	// fctiwz f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f13,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f13.u32);
	// fctiwz f13,f10
	ctx.f13.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// fctiwz f13,f9
	ctx.f13.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f9.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfiwx f0,0,r30
	PPC_STORE_U32(r30.u32, f0.u32);
	// ble cr6,0x826429f0
	if (!cr6.gt) goto loc_826429F0;
	// lwz r10,-172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// lwz r14,-188(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -188);
	// lwz r9,-192(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// subf r22,r14,r10
	r22.s64 = ctx.r10.s64 - r14.s64;
	// lwz r10,-168(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// lwz r15,-160(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// subf r21,r9,r10
	r21.s64 = ctx.r10.s64 - ctx.r9.s64;
loc_82642790:
	// lwz r10,15328(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15328);
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// li r28,1
	r28.s64 = 1;
	// li r20,0
	r20.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82642908
	if (!cr6.gt) goto loc_82642908;
loc_826427AC:
	// lwz r10,-192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// add r6,r6,r14
	ctx.r6.u64 = ctx.r6.u64 + r14.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt cr6,0x826428f8
	if (cr6.lt) goto loc_826428F8;
	// lwz r10,-184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -184);
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// bge cr6,0x826428f8
	if (!cr6.lt) goto loc_826428F8;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826428f8
	if (cr6.lt) goto loc_826428F8;
	// lwz r10,-180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -180);
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// bge cr6,0x826428f8
	if (!cr6.lt) goto loc_826428F8;
	// sraw r10,r6,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r10.s64 = ctx.r6.s32 >> temp.u32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r31,15340(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15340);
	// and r8,r6,r5
	ctx.r8.u64 = ctx.r6.u64 & ctx.r5.u64;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// and r4,r7,r5
	ctx.r4.u64 = ctx.r7.u64 & ctx.r5.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// sraw r30,r7,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	r30.s64 = ctx.r7.s32 >> temp.u32;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// subf r30,r8,r26
	r30.s64 = r26.s64 - ctx.r8.s64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// subf r31,r4,r30
	r31.s64 = r30.s64 - ctx.r4.s64;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r10,r31,r30
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r30.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r28,r4
	ctx.r9.s64 = int64_t(r28.s32) * int64_t(ctx.r4.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r10,r10,r29
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// sraw r10,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// stb r10,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r10.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// beq cr6,0x826428f4
	if (cr6.eq) goto loc_826428F4;
	// srawi r9,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 1;
	// srawi r10,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// and r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 & ctx.r5.u64;
	// and r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 & ctx.r5.u64;
	// li r28,0
	r28.s64 = 0;
	// sraw r8,r6,r19
	temp.u32 = r19.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r8.s64 = ctx.r6.s32 >> temp.u32;
	// mullw r8,r8,r23
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r23.s32);
	// sraw r4,r7,r19
	temp.u32 = r19.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r7.s32 < 0) & (((ctx.r7.s32 >> temp.u32) << temp.u32) != ctx.r7.s32);
	ctx.r4.s64 = ctx.r7.s32 >> temp.u32;
	// add r4,r8,r4
	ctx.r4.u64 = ctx.r8.u64 + ctx.r4.u64;
	// subf r8,r10,r26
	ctx.r8.s64 = r26.s64 - ctx.r10.s64;
	// subf r31,r9,r8
	r31.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r8,r4,r17
	ctx.r8.u64 = ctx.r4.u64 + r17.u64;
	// lbzx r30,r8,r23
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + r23.u32);
	// lbz r14,1(r8)
	r14.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// stw r8,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r8.u32);
	// add r8,r4,r16
	ctx.r8.u64 = ctx.r4.u64 + r16.u64;
	// mullw r4,r30,r10
	ctx.r4.s64 = int64_t(r30.s32) * int64_t(ctx.r10.s32);
	// mullw r30,r14,r9
	r30.s64 = int64_t(r14.s32) * int64_t(ctx.r9.s32);
	// lwz r14,-160(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// mullw r30,r14,r31
	r30.s64 = int64_t(r14.s32) * int64_t(r31.s32);
	// lwz r14,-188(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + -188);
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + r30.u64;
	// mullw r4,r4,r29
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(r29.s32);
	// srawi r4,r4,8
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xFF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 8;
	// sraw r4,r4,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r4.s32 < 0) & (((ctx.r4.s32 >> temp.u32) << temp.u32) != ctx.r4.s32);
	ctx.r4.s64 = ctx.r4.s32 >> temp.u32;
	// stb r4,0(r25)
	PPC_STORE_U8(r25.u32 + 0, ctx.r4.u8);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// lbzx r4,r8,r23
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + r23.u32);
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// mullw r10,r4,r10
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lbz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// mullw r9,r30,r9
	ctx.r9.s64 = int64_t(r30.s32) * int64_t(ctx.r9.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r8,r31
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(r31.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r10,r10,r29
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// sraw r10,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// stb r10,0(r24)
	PPC_STORE_U8(r24.u32 + 0, ctx.r10.u8);
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// b 0x826428f8
	goto loc_826428F8;
loc_826428F4:
	// li r28,1
	r28.s64 = 1;
loc_826428F8:
	// lwz r10,15328(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15328);
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// cmpw cr6,r20,r10
	cr6.compare<int32_t>(r20.s32, ctx.r10.s32, xer);
	// blt cr6,0x826427ac
	if (cr6.lt) goto loc_826427AC;
loc_82642908:
	// lwz r9,-176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// add r28,r21,r15
	r28.u64 = r21.u64 + r15.u64;
	// lwz r10,15328(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15328);
	// addi r20,r18,1
	r20.s64 = r18.s64 + 1;
	// add r22,r22,r9
	r22.u64 = r22.u64 + ctx.r9.u64;
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// ble cr6,0x826429d4
	if (!cr6.gt) goto loc_826429D4;
loc_82642930:
	// lwz r10,-192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	// add r4,r4,r14
	ctx.r4.u64 = ctx.r4.u64 + r14.u64;
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// blt cr6,0x826429c4
	if (cr6.lt) goto loc_826429C4;
	// lwz r10,-184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -184);
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// bge cr6,0x826429c4
	if (!cr6.lt) goto loc_826429C4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// blt cr6,0x826429c4
	if (cr6.lt) goto loc_826429C4;
	// lwz r10,-180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -180);
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// bge cr6,0x826429c4
	if (!cr6.lt) goto loc_826429C4;
	// sraw r10,r4,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r4.s32 < 0) & (((ctx.r4.s32 >> temp.u32) << temp.u32) != ctx.r4.s32);
	ctx.r10.s64 = ctx.r4.s32 >> temp.u32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r31,15340(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15340);
	// and r8,r4,r5
	ctx.r8.u64 = ctx.r4.u64 & ctx.r5.u64;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// and r7,r6,r5
	ctx.r7.u64 = ctx.r6.u64 & ctx.r5.u64;
	// sraw r30,r6,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	r30.s64 = ctx.r6.s32 >> temp.u32;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// subf r30,r8,r26
	r30.s64 = r26.s64 - ctx.r8.s64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// subf r31,r7,r30
	r31.s64 = r30.s64 - ctx.r7.s64;
	// lbzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r18,1(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r10,r31,r30
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r30.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r9,r18,r7
	ctx.r9.s64 = int64_t(r18.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mullw r10,r10,r29
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// sraw r10,r10,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// stb r10,0(r27)
	PPC_STORE_U8(r27.u32 + 0, ctx.r10.u8);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
loc_826429C4:
	// lwz r10,15328(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15328);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// cmpw cr6,r21,r10
	cr6.compare<int32_t>(r21.s32, ctx.r10.s32, xer);
	// blt cr6,0x82642930
	if (cr6.lt) goto loc_82642930;
loc_826429D4:
	// lwz r10,15332(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15332);
	// addi r18,r20,1
	r18.s64 = r20.s64 + 1;
	// lwz r9,-176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// add r21,r28,r15
	r21.u64 = r28.u64 + r15.u64;
	// cmpw cr6,r18,r10
	cr6.compare<int32_t>(r18.s32, ctx.r10.s32, xer);
	// add r22,r22,r9
	r22.u64 = r22.u64 + ctx.r9.u64;
	// blt cr6,0x82642790
	if (cr6.lt) goto loc_82642790;
loc_826429F0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826429F8"))) PPC_WEAK_FUNC(sub_826429F8);
PPC_FUNC_IMPL(__imp__sub_826429F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f0,f3
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f3.f64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// fmr f3,f5
	ctx.f3.f64 = ctx.f5.f64;
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82642b40
	if (cr6.eq) goto loc_82642B40;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// stw r7,15352(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15352, ctx.r7.u32);
	// stw r8,15356(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15356, ctx.r8.u32);
	// stw r9,15360(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15360, ctx.r9.u32);
	// stw r4,15340(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15340, ctx.r4.u32);
	// stw r5,15344(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15344, ctx.r5.u32);
	// lfd f13,-31368(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -31368);
	// stw r6,15348(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15348, ctx.r6.u32);
	// fcmpu cr6,f2,f13
	cr6.compare(ctx.f2.f64, ctx.f13.f64);
	// bne cr6,0x82642b14
	if (!cr6.eq) goto loc_82642B14;
	// fcmpu cr6,f4,f13
	cr6.compare(ctx.f4.f64, ctx.f13.f64);
	// bne cr6,0x82642b14
	if (!cr6.eq) goto loc_82642B14;
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fcmpu cr6,f1,f3
	cr6.compare(ctx.f1.f64, ctx.f3.f64);
	// lfd f13,-31360(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// bne cr6,0x82642ad0
	if (!cr6.eq) goto loc_82642AD0;
	// fcmpu cr6,f1,f13
	cr6.compare(ctx.f1.f64, ctx.f13.f64);
	// bne cr6,0x82642ad0
	if (!cr6.eq) goto loc_82642AD0;
	// fcmpu cr6,f7,f13
	cr6.compare(ctx.f7.f64, ctx.f13.f64);
	// fmr f2,f6
	ctx.f2.f64 = ctx.f6.f64;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
	// bne cr6,0x82642ab4
	if (!cr6.eq) goto loc_82642AB4;
	// bl 0x8263c9b0
	sub_8263C9B0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_82642AB4:
	// fmr f3,f7
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f7.f64;
	// bl 0x8263d740
	sub_8263D740(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_82642AD0:
	// fcmpu cr6,f7,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f13.f64);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fmr f2,f0
	ctx.f2.f64 = f0.f64;
	// bne cr6,0x82642af8
	if (!cr6.eq) goto loc_82642AF8;
	// bl 0x8263f2e0
	sub_8263F2E0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_82642AF8:
	// fmr f5,f7
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f7.f64;
	// bl 0x82640790
	sub_82640790(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_82642B14:
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmr f5,f3
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f3.f64;
	// fmr f3,f0
	ctx.f3.f64 = f0.f64;
	// lfd f13,-31360(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -31360);
	// fcmpu cr6,f7,f13
	cr6.compare(ctx.f7.f64, ctx.f13.f64);
	// bl 0x82642580
	sub_82642580(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_82642B40:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82642B54"))) PPC_WEAK_FUNC(sub_82642B54);
PPC_FUNC_IMPL(__imp__sub_82642B54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82642B58"))) PPC_WEAK_FUNC(sub_82642B58);
PPC_FUNC_IMPL(__imp__sub_82642B58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcdc
	// stfd f30,-112(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -112, f30.u64);
	// stfd f31,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r23,r8
	r23.u64 = ctx.r8.u64;
	// mr r21,r9
	r21.u64 = ctx.r9.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x82642ba4
	if (!cr6.eq) goto loc_82642BA4;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f30,-112(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
loc_82642BA4:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,32
	r30.s64 = 32;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// bge cr6,0x82642c18
	if (!cr6.lt) goto loc_82642C18;
loc_82642BC0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82642c18
	if (cr6.eq) goto loc_82642C18;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82642c08
	if (!cr0.lt) goto loc_82642C08;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642C08:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82642bc0
	if (cr6.gt) goto loc_82642BC0;
loc_82642C18:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82642c54
	if (!cr0.lt) goto loc_82642C54;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642C54:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// beq cr6,0x8264322c
	if (cr6.eq) goto loc_8264322C;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x82642cd4
	if (!cr6.lt) goto loc_82642CD4;
loc_82642C7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82642cd4
	if (cr6.eq) goto loc_82642CD4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82642cc4
	if (!cr0.lt) goto loc_82642CC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642CC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82642c7c
	if (cr6.gt) goto loc_82642C7C;
loc_82642CD4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82642d10
	if (!cr0.lt) goto loc_82642D10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642D10:
	// cmplwi cr6,r30,14
	cr6.compare<uint32_t>(r30.u32, 14, xer);
	// stw r30,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r30.u32);
	// ble cr6,0x82642d30
	if (!cr6.gt) goto loc_82642D30;
loc_82642D1C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f30,-112(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
loc_82642D30:
	// lis r10,-32245
	ctx.r10.s64 = -2113208320;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// cmplwi cr6,r30,7
	cr6.compare<uint32_t>(r30.u32, 7, xer);
	// lfs f30,-2072(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -2072);
	f30.f64 = double(temp.f32);
	// lfs f31,27480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27480);
	f31.f64 = double(temp.f32);
	// bne cr6,0x82642d80
	if (!cr6.eq) goto loc_82642D80;
	// addi r10,r28,24
	ctx.r10.s64 = r28.s64 + 24;
	// addi r9,r28,20
	ctx.r9.s64 = r28.s64 + 20;
	// addi r8,r28,16
	ctx.r8.s64 = r28.s64 + 16;
	// addi r7,r28,12
	ctx.r7.s64 = r28.s64 + 12;
	// addi r6,r28,8
	ctx.r6.s64 = r28.s64 + 8;
	// addi r5,r28,4
	ctx.r5.s64 = r28.s64 + 4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826432f0
	if (!cr6.eq) goto loc_826432F0;
	// lwz r11,15364(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15364);
	// stw r11,15400(r24)
	PPC_STORE_U32(r24.u32 + 15400, r11.u32);
	// b 0x82642fb0
	goto loc_82642FB0;
loc_82642D80:
	// cmplwi cr6,r30,14
	cr6.compare<uint32_t>(r30.u32, 14, xer);
	// bne cr6,0x82642df4
	if (!cr6.eq) goto loc_82642DF4;
	// addi r10,r28,24
	ctx.r10.s64 = r28.s64 + 24;
	// addi r9,r28,20
	ctx.r9.s64 = r28.s64 + 20;
	// addi r8,r28,16
	ctx.r8.s64 = r28.s64 + 16;
	// addi r7,r28,12
	ctx.r7.s64 = r28.s64 + 12;
	// addi r6,r28,8
	ctx.r6.s64 = r28.s64 + 8;
	// addi r5,r28,4
	ctx.r5.s64 = r28.s64 + 4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826432f0
	if (!cr6.eq) goto loc_826432F0;
	// lwz r11,15364(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15364);
	// addi r10,r28,52
	ctx.r10.s64 = r28.s64 + 52;
	// addi r9,r28,48
	ctx.r9.s64 = r28.s64 + 48;
	// addi r8,r28,44
	ctx.r8.s64 = r28.s64 + 44;
	// addi r7,r28,40
	ctx.r7.s64 = r28.s64 + 40;
	// addi r6,r28,36
	ctx.r6.s64 = r28.s64 + 36;
	// addi r5,r28,32
	ctx.r5.s64 = r28.s64 + 32;
	// stw r11,15400(r24)
	PPC_STORE_U32(r24.u32 + 15400, r11.u32);
	// addi r4,r28,28
	ctx.r4.s64 = r28.s64 + 28;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826432f0
	if (!cr6.eq) goto loc_826432F0;
	// lwz r11,15364(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15364);
	// stw r11,15404(r24)
	PPC_STORE_U32(r24.u32 + 15404, r11.u32);
	// b 0x82642fb0
	goto loc_82642FB0;
loc_82642DF4:
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82642fb0
	if (cr6.eq) goto loc_82642FB0;
loc_82642E00:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x82642e78
	if (!cr6.lt) goto loc_82642E78;
loc_82642E1C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82642e78
	if (cr6.eq) goto loc_82642E78;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82642e68
	if (!cr0.lt) goto loc_82642E68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642E68:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82642e1c
	if (cr6.gt) goto loc_82642E1C;
loc_82642E78:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r29
	r27.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82642eb4
	if (!cr0.lt) goto loc_82642EB4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642EB4:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x82642f2c
	if (!cr6.lt) goto loc_82642F2C;
loc_82642ED0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82642f2c
	if (cr6.eq) goto loc_82642F2C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82642f1c
	if (!cr0.lt) goto loc_82642F1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642F1C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82642ed0
	if (cr6.gt) goto loc_82642ED0;
loc_82642F2C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82642f68
	if (!cr0.lt) goto loc_82642F68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82642F68:
	// lwz r11,84(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82642d1c
	if (!cr6.eq) goto loc_82642D1C;
	// rlwinm r11,r27,15,0,16
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 15) & 0xFFFF8000;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r28)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x82642e00
	if (cr6.lt) goto loc_82642E00;
loc_82642FB0:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,16
	r30.s64 = 16;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x82643024
	if (!cr6.lt) goto loc_82643024;
loc_82642FCC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82643024
	if (cr6.eq) goto loc_82643024;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82643014
	if (!cr0.lt) goto loc_82643014;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82643014:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82642fcc
	if (cr6.gt) goto loc_82642FCC;
loc_82643024:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82643060
	if (!cr0.lt) goto loc_82643060;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82643060:
	// cmplwi cr6,r30,100
	cr6.compare<uint32_t>(r30.u32, 100, xer);
	// stw r30,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r30.u32);
	// bgt cr6,0x82642d1c
	if (cr6.gt) goto loc_82642D1C;
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264322c
	if (cr6.eq) goto loc_8264322C;
	// mr r27,r23
	r27.u64 = r23.u64;
loc_8264307C:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x826430f4
	if (!cr6.lt) goto loc_826430F4;
loc_82643098:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826430f4
	if (cr6.eq) goto loc_826430F4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826430e4
	if (!cr0.lt) goto loc_826430E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826430E4:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82643098
	if (cr6.gt) goto loc_82643098;
loc_826430F4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82643130
	if (!cr0.lt) goto loc_82643130;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82643130:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,15
	r30.s64 = 15;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplwi cr6,r11,15
	cr6.compare<uint32_t>(r11.u32, 15, xer);
	// bge cr6,0x826431a8
	if (!cr6.lt) goto loc_826431A8;
loc_8264314C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826431a8
	if (cr6.eq) goto loc_826431A8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82643198
	if (!cr0.lt) goto loc_82643198;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82643198:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264314c
	if (cr6.gt) goto loc_8264314C;
loc_826431A8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826431e4
	if (!cr0.lt) goto loc_826431E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826431E4:
	// lwz r11,84(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82642d1c
	if (!cr6.eq) goto loc_82642D1C;
	// rlwinm r11,r28,15,0,16
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 15) & 0xFFFF8000;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmsubs f0,f0,f31,f30
	f0.f64 = double(float(f0.f64 * f31.f64 - f30.f64));
	// stfs f0,0(r27)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r27.u32 + 0, temp.u32);
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x8264307c
	if (cr6.lt) goto loc_8264307C;
loc_8264322C:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// li r30,1
	r30.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x826432a0
	if (!cr6.lt) goto loc_826432A0;
loc_82643248:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826432a0
	if (cr6.eq) goto loc_826432A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82643290
	if (!cr0.lt) goto loc_82643290;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82643290:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82643248
	if (cr6.gt) goto loc_82643248;
loc_826432A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826432dc
	if (!cr0.lt) goto loc_826432DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826432DC:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
loc_826432F0:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f30,-112(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8239bd2c
	return;
}

__attribute__((alias("__imp__sub_82643300"))) PPC_WEAK_FUNC(sub_82643300);
PPC_FUNC_IMPL(__imp__sub_82643300) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x826433b8
	if (cr6.eq) goto loc_826433B8;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643338
	if (cr6.eq) goto loc_82643338;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
loc_82643338:
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8264334c
	if (cr6.eq) goto loc_8264334C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
loc_8264334C:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643360
	if (cr6.eq) goto loc_82643360;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r30.u32);
loc_82643360:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643374
	if (cr6.eq) goto loc_82643374;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
loc_82643374:
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643388
	if (cr6.eq) goto loc_82643388;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r30.u32);
loc_82643388:
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8264339c
	if (cr6.eq) goto loc_8264339C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r30.u32);
loc_8264339C:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826433b0
	if (cr6.eq) goto loc_826433B0;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r30.u32);
loc_826433B0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_826433B8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826433D0"))) PPC_WEAK_FUNC(sub_826433D0);
PPC_FUNC_IMPL(__imp__sub_826433D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r29,0
	r29.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r27,r29
	r27.u64 = r29.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x8264365c
	if (!cr6.gt) goto loc_8264365C;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8264365c
	if (!cr6.gt) goto loc_8264365C;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643428
	if (cr6.eq) goto loc_82643428;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x82643440
	if (!cr6.lt) goto loc_82643440;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643428
	if (cr6.eq) goto loc_82643428;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r29.u32);
loc_82643428:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_82643440:
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643468
	if (cr6.eq) goto loc_82643468;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x82643480
	if (!cr6.lt) goto loc_82643480;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643468
	if (cr6.eq) goto loc_82643468;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r29.u32);
loc_82643468:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_82643480:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826434a8
	if (cr6.eq) goto loc_826434A8;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x826434c0
	if (!cr6.lt) goto loc_826434C0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826434a8
	if (cr6.eq) goto loc_826434A8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r29.u32);
loc_826434A8:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_826434C0:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826434e8
	if (cr6.eq) goto loc_826434E8;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x82643500
	if (!cr6.lt) goto loc_82643500;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826434e8
	if (cr6.eq) goto loc_826434E8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r29.u32);
loc_826434E8:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r30,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_82643500:
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643528
	if (cr6.eq) goto loc_82643528;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bge cr6,0x82643540
	if (!cr6.lt) goto loc_82643540;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643528
	if (cr6.eq) goto loc_82643528;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r29.u32);
loc_82643528:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,60(r31)
	PPC_STORE_U32(r31.u32 + 60, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_82643540:
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643568
	if (cr6.eq) goto loc_82643568;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bge cr6,0x82643580
	if (!cr6.lt) goto loc_82643580;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643568
	if (cr6.eq) goto loc_82643568;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r29.u32);
loc_82643568:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r28,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,64(r31)
	PPC_STORE_U32(r31.u32 + 64, ctx.r3.u32);
	// beq cr6,0x826435c0
	if (cr6.eq) goto loc_826435C0;
loc_82643580:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826435a8
	if (cr6.eq) goto loc_826435A8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bge cr6,0x82643650
	if (!cr6.lt) goto loc_82643650;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826435a8
	if (cr6.eq) goto loc_826435A8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r29.u32);
loc_826435A8:
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r28,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,68(r31)
	PPC_STORE_U32(r31.u32 + 68, ctx.r3.u32);
	// bne cr6,0x82643650
	if (!cr6.eq) goto loc_82643650;
loc_826435C0:
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r27,2
	r27.s64 = 2;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826435d8
	if (cr6.eq) goto loc_826435D8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r29.u32);
loc_826435D8:
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826435ec
	if (cr6.eq) goto loc_826435EC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r29.u32);
loc_826435EC:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643600
	if (cr6.eq) goto loc_82643600;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r29.u32);
loc_82643600:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643614
	if (cr6.eq) goto loc_82643614;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r29.u32);
loc_82643614:
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643628
	if (cr6.eq) goto loc_82643628;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r29.u32);
loc_82643628:
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8264363c
	if (cr6.eq) goto loc_8264363C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r29.u32);
loc_8264363C:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82643650
	if (cr6.eq) goto loc_82643650;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r29,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r29.u32);
loc_82643650:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_8264365C:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82643668"))) PPC_WEAK_FUNC(sub_82643668);
PPC_FUNC_IMPL(__imp__sub_82643668) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc4
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r18,r5
	r18.u64 = ctx.r5.u64;
	// mr r17,r6
	r17.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r15,0
	cr6.compare<uint32_t>(r15.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// lwz r19,308(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x82643ad4
	if (cr6.eq) goto loc_82643AD4;
	// lwz r11,4(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// li r31,0
	r31.s64 = 0;
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82643864
	if (!cr6.gt) goto loc_82643864;
	// li r26,0
	r26.s64 = 0;
loc_826436FC:
	// lwz r10,20(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// lwz r9,24(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// lwz r8,28(r23)
	ctx.r8.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// lwz r7,32(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + 32);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwzx r30,r26,r10
	r30.u64 = PPC_LOAD_U32(r26.u32 + ctx.r10.u32);
	// lwzx r29,r26,r9
	r29.u64 = PPC_LOAD_U32(r26.u32 + ctx.r9.u32);
	// lwzx r27,r26,r8
	r27.u64 = PPC_LOAD_U32(r26.u32 + ctx.r8.u32);
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// lwzx r25,r26,r7
	r25.u64 = PPC_LOAD_U32(r26.u32 + ctx.r7.u32);
	// bge cr6,0x8264372c
	if (!cr6.lt) goto loc_8264372C;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_8264372C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82643744
	if (!cr6.gt) goto loc_82643744;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// add r4,r31,r24
	ctx.r4.u64 = r31.u64 + r24.u64;
	// add r3,r31,r28
	ctx.r3.u64 = r31.u64 + r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643744:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// bgt cr6,0x82643754
	if (cr6.gt) goto loc_82643754;
	// li r11,0
	r11.s64 = 0;
loc_82643754:
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// addi r5,r10,-1
	ctx.r5.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82643780
	if (!cr6.gt) goto loc_82643780;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r10,r11,r24
	ctx.r10.u64 = r11.u64 + r24.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// addi r3,r11,1
	ctx.r3.s64 = r11.s64 + 1;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643780:
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// addi r10,r29,1
	ctx.r10.s64 = r29.s64 + 1;
	// bgt cr6,0x82643790
	if (cr6.gt) goto loc_82643790;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82643790:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// bge cr6,0x826437a0
	if (!cr6.lt) goto loc_826437A0;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_826437A0:
	// subf r5,r10,r11
	ctx.r5.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x826437bc
	if (!cr6.gt) goto loc_826437BC;
	// add r11,r10,r31
	r11.u64 = ctx.r10.u64 + r31.u64;
	// add r4,r11,r24
	ctx.r4.u64 = r11.u64 + r24.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826437BC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// bgt cr6,0x826437cc
	if (cr6.gt) goto loc_826437CC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_826437CC:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// bge cr6,0x826437e0
	if (!cr6.lt) goto loc_826437E0;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x826437e4
	goto loc_826437E4;
loc_826437E0:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_826437E4:
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82643804
	if (!cr6.gt) goto loc_82643804;
	// add r11,r10,r31
	r11.u64 = ctx.r10.u64 + r31.u64;
	// add r4,r11,r22
	ctx.r4.u64 = r11.u64 + r22.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643804:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x82643818
	if (cr6.lt) goto loc_82643818;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
loc_82643818:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// mr r11,r25
	r11.u64 = r25.u64;
	// bgt cr6,0x82643828
	if (cr6.gt) goto loc_82643828;
	// li r11,0
	r11.s64 = 0;
loc_82643828:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82643848
	if (!cr6.gt) goto loc_82643848;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r4,r11,r22
	ctx.r4.u64 = r11.u64 + r22.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643848:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// lwz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// cmpw cr6,r21,r10
	cr6.compare<int32_t>(r21.s32, ctx.r10.s32, xer);
	// blt cr6,0x826436fc
	if (cr6.lt) goto loc_826436FC;
loc_82643864:
	// lwz r11,4(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// li r29,0
	r29.s64 = 0;
	// li r22,0
	r22.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82643ac8
	if (!cr6.gt) goto loc_82643AC8;
	// li r25,0
	r25.s64 = 0;
loc_8264387C:
	// lwz r10,20(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 20);
	// lwz r9,24(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + 24);
	// lwz r8,28(r23)
	ctx.r8.u64 = PPC_LOAD_U32(r23.u32 + 28);
	// lwz r7,32(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + 32);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// lwzx r28,r25,r10
	r28.u64 = PPC_LOAD_U32(r25.u32 + ctx.r10.u32);
	// lwzx r27,r25,r9
	r27.u64 = PPC_LOAD_U32(r25.u32 + ctx.r9.u32);
	// lwzx r26,r25,r8
	r26.u64 = PPC_LOAD_U32(r25.u32 + ctx.r8.u32);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// lwzx r24,r25,r7
	r24.u64 = PPC_LOAD_U32(r25.u32 + ctx.r7.u32);
	// bge cr6,0x826438ac
	if (!cr6.lt) goto loc_826438AC;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_826438AC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// srawi r31,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r31.s64 = r11.s32 >> 1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x826438dc
	if (!cr6.gt) goto loc_826438DC;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// add r4,r29,r18
	ctx.r4.u64 = r29.u64 + r18.u64;
	// add r3,r29,r19
	ctx.r3.u64 = r29.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// add r4,r29,r17
	ctx.r4.u64 = r29.u64 + r17.u64;
	// add r3,r29,r20
	ctx.r3.u64 = r29.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826438DC:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826438f4
	if (cr6.eq) goto loc_826438F4;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x826438f8
	goto loc_826438F8;
loc_826438F4:
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
loc_826438F8:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// bgt cr6,0x82643908
	if (cr6.gt) goto loc_82643908;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82643908:
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r30,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r30.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x8264394c
	if (!cr6.gt) goto loc_8264394C;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// add r4,r31,r18
	ctx.r4.u64 = r31.u64 + r18.u64;
	// add r3,r31,r19
	ctx.r3.u64 = r31.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r17
	ctx.r4.u64 = r31.u64 + r17.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_8264394C:
	// cmpwi cr6,r27,-1
	cr6.compare<int32_t>(r27.s32, -1, xer);
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// bgt cr6,0x8264395c
	if (cr6.gt) goto loc_8264395C;
	// li r11,0
	r11.s64 = 0;
loc_8264395C:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8264396c
	if (cr6.eq) goto loc_8264396C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8264396C:
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpw cr6,r24,r10
	cr6.compare<int32_t>(r24.s32, ctx.r10.s32, xer);
	// bge cr6,0x8264397c
	if (!cr6.lt) goto loc_8264397C;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_8264397C:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// srawi r30,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r30.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826439b8
	if (!cr6.gt) goto loc_826439B8;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// add r4,r31,r18
	ctx.r4.u64 = r31.u64 + r18.u64;
	// add r3,r31,r19
	ctx.r3.u64 = r31.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r17
	ctx.r4.u64 = r31.u64 + r17.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_826439B8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// bgt cr6,0x826439c8
	if (cr6.gt) goto loc_826439C8;
	// li r11,0
	r11.s64 = 0;
loc_826439C8:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826439d8
	if (cr6.eq) goto loc_826439D8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826439D8:
	// lwz r10,0(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpw cr6,r27,r10
	cr6.compare<int32_t>(r27.s32, ctx.r10.s32, xer);
	// bge cr6,0x826439ec
	if (!cr6.lt) goto loc_826439EC;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// b 0x826439f0
	goto loc_826439F0;
loc_826439EC:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
loc_826439F0:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// srawi r30,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r30.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82643a2c
	if (!cr6.gt) goto loc_82643A2C;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r19
	ctx.r3.u64 = r31.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643A2C:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// bge cr6,0x82643a40
	if (!cr6.lt) goto loc_82643A40;
	// mr r11,r26
	r11.u64 = r26.u64;
	// b 0x82643a44
	goto loc_82643A44;
loc_82643A40:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_82643A44:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82643a54
	if (cr6.eq) goto loc_82643A54;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_82643A54:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// bgt cr6,0x82643a64
	if (cr6.gt) goto loc_82643A64;
	// li r10,0
	ctx.r10.s64 = 0;
loc_82643A64:
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// srawi r30,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r30.s64 = ctx.r10.s32 >> 1;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82643aa8
	if (!cr6.gt) goto loc_82643AA8;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// add r4,r31,r16
	ctx.r4.u64 = r31.u64 + r16.u64;
	// add r3,r31,r19
	ctx.r3.u64 = r31.u64 + r19.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r4,r31,r15
	ctx.r4.u64 = r31.u64 + r15.u64;
	// add r3,r31,r20
	ctx.r3.u64 = r31.u64 + r20.u64;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
loc_82643AA8:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// addi r22,r22,2
	r22.s64 = r22.s64 + 2;
	// lwz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + 4);
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// cmpw cr6,r22,r10
	cr6.compare<int32_t>(r22.s32, ctx.r10.s32, xer);
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// blt cr6,0x8264387c
	if (cr6.lt) goto loc_8264387C;
loc_82643AC8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd14
	return;
loc_82643AD4:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd14
	return;
}

__attribute__((alias("__imp__sub_82643AE0"))) PPC_WEAK_FUNC(sub_82643AE0);
PPC_FUNC_IMPL(__imp__sub_82643AE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// li r31,0
	r31.s64 = 0;
	// li r29,1
	r29.s64 = 1;
	// rlwinm r28,r10,0,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// lwz r10,12(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// mr r11,r31
	r11.u64 = r31.u64;
	// subf r30,r10,r6
	r30.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82643fd8
	if (!cr6.gt) goto loc_82643FD8;
loc_82643B0C:
	// lwz r10,60(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 60);
	// srawi r9,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r9.s64 = r11.s32 >> 3;
	// lbzx r6,r9,r10
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x82643b34
	if (!cr6.eq) goto loc_82643B34;
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// stwx r31,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r31.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// b 0x82643fd0
	goto loc_82643FD0;
loc_82643B34:
	// cmplwi cr6,r6,255
	cr6.compare<uint32_t>(ctx.r6.u32, 255, xer);
	// bne cr6,0x82643e04
	if (!cr6.eq) goto loc_82643E04;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643b8c
	if (cr6.lt) goto loc_82643B8C;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643b8c
	if (!cr6.lt) goto loc_82643B8C;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643b90
	goto loc_82643B90;
loc_82643B8C:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643B90:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643be4
	if (cr6.lt) goto loc_82643BE4;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643be4
	if (!cr6.lt) goto loc_82643BE4;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643be8
	goto loc_82643BE8;
loc_82643BE4:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643BE8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643c3c
	if (cr6.lt) goto loc_82643C3C;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643c3c
	if (!cr6.lt) goto loc_82643C3C;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643c40
	goto loc_82643C40;
loc_82643C3C:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643C40:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643c94
	if (cr6.lt) goto loc_82643C94;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643c94
	if (!cr6.lt) goto loc_82643C94;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643c98
	goto loc_82643C98;
loc_82643C94:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643C98:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643cec
	if (cr6.lt) goto loc_82643CEC;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643cec
	if (!cr6.lt) goto loc_82643CEC;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643cf0
	goto loc_82643CF0;
loc_82643CEC:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643CF0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643d44
	if (cr6.lt) goto loc_82643D44;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643d44
	if (!cr6.lt) goto loc_82643D44;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643d48
	goto loc_82643D48;
loc_82643D44:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643D48:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643d9c
	if (cr6.lt) goto loc_82643D9C;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643d9c
	if (!cr6.lt) goto loc_82643D9C;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643da0
	goto loc_82643DA0;
loc_82643D9C:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643DA0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643df8
	if (cr6.lt) goto loc_82643DF8;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643df8
	if (!cr6.lt) goto loc_82643DF8;
	// lwz r7,64(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r7,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// stbx r10,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// b 0x82643fd0
	goto loc_82643FD0;
loc_82643DF8:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// b 0x82643fd0
	goto loc_82643FD0;
loc_82643E04:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r7,2
	ctx.r7.s64 = 2;
loc_82643E0C:
	// addi r9,r7,-2
	ctx.r9.s64 = ctx.r7.s64 + -2;
	// slw r9,r29,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r9.u8 & 0x3F));
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82643e6c
	if (cr6.eq) goto loc_82643E6C;
	// lwz r9,68(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r9,r9,r30
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r30.s32);
	// srawi r9,r9,20
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 20;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x82643e6c
	if (cr6.lt) goto loc_82643E6C;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643e6c
	if (!cr6.lt) goto loc_82643E6C;
	// lwz r27,64(r5)
	r27.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwzx r9,r27,r10
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + ctx.r10.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r9,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r9.u8);
	// b 0x82643e70
	goto loc_82643E70;
loc_82643E6C:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82643E70:
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r7,-1
	r11.s64 = ctx.r7.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// slw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// and r11,r11,r6
	r11.u64 = r11.u64 & ctx.r6.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82643edc
	if (cr6.eq) goto loc_82643EDC;
	// lwz r11,68(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// srawi r11,r11,20
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFFFFF) != 0);
	r11.s64 = r11.s32 >> 20;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x82643edc
	if (cr6.lt) goto loc_82643EDC;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643edc
	if (!cr6.lt) goto loc_82643EDC;
	// lwz r27,64(r5)
	r27.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// lwzx r11,r27,r10
	r11.u64 = PPC_LOAD_U32(r27.u32 + ctx.r10.u32);
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// addi r11,r10,4
	r11.s64 = ctx.r10.s64 + 4;
	// lbzx r10,r8,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r4.u32);
	// stbx r10,r9,r3
	PPC_STORE_U8(ctx.r9.u32 + ctx.r3.u32, ctx.r10.u8);
	// b 0x82643ee4
	goto loc_82643EE4;
loc_82643EDC:
	// addi r11,r10,4
	r11.s64 = ctx.r10.s64 + 4;
	// stbx r31,r9,r3
	PPC_STORE_U8(ctx.r9.u32 + ctx.r3.u32, r31.u8);
loc_82643EE4:
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// slw r9,r29,r7
	ctx.r9.u64 = ctx.r7.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r7.u8 & 0x3F));
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82643f44
	if (cr6.eq) goto loc_82643F44;
	// lwz r9,68(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// mullw r9,r9,r30
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r30.s32);
	// srawi r9,r9,20
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 20;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x82643f44
	if (cr6.lt) goto loc_82643F44;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643f44
	if (!cr6.lt) goto loc_82643F44;
	// lwz r27,64(r5)
	r27.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r9
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwzx r9,r27,r11
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r9,r10,r3
	PPC_STORE_U8(ctx.r10.u32 + ctx.r3.u32, ctx.r9.u8);
	// b 0x82643f48
	goto loc_82643F48;
loc_82643F44:
	// stbx r31,r10,r3
	PPC_STORE_U8(ctx.r10.u32 + ctx.r3.u32, r31.u8);
loc_82643F48:
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// slw r10,r29,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r10.u8 & 0x3F));
	// and r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 & ctx.r6.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82643fb4
	if (cr6.eq) goto loc_82643FB4;
	// lwz r10,68(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// mullw r10,r10,r30
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r30.s32);
	// srawi r10,r10,20
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 20;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82643fb4
	if (cr6.lt) goto loc_82643FB4;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82643fb4
	if (!cr6.lt) goto loc_82643FB4;
	// lwz r27,64(r5)
	r27.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lwzx r10,r27,r11
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lbzx r11,r8,r4
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r4.u32);
	// stbx r11,r9,r3
	PPC_STORE_U8(ctx.r9.u32 + ctx.r3.u32, r11.u8);
	// b 0x82643fbc
	goto loc_82643FBC;
loc_82643FB4:
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stbx r31,r9,r3
	PPC_STORE_U8(ctx.r9.u32 + ctx.r3.u32, r31.u8);
loc_82643FBC:
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r11,r9,1
	r11.s64 = ctx.r9.s64 + 1;
	// addi r9,r7,-2
	ctx.r9.s64 = ctx.r7.s64 + -2;
	// cmpwi cr6,r9,8
	cr6.compare<int32_t>(ctx.r9.s32, 8, xer);
	// blt cr6,0x82643e0c
	if (cr6.lt) goto loc_82643E0C;
loc_82643FD0:
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// blt cr6,0x82643b0c
	if (cr6.lt) goto loc_82643B0C;
loc_82643FD8:
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmpw cr6,r7,r28
	cr6.compare<int32_t>(ctx.r7.s32, r28.s32, xer);
	// beq cr6,0x82644074
	if (cr6.eq) goto loc_82644074;
	// lwz r10,60(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 60);
	// srawi r9,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r9.s64 = r11.s32 >> 3;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// bge cr6,0x82644074
	if (!cr6.lt) goto loc_82644074;
	// clrlwi r6,r10,24
	ctx.r6.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82644000:
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// slw r9,r29,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r9.u8 & 0x3F));
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8264405c
	if (cr6.eq) goto loc_8264405C;
	// lwz r9,68(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r9,r9,r30
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r30.s32);
	// srawi r9,r9,20
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 20;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x8264405c
	if (cr6.lt) goto loc_8264405C;
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// bge cr6,0x8264405c
	if (!cr6.lt) goto loc_8264405C;
	// lwz r28,64(r5)
	r28.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// mullw r8,r7,r9
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// lwzx r9,r28,r10
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + ctx.r10.u32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r9,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r9.u8);
	// b 0x82644060
	goto loc_82644060;
loc_8264405C:
	// stbx r31,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r31.u8);
loc_82644060:
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x82644000
	if (cr6.lt) goto loc_82644000;
loc_82644074:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82644080"))) PPC_WEAK_FUNC(sub_82644080);
PPC_FUNC_IMPL(__imp__sub_82644080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcec
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// li r28,128
	r28.s64 = 128;
	// srawi r27,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r27.s64 = ctx.r10.s32 >> 1;
	// lwz r10,12(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// rlwinm r25,r27,0,0,29
	r25.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r26,r10,r8
	r26.s64 = ctx.r8.s64 - ctx.r10.s64;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82644494
	if (!cr6.gt) goto loc_82644494;
	// lis r9,-32640
	ctx.r9.s64 = -2139095040;
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r29,r9,32896
	r29.u64 = ctx.r9.u64 | 32896;
loc_826440B8:
	// lwz r9,60(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
	// srawi r8,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r8.s64 = r11.s32 >> 2;
	// lbzx r30,r8,r9
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r9.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826440e0
	if (!cr6.eq) goto loc_826440E0;
	// stwx r29,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r29.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// stwx r29,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, r29.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// b 0x8264448c
	goto loc_8264448C;
loc_826440E0:
	// andi. r9,r30,51
	ctx.r9.u64 = r30.u64 & 51;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8264429c
	if (!cr6.eq) goto loc_8264429C;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// lwzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x8264414c
	if (cr6.lt) goto loc_8264414C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8264414c
	if (cr6.lt) goto loc_8264414C;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x8264414c
	if (!cr6.lt) goto loc_8264414C;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644154
	goto loc_82644154;
loc_8264414C:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644154:
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// lwzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x826441bc
	if (cr6.lt) goto loc_826441BC;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x826441bc
	if (cr6.lt) goto loc_826441BC;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x826441bc
	if (!cr6.lt) goto loc_826441BC;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x826441c4
	goto loc_826441C4;
loc_826441BC:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_826441C4:
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// lwzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x8264422c
	if (cr6.lt) goto loc_8264422C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8264422c
	if (cr6.lt) goto loc_8264422C;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x8264422c
	if (!cr6.lt) goto loc_8264422C;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644234
	goto loc_82644234;
loc_8264422C:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644234:
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r10,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32);
	// lwzx r9,r10,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x8264447c
	if (cr6.lt) goto loc_8264447C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8264447c
	if (cr6.lt) goto loc_8264447C;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x8264447c
	if (!cr6.lt) goto loc_8264447C;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644484
	goto loc_82644484;
loc_8264429C:
	// clrlwi r9,r30,31
	ctx.r9.u64 = r30.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82644308
	if (cr6.eq) goto loc_82644308;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x82644308
	if (cr6.lt) goto loc_82644308;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x82644308
	if (cr6.lt) goto loc_82644308;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x82644308
	if (!cr6.lt) goto loc_82644308;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644310
	goto loc_82644310;
loc_82644308:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644310:
	// rlwinm r9,r30,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82644384
	if (cr6.eq) goto loc_82644384;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x82644384
	if (cr6.lt) goto loc_82644384;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x82644384
	if (cr6.lt) goto loc_82644384;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x82644384
	if (!cr6.lt) goto loc_82644384;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x8264438c
	goto loc_8264438C;
loc_82644384:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_8264438C:
	// rlwinm r9,r30,0,27,27
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x10;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82644400
	if (cr6.eq) goto loc_82644400;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x82644400
	if (cr6.lt) goto loc_82644400;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x82644400
	if (cr6.lt) goto loc_82644400;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x82644400
	if (!cr6.lt) goto loc_82644400;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644408
	goto loc_82644408;
loc_82644400:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644408:
	// rlwinm r9,r30,0,25,25
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x40;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8264447c
	if (cr6.eq) goto loc_8264447C;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x8264447c
	if (cr6.lt) goto loc_8264447C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x8264447c
	if (cr6.lt) goto loc_8264447C;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x8264447c
	if (!cr6.lt) goto loc_8264447C;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644484
	goto loc_82644484;
loc_8264447C:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644484:
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8264448C:
	// cmpw cr6,r11,r25
	cr6.compare<int32_t>(r11.s32, r25.s32, xer);
	// blt cr6,0x826440b8
	if (cr6.lt) goto loc_826440B8;
loc_82644494:
	// cmpw cr6,r25,r27
	cr6.compare<int32_t>(r25.s32, r27.s32, xer);
	// beq cr6,0x82644548
	if (cr6.eq) goto loc_82644548;
	// lwz r10,60(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
	// srawi r9,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r9.s64 = r11.s32 >> 2;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// bge cr6,0x82644548
	if (!cr6.lt) goto loc_82644548;
	// clrlwi r30,r10,24
	r30.u64 = ctx.r10.u32 & 0xFF;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r29,1
	r29.s64 = 1;
loc_826444BC:
	// rlwinm r9,r11,1,29,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x6;
	// slw r9,r29,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r9.u8 & 0x3F));
	// and r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 & r30.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82644530
	if (cr6.eq) goto loc_82644530;
	// lwz r8,68(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
	// lwz r9,64(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// lwz r31,12(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwzx r8,r8,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// mullw r8,r8,r26
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(r26.s32);
	// srawi r8,r8,20
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 20;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// blt cr6,0x82644530
	if (cr6.lt) goto loc_82644530;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt cr6,0x82644530
	if (cr6.lt) goto loc_82644530;
	// lwz r31,4(r7)
	r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// bge cr6,0x82644530
	if (!cr6.lt) goto loc_82644530;
	// srawi r31,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r31.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 1;
	// mullw r9,r31,r27
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r27.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbzx r8,r9,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// stbx r8,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, ctx.r8.u8);
	// lbzx r9,r9,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// stbx r9,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, ctx.r9.u8);
	// b 0x82644538
	goto loc_82644538;
loc_82644530:
	// stbx r28,r11,r3
	PPC_STORE_U8(r11.u32 + ctx.r3.u32, r28.u8);
	// stbx r28,r11,r5
	PPC_STORE_U8(r11.u32 + ctx.r5.u32, r28.u8);
loc_82644538:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpw cr6,r11,r27
	cr6.compare<int32_t>(r11.s32, r27.s32, xer);
	// blt cr6,0x826444bc
	if (cr6.lt) goto loc_826444BC;
loc_82644548:
	// add r3,r27,r3
	ctx.r3.u64 = r27.u64 + ctx.r3.u64;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82644550"))) PPC_WEAK_FUNC(sub_82644550);
PPC_FUNC_IMPL(__imp__sub_82644550) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd8
	// stfd f30,-120(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -120, f30.u64);
	// stfd f31,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, f31.u64);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// lwz r29,308(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// lwz r30,316(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x826448b8
	if (cr6.eq) goto loc_826448B8;
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f0,52(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 52);
	f0.f64 = double(temp.f32);
	// lfs f13,30752(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 30752);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bne cr6,0x82644638
	if (!cr6.eq) goto loc_82644638;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// li r4,128
	ctx.r4.s64 = 128;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addze r5,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r5.s64 = temp.s64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r4,128
	ctx.r4.s64 = 128;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addze r5,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r5.s64 = temp.s64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f30,-120(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f31,-112(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// b 0x8239bd28
	return;
loc_82644638:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82644650
	if (!cr6.lt) goto loc_82644650;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r21,r5
	r21.u64 = ctx.r5.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// b 0x8264466c
	goto loc_8264466C;
loc_82644650:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// mr r20,r9
	r20.u64 = ctx.r9.u64;
	// lfs f13,-32384(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -32384);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stfs f0,52(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 52, temp.u32);
loc_8264466C:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lfs f0,52(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 52);
	f0.f64 = double(temp.f32);
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// lfs f30,56(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 56);
	f30.f64 = double(temp.f32);
	// mr r25,r29
	r25.u64 = r29.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// srawi r23,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r23.s64 = ctx.r9.s32 >> 1;
	// lfd f31,30744(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + 30744);
	// fmul f1,f0,f31
	ctx.f1.f64 = f0.f64 * f31.f64;
	// bl 0x8239ddc0
	sub_8239DDC0(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64;
	// lfs f13,52(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// fmul f1,f13,f31
	ctx.f1.f64 = ctx.f13.f64 * f31.f64;
	// frsp f31,f0
	f31.f64 = double(float(f0.f64));
	// bl 0x8239de90
	sub_8239DE90(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// fmuls f12,f0,f30
	ctx.f12.f64 = double(float(f0.f64 * f30.f64));
	// ble cr6,0x82644814
	if (!cr6.gt) goto loc_82644814;
	// lis r29,32767
	r29.s64 = 2147418112;
	// lis r5,-32244
	ctx.r5.s64 = -2113142784;
	// lis r6,-32244
	ctx.r6.s64 = -2113142784;
	// lis r7,-32249
	ctx.r7.s64 = -2113470464;
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f6,30740(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 30740);
	ctx.f6.f64 = double(temp.f32);
	// ori r27,r29,65535
	r27.u64 = r29.u64 | 65535;
	// lfs f7,30736(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 30736);
	ctx.f7.f64 = double(temp.f32);
	// lis r29,-32768
	r29.s64 = -2147483648;
	// lfs f8,-19984(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -19984);
	ctx.f8.f64 = double(temp.f32);
	// lfd f10,-28592(r8)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -28592);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lfs f11,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	ctx.f11.f64 = double(temp.f32);
	// li r26,1
	r26.s64 = 1;
	// lfs f9,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f9.f64 = double(temp.f32);
	// ori r29,r29,1
	r29.u64 = r29.u64 | 1;
loc_82644714:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - r11.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// fmadds f0,f13,f31,f12
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + ctx.f12.f64));
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// ble cr6,0x826447e0
	if (!cr6.gt) goto loc_826447E0;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// fdivs f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 / f0.f64));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f5,f5
	ctx.f5.f64 = double(ctx.f5.s64);
	// frsp f5,f5
	ctx.f5.f64 = double(float(ctx.f5.f64));
	// fmadds f13,f13,f30,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * f30.f64 + ctx.f5.f64));
	// fadd f13,f13,f10
	ctx.f13.f64 = ctx.f13.f64 + ctx.f10.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r10,64(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwzx r11,r9,r10
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x826447dc
	if (cr6.lt) goto loc_826447DC;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x826447dc
	if (!cr6.lt) goto loc_826447DC;
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// fmuls f0,f0,f12
	f0.f64 = double(float(f0.f64 * ctx.f12.f64));
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// fcmpu cr6,f0,f8
	cr6.compare(f0.f64, ctx.f8.f64);
	// slw r11,r26,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 << (r11.u8 & 0x3F));
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// ble cr6,0x826447b8
	if (!cr6.gt) goto loc_826447B8;
	// stwx r27,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r27.u32);
	// b 0x826447e0
	goto loc_826447E0;
loc_826447B8:
	// fcmpu cr6,f0,f7
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f7.f64);
	// bge cr6,0x826447c8
	if (!cr6.lt) goto loc_826447C8;
	// stwx r29,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r29.u32);
	// b 0x826447e0
	goto loc_826447E0;
loc_826447C8:
	// fmuls f0,f0,f6
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * ctx.f6.f64));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// b 0x826447e0
	goto loc_826447E0;
loc_826447DC:
	// stwx r30,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r30.u32);
loc_826447E0:
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x82644800
	if (!cr6.eq) goto loc_82644800;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// srawi r8,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 3;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stbx r10,r8,r11
	PPC_STORE_U8(ctx.r8.u32 + r11.u32, ctx.r10.u8);
loc_82644800:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// blt cr6,0x82644714
	if (cr6.lt) goto loc_82644714;
loc_82644814:
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264482c
	if (cr6.eq) goto loc_8264482C;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// srawi r10,r4,3
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 3;
	// stbx r3,r10,r11
	PPC_STORE_U8(ctx.r10.u32 + r11.u32, ctx.r3.u8);
loc_8264482C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r29,r30
	r29.u64 = r30.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82644864
	if (!cr6.gt) goto loc_82644864;
loc_8264483C:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82643ae0
	sub_82643AE0(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8264483c
	if (cr6.lt) goto loc_8264483C;
loc_82644864:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826448a4
	if (!cr6.gt) goto loc_826448A4;
loc_82644870:
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82644080
	sub_82644080(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r30,r30,2
	r30.s64 = r30.s64 + 2;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// add r28,r23,r28
	r28.u64 = r23.u64 + r28.u64;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x82644870
	if (cr6.lt) goto loc_82644870;
loc_826448A4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f30,-120(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f31,-112(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// b 0x8239bd28
	return;
loc_826448B8:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f30,-120(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f31,-112(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// b 0x8239bd28
	return;
}

__attribute__((alias("__imp__sub_826448CC"))) PPC_WEAK_FUNC(sub_826448CC);
PPC_FUNC_IMPL(__imp__sub_826448CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826448D0"))) PPC_WEAK_FUNC(sub_826448D0);
PPC_FUNC_IMPL(__imp__sub_826448D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// lfd f9,30816(r9)
	ctx.fpscr.disableFlushMode();
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30816);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lwz r7,44(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r8,36(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// lfs f0,48(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 48);
	f0.f64 = double(temp.f32);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// lwz r6,40(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// lfd f8,30808(r9)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30808);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// extsw r6,r6
	ctx.r6.s64 = ctx.r6.s32;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f10,30800(r9)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30800);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// addi r31,r1,88
	r31.s64 = ctx.r1.s64 + 88;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lfd f7,30792(r9)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30792);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// lfd f6,30784(r9)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30784);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lfd f5,30776(r9)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30776);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// fmul f4,f12,f5
	ctx.f4.f64 = ctx.f12.f64 * ctx.f5.f64;
	// lfd f5,30768(r9)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30768);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// fmul f3,f13,f5
	ctx.f3.f64 = ctx.f13.f64 * ctx.f5.f64;
	// lfd f5,30760(r9)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 30760);
	// fmul f5,f11,f5
	ctx.f5.f64 = ctx.f11.f64 * ctx.f5.f64;
	// fmadd f9,f13,f9,f4
	ctx.f9.f64 = ctx.f13.f64 * ctx.f9.f64 + ctx.f4.f64;
	// fmadd f8,f11,f8,f3
	ctx.f8.f64 = ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64;
	// fmsub f13,f13,f10,f5
	ctx.f13.f64 = ctx.f13.f64 * ctx.f10.f64 - ctx.f5.f64;
	// fmadd f11,f11,f7,f9
	ctx.f11.f64 = ctx.f11.f64 * ctx.f7.f64 + ctx.f9.f64;
	// fmsub f10,f12,f10,f8
	ctx.f10.f64 = ctx.f12.f64 * ctx.f10.f64 - ctx.f8.f64;
	// fnmsub f13,f12,f6,f13
	ctx.f13.f64 = -(ctx.f12.f64 * ctx.f6.f64 - ctx.f13.f64);
	// fmul f12,f11,f0
	ctx.f12.f64 = ctx.f11.f64 * f0.f64;
	// fmul f11,f10,f0
	ctx.f11.f64 = ctx.f10.f64 * f0.f64;
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f13,0,r3
	PPC_STORE_U32(ctx.r3.u32, ctx.f13.u32);
	// fctiwz f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f13,0,r31
	PPC_STORE_U32(r31.u32, ctx.f13.u32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// lwz r23,276(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// lwz r24,284(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x82644ad0
	if (cr6.eq) goto loc_82644AD0;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mullw r31,r9,r11
	r31.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// cmpwi cr6,r7,255
	cr6.compare<int32_t>(ctx.r7.s32, 255, xer);
	// ble cr6,0x82644a38
	if (!cr6.gt) goto loc_82644A38;
	// li r7,255
	ctx.r7.s64 = 255;
	// b 0x82644a44
	goto loc_82644A44;
loc_82644A38:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x82644a44
	if (!cr6.lt) goto loc_82644A44;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82644A44:
	// lwz r29,88(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r29,384
	cr6.compare<int32_t>(r29.s32, 384, xer);
	// ble cr6,0x82644a58
	if (!cr6.gt) goto loc_82644A58;
	// li r29,384
	r29.s64 = 384;
	// b 0x82644a64
	goto loc_82644A64;
loc_82644A58:
	// cmpwi cr6,r29,-384
	cr6.compare<int32_t>(r29.s32, -384, xer);
	// bge cr6,0x82644a64
	if (!cr6.lt) goto loc_82644A64;
	// li r29,-384
	r29.s64 = -384;
loc_82644A64:
	// lwz r30,96(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r30,384
	cr6.compare<int32_t>(r30.s32, 384, xer);
	// ble cr6,0x82644a78
	if (!cr6.gt) goto loc_82644A78;
	// li r30,384
	r30.s64 = 384;
	// b 0x82644a84
	goto loc_82644A84;
loc_82644A78:
	// cmpwi cr6,r30,-384
	cr6.compare<int32_t>(r30.s32, -384, xer);
	// bge cr6,0x82644a84
	if (!cr6.lt) goto loc_82644A84;
	// li r30,-384
	r30.s64 = -384;
loc_82644A84:
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x826423a8
	sub_826423A8(ctx, base);
	// srawi r31,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	r31.s64 = r31.s32 >> 2;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x82642440
	sub_82642440(ctx, base);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82642440
	sub_82642440(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd34
	return;
loc_82644AD0:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82644ADC"))) PPC_WEAK_FUNC(sub_82644ADC);
PPC_FUNC_IMPL(__imp__sub_82644ADC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82644AE0"))) PPC_WEAK_FUNC(sub_82644AE0);
PPC_FUNC_IMPL(__imp__sub_82644AE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// ble cr6,0x82645054
	if (!cr6.gt) goto loc_82645054;
	// fcmpu cr6,f4,f0
	cr6.compare(ctx.f4.f64, f0.f64);
	// ble cr6,0x82645054
	if (!cr6.gt) goto loc_82645054;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,6732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	f0.f64 = double(temp.f32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fmuls f0,f3,f0
	f0.f64 = double(float(ctx.f3.f64 * f0.f64));
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fadds f11,f0,f1
	ctx.f11.f64 = double(float(f0.f64 + ctx.f1.f64));
	// lfd f0,-56(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,5736(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5736);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fsubs f13,f0,f4
	ctx.f13.f64 = double(float(f0.f64 - ctx.f4.f64));
	// fneg f9,f0
	ctx.f9.u64 = f0.u64 ^ 0x8000000000000000;
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fsubs f12,f2,f13
	ctx.f12.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
	// fsubs f13,f2,f0
	ctx.f13.f64 = double(float(ctx.f2.f64 - f0.f64));
	// fsubs f9,f2,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 - ctx.f9.f64));
	// fsubs f10,f2,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f10.f64));
	// fcmpu cr6,f12,f2
	cr6.compare(ctx.f12.f64, ctx.f2.f64);
	// bgt cr6,0x82644b5c
	if (cr6.gt) goto loc_82644B5C;
	// fmr f2,f12
	ctx.f2.f64 = ctx.f12.f64;
loc_82644B5C:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fctiwz f0,f2
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r5,r11
	cr6.compare<int32_t>(ctx.r5.s32, r11.s32, xer);
	// ble cr6,0x82644b7c
	if (!cr6.gt) goto loc_82644B7C;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// stw r5,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, ctx.r5.u32);
loc_82644B7C:
	// fsubs f0,f1,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fsubs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// fdivs f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 / ctx.f12.f64));
	// lfd f0,-28592(r10)
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -28592);
	// blt cr6,0x82644c90
	if (cr6.lt) goto loc_82644C90;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// li r10,2
	ctx.r10.s64 = 2;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82644BA8:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r10,-1
	ctx.r4.s64 = ctx.r10.s64 + -1;
	// extsw r31,r10
	r31.s64 = ctx.r10.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// std r8,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r8.u64);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// std r31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, r31.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r4,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r4.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, r30.u64);
	// lfd f8,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// lfd f7,-48(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f6,-40(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// fcfid f6,f6
	ctx.f6.f64 = double(ctx.f6.s64);
	// lfd f5,-32(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f5,f5
	ctx.f5.f64 = double(ctx.f5.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// frsp f5,f5
	ctx.f5.f64 = double(float(ctx.f5.f64));
	// fsubs f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fsubs f7,f7,f13
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f13.f64));
	// fsubs f6,f6,f13
	ctx.f6.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fsubs f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f13.f64));
	// fmadds f8,f8,f12,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fmadds f7,f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fmadds f6,f6,f12,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fmadds f5,f5,f12,f11
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fadd f6,f6,f0
	ctx.f6.f64 = ctx.f6.f64 + f0.f64;
	// fadd f5,f5,f0
	ctx.f5.f64 = ctx.f5.f64 + f0.f64;
	// fctiwz f8,f8
	ctx.f8.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f8,f7
	ctx.f8.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// fctiwz f7,f6
	ctx.f7.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f6.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f6,f5
	ctx.f6.s64 = (ctx.f5.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f5.f64));
	// stfiwx f7,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f7.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f6,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f6.u32);
	// blt cr6,0x82644ba8
	if (cr6.lt) goto loc_82644BA8;
loc_82644C90:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82644cdc
	if (!cr6.lt) goto loc_82644CDC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82644C9C:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f8,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fsubs f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fmadds f8,f8,f12,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f11.f64));
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fctiwz f8,f8
	ctx.f8.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f8,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f8.u32);
	// blt cr6,0x82644c9c
	if (cr6.lt) goto loc_82644C9C;
loc_82644CDC:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f13,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f10,f13
	cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// bgt cr6,0x82644d00
	if (cr6.gt) goto loc_82644D00;
	// fmr f13,f10
	ctx.f13.f64 = ctx.f10.f64;
loc_82644D00:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x82644d94
	if (cr6.lt) goto loc_82644D94;
	// fadds f12,f1,f13
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// addi r7,r1,-56
	ctx.r7.s64 = ctx.r1.s64 + -56;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82644D58:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r9,r6,r7
	PPC_STORE_U32(ctx.r6.u32 + ctx.r7.u32, ctx.r9.u32);
	// bne cr6,0x82644d58
	if (!cr6.eq) goto loc_82644D58;
loc_82644D94:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82644dd8
	if (!cr6.lt) goto loc_82644DD8;
	// fadds f13,f1,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// addi r8,r1,-56
	ctx.r8.s64 = ctx.r1.s64 + -56;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-56(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82644DC0:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r7,r9
	PPC_STORE_U32(ctx.r7.u32 + ctx.r9.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82644dc0
	if (!cr6.eq) goto loc_82644DC0;
loc_82644DD8:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f13,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f9,f13
	cr6.compare(ctx.f9.f64, ctx.f13.f64);
	// bgt cr6,0x82644dfc
	if (cr6.gt) goto loc_82644DFC;
	// fmr f13,f9
	ctx.f13.f64 = ctx.f9.f64;
loc_82644DFC:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fsubs f12,f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f11.f64 - ctx.f1.f64));
	// fsubs f11,f9,f10
	ctx.f11.f64 = double(float(ctx.f9.f64 - ctx.f10.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// fdivs f13,f12,f11
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f11.f64));
	// blt cr6,0x82644f18
	if (cr6.lt) goto loc_82644F18;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82644E30:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r10,-1
	ctx.r4.s64 = ctx.r10.s64 + -1;
	// extsw r31,r10
	r31.s64 = ctx.r10.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// std r31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, r31.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r4,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r4.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r30,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, r30.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f11,-40(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f9,-48(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f8,-56(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fsubs f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// fsubs f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// fsubs f9,f9,f10
	ctx.f9.f64 = double(float(ctx.f9.f64 - ctx.f10.f64));
	// fsubs f8,f8,f10
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f10.f64));
	// fmadds f12,f12,f13,f1
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmadds f11,f11,f13,f1
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmadds f9,f9,f13,f1
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmadds f8,f8,f13,f1
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f11,f11,f0
	ctx.f11.f64 = ctx.f11.f64 + f0.f64;
	// fadd f9,f9,f0
	ctx.f9.f64 = ctx.f9.f64 + f0.f64;
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f12,f11
	ctx.f12.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f11,f9
	ctx.f11.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f9.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f9,f8
	ctx.f9.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// blt cr6,0x82644e30
	if (cr6.lt) goto loc_82644E30;
loc_82644F18:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82644f64
	if (!cr6.lt) goto loc_82644F64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82644F24:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// fmadds f12,f12,f13,f1
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// blt cr6,0x82644f24
	if (cr6.lt) goto loc_82644F24;
loc_82644F64:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82644fa8
	if (!cr6.gt) goto loc_82644FA8;
	// fadd f13,f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 + f0.f64;
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// li r11,0
	r11.s64 = 0;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82644F8C:
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r9,r8,r11
	PPC_STORE_U32(ctx.r8.u32 + r11.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x82644f8c
	if (cr6.lt) goto loc_82644F8C;
loc_82644FA8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826450f0
	if (!cr6.gt) goto loc_826450F0;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	ctx.f13.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f13,f1,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
loc_82644FC8:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r8.u64);
	// lfd f12,-40(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82644fc8
	if (cr6.lt) goto loc_82644FC8;
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82645054:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826450f0
	if (!cr6.gt) goto loc_826450F0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-56
	ctx.r8.s64 = ctx.r1.s64 + -56;
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f1,f0
	f0.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r8,-56(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-56(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_826450BC:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x826450bc
	if (cr6.lt) goto loc_826450BC;
loc_826450F0:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82645100"))) PPC_WEAK_FUNC(sub_82645100);
PPC_FUNC_IMPL(__imp__sub_82645100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = ctx.f1.f64;
	// lfs f0,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// bgt cr6,0x826451b8
	if (cr6.gt) goto loc_826451B8;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645484
	if (!cr6.gt) goto loc_82645484;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f11,f0
	ctx.f13.f64 = double(float(ctx.f11.f64 + f0.f64));
	// fsubs f12,f11,f0
	ctx.f12.f64 = double(float(ctx.f11.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f11,f0
	f0.f64 = ctx.f11.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r8,-16(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_8264517C:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x8264517c
	if (cr6.lt) goto loc_8264517C;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826451B8:
	// fneg f13,f3
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = ctx.f3.u64 ^ 0x8000000000000000;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// fsubs f0,f2,f3
	f0.f64 = double(float(ctx.f2.f64 - ctx.f3.f64));
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// fsubs f12,f2,f13
	ctx.f12.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
	// lfd f13,-8(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x826451e4
	if (!cr6.gt) goto loc_826451E4;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_826451E4:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// li r8,0
	ctx.r8.s64 = 0;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r5,-8(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// lfd f13,-28592(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -28592);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// lfs f9,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	ctx.f9.f64 = double(temp.f32);
	// blt cr6,0x82645278
	if (cr6.lt) goto loc_82645278;
	// fadds f0,f11,f9
	f0.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// rlwinm r10,r11,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// li r11,0
	r11.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_8264523C:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r11,12
	ctx.r7.s64 = r11.s64 + 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stwx r10,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r10,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r10,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r10,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r10.u32);
	// bne cr6,0x8264523c
	if (!cr6.eq) goto loc_8264523C;
loc_82645278:
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// bge cr6,0x826452bc
	if (!cr6.lt) goto loc_826452BC;
	// fadds f0,f11,f9
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// subf r11,r8,r5
	r11.s64 = ctx.r5.s64 - ctx.r8.s64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_826452A4:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stwx r9,r10,r7
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x826452a4
	if (!cr6.eq) goto loc_826452A4;
loc_826452BC:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fmuls f10,f3,f3
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f0,-16(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fcmpu cr6,f12,f0
	cr6.compare(ctx.f12.f64, f0.f64);
	// bgt cr6,0x826452e4
	if (cr6.gt) goto loc_826452E4;
	// fmr f0,f12
	f0.f64 = ctx.f12.f64;
loc_826452E4:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bge cr6,0x82645360
	if (!cr6.lt) goto loc_82645360;
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f12,-31368(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31368);
loc_82645308:
	// extsw r10,r8
	ctx.r10.s64 = ctx.r8.s32;
	// std r10,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r10.u64);
	// lfd f0,-8(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f2
	f0.f64 = double(float(f0.f64 - ctx.f2.f64));
	// fnmsubs f0,f0,f0,f10
	f0.f64 = double(float(-(f0.f64 * f0.f64 - ctx.f10.f64)));
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// bgt cr6,0x8264533c
	if (cr6.gt) goto loc_8264533C;
	// fadd f0,f11,f13
	f0.f64 = ctx.f11.f64 + ctx.f13.f64;
	// b 0x82645348
	goto loc_82645348;
loc_8264533C:
	// fsqrt f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = sqrt(f0.f64);
	// fsub f0,f11,f0
	f0.f64 = ctx.f11.f64 - f0.f64;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
loc_82645348:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645308
	if (cr6.lt) goto loc_82645308;
loc_82645360:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x826453a4
	if (!cr6.lt) goto loc_826453A4;
	// fadds f0,f11,f9
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// addi r10,r1,-8
	ctx.r10.s64 = ctx.r1.s64 + -8;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f0,f0,f13
	f0.f64 = f0.f64 + ctx.f13.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r10,-8(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82645388:
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stwx r10,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645388
	if (cr6.lt) goto loc_82645388;
loc_826453A4:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826453e8
	if (!cr6.gt) goto loc_826453E8;
	// fadd f0,f11,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f11.f64 + ctx.f13.f64;
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// li r11,0
	r11.s64 = 0;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_826453CC:
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x826453cc
	if (cr6.lt) goto loc_826453CC;
loc_826453E8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645484
	if (!cr6.gt) goto loc_82645484;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 * f0.f64));
loc_82645408:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lfd f12,-8(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 - ctx.f12.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f12,-16(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 - ctx.f12.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645408
	if (cr6.lt) goto loc_82645408;
loc_82645484:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8264548C"))) PPC_WEAK_FUNC(sub_8264548C);
PPC_FUNC_IMPL(__imp__sub_8264548C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82645490"))) PPC_WEAK_FUNC(sub_82645490);
PPC_FUNC_IMPL(__imp__sub_82645490) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// ble cr6,0x82645a20
	if (!cr6.gt) goto loc_82645A20;
	// fcmpu cr6,f4,f0
	cr6.compare(ctx.f4.f64, f0.f64);
	// ble cr6,0x82645a20
	if (!cr6.gt) goto loc_82645A20;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,5736(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	f0.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmuls f0,f4,f0
	f0.f64 = double(float(ctx.f4.f64 * f0.f64));
	// lfs f13,6732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fmuls f11,f3,f13
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// std r11,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, r11.u64);
	// fsubs f13,f2,f0
	ctx.f13.f64 = double(float(ctx.f2.f64 - f0.f64));
	// fadds f11,f11,f1
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// fsubs f10,f2,f12
	ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// lfd f0,-56(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x826454fc
	if (cr6.gt) goto loc_826454FC;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_826454FC:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// li r11,0
	r11.s64 = 0;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// lfs f9,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	ctx.f9.f64 = double(temp.f32);
	// blt cr6,0x82645590
	if (cr6.lt) goto loc_82645590;
	// fadds f12,f1,f9
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82645554:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82645554
	if (!cr6.eq) goto loc_82645554;
loc_82645590:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826455d4
	if (!cr6.lt) goto loc_826455D4;
	// fadds f12,f1,f9
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// addi r8,r1,-56
	ctx.r8.s64 = ctx.r1.s64 + -56;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f12.u32);
	// lwz r8,-56(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_826455BC:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x826455bc
	if (!cr6.eq) goto loc_826455BC;
loc_826455D4:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r10.u64);
	// lfd f12,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f2,f12
	cr6.compare(ctx.f2.f64, ctx.f12.f64);
	// bgt cr6,0x826455f8
	if (cr6.gt) goto loc_826455F8;
	// fmr f12,f2
	ctx.f12.f64 = ctx.f2.f64;
loc_826455F8:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fsubs f8,f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f11.f64 - ctx.f1.f64));
	// fsubs f7,f2,f13
	ctx.f7.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// fdivs f12,f8,f7
	ctx.f12.f64 = double(float(ctx.f8.f64 / ctx.f7.f64));
	// blt cr6,0x82645714
	if (cr6.lt) goto loc_82645714;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8264562C:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r10,-1
	ctx.r4.s64 = ctx.r10.s64 + -1;
	// extsw r31,r10
	r31.s64 = ctx.r10.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// std r8,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// std r31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, r31.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r4,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r4.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, r30.u64);
	// lfd f8,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// lfd f7,-48(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f6,-40(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// fcfid f6,f6
	ctx.f6.f64 = double(ctx.f6.s64);
	// lfd f5,-32(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f5,f5
	ctx.f5.f64 = double(ctx.f5.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// frsp f5,f5
	ctx.f5.f64 = double(float(ctx.f5.f64));
	// fsubs f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fsubs f7,f7,f13
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f13.f64));
	// fsubs f6,f6,f13
	ctx.f6.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fsubs f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f13.f64));
	// fmadds f8,f8,f12,f1
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fmadds f7,f7,f12,f1
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fmadds f6,f6,f12,f1
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fmadds f5,f5,f12,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fadd f6,f6,f0
	ctx.f6.f64 = ctx.f6.f64 + f0.f64;
	// fadd f5,f5,f0
	ctx.f5.f64 = ctx.f5.f64 + f0.f64;
	// fctiwz f8,f8
	ctx.f8.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f8,f7
	ctx.f8.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f7,f6
	ctx.f7.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f6.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f6,f5
	ctx.f6.s64 = (ctx.f5.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f5.f64));
	// stfiwx f7,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f7.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f6,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f6.u32);
	// blt cr6,0x8264562c
	if (cr6.lt) goto loc_8264562C;
loc_82645714:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82645760
	if (!cr6.lt) goto loc_82645760;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82645720:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f8,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fsubs f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fmadds f8,f8,f12,f1
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fctiwz f8,f8
	ctx.f8.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f8,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f8.u32);
	// blt cr6,0x82645720
	if (cr6.lt) goto loc_82645720;
loc_82645760:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f13,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f10,f13
	cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// bgt cr6,0x82645784
	if (cr6.gt) goto loc_82645784;
	// fmr f13,f10
	ctx.f13.f64 = ctx.f10.f64;
loc_82645784:
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fsubs f12,f1,f11
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// fsubs f10,f10,f2
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f2.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// fdivs f13,f12,f10
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f10.f64));
	// blt cr6,0x826458a0
	if (cr6.lt) goto loc_826458A0;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_826457B8:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r10,-1
	ctx.r4.s64 = ctx.r10.s64 + -1;
	// extsw r31,r10
	r31.s64 = ctx.r10.s32;
	// extsw r4,r4
	ctx.r4.s64 = ctx.r4.s32;
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r30,r30
	r30.s64 = r30.s32;
	// std r31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, r31.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r4,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r4.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r30,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, r30.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f10,-40(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f8,-48(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// lfd f7,-56(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// fsubs f12,f12,f2
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f2.f64));
	// fsubs f10,f10,f2
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f2.f64));
	// fsubs f8,f8,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f2.f64));
	// fsubs f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f2.f64));
	// fmadds f12,f12,f13,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fmadds f10,f10,f13,f11
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fmadds f8,f8,f13,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fmadds f7,f7,f13,f11
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f10,f10,f0
	ctx.f10.f64 = ctx.f10.f64 + f0.f64;
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f12,f10
	ctx.f12.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f10,f8
	ctx.f10.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f8,f7
	ctx.f8.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f8,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f8.u32);
	// blt cr6,0x826457b8
	if (cr6.lt) goto loc_826457B8;
loc_826458A0:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826458ec
	if (!cr6.lt) goto loc_826458EC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_826458AC:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f12,f2
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f2.f64));
	// fmadds f12,f12,f13,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// blt cr6,0x826458ac
	if (cr6.lt) goto loc_826458AC;
loc_826458EC:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82645930
	if (!cr6.lt) goto loc_82645930;
	// fadds f13,f1,f9
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82645914:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r9,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x82645914
	if (cr6.lt) goto loc_82645914;
loc_82645930:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645974
	if (!cr6.gt) goto loc_82645974;
	// fadd f13,f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 + f0.f64;
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// li r11,0
	r11.s64 = 0;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82645958:
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x82645958
	if (cr6.lt) goto loc_82645958;
loc_82645974:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645abc
	if (!cr6.gt) goto loc_82645ABC;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	ctx.f13.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f13,f1,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
loc_82645994:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
	// lfd f12,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r8.u64);
	// lfd f12,-40(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645994
	if (cr6.lt) goto loc_82645994;
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82645A20:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645abc
	if (!cr6.gt) goto loc_82645ABC;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-56
	ctx.r8.s64 = ctx.r1.s64 + -56;
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f1,f0
	f0.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lwz r8,-56(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-56(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// addi r9,r1,-56
	ctx.r9.s64 = ctx.r1.s64 + -56;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
loc_82645A88:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x82645a88
	if (cr6.lt) goto loc_82645A88;
loc_82645ABC:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82645ACC"))) PPC_WEAK_FUNC(sub_82645ACC);
PPC_FUNC_IMPL(__imp__sub_82645ACC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82645AD0"))) PPC_WEAK_FUNC(sub_82645AD0);
PPC_FUNC_IMPL(__imp__sub_82645AD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// ble cr6,0x82645d9c
	if (!cr6.gt) goto loc_82645D9C;
	// fcmpu cr6,f4,f0
	cr6.compare(ctx.f4.f64, f0.f64);
	// ble cr6,0x82645d9c
	if (!cr6.gt) goto loc_82645D9C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f0,5736(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	f0.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmuls f0,f4,f0
	f0.f64 = double(float(ctx.f4.f64 * f0.f64));
	// lfs f12,6732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f12,f3,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// lfd f11,-28592(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fneg f13,f0
	ctx.f13.u64 = f0.u64 ^ 0x8000000000000000;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// fsubs f0,f2,f0
	f0.f64 = double(float(ctx.f2.f64 - f0.f64));
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// fsubs f10,f2,f13
	ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
	// fadd f13,f1,f11
	ctx.f13.f64 = ctx.f1.f64 + ctx.f11.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lfd f13,-8(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// lwz r4,-16(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// lfd f9,-8(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// ble cr6,0x82645b64
	if (!cr6.gt) goto loc_82645B64;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_82645B64:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// li r8,0
	ctx.r8.s64 = 0;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r5,-8(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// blt cr6,0x82645be4
	if (cr6.lt) goto loc_82645BE4;
	// fadd f0,f12,f11
	f0.f64 = ctx.f12.f64 + ctx.f11.f64;
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// rlwinm r10,r11,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// li r11,0
	r11.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_82645BA8:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r11,12
	ctx.r7.s64 = r11.s64 + 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stwx r10,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r10,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r10,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r10.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r10,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r10.u32);
	// bne cr6,0x82645ba8
	if (!cr6.eq) goto loc_82645BA8;
loc_82645BE4:
	// cmpw cr6,r8,r5
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, xer);
	// bge cr6,0x82645c24
	if (!cr6.lt) goto loc_82645C24;
	// fadd f0,f12,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64 + ctx.f11.f64;
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// subf r11,r8,r5
	r11.s64 = ctx.r5.s64 - ctx.r8.s64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_82645C0C:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stwx r9,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82645c0c
	if (!cr6.eq) goto loc_82645C0C;
loc_82645C24:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fcmpu cr6,f10,f0
	cr6.compare(ctx.f10.f64, f0.f64);
	// bgt cr6,0x82645c48
	if (cr6.gt) goto loc_82645C48;
	// fmr f0,f10
	f0.f64 = ctx.f10.f64;
loc_82645C48:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x82645c88
	if (!cr6.lt) goto loc_82645C88;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
loc_82645C6C:
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stwx r7,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82645c6c
	if (!cr6.eq) goto loc_82645C6C;
loc_82645C88:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r8,r11
	cr6.compare<int32_t>(ctx.r8.s32, r11.s32, xer);
	// bge cr6,0x82645cc8
	if (!cr6.lt) goto loc_82645CC8;
	// fadd f0,f12,f11
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64 + ctx.f11.f64;
	// addi r10,r1,-8
	ctx.r10.s64 = ctx.r1.s64 + -8;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lwz r10,-8(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82645CAC:
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stwx r10,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645cac
	if (cr6.lt) goto loc_82645CAC;
loc_82645CC8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645cf8
	if (!cr6.gt) goto loc_82645CF8;
	// li r11,0
	r11.s64 = 0;
loc_82645CDC:
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r4,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645cdc
	if (cr6.lt) goto loc_82645CDC;
loc_82645CF8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645e38
	if (!cr6.gt) goto loc_82645E38;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
loc_82645D18:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lfd f13,-8(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fsubs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 - ctx.f13.f64));
	// fadd f13,f13,f11
	ctx.f13.f64 = ctx.f13.f64 + ctx.f11.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fsubs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 - ctx.f13.f64));
	// fadd f13,f13,f11
	ctx.f13.f64 = ctx.f13.f64 + ctx.f11.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82645d18
	if (cr6.lt) goto loc_82645D18;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82645D9C:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82645e38
	if (!cr6.gt) goto loc_82645E38;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-8
	ctx.r8.s64 = ctx.r1.s64 + -8;
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f1,f0
	f0.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-8(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-8(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82645E04:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x82645e04
	if (cr6.lt) goto loc_82645E04;
loc_82645E38:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82645E40"))) PPC_WEAK_FUNC(sub_82645E40);
PPC_FUNC_IMPL(__imp__sub_82645E40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// ble cr6,0x826461a8
	if (!cr6.gt) goto loc_826461A8;
	// fcmpu cr6,f4,f0
	cr6.compare(ctx.f4.f64, f0.f64);
	// ble cr6,0x826461a8
	if (!cr6.gt) goto loc_826461A8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f0,5736(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	f0.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmuls f0,f4,f0
	f0.f64 = double(float(ctx.f4.f64 * f0.f64));
	// lfs f13,6732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6732);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f11,f3,f13
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// fsubs f13,f2,f0
	ctx.f13.f64 = double(float(ctx.f2.f64 - f0.f64));
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// fsubs f10,f2,f12
	ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fadd f12,f1,f0
	ctx.f12.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lfd f12,-8(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// lwz r4,-16(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// lfd f9,-8(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fadds f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// ble cr6,0x82645ed4
	if (!cr6.gt) goto loc_82645ED4;
	// fmr f13,f12
	ctx.f13.f64 = ctx.f12.f64;
loc_82645ED4:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// li r10,0
	ctx.r10.s64 = 0;
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r5,-8(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// lfs f12,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	ctx.f12.f64 = double(temp.f32);
	// blt cr6,0x82645f60
	if (cr6.lt) goto loc_82645F60;
	// fadds f13,f1,f12
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// rlwinm r10,r11,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_82645F24:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r11,12
	ctx.r7.s64 = r11.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82645f24
	if (!cr6.eq) goto loc_82645F24;
loc_82645F60:
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// bge cr6,0x82645fa4
	if (!cr6.lt) goto loc_82645FA4;
	// fadds f13,f1,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// subf r11,r10,r5
	r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-16(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
loc_82645F8C:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82645f8c
	if (!cr6.eq) goto loc_82645F8C;
loc_82645FA4:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f13,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f10,f13
	cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// bgt cr6,0x82645fc8
	if (cr6.gt) goto loc_82645FC8;
	// fmr f13,f10
	ctx.f13.f64 = ctx.f10.f64;
loc_82645FC8:
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(r11.u32, ctx.f13.u32);
	// lwz r5,-8(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// subf r11,r10,r5
	r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x82646050
	if (cr6.lt) goto loc_82646050;
	// subf r11,r10,r5
	r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// fadd f13,f11,f0
	ctx.f13.f64 = ctx.f11.f64 + f0.f64;
	// addi r7,r1,-8
	ctx.r7.s64 = ctx.r1.s64 + -8;
	// addi r9,r11,-4
	ctx.r9.s64 = r11.s64 + -4;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82646014:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r11,12
	ctx.r7.s64 = r11.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82646014
	if (!cr6.eq) goto loc_82646014;
loc_82646050:
	// cmpw cr6,r10,r5
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, xer);
	// bge cr6,0x82646090
	if (!cr6.lt) goto loc_82646090;
	// fadd f13,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f11.f64 + f0.f64;
	// addi r8,r1,-8
	ctx.r8.s64 = ctx.r1.s64 + -8;
	// subf r11,r10,r5
	r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-8(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82646078:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82646078
	if (!cr6.eq) goto loc_82646078;
loc_82646090:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x826460d4
	if (!cr6.lt) goto loc_826460D4;
	// fadds f13,f1,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_826460B8:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x826460b8
	if (cr6.lt) goto loc_826460B8;
loc_826460D4:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82646104
	if (!cr6.gt) goto loc_82646104;
	// li r11,0
	r11.s64 = 0;
loc_826460E8:
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r4,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r4.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x826460e8
	if (cr6.lt) goto loc_826460E8;
loc_82646104:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82646244
	if (!cr6.gt) goto loc_82646244;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	ctx.f13.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f13,f1,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
loc_82646124:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lfd f12,-8(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f12,-16(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82646124
	if (cr6.lt) goto loc_82646124;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_826461A8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82646244
	if (!cr6.gt) goto loc_82646244;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-8
	ctx.r8.s64 = ctx.r1.s64 + -8;
	// lfs f0,2552(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f1,f0
	f0.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-8(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-8(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// addi r9,r1,-8
	ctx.r9.s64 = ctx.r1.s64 + -8;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-8(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
loc_82646210:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x82646210
	if (cr6.lt) goto loc_82646210;
loc_82646244:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8264624C"))) PPC_WEAK_FUNC(sub_8264624C);
PPC_FUNC_IMPL(__imp__sub_8264624C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82646250"))) PPC_WEAK_FUNC(sub_82646250);
PPC_FUNC_IMPL(__imp__sub_82646250) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f29,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f29.u64);
	// stfd f30,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// bgt cr6,0x82646324
	if (cr6.gt) goto loc_82646324;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82646d10
	if (!cr6.gt) goto loc_82646D10;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,-96
	ctx.r8.s64 = ctx.r1.s64 + -96;
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f1,f0
	f0.f64 = ctx.f1.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,-96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,-96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// addi r9,r1,-96
	ctx.r9.s64 = ctx.r1.s64 + -96;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,-96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
loc_826462DC:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r8.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// stwx r7,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r7.u32);
	// lwz r6,32(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x826462dc
	if (cr6.lt) goto loc_826462DC;
	// li r3,0
	ctx.r3.s64 = 0;
	// lfd f29,-56(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f30,-48(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
loc_82646324:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fneg f0,f3
	ctx.fpscr.disableFlushMode();
	f0.u64 = ctx.f3.u64 ^ 0x8000000000000000;
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// lfd f13,-28592(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fadd f11,f1,f13
	ctx.f11.f64 = ctx.f1.f64 + ctx.f13.f64;
	// lfd f12,30880(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30880);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f12,f3,f12
	ctx.f12.f64 = ctx.f3.f64 * ctx.f12.f64;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r4,-96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lfd f12,30872(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30872);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f10,f0,f12
	ctx.f10.f64 = f0.f64 * ctx.f12.f64;
	// std r10,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r10.u64);
	// lfd f12,30864(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30864);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f9,f0,f12
	ctx.f9.f64 = f0.f64 * ctx.f12.f64;
	// lfd f12,30856(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30856);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f8,f0,f12
	ctx.f8.f64 = f0.f64 * ctx.f12.f64;
	// lfd f12,30848(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30848);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f7,f0,f12
	ctx.f7.f64 = f0.f64 * ctx.f12.f64;
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// lfd f12,30840(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30840);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f6,f0,f12
	ctx.f6.f64 = f0.f64 * ctx.f12.f64;
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// lfd f12,30832(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30832);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// fmul f5,f0,f12
	ctx.f5.f64 = f0.f64 * ctx.f12.f64;
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// lfd f12,30824(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 30824);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmul f0,f0,f12
	f0.f64 = f0.f64 * ctx.f12.f64;
	// frsp f12,f10
	ctx.f12.f64 = double(float(ctx.f10.f64));
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// frsp f5,f5
	ctx.f5.f64 = double(float(ctx.f5.f64));
	// frsp f4,f0
	ctx.f4.f64 = double(float(f0.f64));
	// lfs f0,5736(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 5736);
	f0.f64 = double(temp.f32);
	// fadds f10,f12,f3
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f3.f64));
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// fmuls f0,f10,f0
	f0.f64 = double(float(ctx.f10.f64 * f0.f64));
	// lfd f10,-88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// std r11,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, r11.u64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fsubs f11,f0,f11
	ctx.f11.f64 = double(float(f0.f64 - ctx.f11.f64));
	// fsubs f3,f0,f3
	ctx.f3.f64 = double(float(f0.f64 - ctx.f3.f64));
	// fsubs f9,f0,f9
	ctx.f9.f64 = double(float(f0.f64 - ctx.f9.f64));
	// fsubs f30,f0,f12
	f30.f64 = double(float(f0.f64 - ctx.f12.f64));
	// fsubs f0,f0,f8
	f0.f64 = double(float(f0.f64 - ctx.f8.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fadds f12,f3,f2
	ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f2.f64));
	// fadds f31,f9,f2
	f31.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// fadds f3,f11,f2
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f2.f64));
	// fadds f29,f0,f2
	f29.f64 = double(float(f0.f64 + ctx.f2.f64));
	// fadds f9,f11,f2
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f2.f64));
	// fadds f30,f30,f2
	f30.f64 = double(float(f30.f64 + ctx.f2.f64));
	// fcmpu cr6,f12,f10
	cr6.compare(ctx.f12.f64, ctx.f10.f64);
	// lfd f0,-88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fadds f11,f0,f7
	ctx.f11.f64 = double(float(f0.f64 + ctx.f7.f64));
	// fadds f8,f0,f6
	ctx.f8.f64 = double(float(f0.f64 + ctx.f6.f64));
	// fadds f7,f0,f5
	ctx.f7.f64 = double(float(f0.f64 + ctx.f5.f64));
	// fadds f6,f0,f4
	ctx.f6.f64 = double(float(f0.f64 + ctx.f4.f64));
	// bgt cr6,0x82646448
	if (cr6.gt) goto loc_82646448;
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
loc_82646448:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fctiwz f10,f10
	ctx.fpscr.disableFlushMode();
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f10,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f10.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r5,-96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// lfs f5,2552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	ctx.f5.f64 = double(temp.f32);
	// blt cr6,0x826464d4
	if (cr6.lt) goto loc_826464D4;
	// fadds f10,f1,f5
	ctx.f10.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// addi r9,r1,-88
	ctx.r9.s64 = ctx.r1.s64 + -88;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f10.u32);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
loc_82646498:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82646498
	if (!cr6.eq) goto loc_82646498;
loc_826464D4:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82646518
	if (!cr6.lt) goto loc_82646518;
	// fadds f10,f1,f5
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// addi r8,r1,-88
	ctx.r8.s64 = ctx.r1.s64 + -88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// lwz r8,-88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
loc_82646500:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82646500
	if (!cr6.eq) goto loc_82646500;
loc_82646518:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r10.u64);
	// lfd f10,-88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fcmpu cr6,f3,f10
	cr6.compare(ctx.f3.f64, ctx.f10.f64);
	// bgt cr6,0x8264653c
	if (cr6.gt) goto loc_8264653C;
	// fmr f10,f3
	ctx.f10.f64 = ctx.f3.f64;
loc_8264653C:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fsubs f11,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f11.f64 - f0.f64));
	// fsubs f4,f3,f12
	ctx.f4.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f10.u32);
	// lwz r5,-96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// fdivs f11,f11,f4
	ctx.f11.f64 = double(float(ctx.f11.f64 / ctx.f4.f64));
	// blt cr6,0x82646658
	if (cr6.lt) goto loc_82646658;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646570:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r31,r10,-1
	r31.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r31,r31
	r31.s64 = r31.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r8.u64);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r31,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, r31.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, r29.u64);
	// lfd f10,-88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// lfd f4,-80(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// lfd f3,-72(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// fcfid f4,f4
	ctx.f4.f64 = double(ctx.f4.s64);
	// fcfid f3,f3
	ctx.f3.f64 = double(ctx.f3.s64);
	// lfd f2,-64(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f2,f2
	ctx.f2.f64 = double(ctx.f2.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f4,f4
	ctx.f4.f64 = double(float(ctx.f4.f64));
	// frsp f3,f3
	ctx.f3.f64 = double(float(ctx.f3.f64));
	// frsp f2,f2
	ctx.f2.f64 = double(float(ctx.f2.f64));
	// fsubs f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f12.f64));
	// fsubs f4,f4,f12
	ctx.f4.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fsubs f3,f3,f12
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
	// fsubs f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fmadds f10,f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + f0.f64));
	// fmadds f4,f4,f11,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f11.f64 + f0.f64));
	// fmadds f3,f3,f11,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + f0.f64));
	// fmadds f2,f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64 + f0.f64));
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fadd f4,f4,f13
	ctx.f4.f64 = ctx.f4.f64 + ctx.f13.f64;
	// fadd f3,f3,f13
	ctx.f3.f64 = ctx.f3.f64 + ctx.f13.f64;
	// fadd f2,f2,f13
	ctx.f2.f64 = ctx.f2.f64 + ctx.f13.f64;
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f10,f4
	ctx.f10.s64 = (ctx.f4.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f4.f64));
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// fctiwz f4,f3
	ctx.f4.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f3.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f3,f2
	ctx.f3.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f2.f64));
	// stfiwx f4,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f4.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stfiwx f3,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f3.u32);
	// blt cr6,0x82646570
	if (cr6.lt) goto loc_82646570;
loc_82646658:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826466a4
	if (!cr6.lt) goto loc_826466A4;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646664:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// lfd f10,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fsubs f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f12.f64));
	// fmadds f10,f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + f0.f64));
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fctiwz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f10.u32);
	// blt cr6,0x82646664
	if (cr6.lt) goto loc_82646664;
loc_826466A4:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r10.u64);
	// lfd f12,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f31,f12
	cr6.compare(f31.f64, ctx.f12.f64);
	// bgt cr6,0x826466c8
	if (cr6.gt) goto loc_826466C8;
	// fmr f12,f31
	ctx.f12.f64 = f31.f64;
loc_826466C8:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fsubs f11,f7,f8
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f7.f64 - ctx.f8.f64));
	// fsubs f10,f31,f9
	ctx.f10.f64 = double(float(f31.f64 - ctx.f9.f64));
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lwz r5,-96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// fdivs f12,f11,f10
	ctx.f12.f64 = double(float(ctx.f11.f64 / ctx.f10.f64));
	// blt cr6,0x826467e4
	if (cr6.lt) goto loc_826467E4;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_826466FC:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r31,r10,-1
	r31.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r31,r31
	r31.s64 = r31.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r31,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, r31.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, r29.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f10,-72(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f4,-80(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f4,f4
	ctx.f4.f64 = double(ctx.f4.s64);
	// lfd f3,-88(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f3,f3
	ctx.f3.f64 = double(ctx.f3.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f4,f4
	ctx.f4.f64 = double(float(ctx.f4.f64));
	// frsp f3,f3
	ctx.f3.f64 = double(float(ctx.f3.f64));
	// fsubs f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// fsubs f10,f10,f9
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
	// fsubs f4,f4,f9
	ctx.f4.f64 = double(float(ctx.f4.f64 - ctx.f9.f64));
	// fsubs f3,f3,f9
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f9.f64));
	// fmadds f11,f11,f12,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f8.f64));
	// fmadds f10,f10,f12,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 + ctx.f8.f64));
	// fmadds f4,f4,f12,f8
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 + ctx.f8.f64));
	// fmadds f3,f3,f12,f8
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f8.f64));
	// fadd f11,f11,f13
	ctx.f11.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fadd f4,f4,f13
	ctx.f4.f64 = ctx.f4.f64 + ctx.f13.f64;
	// fadd f3,f3,f13
	ctx.f3.f64 = ctx.f3.f64 + ctx.f13.f64;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f11,f10
	ctx.f11.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// fctiwz f10,f4
	ctx.f10.s64 = (ctx.f4.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f4.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f4,f3
	ctx.f4.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f3.f64));
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f4,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f4.u32);
	// blt cr6,0x826466fc
	if (cr6.lt) goto loc_826466FC;
loc_826467E4:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82646830
	if (!cr6.lt) goto loc_82646830;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_826467F0:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fsubs f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// fmadds f11,f11,f12,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f8.f64));
	// fadd f11,f11,f13
	ctx.f11.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// blt cr6,0x826467f0
	if (cr6.lt) goto loc_826467F0;
loc_82646830:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fsubs f11,f30,f31
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(f30.f64 - f31.f64));
	// fsubs f12,f6,f7
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r10.u64);
	// fdivs f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f11.f64));
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fcmpu cr6,f30,f11
	cr6.compare(f30.f64, ctx.f11.f64);
	// bgt cr6,0x82646860
	if (cr6.gt) goto loc_82646860;
	// fmr f11,f30
	ctx.f11.f64 = f30.f64;
loc_82646860:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fctiwz f11,f11
	ctx.fpscr.disableFlushMode();
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r5,-96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x82646970
	if (cr6.lt) goto loc_82646970;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646888:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r31,r10,-1
	r31.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r31,r31
	r31.s64 = r31.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r31,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, r31.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, r29.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f10,-72(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f9,-80(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f8,-88(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fsubs f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 - f31.f64));
	// fsubs f10,f10,f31
	ctx.f10.f64 = double(float(ctx.f10.f64 - f31.f64));
	// fsubs f9,f9,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 - f31.f64));
	// fsubs f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 - f31.f64));
	// fmadds f11,f11,f12,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmadds f10,f10,f12,f7
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmadds f9,f9,f12,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmadds f8,f8,f12,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fadd f11,f11,f13
	ctx.f11.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fadd f9,f9,f13
	ctx.f9.f64 = ctx.f9.f64 + ctx.f13.f64;
	// fadd f8,f8,f13
	ctx.f8.f64 = ctx.f8.f64 + ctx.f13.f64;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// fctiwz f11,f10
	ctx.f11.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f10,f9
	ctx.f10.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f9.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f9,f8
	ctx.f9.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// blt cr6,0x82646888
	if (cr6.lt) goto loc_82646888;
loc_82646970:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826469bc
	if (!cr6.lt) goto loc_826469BC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8264697C:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fsubs f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 - f31.f64));
	// fmadds f11,f11,f12,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fadd f11,f11,f13
	ctx.f11.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fctiwz f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// blt cr6,0x8264697c
	if (cr6.lt) goto loc_8264697C;
loc_826469BC:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82646a00
	if (!cr6.lt) goto loc_82646A00;
	// fadds f12,f1,f5
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// addi r9,r1,-88
	ctx.r9.s64 = ctx.r1.s64 + -88;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
loc_826469E4:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x826469e4
	if (cr6.lt) goto loc_826469E4;
loc_82646A00:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, r11.u64);
	// lfd f12,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f29,f12
	cr6.compare(f29.f64, ctx.f12.f64);
	// bgt cr6,0x82646a24
	if (cr6.gt) goto loc_82646A24;
	// fmr f12,f29
	ctx.f12.f64 = f29.f64;
loc_82646A24:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fctiwz f12,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lwz r6,-96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// blt cr6,0x82646a90
	if (cr6.lt) goto loc_82646A90;
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646A54:
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r8,r10,12
	ctx.r8.s64 = ctx.r10.s64 + 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stwx r4,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r4.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r4,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r4.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stw r4,-4(r7)
	PPC_STORE_U32(ctx.r7.u32 + -4, ctx.r4.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stwx r4,r7,r8
	PPC_STORE_U32(ctx.r7.u32 + ctx.r8.u32, ctx.r4.u32);
	// bne cr6,0x82646a54
	if (!cr6.eq) goto loc_82646A54;
loc_82646A90:
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x82646abc
	if (!cr6.lt) goto loc_82646ABC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_82646AA4:
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r4,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r4.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82646aa4
	if (!cr6.eq) goto loc_82646AA4;
loc_82646ABC:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fsubs f12,f29,f30
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(f29.f64 - f30.f64));
	// fsubs f0,f0,f6
	f0.f64 = double(float(f0.f64 - ctx.f6.f64));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r10.u64);
	// fdivs f0,f0,f12
	f0.f64 = double(float(f0.f64 / ctx.f12.f64));
	// lfd f12,-64(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f30,f12
	cr6.compare(f30.f64, ctx.f12.f64);
	// bgt cr6,0x82646aec
	if (cr6.gt) goto loc_82646AEC;
	// fmr f12,f30
	ctx.f12.f64 = f30.f64;
loc_82646AEC:
	// addi r10,r1,-96
	ctx.r10.s64 = ctx.r1.s64 + -96;
	// fctiwz f12,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lwz r5,-96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x82646bfc
	if (cr6.lt) goto loc_82646BFC;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646B14:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r31,r10,-1
	r31.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r31,r31
	r31.s64 = r31.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r31,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, r31.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, r29.u64);
	// lfd f12,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f11,-72(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f10,-80(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// lfd f9,-88(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fsubs f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 - f30.f64));
	// fsubs f11,f11,f30
	ctx.f11.f64 = double(float(ctx.f11.f64 - f30.f64));
	// fsubs f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 - f30.f64));
	// fsubs f9,f9,f30
	ctx.f9.f64 = double(float(ctx.f9.f64 - f30.f64));
	// fmadds f12,f12,f0,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f6.f64));
	// fmadds f11,f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f11.f64 * f0.f64 + ctx.f6.f64));
	// fmadds f10,f10,f0,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * f0.f64 + ctx.f6.f64));
	// fmadds f9,f9,f0,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * f0.f64 + ctx.f6.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fadd f11,f11,f13
	ctx.f11.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fadd f10,f10,f13
	ctx.f10.f64 = ctx.f10.f64 + ctx.f13.f64;
	// fadd f9,f9,f13
	ctx.f9.f64 = ctx.f9.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// fctiwz f12,f11
	ctx.f12.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f11,f10
	ctx.f11.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f10.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f10,f9
	ctx.f10.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f9.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// blt cr6,0x82646b14
	if (cr6.lt) goto loc_82646B14;
loc_82646BFC:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82646c48
	if (!cr6.lt) goto loc_82646C48;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646C08:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// lfd f12,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 - f30.f64));
	// fmadds f12,f12,f0,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f6.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// blt cr6,0x82646c08
	if (cr6.lt) goto loc_82646C08;
loc_82646C48:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82646c74
	if (!cr6.lt) goto loc_82646C74;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82646C58:
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r4,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82646c58
	if (cr6.lt) goto loc_82646C58;
loc_82646C74:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82646d10
	if (!cr6.gt) goto loc_82646D10;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,560(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 560);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
loc_82646C94:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
	// lfd f12,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 - ctx.f12.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.r8.u64);
	// lfd f12,-72(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 - ctx.f12.f64));
	// fadd f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 + ctx.f13.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82646c94
	if (cr6.lt) goto loc_82646C94;
loc_82646D10:
	// li r3,0
	ctx.r3.s64 = 0;
	// lfd f29,-56(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f30,-48(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82646D24"))) PPC_WEAK_FUNC(sub_82646D24);
PPC_FUNC_IMPL(__imp__sub_82646D24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82646D28"))) PPC_WEAK_FUNC(sub_82646D28);
PPC_FUNC_IMPL(__imp__sub_82646D28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stfd f29,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f29.u64);
	// stfd f30,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f30,f1
	f30.f64 = ctx.f1.f64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f31,f2
	f31.f64 = ctx.f2.f64;
	// lfs f0,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// bgt cr6,0x82646dfc
	if (cr6.gt) goto loc_82646DFC;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82647970
	if (!cr6.gt) goto loc_82647970;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,2552(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2552);
	f0.f64 = double(temp.f32);
	// li r11,0
	r11.s64 = 0;
	// fadds f13,f30,f0
	ctx.f13.f64 = double(float(f30.f64 + f0.f64));
	// fsubs f12,f30,f0
	ctx.f12.f64 = double(float(f30.f64 - f0.f64));
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f0,f30,f0
	f0.f64 = f30.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f12
	ctx.f13.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82646DC4:
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r8,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r8.u32);
	// lwz r6,24(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stwx r9,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// stwx r7,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + r11.u32, ctx.r7.u32);
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x82646dc4
	if (cr6.lt) goto loc_82646DC4;
	// b 0x82647970
	goto loc_82647970;
loc_82646DFC:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfs f0,30752(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 30752);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// blt cr6,0x82646e80
	if (cr6.lt) goto loc_82646E80;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82647970
	if (!cr6.gt) goto loc_82647970;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// li r11,0
	r11.s64 = 0;
	// lfd f0,-28592(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -28592);
	// fadd f0,f30,f0
	f0.f64 = f30.f64 + f0.f64;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_82646E3C:
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r7,r8,r11
	PPC_STORE_U32(ctx.r8.u32 + r11.u32, ctx.r7.u32);
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stwx r9,r8,r11
	PPC_STORE_U32(ctx.r8.u32 + r11.u32, ctx.r9.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r7,28(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stwx r8,r7,r11
	PPC_STORE_U32(ctx.r7.u32 + r11.u32, ctx.r8.u32);
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x82646e3c
	if (cr6.lt) goto loc_82646E3C;
	// b 0x82647970
	goto loc_82647970;
loc_82646E80:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lis r11,-32244
	r11.s64 = -2113142784;
	// lfd f0,30744(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 30744);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// fmul f1,f3,f0
	ctx.f1.f64 = ctx.f3.f64 * f0.f64;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f29,f13,f0
	f29.f64 = double(float(ctx.f13.f64 / f0.f64));
	// bl 0x8239d8c0
	sub_8239D8C0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// fcmpu cr6,f10,f29
	cr6.compare(ctx.f10.f64, f29.f64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// bge cr6,0x82646ef4
	if (!cr6.lt) goto loc_82646EF4;
	// fmuls f0,f12,f10
	f0.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// b 0x82646f10
	goto loc_82646F10;
loc_82646EF4:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_82646F10:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// fsubs f11,f31,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(f31.f64 - f0.f64));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-28592(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -28592);
	// fadd f13,f30,f0
	ctx.f13.f64 = f30.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// lfd f9,88(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// bgt cr6,0x82646f6c
	if (cr6.gt) goto loc_82646F6C;
	// fmr f13,f31
	ctx.f13.f64 = f31.f64;
loc_82646F6C:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// blt cr6,0x8264707c
	if (cr6.lt) goto loc_8264707C;
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// li r10,2
	ctx.r10.s64 = 2;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82646F94:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r3,r10,-1
	ctx.r3.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r3,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r3.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r29.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f8,96(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lfd f7,104(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// lfd f6,112(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f6,f6
	ctx.f6.f64 = double(ctx.f6.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// fsubs f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 - f31.f64));
	// fsubs f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 - f31.f64));
	// fsubs f7,f7,f31
	ctx.f7.f64 = double(float(ctx.f7.f64 - f31.f64));
	// fsubs f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 - f31.f64));
	// fmadds f13,f13,f10,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 + f30.f64));
	// fmadds f8,f8,f10,f30
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 + f30.f64));
	// fmadds f7,f7,f10,f30
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + f30.f64));
	// fmadds f6,f6,f10,f30
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f10.f64 + f30.f64));
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fadd f8,f8,f0
	ctx.f8.f64 = ctx.f8.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fadd f6,f6,f0
	ctx.f6.f64 = ctx.f6.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// fctiwz f13,f8
	ctx.f13.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f8.f64));
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// fctiwz f8,f7
	ctx.f8.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f7,f6
	ctx.f7.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f6.f64));
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stfiwx f7,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f7.u32);
	// blt cr6,0x82646f94
	if (cr6.lt) goto loc_82646F94;
loc_8264707C:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826470c8
	if (!cr6.lt) goto loc_826470C8;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647088:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fsubs f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 - f31.f64));
	// fmadds f13,f13,f10,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 + f30.f64));
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// blt cr6,0x82647088
	if (cr6.lt) goto loc_82647088;
loc_826470C8:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f11,f13
	cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// bgt cr6,0x826470ec
	if (cr6.gt) goto loc_826470EC;
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
loc_826470EC:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x82647174
	if (cr6.lt) goto loc_82647174;
	// fadd f13,f12,f0
	ctx.f13.f64 = ctx.f12.f64 + f0.f64;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82647138:
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82647138
	if (!cr6.eq) goto loc_82647138;
loc_82647174:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826471b4
	if (!cr6.lt) goto loc_826471B4;
	// fadd f13,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64 + f0.f64;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_8264719C:
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x8264719c
	if (!cr6.eq) goto loc_8264719C;
loc_826471B4:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f8,2552(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2552);
	ctx.f8.f64 = double(temp.f32);
	// bge cr6,0x82647200
	if (!cr6.lt) goto loc_82647200;
	// fadds f13,f30,f8
	ctx.f13.f64 = double(float(f30.f64 + ctx.f8.f64));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826471E4:
	// lwz r8,20(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x826471e4
	if (cr6.lt) goto loc_826471E4;
loc_82647200:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// bgt cr6,0x82647224
	if (cr6.gt) goto loc_82647224;
	// fmr f13,f31
	ctx.f13.f64 = f31.f64;
loc_82647224:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// blt cr6,0x82647290
	if (cr6.lt) goto loc_82647290;
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647254:
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r8,r10,12
	ctx.r8.s64 = ctx.r10.s64 + 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stwx r4,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r4.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r4,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r4.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r4,-4(r7)
	PPC_STORE_U32(ctx.r7.u32 + -4, ctx.r4.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stwx r4,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r4.u32);
	// bne cr6,0x82647254
	if (!cr6.eq) goto loc_82647254;
loc_82647290:
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x826472bc
	if (!cr6.lt) goto loc_826472BC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826472A4:
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r4,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r4.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x826472a4
	if (!cr6.eq) goto loc_826472A4;
loc_826472BC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f11,f13
	cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// bgt cr6,0x826472e0
	if (cr6.gt) goto loc_826472E0;
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
loc_826472E0:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x826473f4
	if (cr6.lt) goto loc_826473F4;
	// fdivs f13,f8,f10
	ctx.f13.f64 = double(float(ctx.f8.f64 / ctx.f10.f64));
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8264730C:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r3,r10,-1
	ctx.r3.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r3,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r3.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r29.u64);
	// lfd f12,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f11,104(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// lfd f7,96(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// lfd f6,88(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f6,f6
	ctx.f6.f64 = double(ctx.f6.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// fsubs f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 - f31.f64));
	// fsubs f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 - f31.f64));
	// fsubs f7,f7,f31
	ctx.f7.f64 = double(float(ctx.f7.f64 - f31.f64));
	// fsubs f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 - f31.f64));
	// fnmsubs f12,f12,f13,f30
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f11,f11,f13,f30
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f7,f7,f13,f30
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f6,f6,f13,f30
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f13.f64 - f30.f64)));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f11,f11,f0
	ctx.f11.f64 = ctx.f11.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fadd f6,f6,f0
	ctx.f6.f64 = ctx.f6.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// fctiwz f12,f11
	ctx.f12.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// fctiwz f11,f7
	ctx.f11.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f7,f6
	ctx.f7.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f6.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stfiwx f7,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f7.u32);
	// blt cr6,0x8264730c
	if (cr6.lt) goto loc_8264730C;
loc_826473F4:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82647444
	if (!cr6.lt) goto loc_82647444;
	// fdivs f13,f8,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f8.f64 / ctx.f10.f64));
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647404:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f12,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 - f31.f64));
	// fnmsubs f12,f12,f13,f30
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f13.f64 - f30.f64)));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// blt cr6,0x82647404
	if (cr6.lt) goto loc_82647404;
loc_82647444:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82647470
	if (!cr6.lt) goto loc_82647470;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647454:
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r4,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82647454
	if (cr6.lt) goto loc_82647454;
loc_82647470:
	// fcmpu cr6,f10,f29
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f10.f64, f29.f64);
	// bge cr6,0x82647494
	if (!cr6.lt) goto loc_82647494;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// b 0x826474b0
	goto loc_826474B0;
loc_82647494:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
loc_826474B0:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// fmuls f13,f13,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// fsubs f11,f31,f13
	ctx.f11.f64 = double(float(f31.f64 - ctx.f13.f64));
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// lfd f12,104(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fcmpu cr6,f11,f13
	cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// bgt cr6,0x826474f8
	if (cr6.gt) goto loc_826474F8;
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
loc_826474F8:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// blt cr6,0x8264757c
	if (cr6.lt) goto loc_8264757C;
	// fsubs f13,f30,f8
	ctx.f13.f64 = double(float(f30.f64 - ctx.f8.f64));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r11,r5,-4
	r11.s64 = ctx.r5.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82647540:
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r6,r6,r10
	ctx.r6.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82647540
	if (!cr6.eq) goto loc_82647540;
loc_8264757C:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826475c0
	if (!cr6.lt) goto loc_826475C0;
	// fsubs f13,f30,f8
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f30.f64 - ctx.f8.f64));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_826475A8:
	// lwz r7,28(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x826475a8
	if (!cr6.eq) goto loc_826475A8;
loc_826475C0:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// bgt cr6,0x826475e4
	if (cr6.gt) goto loc_826475E4;
	// fmr f13,f31
	ctx.f13.f64 = f31.f64;
loc_826475E4:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x8264766c
	if (cr6.lt) goto loc_8264766C;
	// fadd f13,f12,f0
	ctx.f13.f64 = ctx.f12.f64 + f0.f64;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// addi r9,r10,-4
	ctx.r9.s64 = ctx.r10.s64 + -4;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82647630:
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r7,r10,12
	ctx.r7.s64 = ctx.r10.s64 + 12;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stwx r9,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// add r6,r7,r6
	ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
	// stw r9,-4(r6)
	PPC_STORE_U32(ctx.r6.u32 + -4, ctx.r9.u32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// stwx r9,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r9.u32);
	// bne cr6,0x82647630
	if (!cr6.eq) goto loc_82647630;
loc_8264766C:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x826476ac
	if (!cr6.lt) goto loc_826476AC;
	// fadd f13,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64 + f0.f64;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_82647694:
	// lwz r7,28(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r8,r9,r7
	PPC_STORE_U32(ctx.r9.u32 + ctx.r7.u32, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x82647694
	if (!cr6.eq) goto loc_82647694;
loc_826476AC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82647700
	if (!cr6.lt) goto loc_82647700;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_826476BC:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,28(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fsubs f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 - f31.f64));
	// fmadds f13,f13,f10,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 + f30.f64));
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x826476bc
	if (cr6.lt) goto loc_826476BC;
loc_82647700:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f11,f13
	cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// bgt cr6,0x82647724
	if (cr6.gt) goto loc_82647724;
	// fmr f13,f11
	ctx.f13.f64 = ctx.f11.f64;
loc_82647724:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// li r11,0
	r11.s64 = 0;
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r6,4
	cr6.compare<int32_t>(ctx.r6.s32, 4, xer);
	// blt cr6,0x82647790
	if (cr6.lt) goto loc_82647790;
	// addi r11,r6,-4
	r11.s64 = ctx.r6.s64 + -4;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647754:
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r8,r10,12
	ctx.r8.s64 = ctx.r10.s64 + 12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stwx r4,r10,r7
	PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, ctx.r4.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r4,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r4.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r4,-4(r7)
	PPC_STORE_U32(ctx.r7.u32 + -4, ctx.r4.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stwx r4,r8,r7
	PPC_STORE_U32(ctx.r8.u32 + ctx.r7.u32, ctx.r4.u32);
	// bne cr6,0x82647754
	if (!cr6.eq) goto loc_82647754;
loc_82647790:
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// bge cr6,0x826477bc
	if (!cr6.lt) goto loc_826477BC;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_826477A4:
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stwx r4,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r4.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x826477a4
	if (!cr6.eq) goto loc_826477A4;
loc_826477BC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// lfd f13,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// bgt cr6,0x826477e0
	if (cr6.gt) goto loc_826477E0;
	// fmr f13,f31
	ctx.f13.f64 = f31.f64;
loc_826477E0:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// blt cr6,0x826478f4
	if (cr6.lt) goto loc_826478F4;
	// fdivs f13,f8,f10
	ctx.f13.f64 = double(float(ctx.f8.f64 / ctx.f10.f64));
	// addi r6,r5,-3
	ctx.r6.s64 = ctx.r5.s64 + -3;
	// addi r10,r11,2
	ctx.r10.s64 = r11.s64 + 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8264780C:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r3,r10,-1
	ctx.r3.s64 = ctx.r10.s64 + -1;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// extsw r3,r3
	ctx.r3.s64 = ctx.r3.s32;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// extsw r29,r29
	r29.s64 = r29.s32;
	// std r30,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r30.u64);
	// addi r8,r9,12
	ctx.r8.s64 = ctx.r9.s64 + 12;
	// std r3,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r3.u64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// std r29,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r29.u64);
	// lfd f12,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f11,104(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// lfd f9,96(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f7,88(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f7,f7
	ctx.f7.f64 = double(ctx.f7.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64));
	// fsubs f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 - f31.f64));
	// fsubs f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 - f31.f64));
	// fsubs f9,f9,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 - f31.f64));
	// fsubs f7,f7,f31
	ctx.f7.f64 = double(float(ctx.f7.f64 - f31.f64));
	// fnmsubs f12,f12,f13,f30
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f11,f11,f13,f30
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f9,f9,f13,f30
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f13.f64 - f30.f64)));
	// fnmsubs f7,f7,f13,f30
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f13.f64 - f30.f64)));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fadd f11,f11,f0
	ctx.f11.f64 = ctx.f11.f64 + f0.f64;
	// fadd f9,f9,f0
	ctx.f9.f64 = ctx.f9.f64 + f0.f64;
	// fadd f7,f7,f0
	ctx.f7.f64 = ctx.f7.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// fctiwz f12,f11
	ctx.f12.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	// fctiwz f11,f9
	ctx.f11.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f9.f64));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fctiwz f9,f7
	ctx.f9.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f7.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// stfiwx f12,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f12.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// blt cr6,0x8264780c
	if (cr6.lt) goto loc_8264780C;
loc_826478F4:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// bge cr6,0x82647944
	if (!cr6.lt) goto loc_82647944;
	// fdivs f13,f8,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f8.f64 / ctx.f10.f64));
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647904:
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f12,112(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fsubs f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 - f31.f64));
	// fnmsubs f12,f12,f13,f30
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f13.f64 - f30.f64)));
	// fadd f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 + f0.f64;
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// blt cr6,0x82647904
	if (cr6.lt) goto loc_82647904;
loc_82647944:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82647970
	if (!cr6.lt) goto loc_82647970;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82647954:
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r4,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82647954
	if (cr6.lt) goto loc_82647954;
loc_82647970:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f29,-56(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f30,-48(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82647988"))) PPC_WEAK_FUNC(sub_82647988);
PPC_FUNC_IMPL(__imp__sub_82647988) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r19,r6
	r19.u64 = ctx.r6.u64;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x82647f28
	if (!cr6.gt) goto loc_82647F28;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82647f28
	if (!cr6.gt) goto loc_82647F28;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x826479d0
	if (!cr6.gt) goto loc_826479D0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
loc_826479D0:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r18,308(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r20,316(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r21,324(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r24,332(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r25,340(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r26,348(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// lwz r27,356(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82647f28
	if (cr6.eq) goto loc_82647F28;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826433d0
	sub_826433D0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82647f2c
	if (!cr6.eq) goto loc_82647F2C;
	// addi r19,r19,-11
	r19.s64 = r19.s64 + -11;
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// cmplwi cr6,r19,20
	cr6.compare<uint32_t>(r19.u32, 20, xer);
	// bgt cr6,0x82647f28
	if (cr6.gt) goto loc_82647F28;
	// lis r12,-32156
	r12.s64 = -2107375616;
	// addi r12,r12,31352
	r12.s64 = r12.s64 + 31352;
	// rlwinm r0,r19,2,0,29
	r0.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r19.u64) {
	case 0:
		goto loc_82647ACC;
	case 1:
		goto loc_82647B0C;
	case 2:
		goto loc_82647F28;
	case 3:
		goto loc_82647B24;
	case 4:
		goto loc_82647B24;
	case 5:
		goto loc_82647B64;
	case 6:
		goto loc_82647B24;
	case 7:
		goto loc_82647C4C;
	case 8:
		goto loc_82647CC4;
	case 9:
		goto loc_82647C84;
	case 10:
		goto loc_82647F28;
	case 11:
		goto loc_82647F28;
	case 12:
		goto loc_82647CC4;
	case 13:
		goto loc_82647CC4;
	case 14:
		goto loc_82647F28;
	case 15:
		goto loc_82647F28;
	case 16:
		goto loc_82647CC4;
	case 17:
		goto loc_82647F28;
	case 18:
		goto loc_82647CC4;
	case 19:
		goto loc_82647D04;
	case 20:
		goto loc_82647D1C;
	default:
		__builtin_unreachable();
	}
	// lwz r19,31436(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31436);
	// lwz r19,31500(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31500);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,31524(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31524);
	// lwz r19,31524(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31524);
	// lwz r19,31588(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31588);
	// lwz r19,31524(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31524);
	// lwz r19,31820(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31820);
	// lwz r19,31940(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31940);
	// lwz r19,31876(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31876);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,31940(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31940);
	// lwz r19,31940(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31940);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,31940(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31940);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,31940(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 31940);
	// lwz r19,32004(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32004);
	// lwz r19,32028(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32028);
loc_82647ACC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f4,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82644ae0
	sub_82644AE0(ctx, base);
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647B0C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f3,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82645100
	sub_82645100(ctx, base);
	// b 0x82647d30
	goto loc_82647D30;
loc_82647B24:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f4,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82645490
	sub_82645490(ctx, base);
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647B64:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// lfs f12,-30144(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -30144);
	ctx.f12.f64 = double(temp.f32);
	// bge cr6,0x82647b88
	if (!cr6.lt) goto loc_82647B88;
	// li r11,0
	r11.s64 = 0;
	// b 0x82647ba8
	goto loc_82647BA8;
loc_82647B88:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f12.f64);
	// ble cr6,0x82647b98
	if (!cr6.gt) goto loc_82647B98;
	// li r11,255
	r11.s64 = 255;
	// b 0x82647ba8
	goto loc_82647BA8;
loc_82647B98:
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
loc_82647BA8:
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// lfs f0,4(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82647bc0
	if (!cr6.lt) goto loc_82647BC0;
	// li r11,0
	r11.s64 = 0;
	// b 0x82647be0
	goto loc_82647BE0;
loc_82647BC0:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f12.f64);
	// ble cr6,0x82647bd0
	if (!cr6.gt) goto loc_82647BD0;
	// li r11,255
	r11.s64 = 255;
	// b 0x82647be0
	goto loc_82647BE0;
loc_82647BD0:
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
loc_82647BE0:
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// lfs f0,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82647c08
	if (!cr6.lt) goto loc_82647C08;
	// li r11,0
	r11.s64 = 0;
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,48(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647C08:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f12.f64);
	// ble cr6,0x82647c28
	if (!cr6.gt) goto loc_82647C28;
	// li r11,255
	r11.s64 = 255;
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,48(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647C28:
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,48(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647C4C:
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	f0.f64 = double(temp.f32);
	// stfs f0,52(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 52, temp.u32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,56(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 56, temp.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647C84:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f4,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82645ad0
	sub_82645AD0(ctx, base);
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647CC4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f4,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82645e40
	sub_82645E40(ctx, base);
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	f0.f64 = double(temp.f32);
	// b 0x82647d54
	goto loc_82647D54;
loc_82647D04:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f3,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82646250
	sub_82646250(ctx, base);
	// b 0x82647d30
	goto loc_82647D30;
loc_82647D1C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f3,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82646d28
	sub_82646D28(ctx, base);
loc_82647D30:
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 4);
	f0.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
loc_82647D54:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// li r12,16
	r12.s64 = 16;
	// stfiwx f0,r31,r12
	PPC_STORE_U32(r31.u32 + r12.u32, f0.u32);
	// lis r12,-32156
	r12.s64 = -2107375616;
	// addi r12,r12,32120
	r12.s64 = r12.s64 + 32120;
	// rlwinm r0,r19,2,0,29
	r0.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32436(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32436);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32320(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32320);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32552(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32552);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r19,32204(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32204);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82647e10
	if (cr6.eq) goto loc_82647E10;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x82643668
	sub_82643668(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_82647E10:
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82643668
	sub_82643668(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82647e84
	if (cr6.eq) goto loc_82647E84;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x82644550
	sub_82644550(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_82647E84:
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x82644550
	sub_82644550(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82647ef8
	if (cr6.eq) goto loc_82647EF8;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x826448d0
	sub_826448D0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_82647EF8:
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// bl 0x826448d0
	sub_826448D0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
loc_82647F28:
	// li r3,7
	ctx.r3.s64 = 7;
loc_82647F2C:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd20
	return;
}

__attribute__((alias("__imp__sub_82647F34"))) PPC_WEAK_FUNC(sub_82647F34);
PPC_FUNC_IMPL(__imp__sub_82647F34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82647F38"))) PPC_WEAK_FUNC(sub_82647F38);
PPC_FUNC_IMPL(__imp__sub_82647F38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,15372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82647f94
	if (!cr6.eq) goto loc_82647F94;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,72
	ctx.r3.s64 = 72;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82647f80
	if (cr6.eq) goto loc_82647F80;
	// li r5,72
	ctx.r5.s64 = 72;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
loc_82647F80:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,15372(r31)
	PPC_STORE_U32(r31.u32 + 15372, r30.u32);
	// bne cr6,0x82647f94
	if (!cr6.eq) goto loc_82647F94;
loc_82647F8C:
	// li r3,2
	ctx.r3.s64 = 2;
	// b 0x82647fe4
	goto loc_82647FE4;
loc_82647F94:
	// lwz r11,15384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82647fb8
	if (!cr6.eq) goto loc_82647FB8;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,56
	ctx.r3.s64 = 56;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15384(r31)
	PPC_STORE_U32(r31.u32 + 15384, ctx.r3.u32);
	// beq cr6,0x82647f8c
	if (cr6.eq) goto loc_82647F8C;
loc_82647FB8:
	// lwz r11,15392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82647fe0
	if (!cr6.eq) goto loc_82647FE0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,400
	ctx.r3.s64 = 400;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,15392(r31)
	PPC_STORE_U32(r31.u32 + 15392, ctx.r3.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// beq cr6,0x82647fe4
	if (cr6.eq) goto loc_82647FE4;
loc_82647FE0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82647FE4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82647FFC"))) PPC_WEAK_FUNC(sub_82647FFC);
PPC_FUNC_IMPL(__imp__sub_82647FFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82648000"))) PPC_WEAK_FUNC(sub_82648000);
PPC_FUNC_IMPL(__imp__sub_82648000) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d5e4
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82648038
	if (!cr6.eq) goto loc_82648038;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648038:
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r9,r1,164
	ctx.r9.s64 = ctx.r1.s64 + 164;
	// addi r8,r1,184
	ctx.r8.s64 = ctx.r1.s64 + 184;
	// addi r7,r1,180
	ctx.r7.s64 = ctx.r1.s64 + 180;
	// addi r6,r1,168
	ctx.r6.s64 = ctx.r1.s64 + 168;
	// addi r5,r1,176
	ctx.r5.s64 = ctx.r1.s64 + 176;
	// addi r4,r1,172
	ctx.r4.s64 = ctx.r1.s64 + 172;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264897c
	if (!cr6.eq) goto loc_8264897C;
	// lwz r11,15304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// lwz r27,15364(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 15364);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x826480a0
	if (!cr6.eq) goto loc_826480A0;
	// addi r10,r1,152
	ctx.r10.s64 = ctx.r1.s64 + 152;
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// addi r8,r1,200
	ctx.r8.s64 = ctx.r1.s64 + 200;
	// addi r7,r1,196
	ctx.r7.s64 = ctx.r1.s64 + 196;
	// addi r6,r1,188
	ctx.r6.s64 = ctx.r1.s64 + 188;
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// addi r4,r1,204
	ctx.r4.s64 = ctx.r1.s64 + 204;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825e2f38
	sub_825E2F38(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264897c
	if (!cr6.eq) goto loc_8264897C;
loc_826480A0:
	// addi r24,r31,15388
	r24.s64 = r31.s64 + 15388;
	// lwz r8,15392(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// addi r30,r31,15380
	r30.s64 = r31.s64 + 15380;
	// lwz r6,15384(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// addi r26,r31,15376
	r26.s64 = r31.s64 + 15376;
	// addi r9,r31,15396
	ctx.r9.s64 = r31.s64 + 15396;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82642b58
	sub_82642B58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264897c
	if (!cr6.eq) goto loc_8264897C;
	// lwz r5,0(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82648204
	if (!cr6.gt) goto loc_82648204;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x826481e0
	if (cr6.eq) goto loc_826481E0;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x82648194
	if (cr6.eq) goto loc_82648194;
	// cmpwi cr6,r11,14
	cr6.compare<int32_t>(r11.s32, 14, xer);
	// beq cr6,0x82648110
	if (cr6.eq) goto loc_82648110;
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648110:
	// li r10,2
	ctx.r10.s64 = 2;
	// lwz r11,15384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// lwz r27,15404(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 15404);
	// stw r10,15304(r31)
	PPC_STORE_U32(r31.u32 + 15304, ctx.r10.u32);
	// lfs f1,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,172(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,176(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lfs f3,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,168(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// lfs f0,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,180(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f0,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f0.f64 = double(temp.f32);
	// stfs f0,184(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// lfs f6,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,164(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// lfs f0,24(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24);
	f0.f64 = double(temp.f32);
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// lfs f13,28(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,204(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f13,32(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,192(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// lfs f13,36(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,188(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// lfs f13,40(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,196(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// lfs f13,44(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,200(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// lfs f13,48(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,156(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// lfs f7,52(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 52);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,152(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// b 0x82648218
	goto loc_82648218;
loc_82648194:
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,15384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// lwz r27,15400(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 15400);
	// stw r10,15304(r31)
	PPC_STORE_U32(r31.u32 + 15304, ctx.r10.u32);
	// lfs f1,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,172(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	f0.f64 = double(temp.f32);
	// stfs f0,176(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lfs f3,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,168(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// lfs f0,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,180(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f0,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f0.f64 = double(temp.f32);
	// stfs f0,184(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// lfs f6,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,164(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// lfs f0,24(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24);
	f0.f64 = double(temp.f32);
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// b 0x82648214
	goto loc_82648214;
loc_826481E0:
	// lwz r11,15384(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15384);
	// lfs f6,164(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,152(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// b 0x82648218
	goto loc_82648218;
loc_82648204:
	// lfs f0,160(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	f0.f64 = double(temp.f32);
	// lfs f6,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
loc_82648214:
	// lfs f7,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f7.f64 = double(temp.f32);
loc_82648218:
	// lwz r6,15304(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// lwz r30,3776(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r29,3780(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// cmpwi cr6,r6,2
	cr6.compare<int32_t>(ctx.r6.s32, 2, xer);
	// lwz r28,3784(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// bne cr6,0x8264823c
	if (!cr6.eq) goto loc_8264823C;
	// lwz r30,3744(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// lwz r29,3748(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// lwz r28,3752(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3752);
loc_8264823C:
	// lis r7,-32249
	ctx.r7.s64 = -2113470464;
	// lfs f13,15408(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15408);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32249
	ctx.r8.s64 = -2113470464;
	// fcmpu cr6,f1,f13
	cr6.compare(ctx.f1.f64, ctx.f13.f64);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f31,-31368(r7)
	f31.u64 = PPC_LOAD_U64(ctx.r7.u32 + -31368);
	// lfd f30,-31360(r8)
	f30.u64 = PPC_LOAD_U64(ctx.r8.u32 + -31360);
	// lfs f29,2552(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2552);
	f29.f64 = double(temp.f32);
	// lfs f28,30752(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 30752);
	f28.f64 = double(temp.f32);
	// lfs f27,2480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2480);
	f27.f64 = double(temp.f32);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15412(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15412);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15416(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15416);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f3,f13
	cr6.compare(ctx.f3.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15420(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15420);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15424(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15424);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15428(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15428);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f6,f13
	cr6.compare(ctx.f6.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lfs f13,15432(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15432);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// lwz r11,288(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826482e0
	if (cr6.eq) goto loc_826482E0;
	// cmpw cr6,r25,r6
	cr6.compare<int32_t>(r25.s32, ctx.r6.s32, xer);
	// bne cr6,0x826482e0
	if (!cr6.eq) goto loc_826482E0;
	// cmpwi cr6,r5,18
	cr6.compare<int32_t>(ctx.r5.s32, 18, xer);
	// bne cr6,0x82648398
	if (!cr6.eq) goto loc_82648398;
loc_826482E0:
	// cmpwi cr6,r5,18
	cr6.compare<int32_t>(ctx.r5.s32, 18, xer);
	// bne cr6,0x82648318
	if (!cr6.eq) goto loc_82648318;
	// lwz r11,15392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// lfs f13,16(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f27
	cr6.compare(ctx.f13.f64, f27.f64);
	// bne cr6,0x82648304
	if (!cr6.eq) goto loc_82648304;
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f28
	cr6.compare(ctx.f12.f64, f28.f64);
	// ble cr6,0x82648398
	if (!cr6.gt) goto loc_82648398;
loc_82648304:
	// fcmpu cr6,f13,f29
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, f29.f64);
	// bne cr6,0x82648318
	if (!cr6.eq) goto loc_82648318;
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f28
	cr6.compare(ctx.f13.f64, f28.f64);
	// bgt cr6,0x82648398
	if (cr6.gt) goto loc_82648398;
loc_82648318:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82648334
	if (!cr6.eq) goto loc_82648334;
	// fmr f5,f30
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// b 0x82648358
	goto loc_82648358;
loc_82648334:
	// cmpwi cr6,r27,1
	cr6.compare<int32_t>(r27.s32, 1, xer);
	// bne cr6,0x8264834c
	if (!cr6.eq) goto loc_8264834C;
	// fmr f4,f31
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f5,f1
	ctx.f5.f64 = ctx.f1.f64;
	// b 0x82648358
	goto loc_82648358;
loc_8264834C:
	// lfs f5,184(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f2.f64 = double(temp.f32);
loc_82648358:
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3728);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3724);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3720);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fmr f7,f0
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = f0.f64;
	// bl 0x826429f8
	sub_826429F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264897c
	if (!cr6.eq) goto loc_8264897C;
	// lfs f0,160(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	f0.f64 = double(temp.f32);
	// lfs f6,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f7.f64 = double(temp.f32);
loc_82648398:
	// lfs f13,176(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,15304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// stfs f13,15412(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 15412, temp.u32);
	// lfs f13,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f13.f64 = double(temp.f32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// stfs f13,15420(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 15420, temp.u32);
	// lfs f13,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f13.f64 = double(temp.f32);
	// stfs f1,15408(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r31.u32 + 15408, temp.u32);
	// stfs f3,15416(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(r31.u32 + 15416, temp.u32);
	// stfs f13,15424(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 15424, temp.u32);
	// stfs f6,15428(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r31.u32 + 15428, temp.u32);
	// stfs f0,15432(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15432, temp.u32);
	// bne cr6,0x8264853c
	if (!cr6.eq) goto loc_8264853C;
	// lfs f0,15436(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15436);
	f0.f64 = double(temp.f32);
	// lfs f1,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f3.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15440(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15440);
	f0.f64 = double(temp.f32);
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15444(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15444);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f3,f0
	cr6.compare(ctx.f3.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15448(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15448);
	f0.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15452(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15452);
	f0.f64 = double(temp.f32);
	// lfs f13,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15456(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15456);
	f0.f64 = double(temp.f32);
	// lfs f13,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lfs f0,15460(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 15460);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f7,f0
	cr6.compare(ctx.f7.f64, f0.f64);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lwz r11,288(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82648458
	if (cr6.eq) goto loc_82648458;
	// cmpwi cr6,r25,2
	cr6.compare<int32_t>(r25.s32, 2, xer);
	// bne cr6,0x82648458
	if (!cr6.eq) goto loc_82648458;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r11,18
	cr6.compare<int32_t>(r11.s32, 18, xer);
	// bne cr6,0x82648508
	if (!cr6.eq) goto loc_82648508;
loc_82648458:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmpwi cr6,r11,18
	cr6.compare<int32_t>(r11.s32, 18, xer);
	// bne cr6,0x82648494
	if (!cr6.eq) goto loc_82648494;
	// lwz r11,15392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// lfs f0,16(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f27
	cr6.compare(f0.f64, f27.f64);
	// bne cr6,0x82648480
	if (!cr6.eq) goto loc_82648480;
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f28
	cr6.compare(ctx.f13.f64, f28.f64);
	// bgt cr6,0x82648508
	if (cr6.gt) goto loc_82648508;
loc_82648480:
	// fcmpu cr6,f0,f29
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f29.f64);
	// bne cr6,0x82648494
	if (!cr6.eq) goto loc_82648494;
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// ble cr6,0x82648508
	if (!cr6.gt) goto loc_82648508;
loc_82648494:
	// lwz r11,15364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15364);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826484b4
	if (!cr6.eq) goto loc_826484B4;
	// fmr f5,f30
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// b 0x826484d8
	goto loc_826484D8;
loc_826484B4:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826484cc
	if (!cr6.eq) goto loc_826484CC;
	// fmr f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f1.f64;
	// fmr f4,f31
	ctx.f4.f64 = f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// b 0x826484d8
	goto loc_826484D8;
loc_826484CC:
	// lfs f5,200(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f2.f64 = double(temp.f32);
loc_826484D8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f6,156(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f6.f64 = double(temp.f32);
	// lwz r9,3796(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// lwz r8,3792(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// lwz r7,3788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// bl 0x826429f8
	sub_826429F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264897c
	if (!cr6.eq) goto loc_8264897C;
	// lfs f7,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f7.f64 = double(temp.f32);
loc_82648508:
	// lfs f0,204(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	f0.f64 = double(temp.f32);
	// stfs f0,15436(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15436, temp.u32);
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	f0.f64 = double(temp.f32);
	// stfs f0,15440(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15440, temp.u32);
	// lfs f0,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	f0.f64 = double(temp.f32);
	// stfs f0,15444(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15444, temp.u32);
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	f0.f64 = double(temp.f32);
	// stfs f0,15448(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15448, temp.u32);
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	f0.f64 = double(temp.f32);
	// stfs f0,15452(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15452, temp.u32);
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	f0.f64 = double(temp.f32);
	// stfs f0,15456(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 15456, temp.u32);
	// stfs f7,15460(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r31.u32 + 15460, temp.u32);
loc_8264853C:
	// lwz r6,0(r26)
	ctx.r6.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// addi r11,r6,-11
	r11.s64 = ctx.r6.s64 + -11;
	// cmplwi cr6,r11,20
	cr6.compare<uint32_t>(r11.u32, 20, xer);
	// bgt cr6,0x82648920
	if (cr6.gt) goto loc_82648920;
	// lis r12,-32155
	r12.s64 = -2107310080;
	// addi r12,r12,-31388
	r12.s64 = r12.s64 + -31388;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_826485B8;
	case 1:
		goto loc_826485B8;
	case 2:
		goto loc_82648920;
	case 3:
		goto loc_826485B8;
	case 4:
		goto loc_826485B8;
	case 5:
		goto loc_826485B8;
	case 6:
		goto loc_826485B8;
	case 7:
		goto loc_826485B8;
	case 8:
		goto loc_826485B8;
	case 9:
		goto loc_826485B8;
	case 10:
		goto loc_82648628;
	case 11:
		goto loc_82648920;
	case 12:
		goto loc_826485B8;
	case 13:
		goto loc_826485B8;
	case 14:
		goto loc_82648920;
	case 15:
		goto loc_82648920;
	case 16:
		goto loc_826485B8;
	case 17:
		goto loc_82648920;
	case 18:
		goto loc_826485B8;
	case 19:
		goto loc_826485B8;
	case 20:
		goto loc_826485B8;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-30432(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -30432);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31192(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31192);
	// lwz r19,-30432(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -30432);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-30432(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -30432);
	// lwz r19,-30432(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -30432);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-30432(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -30432);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
	// lwz r19,-31304(r4)
	r19.u64 = PPC_LOAD_U32(ctx.r4.u32 + -31304);
loc_826485B8:
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r28,3752(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 3752);
	// lwz r27,3748(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// lwz r26,3744(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// lwz r25,3796(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// lwz r10,3792(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// lwz r9,3788(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// lwz r8,15392(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// lwz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r5,15312(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r4,15308(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r3,15372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15372);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// stw r30,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r30.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
	// stw r28,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r28.u32);
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r27.u32);
	// stw r26,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r26.u32);
	// stw r25,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r25.u32);
	// bl 0x82647988
	sub_82647988(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82648978
	if (cr6.eq) goto loc_82648978;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648628:
	// lwz r11,15392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15392);
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// addi r9,r1,148
	ctx.r9.s64 = ctx.r1.s64 + 148;
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fctiwz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// fcmpu cr6,f11,f27
	cr6.compare(ctx.f11.f64, f27.f64);
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// stfiwx f12,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f12.u32);
	// bne cr6,0x82648684
	if (!cr6.eq) goto loc_82648684;
	// lwz r5,3788(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// lwz r6,3792(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// lwz r7,3796(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// lwz r8,3744(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// lwz r9,3748(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// lwz r10,3752(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3752);
	// b 0x8264869c
	goto loc_8264869C;
loc_82648684:
	// lwz r5,3744(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3744);
	// lwz r6,3748(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3748);
	// lwz r7,3752(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 3752);
	// lwz r8,3788(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// lwz r9,3792(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// lwz r10,3796(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3796);
loc_8264869C:
	// lwz r11,208(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x826486f0
	if (!cr6.eq) goto loc_826486F0;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82672808
	sub_82672808(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_826486F0:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82648740
	if (!cr6.eq) goto loc_82648740;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82673570
	sub_82673570(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648740:
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x82648790
	if (!cr6.eq) goto loc_82648790;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82672ef8
	sub_82672EF8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648790:
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x826487e0
	if (!cr6.eq) goto loc_826487E0;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82672130
	sub_82672130(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_826487E0:
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// bne cr6,0x82648830
	if (!cr6.eq) goto loc_82648830;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82671ac8
	sub_82671AC8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648830:
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82648880
	if (!cr6.eq) goto loc_82648880;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x82671878
	sub_82671878(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648880:
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x826488d0
	if (!cr6.eq) goto loc_826488D0;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x826714b8
	sub_826714B8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_826488D0:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bne cr6,0x82648978
	if (!cr6.eq) goto loc_82648978;
	// lwz r11,3784(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r30,3780(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// lwz r29,3776(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// lwz r4,15312(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 15312);
	// lwz r3,15308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15308);
	// lwz r31,144(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// lwz r31,148(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// bl 0x826710f8
	sub_826710F8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
loc_82648920:
	// lwz r11,15304(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15304);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82648978
	if (!cr6.eq) goto loc_82648978;
	// lwz r11,15332(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15332);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,15328(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 15328);
	// lwz r5,3776(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3776);
	// mullw r30,r11,r10
	r30.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r4,3788(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3788);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// bl 0x82641fe0
	sub_82641FE0(ctx, base);
	// srawi r30,r30,2
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3) != 0);
	r30.s64 = r30.s32 >> 2;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,3780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3780);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r4,3792(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3792);
	// bl 0x82642140
	sub_82642140(ctx, base);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r5,3784(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 3784);
	// lwz r4,3796(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3796);
	// bl 0x82642140
	sub_82642140(ctx, base);
loc_82648978:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8264897C:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-72
	r12.s64 = ctx.r1.s64 + -72;
	// bl 0x8239d630
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_8264898C"))) PPC_WEAK_FUNC(sub_8264898C);
PPC_FUNC_IMPL(__imp__sub_8264898C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82648990"))) PPC_WEAK_FUNC(sub_82648990);
PPC_FUNC_IMPL(__imp__sub_82648990) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,1
	r11.s64 = 1;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,15656(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15656);
	// stw r11,3360(r31)
	PPC_STORE_U32(r31.u32 + 3360, r11.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826489c8
	if (cr6.eq) goto loc_826489C8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15656(r31)
	PPC_STORE_U32(r31.u32 + 15656, r30.u32);
loc_826489C8:
	// lwz r3,15664(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15664);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826489dc
	if (cr6.eq) goto loc_826489DC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15664(r31)
	PPC_STORE_U32(r31.u32 + 15664, r30.u32);
loc_826489DC:
	// lwz r3,15660(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15660);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x826489f0
	if (cr6.eq) goto loc_826489F0;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15660(r31)
	PPC_STORE_U32(r31.u32 + 15660, r30.u32);
loc_826489F0:
	// lwz r3,15668(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15668);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648a04
	if (cr6.eq) goto loc_82648A04;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15668(r31)
	PPC_STORE_U32(r31.u32 + 15668, r30.u32);
loc_82648A04:
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82648a4c
	if (cr6.eq) goto loc_82648A4C;
	// lwz r11,16472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x82648a4c
	if (cr6.eq) goto loc_82648A4C;
	// stw r30,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, r30.u32);
	// stw r30,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, r30.u32);
	// stw r30,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, r30.u32);
	// stw r30,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, r30.u32);
	// stw r30,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, r30.u32);
	// stw r30,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, r30.u32);
	// stw r30,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, r30.u32);
	// stw r30,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, r30.u32);
	// stw r30,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, r30.u32);
	// stw r30,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, r30.u32);
	// stw r30,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, r30.u32);
	// b 0x82648b38
	goto loc_82648B38;
loc_82648A4C:
	// lwz r3,15672(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15672);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648a60
	if (cr6.eq) goto loc_82648A60;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, r30.u32);
loc_82648A60:
	// lwz r3,15680(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15680);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648a74
	if (cr6.eq) goto loc_82648A74;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, r30.u32);
loc_82648A74:
	// lwz r3,15688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15688);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648a88
	if (cr6.eq) goto loc_82648A88;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, r30.u32);
loc_82648A88:
	// lwz r3,15696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15696);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648a9c
	if (cr6.eq) goto loc_82648A9C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, r30.u32);
loc_82648A9C:
	// lwz r3,15704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15704);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648ab0
	if (cr6.eq) goto loc_82648AB0;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, r30.u32);
loc_82648AB0:
	// lwz r3,15712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15712);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648ac4
	if (cr6.eq) goto loc_82648AC4;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, r30.u32);
loc_82648AC4:
	// lwz r3,15720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15720);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648ad8
	if (cr6.eq) goto loc_82648AD8;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, r30.u32);
loc_82648AD8:
	// lwz r3,15728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15728);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648aec
	if (cr6.eq) goto loc_82648AEC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, r30.u32);
loc_82648AEC:
	// lwz r3,15736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15736);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648b00
	if (cr6.eq) goto loc_82648B00;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, r30.u32);
loc_82648B00:
	// lwz r3,15744(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15744);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648b14
	if (cr6.eq) goto loc_82648B14;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, r30.u32);
loc_82648B14:
	// lwz r3,15752(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15752);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648b28
	if (cr6.eq) goto loc_82648B28;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, r30.u32);
loc_82648B28:
	// lwz r3,15760(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 15760);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82648b3c
	if (cr6.eq) goto loc_82648B3C;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82648B38:
	// stw r30,15760(r31)
	PPC_STORE_U32(r31.u32 + 15760, r30.u32);
loc_82648B3C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82648B54"))) PPC_WEAK_FUNC(sub_82648B54);
PPC_FUNC_IMPL(__imp__sub_82648B54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82648B58"))) PPC_WEAK_FUNC(sub_82648B58);
PPC_FUNC_IMPL(__imp__sub_82648B58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r10,3356(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 3356);
	// lwz r8,188(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 188);
	// lwz r5,200(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 200);
	// twllei r10,0
	// lwz r4,140(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// twllei r10,0
	// lwz r3,136(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// twllei r10,0
	// stw r9,3820(r11)
	PPC_STORE_U32(r11.u32 + 3820, ctx.r9.u32);
	// twllei r10,0
	// stw r9,3828(r11)
	PPC_STORE_U32(r11.u32 + 3828, ctx.r9.u32);
	// divwu r9,r8,r10
	ctx.r9.u32 = ctx.r8.u32 / ctx.r10.u32;
	// divwu r8,r5,r10
	ctx.r8.u32 = ctx.r5.u32 / ctx.r10.u32;
	// lwz r7,220(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 220);
	// lwz r6,224(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 224);
	// divwu r5,r4,r10
	ctx.r5.u32 = ctx.r4.u32 / ctx.r10.u32;
	// divwu r31,r3,r10
	r31.u32 = ctx.r3.u32 / ctx.r10.u32;
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// stw r9,3824(r11)
	PPC_STORE_U32(r11.u32 + 3824, ctx.r9.u32);
	// stw r7,3836(r11)
	PPC_STORE_U32(r11.u32 + 3836, ctx.r7.u32);
	// stw r6,3840(r11)
	PPC_STORE_U32(r11.u32 + 3840, ctx.r6.u32);
	// stw r8,3832(r11)
	PPC_STORE_U32(r11.u32 + 3832, ctx.r8.u32);
	// stw r5,3812(r11)
	PPC_STORE_U32(r11.u32 + 3812, ctx.r5.u32);
	// stw r31,3816(r11)
	PPC_STORE_U32(r11.u32 + 3816, r31.u32);
	// blt cr6,0x82648c4c
	if (cr6.lt) goto loc_82648C4C;
	// lwz r30,204(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 204);
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// lwz r28,208(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 208);
	// mullw r29,r30,r9
	r29.s64 = int64_t(r30.s32) * int64_t(ctx.r9.s32);
	// stw r9,3856(r11)
	PPC_STORE_U32(r11.u32 + 3856, ctx.r9.u32);
	// stw r8,3864(r11)
	PPC_STORE_U32(r11.u32 + 3864, ctx.r8.u32);
	// stw r5,3844(r11)
	PPC_STORE_U32(r11.u32 + 3844, ctx.r5.u32);
	// mullw r30,r28,r8
	r30.s64 = int64_t(r28.s32) * int64_t(ctx.r8.s32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r28,r8,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r29,r7
	ctx.r7.u64 = r29.u64 + ctx.r7.u64;
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// stw r9,3860(r11)
	PPC_STORE_U32(r11.u32 + 3860, ctx.r9.u32);
	// stw r28,3868(r11)
	PPC_STORE_U32(r11.u32 + 3868, r28.u32);
	// stw r7,3872(r11)
	PPC_STORE_U32(r11.u32 + 3872, ctx.r7.u32);
	// stw r6,3876(r11)
	PPC_STORE_U32(r11.u32 + 3876, ctx.r6.u32);
	// bne cr6,0x82648c24
	if (!cr6.eq) goto loc_82648C24;
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,3848(r11)
	PPC_STORE_U32(r11.u32 + 3848, ctx.r10.u32);
	// stw r9,3852(r11)
	PPC_STORE_U32(r11.u32 + 3852, ctx.r9.u32);
	// b 0x82648c2c
	goto loc_82648C2C;
loc_82648C24:
	// stw r4,3848(r11)
	PPC_STORE_U32(r11.u32 + 3848, ctx.r4.u32);
	// stw r3,3852(r11)
	PPC_STORE_U32(r11.u32 + 3852, ctx.r3.u32);
loc_82648C2C:
	// lwz r10,204(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 204);
	// lwz r9,208(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 208);
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r9,r5
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,15168(r11)
	PPC_STORE_U32(r11.u32 + 15168, ctx.r10.u32);
	// stw r9,15172(r11)
	PPC_STORE_U32(r11.u32 + 15172, ctx.r9.u32);
loc_82648C4C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82648C54"))) PPC_WEAK_FUNC(sub_82648C54);
PPC_FUNC_IMPL(__imp__sub_82648C54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82648C58"))) PPC_WEAK_FUNC(sub_82648C58);
PPC_FUNC_IMPL(__imp__sub_82648C58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// li r31,0
	r31.s64 = 0;
	// lwz r11,3356(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// lwz r30,188(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lwz r29,200(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// twllei r11,0
	// lwz r28,136(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// twllei r11,0
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// twllei r11,0
	// stw r31,3820(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3820, r31.u32);
	// twllei r11,0
	// stw r31,3828(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3828, r31.u32);
	// divwu r31,r30,r11
	r31.u32 = r30.u32 / r11.u32;
	// divwu r30,r29,r11
	r30.u32 = r29.u32 / r11.u32;
	// lwz r10,220(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// divwu r29,r28,r11
	r29.u32 = r28.u32 / r11.u32;
	// lwz r4,3732(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// divwu r11,r8,r11
	r11.u32 = ctx.r8.u32 / r11.u32;
	// lwz r5,3720(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3720);
	// lwz r9,224(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lwz r6,3724(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3724);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lwz r7,3728(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3728);
	// stw r10,3836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3836, ctx.r10.u32);
	// stw r11,3812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3812, r11.u32);
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// stw r31,3824(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3824, r31.u32);
	// stw r30,3832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3832, r30.u32);
	// stw r29,3816(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3816, r29.u32);
	// stw r9,3840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3840, ctx.r9.u32);
	// stw r8,3848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3848, ctx.r8.u32);
	// stw r4,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, ctx.r4.u32);
	// stw r5,3800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3800, ctx.r5.u32);
	// stw r11,3804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3804, r11.u32);
	// stw r10,3808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3808, ctx.r10.u32);
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82648CF8"))) PPC_WEAK_FUNC(sub_82648CF8);
PPC_FUNC_IMPL(__imp__sub_82648CF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r4,15
	r11.s64 = ctx.r4.s64 + 15;
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r29,r11,0,0,27
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// addi r11,r5,15
	r11.s64 = ctx.r5.s64 + 15;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// srawi r26,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r26.s64 = r29.s32 >> 1;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// srawi r25,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r25.s64 = r29.s32 >> 4;
	// lwz r10,3356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3356);
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// mullw r9,r8,r9
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r3,r9,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// divwu r27,r11,r10
	r27.u32 = r11.u32 / ctx.r10.u32;
	// twllei r10,0
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mullw r3,r10,r9
	ctx.r3.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// stw r11,15656(r31)
	PPC_STORE_U32(r31.u32 + 15656, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// lwz r11,15656(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15656);
	// stw r3,15664(r31)
	PPC_STORE_U32(r31.u32 + 15664, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// mullw r3,r10,r9
	ctx.r3.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// stw r11,15660(r31)
	PPC_STORE_U32(r31.u32 + 15660, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// lwz r11,15660(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15660);
	// stw r3,15668(r31)
	PPC_STORE_U32(r31.u32 + 15668, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,23968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23968);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82648e4c
	if (cr6.eq) goto loc_82648E4C;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16472);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x82648e4c
	if (cr6.eq) goto loc_82648E4C;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r10,15672(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15672);
	// stw r10,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, ctx.r10.u32);
	// lwz r10,15680(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15680);
	// stw r10,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, ctx.r10.u32);
	// lwz r10,15688(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15688);
	// stw r10,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, ctx.r10.u32);
	// lwz r10,15696(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15696);
	// stw r10,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, ctx.r10.u32);
	// lwz r10,15704(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15704);
	// stw r10,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, ctx.r10.u32);
	// lwz r10,15712(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15712);
	// stw r10,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, ctx.r10.u32);
	// lwz r10,15720(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15720);
	// stw r10,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, ctx.r10.u32);
	// lwz r10,15728(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15728);
	// stw r10,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, ctx.r10.u32);
	// lwz r10,15736(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15736);
	// stw r10,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, ctx.r10.u32);
	// lwz r10,15744(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15744);
	// stw r10,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, ctx.r10.u32);
	// lwz r10,15752(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 15752);
	// stw r10,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, ctx.r10.u32);
	// lwz r11,15760(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 15760);
	// stw r11,15760(r31)
	PPC_STORE_U32(r31.u32 + 15760, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82648E4C:
	// addi r11,r29,31
	r11.s64 = r29.s64 + 31;
	// rlwinm r30,r27,1,0,30
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r29,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r29.s64 = r11.s32 >> 5;
	// addi r28,r30,-1
	r28.s64 = r30.s64 + -1;
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r3,r29,r28
	ctx.r3.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// stw r3,15672(r31)
	PPC_STORE_U32(r31.u32 + 15672, ctx.r3.u32);
	// mullw r11,r29,r27
	r11.s64 = int64_t(r29.s32) * int64_t(r27.s32);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// addi r11,r26,31
	r11.s64 = r26.s64 + 31;
	// rlwinm r10,r28,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r3,15680(r31)
	PPC_STORE_U32(r31.u32 + 15680, ctx.r3.u32);
	// srawi r29,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r29.s64 = r11.s32 >> 5;
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r28,r10,r29
	r28.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// rlwinm r11,r30,31,1,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r3,15688(r31)
	PPC_STORE_U32(r31.u32 + 15688, ctx.r3.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r30,r11,r29
	r30.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,15696(r31)
	PPC_STORE_U32(r31.u32 + 15696, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,15704(r31)
	PPC_STORE_U32(r31.u32 + 15704, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// lwz r11,15672(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15672);
	// stw r3,15712(r31)
	PPC_STORE_U32(r31.u32 + 15712, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15680(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15680);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15688);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15696(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15696);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15704);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// rlwinm r29,r27,4,0,27
	r29.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r30,r25,1,0,30
	r30.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r29,31
	r11.s64 = r29.s64 + 31;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// srawi r28,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r28.s64 = r11.s32 >> 5;
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r3,r10,r28
	ctx.r3.s64 = int64_t(ctx.r10.s32) * int64_t(r28.s32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// stw r3,15720(r31)
	PPC_STORE_U32(r31.u32 + 15720, ctx.r3.u32);
	// mullw r11,r28,r25
	r11.s64 = int64_t(r28.s32) * int64_t(r25.s32);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// srawi r11,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r11.s64 = r29.s32 >> 1;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// stw r3,15728(r31)
	PPC_STORE_U32(r31.u32 + 15728, ctx.r3.u32);
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// li r4,0
	ctx.r4.s64 = 0;
	// srawi r29,r11,5
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1F) != 0);
	r29.s64 = r11.s32 >> 5;
	// srawi r11,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r11.s64 = ctx.r10.s32 >> 1;
	// mullw r28,r11,r29
	r28.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// stw r3,15736(r31)
	PPC_STORE_U32(r31.u32 + 15736, ctx.r3.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r30,r11,r29
	r30.s64 = int64_t(r11.s32) * int64_t(r29.s32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,15744(r31)
	PPC_STORE_U32(r31.u32 + 15744, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,15752(r31)
	PPC_STORE_U32(r31.u32 + 15752, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// lwz r11,15720(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15720);
	// stw r3,15760(r31)
	PPC_STORE_U32(r31.u32 + 15760, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15728(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15728);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15736(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15736);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15744(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15744);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// lwz r11,15752(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15752);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82649014
	if (cr6.eq) goto loc_82649014;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82649014:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82649020"))) PPC_WEAK_FUNC(sub_82649020);
PPC_FUNC_IMPL(__imp__sub_82649020) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// li r5,252
	ctx.r5.s64 = 252;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1756(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1756);
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,0,27,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649100
	if (cr6.eq) goto loc_82649100;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r9,r30,r29
	ctx.r9.s64 = r29.s64 - r30.s64;
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// li r29,0
	r29.s64 = 0;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// li r10,7
	ctx.r10.s64 = 7;
	// beq cr6,0x826490c4
	if (cr6.eq) goto loc_826490C4;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
loc_8264908C:
	// sth r29,16(r11)
	PPC_STORE_U16(r11.u32 + 16, r29.u16);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x8264908c
	if (!cr6.eq) goto loc_8264908C;
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826490bc
	if (cr6.eq) goto loc_826490BC;
	// lwz r7,1820(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1820);
	// b 0x82649154
	goto loc_82649154;
loc_826490BC:
	// lwz r7,1804(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1804);
	// b 0x82649154
	goto loc_82649154;
loc_826490C4:
	// addi r11,r30,18
	r11.s64 = r30.s64 + 18;
loc_826490C8:
	// lhzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// sth r29,-16(r11)
	PPC_STORE_U16(r11.u32 + -16, r29.u16);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne cr6,0x826490c8
	if (!cr6.eq) goto loc_826490C8;
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826490f8
	if (cr6.eq) goto loc_826490F8;
	// lwz r7,1816(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1816);
	// b 0x82649154
	goto loc_82649154;
loc_826490F8:
	// lwz r7,1808(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1808);
	// b 0x82649154
	goto loc_82649154;
loc_82649100:
	// li r29,0
	r29.s64 = 0;
	// sth r29,2(r30)
	PPC_STORE_U16(r30.u32 + 2, r29.u16);
	// sth r29,18(r30)
	PPC_STORE_U16(r30.u32 + 18, r29.u16);
	// sth r29,4(r30)
	PPC_STORE_U16(r30.u32 + 4, r29.u16);
	// sth r29,20(r30)
	PPC_STORE_U16(r30.u32 + 20, r29.u16);
	// sth r29,6(r30)
	PPC_STORE_U16(r30.u32 + 6, r29.u16);
	// sth r29,22(r30)
	PPC_STORE_U16(r30.u32 + 22, r29.u16);
	// sth r29,8(r30)
	PPC_STORE_U16(r30.u32 + 8, r29.u16);
	// sth r29,24(r30)
	PPC_STORE_U16(r30.u32 + 24, r29.u16);
	// sth r29,10(r30)
	PPC_STORE_U16(r30.u32 + 10, r29.u16);
	// sth r29,26(r30)
	PPC_STORE_U16(r30.u32 + 26, r29.u16);
	// sth r29,12(r30)
	PPC_STORE_U16(r30.u32 + 12, r29.u16);
	// sth r29,28(r30)
	PPC_STORE_U16(r30.u32 + 28, r29.u16);
	// sth r29,14(r30)
	PPC_STORE_U16(r30.u32 + 14, r29.u16);
	// sth r29,30(r30)
	PPC_STORE_U16(r30.u32 + 30, r29.u16);
	// lwz r11,1796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1796);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82649150
	if (cr6.eq) goto loc_82649150;
	// lwz r7,1812(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1812);
	// b 0x82649154
	goto loc_82649154;
loc_82649150:
	// lwz r7,1800(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1800);
loc_82649154:
	// lwz r11,15472(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 15472);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// blt cr6,0x8264917c
	if (cr6.lt) goto loc_8264917C;
	// add r11,r26,r27
	r11.u64 = r26.u64 + r27.u64;
	// lbz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// bl 0x826551b8
	sub_826551B8(ctx, base);
	// b 0x82649198
	goto loc_82649198;
loc_8264917C:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// add r11,r26,r27
	r11.u64 = r26.u64 + r27.u64;
	// lbz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// beq cr6,0x82649194
	if (cr6.eq) goto loc_82649194;
	// bl 0x826551b8
	sub_826551B8(ctx, base);
	// b 0x82649198
	goto loc_82649198;
loc_82649194:
	// bl 0x825fb528
	sub_825FB528(ctx, base);
loc_82649198:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x826494b8
	if (!cr6.eq) goto loc_826494B8;
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264949c
	if (cr6.eq) goto loc_8264949C;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_826491B4:
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// addi r9,r11,12
	ctx.r9.s64 = r11.s64 + 12;
	// lwz r6,1884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// addi r8,r10,6
	ctx.r8.s64 = ctx.r10.s64 + 6;
	// lwzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
	// sthx r7,r6,r10
	PPC_STORE_U16(ctx.r6.u32 + ctx.r10.u32, ctx.r7.u16);
	// lwz r6,1760(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lwz r7,1884(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// cmpwi cr6,r11,256
	cr6.compare<int32_t>(r11.s32, 256, xer);
	// lwz r6,4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// sth r6,2(r7)
	PPC_STORE_U16(ctx.r7.u32 + 2, ctx.r6.u16);
	// lwz r6,1760(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lwz r7,1884(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r6,-4(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	// sth r6,-2(r7)
	PPC_STORE_U16(ctx.r7.u32 + -2, ctx.r6.u16);
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// lwz r6,1884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// lwzx r9,r7,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// sthx r9,r6,r8
	PPC_STORE_U16(ctx.r6.u32 + ctx.r8.u32, ctx.r9.u16);
	// blt cr6,0x826491b4
	if (cr6.lt) goto loc_826491B4;
	// lwz r4,1884(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r6,255
	ctx.r6.s64 = 255;
	// lwz r11,3180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3180);
	// li r5,8
	ctx.r5.s64 = 8;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,2968(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2968);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x826492bc
	if (cr6.eq) goto loc_826492BC;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x826492bc
	if (cr6.eq) goto loc_826492BC;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,0,20,20
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x800;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x826492bc
	if (!cr6.eq) goto loc_826492BC;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// li r7,8
	ctx.r7.s64 = 8;
loc_82649264:
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// li r9,128
	ctx.r9.s64 = 128;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82649274:
	// stb r9,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bdnz 0x82649274
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82649274;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82649264
	if (!cr6.eq) goto loc_82649264;
	// lwz r7,260(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 260);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// lwz r5,1884(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// lwz r11,3148(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3148);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_826492BC:
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// bge cr6,0x826492f8
	if (!cr6.lt) goto loc_826492F8;
	// rlwinm r10,r27,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x2;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// clrlwi r11,r27,31
	r11.u64 = r27.u32 & 0x1;
	// addi r8,r10,745
	ctx.r8.s64 = ctx.r10.s64 + 745;
	// lwz r10,252(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r9,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// b 0x82649320
	goto loc_82649320;
loc_826492F8:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpwi cr6,r27,4
	cr6.compare<int32_t>(r27.s32, 4, xer);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bne cr6,0x82649310
	if (!cr6.eq) goto loc_82649310;
	// lwz r11,2992(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// b 0x82649314
	goto loc_82649314;
loc_82649310:
	// lwz r11,3000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3000);
loc_82649314:
	// lwz r9,252(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_82649320:
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82649330:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x82649330
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82649330;
	// lwz r8,1884(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// li r6,8
	ctx.r6.s64 = 8;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + r11.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_8264935C:
	// lhz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// sth r6,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r6.u16);
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// bdnz 0x8264935c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264935C;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r7,8
	ctx.r7.s64 = 8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82649388:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x82649388
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82649388;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r7,8
	ctx.r7.s64 = 8;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,48
	ctx.r9.s64 = ctx.r9.s64 + 48;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_826493BC:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x826493bc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826493BC;
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// li r7,8
	ctx.r7.s64 = 8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_826493E8:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x826493e8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_826493E8;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r7,8
	ctx.r7.s64 = 8;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,80
	ctx.r9.s64 = ctx.r9.s64 + 80;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264941C:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x8264941c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264941C;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1884(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r7,8
	ctx.r7.s64 = 8;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r9,r9,96
	ctx.r9.s64 = ctx.r9.s64 + 96;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82649450:
	// lhz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// sth r7,0(r8)
	PPC_STORE_U16(ctx.r8.u32 + 0, ctx.r7.u16);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// bdnz 0x82649450
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82649450;
	// mulli r10,r10,14
	ctx.r10.s64 = ctx.r10.s64 * 14;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,1884(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1884);
	// li r9,8
	ctx.r9.s64 = 8;
	// addi r11,r11,112
	r11.s64 = r11.s64 + 112;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8264947C:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bdnz 0x8264947c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264947C;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
loc_8264949C:
	// lwz r5,1760(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r11,3160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3160);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_826494B8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826494C0"))) PPC_WEAK_FUNC(sub_826494C0);
PPC_FUNC_IMPL(__imp__sub_826494C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r7,4
	r11.s64 = ctx.r7.s64 + 4;
	// lwz r9,1936(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1936);
	// addi r7,r6,2
	ctx.r7.s64 = ctx.r6.s64 + 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r4
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// lwzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	// lwzx r3,r7,r4
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r7,r7,r11
	ctx.r7.s64 = r11.s64 - ctx.r7.s64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r4,r11,r5
	ctx.r4.u64 = r11.u64 ^ ctx.r5.u64;
	// subf r11,r6,r7
	r11.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r7,r5,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x82649538
	if (!cr6.lt) goto loc_82649538;
	// li r11,0
	r11.s64 = 0;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// blr 
	return;
loc_82649538:
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82649544"))) PPC_WEAK_FUNC(sub_82649544);
PPC_FUNC_IMPL(__imp__sub_82649544) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82649548"))) PPC_WEAK_FUNC(sub_82649548);
PPC_FUNC_IMPL(__imp__sub_82649548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,248(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 248);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// blt cr6,0x826495f4
	if (cr6.lt) goto loc_826495F4;
	// lwz r11,284(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82649578
	if (cr6.eq) goto loc_82649578;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x82649584
	if (!cr6.eq) goto loc_82649584;
loc_82649578:
	// lwz r11,20056(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20056);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82649590
	if (cr6.eq) goto loc_82649590;
loc_82649584:
	// lwz r11,280(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 280);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826495f4
	if (!cr6.eq) goto loc_826495F4;
loc_82649590:
	// li r30,0
	r30.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_82649598:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x826495e4
	if (!cr6.eq) goto loc_826495E4;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826495cc
	if (!cr0.lt) goto loc_826495CC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826495CC:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// mr r11,r31
	r11.u64 = r31.u64;
	// cmpwi cr6,r30,6
	cr6.compare<int32_t>(r30.s32, 6, xer);
	// blt cr6,0x82649598
	if (cr6.lt) goto loc_82649598;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x826495ec
	if (cr6.eq) goto loc_826495EC;
loc_826495E4:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// b 0x826497a4
	goto loc_826497A4;
loc_826495EC:
	// li r11,8
	r11.s64 = 8;
	// b 0x826497a4
	goto loc_826497A4;
loc_826495F4:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82649668
	if (!cr6.lt) goto loc_82649668;
loc_82649610:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649668
	if (cr6.eq) goto loc_82649668;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82649658
	if (!cr0.lt) goto loc_82649658;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649658:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82649610
	if (cr6.gt) goto loc_82649610;
loc_82649668:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826496a4
	if (!cr0.lt) goto loc_826496A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826496A4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// stw r30,1948(r28)
	PPC_STORE_U32(r28.u32 + 1948, r30.u32);
	// bne cr6,0x826497a8
	if (!cr6.eq) goto loc_826497A8;
	// lwz r11,15472(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82649774
	if (cr6.lt) goto loc_82649774;
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82649730
	if (!cr6.lt) goto loc_82649730;
loc_826496D8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649730
	if (cr6.eq) goto loc_82649730;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82649720
	if (!cr0.lt) goto loc_82649720;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649720:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826496d8
	if (cr6.gt) goto loc_826496D8;
loc_82649730:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264976c
	if (!cr0.lt) goto loc_8264976C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264976C:
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// b 0x826497a4
	goto loc_826497A4;
loc_82649774:
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x826497a0
	if (!cr0.lt) goto loc_826497A0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826497A0:
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
loc_826497A4:
	// stw r11,1948(r28)
	PPC_STORE_U32(r28.u32 + 1948, r11.u32);
loc_826497A8:
	// lwz r31,84(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8264981c
	if (!cr6.lt) goto loc_8264981C;
loc_826497C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264981c
	if (cr6.eq) goto loc_8264981C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264980c
	if (!cr0.lt) goto loc_8264980C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264980C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826497c4
	if (cr6.gt) goto loc_826497C4;
loc_8264981C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82649858
	if (!cr0.lt) goto loc_82649858;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649858:
	// addi r11,r30,3
	r11.s64 = r30.s64 + 3;
	// stw r11,1952(r28)
	PPC_STORE_U32(r28.u32 + 1952, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82649868"))) PPC_WEAK_FUNC(sub_82649868);
PPC_FUNC_IMPL(__imp__sub_82649868) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r21,0
	r21.s64 = 0;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r24,r21
	r24.u64 = r21.u64;
	// mr r23,r21
	r23.u64 = r21.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r28,4(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r18,r10,1
	r18.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r9,15472(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// lwz r25,0(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r19,28(r11)
	r19.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmpwi cr6,r9,6
	cr6.compare<int32_t>(ctx.r9.s32, 6, xer);
	// lwz r20,32(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r14,24(r11)
	r14.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r21,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r21.u32);
	// stw r28,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r28.u32);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// blt cr6,0x826498ec
	if (cr6.lt) goto loc_826498EC;
	// lwz r11,8(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// lwz r17,0(r7)
	r17.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r16,4(r7)
	r16.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// b 0x82649900
	goto loc_82649900;
loc_826498EC:
	// lwz r11,304(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 304);
	// lwz r17,312(r27)
	r17.u64 = PPC_LOAD_U32(r27.u32 + 312);
	// lwz r16,316(r27)
	r16.u64 = PPC_LOAD_U32(r27.u32 + 316);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r11,308(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 308);
loc_82649900:
	// li r5,256
	ctx.r5.s64 = 256;
	// lwz r3,1760(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// li r15,1
	r15.s64 = 1;
	// ori r26,r11,32768
	r26.u64 = r11.u64 | 32768;
loc_82649920:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r25)
	ctx.r4.u64 = PPC_LOAD_U8(r25.u32 + 8);
	// lwz r29,0(r25)
	r29.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649a10
	if (cr6.lt) goto loc_82649A10;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82649a08
	if (!cr6.lt) goto loc_82649A08;
loc_82649970:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264999c
	if (cr6.lt) goto loc_8264999C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82649970
	if (cr6.eq) goto loc_82649970;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649a4c
	goto loc_82649A4C;
loc_8264999C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82649A08:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649a4c
	goto loc_82649A4C;
loc_82649A10:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82649A18:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649a18
	if (cr6.lt) goto loc_82649A18;
loc_82649A4C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r31,r28
	cr6.compare<int32_t>(r31.s32, r28.s32, xer);
	// beq cr6,0x82649ac4
	if (cr6.eq) goto loc_82649AC4;
	// cmplw cr6,r31,r18
	cr6.compare<uint32_t>(r31.u32, r18.u32, xer);
	// blt cr6,0x82649a78
	if (cr6.lt) goto loc_82649A78;
	// mr r24,r15
	r24.u64 = r15.u64;
loc_82649A78:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lbzx r28,r31,r20
	r28.u64 = PPC_LOAD_U8(r31.u32 + r20.u32);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649aa4
	if (!cr0.lt) goto loc_82649AA4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649AA4:
	// lbzx r11,r31,r19
	r11.u64 = PPC_LOAD_U8(r31.u32 + r19.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82649abc
	if (cr6.eq) goto loc_82649ABC;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// neg r31,r11
	r31.s64 = -r11.s64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_82649ABC:
	// extsb r31,r11
	r31.s64 = r11.s8;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_82649AC4:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649aec
	if (!cr0.lt) goto loc_82649AEC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649AEC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82649cb4
	if (cr6.eq) goto loc_82649CB4;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// lbz r4,8(r25)
	ctx.r4.u64 = PPC_LOAD_U8(r25.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r25)
	r29.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649bf0
	if (cr6.lt) goto loc_82649BF0;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82649be8
	if (!cr6.lt) goto loc_82649BE8;
loc_82649B50:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82649b7c
	if (cr6.lt) goto loc_82649B7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82649b50
	if (cr6.eq) goto loc_82649B50;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649c2c
	goto loc_82649C2C;
loc_82649B7C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82649BE8:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649c2c
	goto loc_82649C2C;
loc_82649BF0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82649BF8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649bf8
	if (cr6.lt) goto loc_82649BF8;
loc_82649C2C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// beq cr6,0x8264a3b4
	if (cr6.eq) goto loc_8264A3B4;
	// lbzx r10,r11,r19
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r19.u32);
	// cmplw cr6,r11,r18
	cr6.compare<uint32_t>(r11.u32, r18.u32, xer);
	// lbzx r28,r11,r20
	r28.u64 = PPC_LOAD_U8(r11.u32 + r20.u32);
	// extsb r11,r10
	r11.s64 = ctx.r10.s8;
	// blt cr6,0x82649c6c
	if (cr6.lt) goto loc_82649C6C;
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r24,r15
	r24.u64 = r15.u64;
	// b 0x82649c70
	goto loc_82649C70;
loc_82649C6C:
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
loc_82649C70:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649ca4
	if (!cr0.lt) goto loc_82649CA4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649CA4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264a2e0
	if (cr6.eq) goto loc_8264A2E0;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_82649CB4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649ce0
	if (!cr0.lt) goto loc_82649CE0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649CE0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82649eac
	if (cr6.eq) goto loc_82649EAC;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// lbz r4,8(r25)
	ctx.r4.u64 = PPC_LOAD_U8(r25.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r25)
	r29.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649de4
	if (cr6.lt) goto loc_82649DE4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82649ddc
	if (!cr6.lt) goto loc_82649DDC;
loc_82649D44:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82649d70
	if (cr6.lt) goto loc_82649D70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82649d44
	if (cr6.eq) goto loc_82649D44;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649e20
	goto loc_82649E20;
loc_82649D70:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82649DDC:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82649e20
	goto loc_82649E20;
loc_82649DE4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82649DEC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r26
	r11.u64 = r30.u64 + r26.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82649dec
	if (cr6.lt) goto loc_82649DEC;
loc_82649E20:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r10,r28
	cr6.compare<int32_t>(ctx.r10.s32, r28.s32, xer);
	// beq cr6,0x8264a3b4
	if (cr6.eq) goto loc_8264A3B4;
	// lbzx r9,r10,r19
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r19.u32);
	// cmplw cr6,r10,r18
	cr6.compare<uint32_t>(ctx.r10.u32, r18.u32, xer);
	// lbzx r11,r10,r20
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r20.u32);
	// extsb r31,r9
	r31.s64 = ctx.r9.s8;
	// lwz r9,1932(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1932);
	// blt cr6,0x82649e64
	if (cr6.lt) goto loc_82649E64;
	// lbzx r10,r31,r14
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + r14.u32);
	// mr r24,r15
	r24.u64 = r15.u64;
	// b 0x82649e6c
	goto loc_82649E6C;
loc_82649E64:
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lbzx r10,r31,r10
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + ctx.r10.u32);
loc_82649E6C:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649e9c
	if (!cr0.lt) goto loc_82649E9C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649E9C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264a2e0
	if (cr6.eq) goto loc_8264A2E0;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_82649EAC:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649ed8
	if (!cr0.lt) goto loc_82649ED8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649ED8:
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// mr r24,r31
	r24.u64 = r31.u64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x8264a178
	if (cr6.lt) goto loc_8264A178;
	// lwz r11,1944(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1944);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82649f00
	if (cr6.eq) goto loc_82649F00;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82649548
	sub_82649548(ctx, base);
	// stw r21,1944(r27)
	PPC_STORE_U32(r27.u32 + 1944, r21.u32);
loc_82649F00:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r30,1952(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1952);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82649f24
	if (!cr6.eq) goto loc_82649F24;
	// mr r28,r21
	r28.u64 = r21.u64;
	// b 0x82649fc4
	goto loc_82649FC4;
loc_82649F24:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82649f84
	if (!cr6.gt) goto loc_82649F84;
loc_82649F2C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82649f84
	if (cr6.eq) goto loc_82649F84;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82649f74
	if (!cr0.lt) goto loc_82649F74;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649F74:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82649f2c
	if (cr6.gt) goto loc_82649F2C;
loc_82649F84:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82649fc0
	if (!cr0.lt) goto loc_82649FC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649FC0:
	// mr r28,r30
	r28.u64 = r30.u64;
loc_82649FC4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82649ff0
	if (!cr0.lt) goto loc_82649FF0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82649FF0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r30,1948(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1948);
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x8264a0c4
	if (cr6.eq) goto loc_8264A0C4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264a020
	if (!cr6.eq) goto loc_8264A020;
	// mr r30,r21
	r30.u64 = r21.u64;
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_8264A020:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264a080
	if (!cr6.gt) goto loc_8264A080;
loc_8264A028:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a080
	if (cr6.eq) goto loc_8264A080;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264a070
	if (!cr0.lt) goto loc_8264A070;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A070:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264a028
	if (cr6.gt) goto loc_8264A028;
loc_8264A080:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264a0bc
	if (!cr0.lt) goto loc_8264A0BC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A0BC:
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_8264A0C4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264a0d4
	if (!cr6.eq) goto loc_8264A0D4;
	// mr r31,r21
	r31.u64 = r21.u64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_8264A0D4:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264a134
	if (!cr6.gt) goto loc_8264A134;
loc_8264A0DC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a134
	if (cr6.eq) goto loc_8264A134;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264a124
	if (!cr0.lt) goto loc_8264A124;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A124:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264a0dc
	if (cr6.gt) goto loc_8264A0DC;
loc_8264A134:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264a170
	if (!cr0.lt) goto loc_8264A170;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A170:
	// mr r31,r30
	r31.u64 = r30.u64;
	// b 0x8264a2e0
	goto loc_8264A2E0;
loc_8264A178:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8264a1ec
	if (!cr6.lt) goto loc_8264A1EC;
loc_8264A194:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a1ec
	if (cr6.eq) goto loc_8264A1EC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264a1dc
	if (!cr0.lt) goto loc_8264A1DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A1DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264a194
	if (cr6.gt) goto loc_8264A194;
loc_8264A1EC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264a228
	if (!cr0.lt) goto loc_8264A228;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A228:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r30,8
	r30.s64 = 8;
	// mr r29,r21
	r29.u64 = r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8264a2a0
	if (!cr6.lt) goto loc_8264A2A0;
loc_8264A248:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a2a0
	if (cr6.eq) goto loc_8264A2A0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264a290
	if (!cr0.lt) goto loc_8264A290;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A290:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264a248
	if (cr6.gt) goto loc_8264A248;
loc_8264A2A0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264a2dc
	if (!cr0.lt) goto loc_8264A2DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A2DC:
	// extsb r31,r30
	r31.s64 = r30.s8;
loc_8264A2E0:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264a3b4
	if (!cr6.eq) goto loc_8264A3B4;
	// add r11,r28,r23
	r11.u64 = r28.u64 + r23.u64;
	// cmplwi cr6,r11,64
	cr6.compare<uint32_t>(r11.u32, 64, xer);
	// bge cr6,0x8264a3b4
	if (!cr6.lt) goto loc_8264A3B4;
	// lwz r10,1828(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 1828);
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8264a328
	if (cr6.eq) goto loc_8264A328;
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// slw r10,r15,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (r15.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_8264A328:
	// cmpwi cr6,r31,1
	cr6.compare<int32_t>(r31.s32, 1, xer);
	// bne cr6,0x8264a348
	if (!cr6.eq) goto loc_8264A348;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x8264a3a0
	goto loc_8264A3A0;
loc_8264A348:
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x8264a368
	if (!cr6.eq) goto loc_8264A368;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x8264a3a0
	goto loc_8264A3A0;
loc_8264A368:
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x8264a38c
	if (!cr6.gt) goto loc_8264A38C;
	// lbzx r9,r11,r22
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// mullw r10,r31,r17
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(r17.s32);
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + r16.u64;
	// rotlwi r9,r9,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
	// b 0x8264a3a0
	goto loc_8264A3A0;
loc_8264A38C:
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// mullw r9,r31,r17
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r17.s32);
	// subf r9,r16,r9
	ctx.r9.s64 = ctx.r9.s64 - r16.s64;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
loc_8264A3A0:
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// bne cr6,0x8264a3c0
	if (!cr6.eq) goto loc_8264A3C0;
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// b 0x82649920
	goto loc_82649920;
loc_8264A3B4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
loc_8264A3C0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,1940(r27)
	PPC_STORE_U32(r27.u32 + 1940, r11.u32);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264A3D4"))) PPC_WEAK_FUNC(sub_8264A3D4);
PPC_FUNC_IMPL(__imp__sub_8264A3D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8264A3D8"))) PPC_WEAK_FUNC(sub_8264A3D8);
PPC_FUNC_IMPL(__imp__sub_8264A3D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r8,204(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x8264a448
	if (!cr6.eq) goto loc_8264A448;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8264a440
	if (cr6.eq) goto loc_8264A440;
	// lwz r30,4(r5)
	r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// addi r3,r11,-8
	ctx.r3.s64 = r11.s64 + -8;
	// lwz r10,2952(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2952);
	// lwz r5,220(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r4,212(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// extsh r11,r3
	r11.s64 = ctx.r3.s16;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r10,1908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1908);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r3,1908(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1908);
	// b 0x8264a494
	goto loc_8264A494;
loc_8264A440:
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x8264a490
	goto loc_8264A490;
loc_8264A448:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8264a488
	if (cr6.eq) goto loc_8264A488;
	// lwz r4,212(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// lwz r30,12(r5)
	r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r5,220(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// lwz r9,2952(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 2952);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// extsh r11,r3
	r11.s64 = ctx.r3.s16;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r10,1912(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1912);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, r11.u16);
	// lwz r3,1912(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1912);
	// b 0x8264a494
	goto loc_8264A494;
loc_8264A488:
	// addi r11,r7,2
	r11.s64 = ctx.r7.s64 + 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8264A490:
	// lwzx r3,r11,r5
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r5.u32);
loc_8264A494:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8264A4AC"))) PPC_WEAK_FUNC(sub_8264A4AC);
PPC_FUNC_IMPL(__imp__sub_8264A4AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8264A4B0"))) PPC_WEAK_FUNC(sub_8264A4B0);
PPC_FUNC_IMPL(__imp__sub_8264A4B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r17,r6
	r17.u64 = ctx.r6.u64;
	// lwz r6,324(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r7,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r7.u32);
	// srawi r26,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	r26.s64 = ctx.r9.s32 >> 1;
	// srawi r30,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	r30.s64 = ctx.r8.s32 >> 1;
	// srawi r25,r6,1
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	r25.s64 = ctx.r6.s32 >> 1;
	// srawi r27,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	r27.s64 = ctx.r10.s32 >> 1;
	// not r10,r10
	ctx.r10.u64 = ~ctx.r10.u64;
	// lwz r11,19700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 19700);
	// not r9,r9
	ctx.r9.u64 = ~ctx.r9.u64;
	// lwz r7,19696(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 19696);
	// mr r18,r4
	r18.u64 = ctx.r4.u64;
	// lwz r20,328(r31)
	r20.u64 = PPC_LOAD_U32(r31.u32 + 328);
	// clrlwi r15,r10,31
	r15.u64 = ctx.r10.u32 & 0x1;
	// lwz r28,6548(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 6548);
	// clrlwi r22,r9,31
	r22.u64 = ctx.r9.u32 & 0x1;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r10,r25,r11
	ctx.r10.u64 = r25.u64 + r11.u64;
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 3736);
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// lwz r29,3732(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3732);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lbz r5,4(r18)
	ctx.r5.u64 = PPC_LOAD_U8(r18.u32 + 4);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3740);
	// lwz r21,336(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// add r9,r10,r27
	ctx.r9.u64 = ctx.r10.u64 + r27.u64;
	// rotlwi r10,r5,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// not r8,r8
	ctx.r8.u64 = ~ctx.r8.u64;
	// not r6,r6
	ctx.r6.u64 = ~ctx.r6.u64;
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// clrlwi r23,r8,31
	r23.u64 = ctx.r8.u32 & 0x1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r14,r6,31
	r14.u64 = ctx.r6.u32 & 0x1;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r8,r26,r7
	ctx.r8.u64 = r26.u64 + ctx.r7.u64;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// add r20,r10,r28
	r20.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// mullw r10,r8,r6
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
	// add r16,r3,r11
	r16.u64 = ctx.r3.u64 + r11.u64;
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r28,r10,r7
	r28.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x8264a584
	if (cr6.eq) goto loc_8264A584;
	// lbz r11,0(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// clrlwi r21,r11,29
	r21.u64 = r11.u32 & 0x7;
loc_8264A584:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264a5a8
	if (cr6.eq) goto loc_8264A5A8;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,10,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x3;
	// addi r11,r11,726
	r11.s64 = r11.s64 + 726;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r19,r11,r31
	r19.u64 = r11.u64 + r31.u64;
	// b 0x8264a5ac
	goto loc_8264A5AC;
loc_8264A5A8:
	// addi r19,r31,2880
	r19.s64 = r31.s64 + 2880;
loc_8264A5AC:
	// lbz r11,17(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 17);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a8fc
	if (cr6.eq) goto loc_8264A8FC;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a644
	if (cr6.eq) goto loc_8264A644;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a5f4
	if (!cr0.lt) goto loc_8264A5F4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A5F4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264a604
	if (!cr6.eq) goto loc_8264A604;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264a64c
	goto loc_8264A64C;
loc_8264A604:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a630
	if (!cr0.lt) goto loc_8264A630;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A630:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264a694
	goto loc_8264A694;
loc_8264A644:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264a694
	if (!cr6.eq) goto loc_8264A694;
loc_8264A64C:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264A694:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264a7b0
	if (!cr6.eq) goto loc_8264A7B0;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a6e0
	if (!cr0.lt) goto loc_8264A6E0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A6E0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264a76c
	if (cr6.eq) goto loc_8264A76C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a714
	if (!cr0.lt) goto loc_8264A714;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A714:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264a720
	if (!cr6.eq) goto loc_8264A720;
	// li r29,0
	r29.s64 = 0;
loc_8264A720:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264a7b0
	if (cr6.eq) goto loc_8264A7B0;
loc_8264A76C:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264A7B0:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264a8cc
	if (!cr6.eq) goto loc_8264A8CC;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a7fc
	if (!cr0.lt) goto loc_8264A7FC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A7FC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264a888
	if (cr6.eq) goto loc_8264A888;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a830
	if (!cr0.lt) goto loc_8264A830;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A830:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264a83c
	if (!cr6.eq) goto loc_8264A83C;
	// li r29,0
	r29.s64 = 0;
loc_8264A83C:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264a8cc
	if (cr6.eq) goto loc_8264A8CC;
loc_8264A888:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264A8CC:
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// lwz r11,3080(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3080);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8264a920
	goto loc_8264A920;
loc_8264A8FC:
	// lwz r11,3084(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3084);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264A920:
	// lbz r11,16(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 16);
	// addi r27,r24,8
	r27.s64 = r24.s64 + 8;
	// addi r26,r28,8
	r26.s64 = r28.s64 + 8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264ac78
	if (cr6.eq) goto loc_8264AC78;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264a9c0
	if (cr6.eq) goto loc_8264A9C0;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a970
	if (!cr0.lt) goto loc_8264A970;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A970:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264a980
	if (!cr6.eq) goto loc_8264A980;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264a9c8
	goto loc_8264A9C8;
loc_8264A980:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264a9ac
	if (!cr0.lt) goto loc_8264A9AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264A9AC:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264aa10
	goto loc_8264AA10;
loc_8264A9C0:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264aa10
	if (!cr6.eq) goto loc_8264AA10;
loc_8264A9C8:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AA10:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264ab2c
	if (!cr6.eq) goto loc_8264AB2C;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264aa5c
	if (!cr0.lt) goto loc_8264AA5C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AA5C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264aae8
	if (cr6.eq) goto loc_8264AAE8;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264aa90
	if (!cr0.lt) goto loc_8264AA90;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AA90:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264aa9c
	if (!cr6.eq) goto loc_8264AA9C;
	// li r29,0
	r29.s64 = 0;
loc_8264AA9C:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264ab2c
	if (cr6.eq) goto loc_8264AB2C;
loc_8264AAE8:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AB2C:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264ac48
	if (!cr6.eq) goto loc_8264AC48;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264ab78
	if (!cr0.lt) goto loc_8264AB78;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AB78:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264ac04
	if (cr6.eq) goto loc_8264AC04;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264abac
	if (!cr0.lt) goto loc_8264ABAC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264ABAC:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264abb8
	if (!cr6.eq) goto loc_8264ABB8;
	// li r29,0
	r29.s64 = 0;
loc_8264ABB8:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264ac48
	if (cr6.eq) goto loc_8264AC48;
loc_8264AC04:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AC48:
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r11,3080(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3080);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8264aca0
	goto loc_8264ACA0;
loc_8264AC78:
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// lwz r11,3084(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3084);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264ACA0:
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// lbz r10,15(r18)
	ctx.r10.u64 = PPC_LOAD_U8(r18.u32 + 15);
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r27,r11,r26
	r27.u64 = r11.u64 + r26.u64;
	// beq cr6,0x8264affc
	if (cr6.eq) goto loc_8264AFFC;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264ad44
	if (cr6.eq) goto loc_8264AD44;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264acf4
	if (!cr0.lt) goto loc_8264ACF4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264ACF4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264ad04
	if (!cr6.eq) goto loc_8264AD04;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264ad4c
	goto loc_8264AD4C;
loc_8264AD04:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264ad30
	if (!cr0.lt) goto loc_8264AD30;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AD30:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264ad94
	goto loc_8264AD94;
loc_8264AD44:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264ad94
	if (!cr6.eq) goto loc_8264AD94;
loc_8264AD4C:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AD94:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264aeb0
	if (!cr6.eq) goto loc_8264AEB0;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264ade0
	if (!cr0.lt) goto loc_8264ADE0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264ADE0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264ae6c
	if (cr6.eq) goto loc_8264AE6C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264ae14
	if (!cr0.lt) goto loc_8264AE14;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AE14:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264ae20
	if (!cr6.eq) goto loc_8264AE20;
	// li r29,0
	r29.s64 = 0;
loc_8264AE20:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264aeb0
	if (cr6.eq) goto loc_8264AEB0;
loc_8264AE6C:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AEB0:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264afcc
	if (!cr6.eq) goto loc_8264AFCC;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264aefc
	if (!cr0.lt) goto loc_8264AEFC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AEFC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264af88
	if (cr6.eq) goto loc_8264AF88;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264af30
	if (!cr0.lt) goto loc_8264AF30;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264AF30:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264af3c
	if (!cr6.eq) goto loc_8264AF3C;
	// li r29,0
	r29.s64 = 0;
loc_8264AF3C:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264afcc
	if (cr6.eq) goto loc_8264AFCC;
loc_8264AF88:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264AFCC:
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r11,3080(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3080);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8264b024
	goto loc_8264B024;
loc_8264AFFC:
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// lwz r11,3084(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3084);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B024:
	// lbz r11,14(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 14);
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264b37c
	if (cr6.eq) goto loc_8264B37C;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264b0c4
	if (cr6.eq) goto loc_8264B0C4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b074
	if (!cr0.lt) goto loc_8264B074;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B074:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264b084
	if (!cr6.eq) goto loc_8264B084;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264b0cc
	goto loc_8264B0CC;
loc_8264B084:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b0b0
	if (!cr0.lt) goto loc_8264B0B0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B0B0:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264b114
	goto loc_8264B114;
loc_8264B0C4:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264b114
	if (!cr6.eq) goto loc_8264B114;
loc_8264B0CC:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B114:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264b230
	if (!cr6.eq) goto loc_8264B230;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b160
	if (!cr0.lt) goto loc_8264B160;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B160:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b1ec
	if (cr6.eq) goto loc_8264B1EC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b194
	if (!cr0.lt) goto loc_8264B194;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B194:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b1a0
	if (!cr6.eq) goto loc_8264B1A0;
	// li r29,0
	r29.s64 = 0;
loc_8264B1A0:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264b230
	if (cr6.eq) goto loc_8264B230;
loc_8264B1EC:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B230:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264b34c
	if (!cr6.eq) goto loc_8264B34C;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b27c
	if (!cr0.lt) goto loc_8264B27C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B27C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b308
	if (cr6.eq) goto loc_8264B308;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b2b0
	if (!cr0.lt) goto loc_8264B2B0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B2B0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b2bc
	if (!cr6.eq) goto loc_8264B2BC;
	// li r29,0
	r29.s64 = 0;
loc_8264B2BC:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264b34c
	if (cr6.eq) goto loc_8264B34C;
loc_8264B308:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B34C:
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r11,3080(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3080);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8264b3a4
	goto loc_8264B3A4;
loc_8264B37C:
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// lwz r6,204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// lwz r11,3084(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3084);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B3A4:
	// lbz r11,13(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 13);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264b6f4
	if (cr6.eq) goto loc_8264B6F4;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264b43c
	if (cr6.eq) goto loc_8264B43C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b3ec
	if (!cr0.lt) goto loc_8264B3EC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B3EC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264b3fc
	if (!cr6.eq) goto loc_8264B3FC;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264b444
	goto loc_8264B444;
loc_8264B3FC:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b428
	if (!cr0.lt) goto loc_8264B428;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B428:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264b48c
	goto loc_8264B48C;
loc_8264B43C:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264b48c
	if (!cr6.eq) goto loc_8264B48C;
loc_8264B444:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B48C:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264b5a8
	if (!cr6.eq) goto loc_8264B5A8;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b4d8
	if (!cr0.lt) goto loc_8264B4D8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B4D8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b564
	if (cr6.eq) goto loc_8264B564;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b50c
	if (!cr0.lt) goto loc_8264B50C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B50C:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b518
	if (!cr6.eq) goto loc_8264B518;
	// li r29,0
	r29.s64 = 0;
loc_8264B518:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264b5a8
	if (cr6.eq) goto loc_8264B5A8;
loc_8264B564:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B5A8:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264b6c4
	if (!cr6.eq) goto loc_8264B6C4;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b5f4
	if (!cr0.lt) goto loc_8264B5F4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B5F4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b680
	if (cr6.eq) goto loc_8264B680;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b628
	if (!cr0.lt) goto loc_8264B628;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B628:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b634
	if (!cr6.eq) goto loc_8264B634;
	// li r29,0
	r29.s64 = 0;
loc_8264B634:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264b6c4
	if (cr6.eq) goto loc_8264B6C4;
loc_8264B680:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B6C4:
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// lwz r11,3108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3108);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x8264b71c
	goto loc_8264B71C;
loc_8264B6F4:
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r8,r14
	ctx.r8.u64 = r14.u64;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
	// lwz r11,3104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3104);
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B71C:
	// lbz r11,12(r18)
	r11.u64 = PPC_LOAD_U8(r18.u32 + 12);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264ba74
	if (cr6.eq) goto loc_8264BA74;
	// lwz r11,0(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + 0);
	// rlwinm r11,r11,0,3,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264b7b4
	if (cr6.eq) goto loc_8264B7B4;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b764
	if (!cr0.lt) goto loc_8264B764;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B764:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264b774
	if (!cr6.eq) goto loc_8264B774;
	// li r21,0
	r21.s64 = 0;
	// b 0x8264b7bc
	goto loc_8264B7BC;
loc_8264B774:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b7a0
	if (!cr0.lt) goto loc_8264B7A0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B7A0:
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r21,r11,1
	r21.s64 = r11.s64 + 1;
	// b 0x8264b804
	goto loc_8264B804;
loc_8264B7B4:
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// bne cr6,0x8264b804
	if (!cr6.eq) goto loc_8264B804;
loc_8264B7BC:
	// lwz r25,1768(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r5,1832(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1832);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r6,1940(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1940);
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r11,3164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3164);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B804:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,1
	cr6.compare<int32_t>(r21.s32, 1, xer);
	// bne cr6,0x8264b920
	if (!cr6.eq) goto loc_8264B920;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b850
	if (!cr0.lt) goto loc_8264B850;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B850:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b8dc
	if (cr6.eq) goto loc_8264B8DC;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b884
	if (!cr0.lt) goto loc_8264B884;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B884:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b890
	if (!cr6.eq) goto loc_8264B890;
	// li r29,0
	r29.s64 = 0;
loc_8264B890:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264b920
	if (cr6.eq) goto loc_8264B920;
loc_8264B8DC:
	// lwz r5,1852(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1852);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3168);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264B920:
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r21,2
	cr6.compare<int32_t>(r21.s32, 2, xer);
	// bne cr6,0x8264ba3c
	if (!cr6.eq) goto loc_8264BA3C;
	// lwz r25,1764(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 1764);
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b96c
	if (!cr0.lt) goto loc_8264B96C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B96C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264b9f8
	if (cr6.eq) goto loc_8264B9F8;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264b9a0
	if (!cr0.lt) goto loc_8264B9A0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264B9A0:
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// bne cr6,0x8264b9ac
	if (!cr6.eq) goto loc_8264B9AC;
	// li r29,0
	r29.s64 = 0;
loc_8264B9AC:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264ba3c
	if (cr6.eq) goto loc_8264BA3C;
loc_8264B9F8:
	// lwz r5,1856(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1856);
	// mr r7,r20
	ctx.r7.u64 = r20.u64;
	// lwz r11,3156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3156);
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264baa0
	if (!cr6.eq) goto loc_8264BAA0;
	// lwz r5,1768(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,3172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3172);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8264BA3C:
	// lwz r10,332(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r9,r14
	ctx.r9.u64 = r14.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r8,r15
	ctx.r8.u64 = r15.u64;
	// lwz r11,3108(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3108);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,292(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
loc_8264BA74:
	// lwz r9,332(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 332);
	// mr r8,r14
	ctx.r8.u64 = r14.u64;
	// lwz r6,208(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r7,r15
	ctx.r7.u64 = r15.u64;
	// lwz r11,3104(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3104);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r4,292(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_8264BAA0:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264BAA8"))) PPC_WEAK_FUNC(sub_8264BAA8);
PPC_FUNC_IMPL(__imp__sub_8264BAA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r18,r6
	r18.u64 = ctx.r6.u64;
	// mr r17,r7
	r17.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// lwz r11,20056(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20056);
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264bb1c
	if (cr6.eq) goto loc_8264BB1C;
	// lwz r11,352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r21,0
	r21.s64 = 0;
	// lwz r8,388(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// li r10,0
	ctx.r10.s64 = 0;
	// srawi r9,r8,16
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 16;
	// clrlwi r8,r8,16
	ctx.r8.u64 = ctx.r8.u32 & 0xFFFF;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
	// ori r31,r31,4
	r31.u64 = r31.u64 | 4;
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// bl 0x8262d4f0
	sub_8262D4F0(ctx, base);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd18
	return;
loc_8264BB1C:
	// lwz r19,380(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// lwz r22,372(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// stw r23,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r23.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r19,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r19.u32);
	// stw r22,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r22.u32);
	// stw r19,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r19.u32);
	// beq cr6,0x8264bb64
	if (cr6.eq) goto loc_8264BB64;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// rlwinm r11,r11,10,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r20,r10,r31
	r20.u64 = ctx.r10.u64 + r31.u64;
	// add r25,r11,r31
	r25.u64 = r11.u64 + r31.u64;
	// b 0x8264bb6c
	goto loc_8264BB6C;
loc_8264BB64:
	// addi r20,r31,2880
	r20.s64 = r31.s64 + 2880;
	// addi r25,r31,2892
	r25.s64 = r31.s64 + 2892;
loc_8264BB6C:
	// li r21,0
	r21.s64 = 0;
	// lwz r24,388(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	// mr r30,r21
	r30.u64 = r21.u64;
loc_8264BB78:
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264be48
	if (!cr6.eq) goto loc_8264BE48;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lwz r8,1936(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1936);
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r22,2
	ctx.r7.s64 = r22.s64 + 2;
	// rlwinm r9,r23,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwzx r10,r9,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r29.u32);
	// lwzx r9,r7,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + r29.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// subf r7,r7,r11
	ctx.r7.s64 = r11.s64 - ctx.r7.s64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r4,r11,r5
	ctx.r4.u64 = r11.u64 ^ ctx.r5.u64;
	// subf r11,r6,r7
	r11.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r7,r5,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x8264bc18
	if (!cr6.lt) goto loc_8264BC18;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// b 0x8264bc20
	goto loc_8264BC20;
loc_8264BC18:
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_8264BC20:
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r28)
	PPC_STORE_U16(r28.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r16,1760(r31)
	r16.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r16)
	PPC_STORE_U32(r16.u32 + 0, r11.u32);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264be48
	if (!cr6.eq) goto loc_8264BE48;
	// cmplwi cr6,r30,1
	cr6.compare<uint32_t>(r30.u32, 1, xer);
	// li r11,8
	r11.s64 = 8;
	// bne cr6,0x8264bc90
	if (!cr6.eq) goto loc_8264BC90;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
loc_8264BC90:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// addi r28,r28,32
	r28.s64 = r28.s64 + 32;
	// addi r29,r29,24
	r29.s64 = r29.s64 + 24;
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// blt cr6,0x8264bb78
	if (cr6.lt) goto loc_8264BB78;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264be48
	if (!cr6.eq) goto loc_8264BE48;
	// addi r11,r19,4
	r11.s64 = r19.s64 + 4;
	// lwz r8,1936(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1936);
	// addi r9,r22,2
	ctx.r9.s64 = r22.s64 + 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r23,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r29
	r11.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// lwzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r29.u32);
	// lwzx r9,r9,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r29.u32);
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lhz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// subf r7,r7,r11
	ctx.r7.s64 = r11.s64 - ctx.r7.s64;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// srawi r6,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// srawi r5,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 31;
	// xor r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r6.u64;
	// xor r4,r11,r5
	ctx.r4.u64 = r11.u64 ^ ctx.r5.u64;
	// subf r11,r6,r7
	r11.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r7,r5,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bge cr6,0x8264bd3c
	if (!cr6.lt) goto loc_8264BD3C;
	// mr r8,r21
	ctx.r8.u64 = r21.u64;
	// b 0x8264bd44
	goto loc_8264BD44;
loc_8264BD3C:
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_8264BD44:
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// lwz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r28)
	PPC_STORE_U16(r28.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r30,1760(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264be48
	if (!cr6.eq) goto loc_8264BE48;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r30,r28,32
	r30.s64 = r28.s64 + 32;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264be48
	if (!cr6.eq) goto loc_8264BE48;
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// addi r4,r29,24
	ctx.r4.s64 = r29.s64 + 24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826494c0
	sub_826494C0(ctx, base);
	// lwz r7,116(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// li r8,5
	ctx.r8.s64 = 5;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(r30.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r30,1760(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82649020
	sub_82649020(ctx, base);
loc_8264BE48:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_8264BE50"))) PPC_WEAK_FUNC(sub_8264BE50);
PPC_FUNC_IMPL(__imp__sub_8264BE50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,492(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r10,476(r1)
	PPC_STORE_U32(ctx.r1.u32 + 476, ctx.r10.u32);
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// li r19,0
	r19.s64 = 0;
	// stw r7,452(r1)
	PPC_STORE_U32(ctx.r1.u32 + 452, ctx.r7.u32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stw r9,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, ctx.r9.u32);
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r5,128
	ctx.r5.s64 = 128;
	// lwz r25,16(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r26,1760(r27)
	r26.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// stw r28,460(r1)
	PPC_STORE_U32(ctx.r1.u32 + 460, r28.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// sth r19,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r19.u16);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,1832(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1832);
	// stw r25,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r25.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lwz r11,20056(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20056);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264bf14
	if (cr6.eq) goto loc_8264BF14;
	// lwz r11,284(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 284);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264bee0
	if (cr6.eq) goto loc_8264BEE0;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x8264bf14
	if (!cr6.eq) goto loc_8264BF14;
loc_8264BEE0:
	// lwz r11,484(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,27,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x18;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264bf0c
	if (cr6.eq) goto loc_8264BF0C;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8264bf04
	if (cr6.eq) goto loc_8264BF04;
	// lwz r11,1816(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1816);
	// b 0x8264bf10
	goto loc_8264BF10;
loc_8264BF04:
	// lwz r11,1820(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1820);
	// b 0x8264bf10
	goto loc_8264BF10;
loc_8264BF0C:
	// lwz r11,1812(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1812);
loc_8264BF10:
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_8264BF14:
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// li r6,119
	ctx.r6.s64 = 119;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bge cr6,0x8264bf34
	if (!cr6.lt) goto loc_8264BF34;
	// lwz r5,2092(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 2092);
	// b 0x8264bf38
	goto loc_8264BF38;
loc_8264BF34:
	// lwz r5,2096(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 2096);
loc_8264BF38:
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r11,1760(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// sth r19,2(r26)
	PPC_STORE_U16(r26.u32 + 2, r19.u16);
	// sth r11,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r11.u16);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264cd70
	if (!cr6.eq) goto loc_8264CD70;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264c9b8
	if (cr6.eq) goto loc_8264C9B8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r24,r19
	r24.u64 = r19.u64;
	// li r18,1
	r18.s64 = 1;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r23,0(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r21,r10,1
	r21.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r20,28(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r22,32(r11)
	r22.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r14,16(r11)
	r14.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r15,20(r11)
	r15.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lwz r16,24(r11)
	r16.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r17,4(r11)
	r17.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lis r11,0
	r11.s64 = 0;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ori r25,r11,32768
	r25.u64 = r11.u64 | 32768;
loc_8264BFA0:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c090
	if (cr6.lt) goto loc_8264C090;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264c088
	if (!cr6.lt) goto loc_8264C088;
loc_8264BFF0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264c01c
	if (cr6.lt) goto loc_8264C01C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264bff0
	if (cr6.eq) goto loc_8264BFF0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c0cc
	goto loc_8264C0CC;
loc_8264C01C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264C088:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c0cc
	goto loc_8264C0CC;
loc_8264C090:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264C098:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c098
	if (cr6.lt) goto loc_8264C098;
loc_8264C0CC:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r31,r17
	cr6.compare<int32_t>(r31.s32, r17.s32, xer);
	// bgt cr6,0x8264c9d8
	if (cr6.gt) goto loc_8264C9D8;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// beq cr6,0x8264c148
	if (cr6.eq) goto loc_8264C148;
	// subfc r11,r21,r31
	xer.ca = r31.u32 >= r21.u32;
	r11.s64 = r31.s64 - r21.s64;
	// lbzx r28,r31,r22
	r28.u64 = PPC_LOAD_U8(r31.u32 + r22.u32);
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r24,r11,1
	r24.s64 = r11.s64 + 1;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c128
	if (!cr0.lt) goto loc_8264C128;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C128:
	// lbzx r11,r31,r20
	r11.u64 = PPC_LOAD_U8(r31.u32 + r20.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264c140
	if (cr6.eq) goto loc_8264C140;
	// extsb r11,r11
	r11.s64 = r11.s8;
	// neg r31,r11
	r31.s64 = -r11.s64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C140:
	// extsb r31,r11
	r31.s64 = r11.s8;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C148:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c16c
	if (!cr0.lt) goto loc_8264C16C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C16C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264c334
	if (cr6.eq) goto loc_8264C334;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c270
	if (cr6.lt) goto loc_8264C270;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264c268
	if (!cr6.lt) goto loc_8264C268;
loc_8264C1D0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264c1fc
	if (cr6.lt) goto loc_8264C1FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264c1d0
	if (cr6.eq) goto loc_8264C1D0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c2ac
	goto loc_8264C2AC;
loc_8264C1FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264C268:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c2ac
	goto loc_8264C2AC;
loc_8264C270:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264C278:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c278
	if (cr6.lt) goto loc_8264C278;
loc_8264C2AC:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r11,r17
	cr6.compare<int32_t>(r11.s32, r17.s32, xer);
	// beq cr6,0x8264c9d8
	if (cr6.eq) goto loc_8264C9D8;
	// lbzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + r20.u32);
	// cmplw cr6,r11,r21
	cr6.compare<uint32_t>(r11.u32, r21.u32, xer);
	// lbzx r28,r11,r22
	r28.u64 = PPC_LOAD_U8(r11.u32 + r22.u32);
	// extsb r11,r10
	r11.s64 = ctx.r10.s8;
	// blt cr6,0x8264c2ec
	if (cr6.lt) goto loc_8264C2EC;
	// lbzx r10,r28,r14
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + r14.u32);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264c2f4
	goto loc_8264C2F4;
loc_8264C2EC:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
loc_8264C2F4:
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c324
	if (!cr0.lt) goto loc_8264C324;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C324:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264c95c
	if (cr6.eq) goto loc_8264C95C;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C334:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c360
	if (!cr0.lt) goto loc_8264C360;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C360:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264c528
	if (cr6.eq) goto loc_8264C528;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c464
	if (cr6.lt) goto loc_8264C464;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264c45c
	if (!cr6.lt) goto loc_8264C45C;
loc_8264C3C4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264c3f0
	if (cr6.lt) goto loc_8264C3F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264c3c4
	if (cr6.eq) goto loc_8264C3C4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c4a0
	goto loc_8264C4A0;
loc_8264C3F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264C45C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264c4a0
	goto loc_8264C4A0;
loc_8264C464:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264C46C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264c46c
	if (cr6.lt) goto loc_8264C46C;
loc_8264C4A0:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// cmpw cr6,r10,r17
	cr6.compare<int32_t>(ctx.r10.s32, r17.s32, xer);
	// beq cr6,0x8264c9d8
	if (cr6.eq) goto loc_8264C9D8;
	// lbzx r9,r10,r20
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r20.u32);
	// cmplw cr6,r10,r21
	cr6.compare<uint32_t>(ctx.r10.u32, r21.u32, xer);
	// lbzx r11,r10,r22
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r22.u32);
	// extsb r31,r9
	r31.s64 = ctx.r9.s8;
	// lwz r9,1932(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1932);
	// blt cr6,0x8264c4e4
	if (cr6.lt) goto loc_8264C4E4;
	// lbzx r10,r31,r16
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + r16.u32);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264c4e8
	goto loc_8264C4E8;
loc_8264C4E4:
	// lbzx r10,r31,r15
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + r15.u32);
loc_8264C4E8:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c518
	if (!cr0.lt) goto loc_8264C518;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C518:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8264c95c
	if (cr6.eq) goto loc_8264C95C;
	// neg r31,r31
	r31.s64 = -r31.s64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C528:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c554
	if (!cr0.lt) goto loc_8264C554;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C554:
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// mr r24,r31
	r24.u64 = r31.u64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x8264c7f4
	if (cr6.lt) goto loc_8264C7F4;
	// lwz r11,1944(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1944);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264c57c
	if (cr6.eq) goto loc_8264C57C;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82649548
	sub_82649548(ctx, base);
	// stw r19,1944(r27)
	PPC_STORE_U32(r27.u32 + 1944, r19.u32);
loc_8264C57C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r30,1952(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1952);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x8264c5a0
	if (!cr6.eq) goto loc_8264C5A0;
	// mr r28,r19
	r28.u64 = r19.u64;
	// b 0x8264c640
	goto loc_8264C640;
loc_8264C5A0:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264c600
	if (!cr6.gt) goto loc_8264C600;
loc_8264C5A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264c600
	if (cr6.eq) goto loc_8264C600;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264c5f0
	if (!cr0.lt) goto loc_8264C5F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C5F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264c5a8
	if (cr6.gt) goto loc_8264C5A8;
loc_8264C600:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264c63c
	if (!cr0.lt) goto loc_8264C63C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C63C:
	// mr r28,r30
	r28.u64 = r30.u64;
loc_8264C640:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264c66c
	if (!cr0.lt) goto loc_8264C66C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C66C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r30,1948(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1948);
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x8264c740
	if (cr6.eq) goto loc_8264C740;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264c69c
	if (!cr6.eq) goto loc_8264C69C;
	// mr r30,r19
	r30.u64 = r19.u64;
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C69C:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264c6fc
	if (!cr6.gt) goto loc_8264C6FC;
loc_8264C6A4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264c6fc
	if (cr6.eq) goto loc_8264C6FC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264c6ec
	if (!cr0.lt) goto loc_8264C6EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C6EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264c6a4
	if (cr6.gt) goto loc_8264C6A4;
loc_8264C6FC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264c738
	if (!cr0.lt) goto loc_8264C738;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C738:
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C740:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264c750
	if (!cr6.eq) goto loc_8264C750;
	// mr r31,r19
	r31.u64 = r19.u64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C750:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264c7b0
	if (!cr6.gt) goto loc_8264C7B0;
loc_8264C758:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264c7b0
	if (cr6.eq) goto loc_8264C7B0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264c7a0
	if (!cr0.lt) goto loc_8264C7A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C7A0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264c758
	if (cr6.gt) goto loc_8264C758;
loc_8264C7B0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264c7ec
	if (!cr0.lt) goto loc_8264C7EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C7EC:
	// mr r31,r30
	r31.u64 = r30.u64;
	// b 0x8264c95c
	goto loc_8264C95C;
loc_8264C7F4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8264c868
	if (!cr6.lt) goto loc_8264C868;
loc_8264C810:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264c868
	if (cr6.eq) goto loc_8264C868;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264c858
	if (!cr0.lt) goto loc_8264C858;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C858:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264c810
	if (cr6.gt) goto loc_8264C810;
loc_8264C868:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264c8a4
	if (!cr0.lt) goto loc_8264C8A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C8A4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r30,8
	r30.s64 = 8;
	// mr r29,r19
	r29.u64 = r19.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8264c91c
	if (!cr6.lt) goto loc_8264C91C;
loc_8264C8C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264c91c
	if (cr6.eq) goto loc_8264C91C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264c90c
	if (!cr0.lt) goto loc_8264C90C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C90C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264c8c4
	if (cr6.gt) goto loc_8264C8C4;
loc_8264C91C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264c958
	if (!cr0.lt) goto loc_8264C958;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264C958:
	// extsb r31,r30
	r31.s64 = r30.s8;
loc_8264C95C:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264c9d8
	if (!cr6.eq) goto loc_8264C9D8;
	// add r9,r28,r18
	ctx.r9.u64 = r28.u64 + r18.u64;
	// cmplwi cr6,r9,64
	cr6.compare<uint32_t>(ctx.r9.u32, 64, xer);
	// bge cr6,0x8264c9d8
	if (!cr6.lt) goto loc_8264C9D8;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x8264c9d8
	if (cr6.eq) goto loc_8264C9D8;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r18,r9,1
	r18.s64 = ctx.r9.s64 + 1;
	// lhz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lbzx r11,r9,r11
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(r11.u32, 1);
	// sthx r11,r6,r8
	PPC_STORE_U16(ctx.r6.u32 + ctx.r8.u32, r11.u16);
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// sthx r31,r9,r26
	PPC_STORE_U16(ctx.r9.u32 + r26.u32, r31.u16);
	// beq cr6,0x8264bfa0
	if (cr6.eq) goto loc_8264BFA0;
loc_8264C9B8:
	// lwz r10,452(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8264cc4c
	if (cr6.eq) goto loc_8264CC4C;
	// lwz r11,460(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264c9e4
	if (cr6.eq) goto loc_8264C9E4;
	// lwz r11,1920(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1920);
	// b 0x8264c9e8
	goto loc_8264C9E8;
loc_8264C9D8:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8239bd10
	return;
loc_8264C9E4:
	// lwz r11,1916(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1916);
loc_8264C9E8:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,468(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lhz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// cmpwi cr6,r6,-1
	cr6.compare<int32_t>(ctx.r6.s32, -1, xer);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sth r10,0(r26)
	PPC_STORE_U16(r26.u32 + 0, ctx.r10.u16);
	// beq cr6,0x8264cc4c
	if (cr6.eq) goto loc_8264CC4C;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264ca3c
	if (cr6.eq) goto loc_8264CA3C;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264ca68
	goto loc_8264CA68;
loc_8264CA3C:
	// lhz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r4,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r4.u16);
	// sthx r10,r5,r8
	PPC_STORE_U16(ctx.r5.u32 + ctx.r8.u32, ctx.r10.u16);
loc_8264CA68:
	// li r10,2
	ctx.r10.s64 = 2;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264ca90
	if (cr6.eq) goto loc_8264CA90;
	// lhz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264cab8
	goto loc_8264CAB8;
loc_8264CA90:
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lhz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r10,r4,r5
	PPC_STORE_U16(ctx.r4.u32 + ctx.r5.u32, ctx.r10.u16);
	// sthx r8,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r8.u16);
loc_8264CAB8:
	// li r10,3
	ctx.r10.s64 = 3;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264cae0
	if (cr6.eq) goto loc_8264CAE0;
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264cb08
	goto loc_8264CB08;
loc_8264CAE0:
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lhz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r10,r4,r5
	PPC_STORE_U16(ctx.r4.u32 + ctx.r5.u32, ctx.r10.u16);
	// sthx r8,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r8.u16);
loc_8264CB08:
	// li r10,4
	ctx.r10.s64 = 4;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264cb30
	if (cr6.eq) goto loc_8264CB30;
	// lhz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264cb58
	goto loc_8264CB58;
loc_8264CB30:
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lhz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r10,r4,r5
	PPC_STORE_U16(ctx.r4.u32 + ctx.r5.u32, ctx.r10.u16);
	// sthx r8,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r8.u16);
loc_8264CB58:
	// li r10,5
	ctx.r10.s64 = 5;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264cb80
	if (cr6.eq) goto loc_8264CB80;
	// lhz r10,10(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264cba8
	goto loc_8264CBA8;
loc_8264CB80:
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lhz r8,10(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r10,r4,r5
	PPC_STORE_U16(ctx.r4.u32 + ctx.r5.u32, ctx.r10.u16);
	// sthx r8,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r8.u16);
loc_8264CBA8:
	// li r10,6
	ctx.r10.s64 = 6;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264cbd0
	if (cr6.eq) goto loc_8264CBD0;
	// lhz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sthx r10,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, ctx.r10.u16);
	// b 0x8264cbf8
	goto loc_8264CBF8;
loc_8264CBD0:
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// lhz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// rlwinm r3,r10,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// sthx r10,r4,r5
	PPC_STORE_U16(ctx.r4.u32 + ctx.r5.u32, ctx.r10.u16);
	// sthx r8,r3,r26
	PPC_STORE_U16(ctx.r3.u32 + r26.u32, ctx.r8.u16);
loc_8264CBF8:
	// li r10,7
	ctx.r10.s64 = 7;
	// slw r10,r10,r6
	ctx.r10.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r8,r26
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r8.u32 + r26.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8264cc20
	if (cr6.eq) goto loc_8264CC20;
	// lhz r11,14(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sthx r11,r8,r26
	PPC_STORE_U16(ctx.r8.u32 + r26.u32, r11.u16);
	// b 0x8264cc50
	goto loc_8264CC50;
loc_8264CC20:
	// lhz r9,14(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 14);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// rlwinm r6,r11,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// rlwinm r5,r10,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// extsh r7,r11
	ctx.r7.s64 = r11.s16;
	// sthx r10,r6,r8
	PPC_STORE_U16(ctx.r6.u32 + ctx.r8.u32, ctx.r10.u16);
	// sthx r9,r5,r26
	PPC_STORE_U16(ctx.r5.u32 + r26.u32, ctx.r9.u16);
	// b 0x8264cc50
	goto loc_8264CC50;
loc_8264CC4C:
	// lhz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
loc_8264CC50:
	// lhz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// extsh r11,r7
	r11.s64 = ctx.r7.s16;
	// lwz r10,476(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// sth r9,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r9.u16);
	// lhz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// sth r9,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r9.u16);
	// lhz r9,2(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 2);
	// sth r9,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, ctx.r9.u16);
	// lhz r9,16(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 16);
	// sth r9,18(r10)
	PPC_STORE_U16(ctx.r10.u32 + 18, ctx.r9.u16);
	// lhz r9,4(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 4);
	// sth r9,4(r10)
	PPC_STORE_U16(ctx.r10.u32 + 4, ctx.r9.u16);
	// lhz r9,32(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 32);
	// sth r9,20(r10)
	PPC_STORE_U16(ctx.r10.u32 + 20, ctx.r9.u16);
	// lhz r9,6(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 6);
	// sth r9,6(r10)
	PPC_STORE_U16(ctx.r10.u32 + 6, ctx.r9.u16);
	// lhz r9,48(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 48);
	// sth r9,22(r10)
	PPC_STORE_U16(ctx.r10.u32 + 22, ctx.r9.u16);
	// lhz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 8);
	// sth r9,8(r10)
	PPC_STORE_U16(ctx.r10.u32 + 8, ctx.r9.u16);
	// lhz r9,64(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 64);
	// sth r9,24(r10)
	PPC_STORE_U16(ctx.r10.u32 + 24, ctx.r9.u16);
	// lhz r9,10(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 10);
	// sth r9,10(r10)
	PPC_STORE_U16(ctx.r10.u32 + 10, ctx.r9.u16);
	// lhz r9,80(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 80);
	// sth r9,26(r10)
	PPC_STORE_U16(ctx.r10.u32 + 26, ctx.r9.u16);
	// lhz r9,12(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 12);
	// sth r9,12(r10)
	PPC_STORE_U16(ctx.r10.u32 + 12, ctx.r9.u16);
	// lhz r9,96(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 96);
	// sth r9,28(r10)
	PPC_STORE_U16(ctx.r10.u32 + 28, ctx.r9.u16);
	// lhz r9,14(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 14);
	// sth r9,14(r10)
	PPC_STORE_U16(ctx.r10.u32 + 14, ctx.r9.u16);
	// lhz r9,112(r26)
	ctx.r9.u64 = PPC_LOAD_U16(r26.u32 + 112);
	// sth r9,30(r10)
	PPC_STORE_U16(ctx.r10.u32 + 30, ctx.r9.u16);
	// lhz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// sth r10,0(r26)
	PPC_STORE_U16(r26.u32 + 0, ctx.r10.u16);
	// ble cr6,0x8264cd48
	if (!cr6.gt) goto loc_8264CD48;
	// lwz r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_8264CD04:
	// lhz r11,0(r7)
	r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r10,r26
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r26.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264cd38
	if (cr6.eq) goto loc_8264CD38;
	// mullw r8,r11,r5
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// xor r9,r11,r4
	ctx.r9.u64 = r11.u64 ^ ctx.r4.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// sthx r11,r10,r26
	PPC_STORE_U16(ctx.r10.u32 + r26.u32, r11.u16);
loc_8264CD38:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// addi r7,r7,2
	ctx.r7.s64 = ctx.r7.s64 + 2;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x8264cd04
	if (!cr6.eq) goto loc_8264CD04;
loc_8264CD48:
	// li r11,255
	r11.s64 = 255;
	// lwz r4,500(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	// lwz r10,3180(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 3180);
	// li r6,255
	ctx.r6.s64 = 255;
	// lwz r5,508(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// stw r11,1940(r27)
	PPC_STORE_U32(r27.u32 + 1940, r11.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_8264CD70:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264CD78"))) PPC_WEAK_FUNC(sub_8264CD78);
PPC_FUNC_IMPL(__imp__sub_8264CD78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r16,r10
	r16.u64 = ctx.r10.u64;
	// stw r6,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r6.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r7,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r7.u32);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// cmpwi cr6,r16,0
	cr6.compare<int32_t>(r16.s32, 0, xer);
	// li r24,0
	r24.s64 = 0;
	// beq cr6,0x8264cdd8
	if (cr6.eq) goto loc_8264CDD8;
	// lwz r11,-20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -20);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264cdcc
	if (!cr6.eq) goto loc_8264CDCC;
	// li r11,1
	r11.s64 = 1;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
	// b 0x8264cddc
	goto loc_8264CDDC;
loc_8264CDCC:
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r24.u32);
	// li r18,1
	r18.s64 = 1;
	// b 0x8264cde0
	goto loc_8264CDE0;
loc_8264CDD8:
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r24.u32);
loc_8264CDDC:
	// mr r18,r24
	r18.u64 = r24.u64;
loc_8264CDE0:
	// lwz r29,372(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8264ce24
	if (cr6.eq) goto loc_8264CE24;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264ce18
	if (!cr6.eq) goto loc_8264CE18;
	// li r15,1
	r15.s64 = 1;
	// b 0x8264ce28
	goto loc_8264CE28;
loc_8264CE18:
	// mr r15,r24
	r15.u64 = r24.u64;
	// li r21,1
	r21.s64 = 1;
	// b 0x8264ce2c
	goto loc_8264CE2C;
loc_8264CE24:
	// mr r15,r24
	r15.u64 = r24.u64;
loc_8264CE28:
	// mr r21,r24
	r21.u64 = r24.u64;
loc_8264CE2C:
	// lwz r20,380(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// beq cr6,0x8264ce74
	if (cr6.eq) goto loc_8264CE74;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8264ce68
	if (!cr6.eq) goto loc_8264CE68;
	// li r17,1
	r17.s64 = 1;
	// b 0x8264ce78
	goto loc_8264CE78;
loc_8264CE68:
	// mr r17,r24
	r17.u64 = r24.u64;
	// li r22,1
	r22.s64 = 1;
	// b 0x8264ce7c
	goto loc_8264CE7C;
loc_8264CE74:
	// mr r17,r24
	r17.u64 = r24.u64;
loc_8264CE78:
	// mr r22,r24
	r22.u64 = r24.u64;
loc_8264CE7C:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264ceac
	if (cr6.eq) goto loc_8264CEAC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r11,r11,10,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = r11.s64 + 726;
	// addi r11,r11,729
	r11.s64 = r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r19,r10,r31
	r19.u64 = ctx.r10.u64 + r31.u64;
	// add r25,r11,r31
	r25.u64 = r11.u64 + r31.u64;
	// b 0x8264ceb4
	goto loc_8264CEB4;
loc_8264CEAC:
	// addi r19,r31,2880
	r19.s64 = r31.s64 + 2880;
	// addi r25,r31,2892
	r25.s64 = r31.s64 + 2892;
loc_8264CEB4:
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lbz r29,18(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 18);
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// rlwinm r29,r29,31,31,31
	r29.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x1;
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// bl 0x8264a3d8
	sub_8264A3D8(ctx, base);
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r26)
	PPC_STORE_U16(r26.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r29,1760(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// addi r27,r28,8
	r27.s64 = r28.s64 + 8;
	// addi r29,r26,32
	r29.s64 = r26.s64 + 32;
	// addi r26,r23,24
	r26.s64 = r23.s64 + 24;
	// li r28,1
	r28.s64 = 1;
loc_8264CF98:
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// addi r23,r28,1
	r23.s64 = r28.s64 + 1;
	// lbz r11,18(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 18);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r10,r10,r23
	ctx.r10.u64 = r23.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r23.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// sraw r11,r11,r23
	temp.u32 = r23.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r11.s32 < 0) & (((r11.s32 >> temp.u32) << temp.u32) != r11.s32);
	r11.s64 = r11.s32 >> temp.u32;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// beq cr6,0x8264cfe8
	if (cr6.eq) goto loc_8264CFE8;
	// addi r10,r15,2
	ctx.r10.s64 = r15.s64 + 2;
loc_8264CFE8:
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,1760(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwzx r10,r10,r26
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r26.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r14,1760(r31)
	r14.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r11.u32);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// cmplwi cr6,r28,1
	cr6.compare<uint32_t>(r28.u32, 1, xer);
	// li r11,8
	r11.s64 = 8;
	// bne cr6,0x8264d064
	if (!cr6.eq) goto loc_8264D064;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
loc_8264D064:
	// mr r28,r23
	r28.u64 = r23.u64;
	// add r27,r11,r27
	r27.u64 = r11.u64 + r27.u64;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// addi r26,r26,24
	r26.s64 = r26.s64 + 24;
	// cmplwi cr6,r28,3
	cr6.compare<uint32_t>(r28.u32, 3, xer);
	// blt cr6,0x8264cf98
	if (cr6.lt) goto loc_8264CF98;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2092);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// lwz r5,120(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// mr r7,r17
	ctx.r7.u64 = r17.u64;
	// mr r6,r15
	ctx.r6.u64 = r15.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x826494c0
	sub_826494C0(ctx, base);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// li r8,3
	ctx.r8.s64 = 3;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// lwz r11,296(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 296);
	// lwz r28,1760(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// addi r28,r26,24
	r28.s64 = r26.s64 + 24;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// lbz r27,18(r30)
	r27.u64 = PPC_LOAD_U8(r30.u32 + 18);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// lwz r25,372(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// rlwinm r27,r27,27,31,31
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x1;
	// lwz r26,332(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// lwz r11,208(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// stw r27,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r27.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// bl 0x8264a3d8
	sub_8264A3D8(ctx, base);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// li r8,4
	ctx.r8.s64 = 4;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r27,1760(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82649020
	sub_82649020(ctx, base);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 2096);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r29,r29,32
	r29.s64 = r29.s64 + 32;
	// bl 0x82655558
	sub_82655558(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x8264d2d8
	if (!cr6.eq) goto loc_8264D2D8;
	// lwz r4,208(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// lbz r26,18(r30)
	r26.u64 = PPC_LOAD_U8(r30.u32 + 18);
	// addi r5,r28,24
	ctx.r5.s64 = r28.s64 + 24;
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// rlwinm r28,r26,26,31,31
	r28.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 26) & 0x1;
	// lwz r27,340(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// mr r9,r18
	ctx.r9.u64 = r18.u64;
	// stw r22,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r22.u32);
	// stw r4,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r4.u32);
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// mr r6,r16
	ctx.r6.u64 = r16.u64;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r28,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r28.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8264a3d8
	sub_8264A3D8(ctx, base);
	// lwz r11,1760(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// li r8,5
	ctx.r8.s64 = 5;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// extsh r11,r11
	r11.s64 = r11.s16;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// lwz r30,1760(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 1760);
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// bl 0x82649020
	sub_82649020(ctx, base);
loc_8264D2D8:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264D2E0"))) PPC_WEAK_FUNC(sub_8264D2E0);
PPC_FUNC_IMPL(__imp__sub_8264D2E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r22,0
	r22.s64 = 0;
	// lwz r15,0(r7)
	r15.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r16,r5
	r16.u64 = ctx.r5.u64;
	// lwz r14,4(r7)
	r14.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// li r5,128
	ctx.r5.s64 = 128;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// lwz r28,4(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r24,r22
	r24.u64 = r22.u64;
	// addi r17,r10,1
	r17.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r18,1760(r27)
	r18.u64 = PPC_LOAD_U32(r27.u32 + 1760);
	// mr r19,r22
	r19.u64 = r22.u64;
	// lwz r23,0(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r26,r22
	r26.u64 = r22.u64;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// lwz r20,28(r11)
	r20.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// lwz r21,32(r11)
	r21.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// stw r28,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r28.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r5,128
	ctx.r5.s64 = 128;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ori r25,r11,32768
	r25.u64 = r11.u64 | 32768;
	// bne cr6,0x8264dd64
	if (!cr6.eq) goto loc_8264DD64;
loc_8264D384:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d474
	if (cr6.lt) goto loc_8264D474;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264d46c
	if (!cr6.lt) goto loc_8264D46C;
loc_8264D3D4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264d400
	if (cr6.lt) goto loc_8264D400;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264d3d4
	if (cr6.eq) goto loc_8264D3D4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d4b0
	goto loc_8264D4B0;
loc_8264D400:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264D46C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d4b0
	goto loc_8264D4B0;
loc_8264D474:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264D47C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d47c
	if (cr6.lt) goto loc_8264D47C;
loc_8264D4B0:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264d500
	if (cr6.eq) goto loc_8264D500;
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// blt cr6,0x8264d4c4
	if (cr6.lt) goto loc_8264D4C4;
	// li r24,1
	r24.s64 = 1;
loc_8264D4C4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbzx r11,r30,r20
	r11.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// lbzx r28,r30,r21
	r28.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// extsb r31,r11
	r31.s64 = r11.s8;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264dcd8
	if (!cr0.lt) goto loc_8264DCD8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// b 0x8264dcd8
	goto loc_8264DCD8;
loc_8264D500:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264d52c
	if (!cr0.lt) goto loc_8264D52C;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D52C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264d6c8
	if (cr6.eq) goto loc_8264D6C8;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d624
	if (cr6.lt) goto loc_8264D624;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264d61c
	if (!cr6.lt) goto loc_8264D61C;
loc_8264D584:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264d5b0
	if (cr6.lt) goto loc_8264D5B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264d584
	if (cr6.eq) goto loc_8264D584;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d660
	goto loc_8264D660;
loc_8264D5B0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264D61C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d660
	goto loc_8264D660;
loc_8264D624:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264D62C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d62c
	if (cr6.lt) goto loc_8264D62C;
loc_8264D660:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264e728
	if (cr6.eq) goto loc_8264E728;
	// lbzx r11,r30,r20
	r11.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// lbzx r28,r30,r21
	r28.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// blt cr6,0x8264d688
	if (cr6.lt) goto loc_8264D688;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264d68c
	goto loc_8264D68C;
loc_8264D688:
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8264D68C:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264dcd8
	if (!cr0.lt) goto loc_8264DCD8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// b 0x8264dcd8
	goto loc_8264DCD8;
loc_8264D6C8:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264d6f4
	if (!cr0.lt) goto loc_8264D6F4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D6F4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264d894
	if (cr6.eq) goto loc_8264D894;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d7ec
	if (cr6.lt) goto loc_8264D7EC;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264d7e4
	if (!cr6.lt) goto loc_8264D7E4;
loc_8264D74C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264d778
	if (cr6.lt) goto loc_8264D778;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264d74c
	if (cr6.eq) goto loc_8264D74C;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d828
	goto loc_8264D828;
loc_8264D778:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264D7E4:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264d828
	goto loc_8264D828;
loc_8264D7EC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264D7F4:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264d7f4
	if (cr6.lt) goto loc_8264D7F4;
loc_8264D828:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264e728
	if (cr6.eq) goto loc_8264E728;
	// lbzx r10,r30,r20
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// lbzx r11,r30,r21
	r11.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// lwz r9,1932(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1932);
	// extsb r31,r10
	r31.s64 = ctx.r10.s8;
	// blt cr6,0x8264d854
	if (cr6.lt) goto loc_8264D854;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264d858
	goto loc_8264D858;
loc_8264D854:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8264D858:
	// lbzx r10,r31,r10
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + ctx.r10.u32);
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264dcd8
	if (!cr0.lt) goto loc_8264DCD8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// b 0x8264dcd8
	goto loc_8264DCD8;
loc_8264D894:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264d8c0
	if (!cr0.lt) goto loc_8264D8C0;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D8C0:
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// mr r24,r31
	r24.u64 = r31.u64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x8264db60
	if (cr6.lt) goto loc_8264DB60;
	// lwz r11,1944(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1944);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264d8e8
	if (cr6.eq) goto loc_8264D8E8;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82649548
	sub_82649548(ctx, base);
	// stw r22,1944(r27)
	PPC_STORE_U32(r27.u32 + 1944, r22.u32);
loc_8264D8E8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r30,1952(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1952);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x8264d90c
	if (!cr6.eq) goto loc_8264D90C;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x8264d9ac
	goto loc_8264D9AC;
loc_8264D90C:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264d96c
	if (!cr6.gt) goto loc_8264D96C;
loc_8264D914:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264d96c
	if (cr6.eq) goto loc_8264D96C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264d95c
	if (!cr0.lt) goto loc_8264D95C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D95C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264d914
	if (cr6.gt) goto loc_8264D914;
loc_8264D96C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264d9a8
	if (!cr0.lt) goto loc_8264D9A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D9A8:
	// mr r28,r30
	r28.u64 = r30.u64;
loc_8264D9AC:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264d9d8
	if (!cr0.lt) goto loc_8264D9D8;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264D9D8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r30,1948(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1948);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x8264daac
	if (cr6.eq) goto loc_8264DAAC;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264da08
	if (!cr6.eq) goto loc_8264DA08;
	// mr r30,r22
	r30.u64 = r22.u64;
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264dcc8
	goto loc_8264DCC8;
loc_8264DA08:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264da68
	if (!cr6.gt) goto loc_8264DA68;
loc_8264DA10:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264da68
	if (cr6.eq) goto loc_8264DA68;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264da58
	if (!cr0.lt) goto loc_8264DA58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DA58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264da10
	if (cr6.gt) goto loc_8264DA10;
loc_8264DA68:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264daa4
	if (!cr0.lt) goto loc_8264DAA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DAA4:
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264dcc8
	goto loc_8264DCC8;
loc_8264DAAC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264dabc
	if (!cr6.eq) goto loc_8264DABC;
	// mr r31,r22
	r31.u64 = r22.u64;
	// b 0x8264dcc8
	goto loc_8264DCC8;
loc_8264DABC:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264db1c
	if (!cr6.gt) goto loc_8264DB1C;
loc_8264DAC4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264db1c
	if (cr6.eq) goto loc_8264DB1C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264db0c
	if (!cr0.lt) goto loc_8264DB0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DB0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264dac4
	if (cr6.gt) goto loc_8264DAC4;
loc_8264DB1C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264db58
	if (!cr0.lt) goto loc_8264DB58;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DB58:
	// mr r31,r30
	r31.u64 = r30.u64;
	// b 0x8264dcc8
	goto loc_8264DCC8;
loc_8264DB60:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8264dbd4
	if (!cr6.lt) goto loc_8264DBD4;
loc_8264DB7C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264dbd4
	if (cr6.eq) goto loc_8264DBD4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264dbc4
	if (!cr0.lt) goto loc_8264DBC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DBC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264db7c
	if (cr6.gt) goto loc_8264DB7C;
loc_8264DBD4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264dc10
	if (!cr0.lt) goto loc_8264DC10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DC10:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8264dc88
	if (!cr6.lt) goto loc_8264DC88;
loc_8264DC30:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264dc88
	if (cr6.eq) goto loc_8264DC88;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264dc78
	if (!cr0.lt) goto loc_8264DC78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DC78:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264dc30
	if (cr6.gt) goto loc_8264DC30;
loc_8264DC88:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264dcc4
	if (!cr0.lt) goto loc_8264DCC4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DCC4:
	// extsb r31,r30
	r31.s64 = r30.s8;
loc_8264DCC8:
	// rlwinm r30,r31,1,31,31
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x8264dcd8
	if (!cr6.lt) goto loc_8264DCD8;
	// neg r31,r31
	r31.s64 = -r31.s64;
loc_8264DCD8:
	// lwz r11,84(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// add r10,r28,r19
	ctx.r10.u64 = r28.u64 + r19.u64;
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8264e728
	if (!cr6.eq) goto loc_8264E728;
	// cmplwi cr6,r10,64
	cr6.compare<uint32_t>(ctx.r10.u32, 64, xer);
	// bge cr6,0x8264e728
	if (!cr6.lt) goto loc_8264E728;
	// lwz r11,1828(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1828);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x8264dd1c
	if (cr6.eq) goto loc_8264DD1C;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// li r9,1
	ctx.r9.s64 = 1;
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// or r26,r11,r26
	r26.u64 = r11.u64 | r26.u64;
loc_8264DD1C:
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// lbzx r8,r10,r16
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r16.u32);
	// mullw r9,r31,r15
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r15.s32);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// add r9,r9,r14
	ctx.r9.u64 = ctx.r9.u64 + r14.u64;
	// addi r19,r10,1
	r19.s64 = ctx.r10.s64 + 1;
	// xor r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 ^ r11.u64;
	// rotlwi r8,r8,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// sthx r11,r8,r18
	PPC_STORE_U16(ctx.r8.u32 + r18.u32, r11.u16);
	// bne cr6,0x8264dd54
	if (!cr6.eq) goto loc_8264DD54;
	// lwz r28,96(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// b 0x8264d384
	goto loc_8264D384;
loc_8264DD54:
	// stw r26,1940(r27)
	PPC_STORE_U32(r27.u32 + 1940, r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_8264DD64:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264de54
	if (cr6.lt) goto loc_8264DE54;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264de4c
	if (!cr6.lt) goto loc_8264DE4C;
loc_8264DDB4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264dde0
	if (cr6.lt) goto loc_8264DDE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264ddb4
	if (cr6.eq) goto loc_8264DDB4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264de90
	goto loc_8264DE90;
loc_8264DDE0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264DE4C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264de90
	goto loc_8264DE90;
loc_8264DE54:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264DE5C:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264de5c
	if (cr6.lt) goto loc_8264DE5C;
loc_8264DE90:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264dee4
	if (cr6.eq) goto loc_8264DEE4;
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// blt cr6,0x8264dea4
	if (cr6.lt) goto loc_8264DEA4;
	// li r24,1
	r24.s64 = 1;
loc_8264DEA4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbzx r28,r30,r21
	r28.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264ded4
	if (!cr0.lt) goto loc_8264DED4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DED4:
	// lbzx r10,r30,r20
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// mr r11,r31
	r11.u64 = r31.u64;
	// extsb r31,r10
	r31.s64 = ctx.r10.s8;
	// b 0x8264e6c4
	goto loc_8264E6C4;
loc_8264DEE4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264df10
	if (!cr0.lt) goto loc_8264DF10;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264DF10:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264e0b0
	if (cr6.eq) goto loc_8264E0B0;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264e008
	if (cr6.lt) goto loc_8264E008;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264e000
	if (!cr6.lt) goto loc_8264E000;
loc_8264DF68:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264df94
	if (cr6.lt) goto loc_8264DF94;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264df68
	if (cr6.eq) goto loc_8264DF68;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264e044
	goto loc_8264E044;
loc_8264DF94:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264E000:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264e044
	goto loc_8264E044;
loc_8264E008:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264E010:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264e010
	if (cr6.lt) goto loc_8264E010;
loc_8264E044:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264e728
	if (cr6.eq) goto loc_8264E728;
	// lbzx r11,r30,r20
	r11.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// lbzx r28,r30,r21
	r28.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// blt cr6,0x8264e06c
	if (cr6.lt) goto loc_8264E06C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264e070
	goto loc_8264E070;
loc_8264E06C:
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8264E070:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264e278
	if (!cr0.lt) goto loc_8264E278;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x8264e6c4
	goto loc_8264E6C4;
loc_8264E0B0:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264e0dc
	if (!cr0.lt) goto loc_8264E0DC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E0DC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8264e280
	if (cr6.eq) goto loc_8264E280;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lbz r4,8(r23)
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + 8);
	// lwz r29,0(r23)
	r29.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264e1d4
	if (cr6.lt) goto loc_8264E1D4;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8264e1cc
	if (!cr6.lt) goto loc_8264E1CC;
loc_8264E134:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8264e160
	if (cr6.lt) goto loc_8264E160;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8264e134
	if (cr6.eq) goto loc_8264E134;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264e210
	goto loc_8264E210;
loc_8264E160:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8264E1CC:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8264e210
	goto loc_8264E210;
loc_8264E1D4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8264E1DC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r25
	r11.u64 = r30.u64 + r25.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8264e1dc
	if (cr6.lt) goto loc_8264E1DC;
loc_8264E210:
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// beq cr6,0x8264e728
	if (cr6.eq) goto loc_8264E728;
	// lbzx r10,r30,r20
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + r20.u32);
	// cmpw cr6,r30,r17
	cr6.compare<int32_t>(r30.s32, r17.s32, xer);
	// lbzx r11,r30,r21
	r11.u64 = PPC_LOAD_U8(r30.u32 + r21.u32);
	// lwz r9,1932(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 1932);
	// extsb r31,r10
	r31.s64 = ctx.r10.s8;
	// blt cr6,0x8264e23c
	if (cr6.lt) goto loc_8264E23C;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r24,1
	r24.s64 = 1;
	// b 0x8264e240
	goto loc_8264E240;
loc_8264E23C:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_8264E240:
	// lbzx r10,r31,r10
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + ctx.r10.u32);
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264e278
	if (!cr0.lt) goto loc_8264E278;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E278:
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x8264e6c4
	goto loc_8264E6C4;
loc_8264E280:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264e2ac
	if (!cr0.lt) goto loc_8264E2AC;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E2AC:
	// lwz r11,15472(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 15472);
	// mr r24,r31
	r24.u64 = r31.u64;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x8264e54c
	if (cr6.lt) goto loc_8264E54C;
	// lwz r11,1944(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1944);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8264e2d4
	if (cr6.eq) goto loc_8264E2D4;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82649548
	sub_82649548(ctx, base);
	// stw r22,1944(r27)
	PPC_STORE_U32(r27.u32 + 1944, r22.u32);
loc_8264E2D4:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r30,1952(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1952);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x8264e2f8
	if (!cr6.eq) goto loc_8264E2F8;
	// mr r28,r22
	r28.u64 = r22.u64;
	// b 0x8264e398
	goto loc_8264E398;
loc_8264E2F8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264e358
	if (!cr6.gt) goto loc_8264E358;
loc_8264E300:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264e358
	if (cr6.eq) goto loc_8264E358;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264e348
	if (!cr0.lt) goto loc_8264E348;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E348:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264e300
	if (cr6.gt) goto loc_8264E300;
loc_8264E358:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264e394
	if (!cr0.lt) goto loc_8264E394;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E394:
	// mr r28,r30
	r28.u64 = r30.u64;
loc_8264E398:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r31,r8,0
	r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x8264e3c4
	if (!cr0.lt) goto loc_8264E3C4;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E3C4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// lwz r30,1948(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 1948);
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x8264e498
	if (cr6.eq) goto loc_8264E498;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264e3f4
	if (!cr6.eq) goto loc_8264E3F4;
	// mr r30,r22
	r30.u64 = r22.u64;
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264e6b4
	goto loc_8264E6B4;
loc_8264E3F4:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264e454
	if (!cr6.gt) goto loc_8264E454;
loc_8264E3FC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264e454
	if (cr6.eq) goto loc_8264E454;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264e444
	if (!cr0.lt) goto loc_8264E444;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E444:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264e3fc
	if (cr6.gt) goto loc_8264E3FC;
loc_8264E454:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264e490
	if (!cr0.lt) goto loc_8264E490;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E490:
	// neg r31,r30
	r31.s64 = -r30.s64;
	// b 0x8264e6b4
	goto loc_8264E6B4;
loc_8264E498:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8264e4a8
	if (!cr6.eq) goto loc_8264E4A8;
	// mr r31,r22
	r31.u64 = r22.u64;
	// b 0x8264e6b4
	goto loc_8264E6B4;
loc_8264E4A8:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8264e508
	if (!cr6.gt) goto loc_8264E508;
loc_8264E4B0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264e508
	if (cr6.eq) goto loc_8264E508;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264e4f8
	if (!cr0.lt) goto loc_8264E4F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E4F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264e4b0
	if (cr6.gt) goto loc_8264E4B0;
loc_8264E508:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264e544
	if (!cr0.lt) goto loc_8264E544;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E544:
	// mr r31,r30
	r31.u64 = r30.u64;
	// b 0x8264e6b4
	goto loc_8264E6B4;
loc_8264E54C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,6
	r30.s64 = 6;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x8264e5c0
	if (!cr6.lt) goto loc_8264E5C0;
loc_8264E568:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264e5c0
	if (cr6.eq) goto loc_8264E5C0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264e5b0
	if (!cr0.lt) goto loc_8264E5B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E5B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264e568
	if (cr6.gt) goto loc_8264E568;
loc_8264E5C0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264e5fc
	if (!cr0.lt) goto loc_8264E5FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E5FC:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r30,8
	r30.s64 = 8;
	// mr r29,r22
	r29.u64 = r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x8264e674
	if (!cr6.lt) goto loc_8264E674;
loc_8264E61C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8264e674
	if (cr6.eq) goto loc_8264E674;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8264e664
	if (!cr0.lt) goto loc_8264E664;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E664:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8264e61c
	if (cr6.gt) goto loc_8264E61C;
loc_8264E674:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8264e6b0
	if (!cr0.lt) goto loc_8264E6B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8264E6B0:
	// extsb r31,r30
	r31.s64 = r30.s8;
loc_8264E6B4:
	// rlwinm r11,r31,1,31,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0x1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x8264e6c4
	if (!cr6.lt) goto loc_8264E6C4;
	// neg r31,r31
	r31.s64 = -r31.s64;
loc_8264E6C4:
	// lwz r9,84(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// add r10,r28,r19
	ctx.r10.u64 = r28.u64 + r19.u64;
	// lwz r9,20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8264e728
	if (!cr6.eq) goto loc_8264E728;
	// cmplwi cr6,r10,32
	cr6.compare<uint32_t>(ctx.r10.u32, 32, xer);
	// bge cr6,0x8264e728
	if (!cr6.lt) goto loc_8264E728;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lbzx r8,r10,r16
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + r16.u32);
	// mullw r9,r31,r15
	ctx.r9.s64 = int64_t(r31.s32) * int64_t(r15.s32);
	// not r11,r11
	r11.u64 = ~r11.u64;
	// add r9,r9,r14
	ctx.r9.u64 = ctx.r9.u64 + r14.u64;
	// addi r19,r10,1
	r19.s64 = ctx.r10.s64 + 1;
	// xor r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 ^ r11.u64;
	// rotlwi r8,r8,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// sthx r11,r8,r18
	PPC_STORE_U16(ctx.r8.u32 + r18.u32, r11.u16);
	// bne cr6,0x8264e718
	if (!cr6.eq) goto loc_8264E718;
	// lwz r28,96(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// b 0x8264dd64
	goto loc_8264DD64;
loc_8264E718:
	// stw r22,1940(r27)
	PPC_STORE_U32(r27.u32 + 1940, r22.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
loc_8264E728:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264E734"))) PPC_WEAK_FUNC(sub_8264E734);
PPC_FUNC_IMPL(__imp__sub_8264E734) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8264E738"))) PPC_WEAK_FUNC(sub_8264E738);
PPC_FUNC_IMPL(__imp__sub_8264E738) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r10,r1,-241
	ctx.r10.s64 = ctx.r1.s64 + -241;
	// lwz r11,256(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r15,r10,0,0,27
	r15.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r15,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r15.u32);
	// beq cr6,0x8264ed3c
	if (cr6.eq) goto loc_8264ED3C;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8264e8b8
	if (cr6.eq) goto loc_8264E8B8;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8264E774:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x8264e774
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E774;
	// add r11,r5,r6
	r11.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r10,r4,r6
	ctx.r10.u64 = ctx.r4.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E7A0:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e7a0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E7A0;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E7CC:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e7cc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E7CC;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E7F8:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e7f8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E7F8;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E824:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e824
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E824;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E850:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e850
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E850;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_8264E87C:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x8264e87c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E87C;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8264E8A0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x8264e8a0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8264E8A0;
	// b 0x8239bd10
	return;
loc_8264E8B8:
	// addi r10,r5,1
	ctx.r10.s64 = ctx.r5.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r9,r5,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r7,r4,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r4.s64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bne cr6,0x8264ea94
	if (!cr6.eq) goto loc_8264EA94;
	// li r5,8
	ctx.r5.s64 = 8;
loc_8264E8D8:
	// lbz r3,-1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r8,r10,5
	ctx.r8.s64 = ctx.r10.s64 + 5;
	// lbz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,-2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbzx r30,r7,r9
	r30.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r4.u8);
	// lbz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r4,r7,r9
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// lbz r31,-1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,2(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r4.u8);
	// lbz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbzx r4,r7,r9
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r4.u8);
	// lbz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r4,3(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbzx r31,r7,r9
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r4.u8);
	// lbz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r4,3(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r4.u8);
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,6(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r4.u8);
	// lbz r4,6(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r30,7(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// rlwinm r3,r4,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - r30.s64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// stb r4,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, ctx.r4.u8);
	// lbz r3,6(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lbz r4,7(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// lbz r31,8(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// subf r8,r3,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r3.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, ctx.r8.u8);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// bne cr6,0x8264e8d8
	if (!cr6.eq) goto loc_8264E8D8;
	// b 0x8239bd10
	return;
loc_8264EA94:
	// li r3,8
	ctx.r3.s64 = 8;
loc_8264EA98:
	// lbz r5,-1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r4,-2(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r31,r7,r9
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r8.u8);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r5,-2(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + -2);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r8.u8);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r8,r7,r9
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// lbz r4,-1(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r8.u8);
	// lbzx r8,r7,r9
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// lbz r5,-1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r8.u8);
	// lbz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbzx r8,r7,r9
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// lbz r4,3(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// lbz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// lbz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r8,3(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbzx r4,r7,r9
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + ctx.r9.u32);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// subf r8,r4,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r5,r8,8
	ctx.r5.s64 = ctx.r8.s64 + 8;
	// addi r8,r10,5
	ctx.r8.s64 = ctx.r10.s64 + 5;
	// srawi r4,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r5.s32 >> 4;
	// addi r5,r10,7
	ctx.r5.s64 = ctx.r10.s64 + 7;
	// lbzx r31,r4,r11
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// addi r4,r9,5
	ctx.r4.s64 = ctx.r9.s64 + 5;
	// stb r31,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, r31.u8);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r30,1(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stb r31,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, r31.u8);
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r29,2(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r28,0(r8)
	r28.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, r31.u8);
	// clrlwi r31,r31,24
	r31.u64 = r31.u32 & 0xFF;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, r31.u8);
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r29,3(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r28,6(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, r31.u8);
	// lbz r31,0(r8)
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r30,3(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, r31.u8);
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r31,6(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lbz r29,4(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r28,0(r5)
	r28.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r31.u8);
	// clrlwi r31,r31,24
	r31.u64 = r31.u32 & 0xFF;
	// lbz r30,6(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r31.u8);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbz r30,6(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lbz r31,0(r5)
	r31.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbz r29,8(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r8,r31,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// subf r8,r29,r8
	ctx.r8.s64 = ctx.r8.s64 - r29.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r8.u8);
	// lbz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r8.u8);
	// bne cr6,0x8264ea98
	if (!cr6.eq) goto loc_8264EA98;
	// b 0x8239bd10
	return;
loc_8264ED3C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// beq cr6,0x8264f37c
	if (cr6.eq) goto loc_8264F37C;
	// subf r7,r6,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r6.s64;
	// stw r10,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, ctx.r10.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r5,r10,r6
	ctx.r5.u64 = ctx.r10.u64 + ctx.r6.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// subf r3,r6,r7
	ctx.r3.s64 = ctx.r7.s64 - ctx.r6.s64;
	// bne cr6,0x8264ef50
	if (!cr6.eq) goto loc_8264EF50;
	// subfic r31,r6,1
	xer.ca = ctx.r6.u32 <= 1;
	r31.s64 = 1 - ctx.r6.s64;
	// subfic r30,r6,2
	xer.ca = ctx.r6.u32 <= 2;
	r30.s64 = 2 - ctx.r6.s64;
	// subfic r29,r6,3
	xer.ca = ctx.r6.u32 <= 3;
	r29.s64 = 3 - ctx.r6.s64;
	// subfic r28,r6,4
	xer.ca = ctx.r6.u32 <= 4;
	r28.s64 = 4 - ctx.r6.s64;
	// addi r9,r7,2
	ctx.r9.s64 = ctx.r7.s64 + 2;
	// addi r8,r4,2
	ctx.r8.s64 = ctx.r4.s64 + 2;
	// addi r10,r5,2
	ctx.r10.s64 = ctx.r5.s64 + 2;
	// addi r27,r6,-1
	r27.s64 = ctx.r6.s64 + -1;
	// addi r26,r6,-2
	r26.s64 = ctx.r6.s64 + -2;
	// subfic r25,r6,5
	xer.ca = ctx.r6.u32 <= 5;
	r25.s64 = 5 - ctx.r6.s64;
	// li r7,8
	ctx.r7.s64 = 8;
loc_8264ED90:
	// lbz r4,-2(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + -2);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lbzx r5,r9,r26
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r26.u32);
	// lbz r24,-2(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,0(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r5.u8);
	// lbz r4,-1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// lbzx r5,r27,r9
	ctx.r5.u64 = PPC_LOAD_U8(r27.u32 + ctx.r9.u32);
	// lbz r24,-1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,1(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r5.u8);
	// lbz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbzx r5,r9,r6
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
	// lbz r24,2(r3)
	r24.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,0(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r5.u8);
	// lbz r4,1(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r5,r10,r31
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + r31.u32);
	// lbz r24,1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,3(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r5.u8);
	// lbz r4,2(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbzx r5,r10,r30
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + r30.u32);
	// lbz r24,2(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,4(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, ctx.r5.u8);
	// lbz r4,3(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// lbzx r5,r10,r29
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + r29.u32);
	// lbz r24,3(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,5(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, ctx.r5.u8);
	// lbzx r5,r10,r28
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + r28.u32);
	// lbz r4,4(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r24,4(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r23,6(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 6);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, ctx.r5.u8);
	// lbz r4,5(r9)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lbzx r5,r10,r25
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + r25.u32);
	// lbz r24,5(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r23,7(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 7);
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// rlwinm r4,r5,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// subf r5,r24,r5
	ctx.r5.s64 = ctx.r5.s64 - r24.s64;
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, ctx.r5.u8);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// bne cr6,0x8264ed90
	if (!cr6.eq) goto loc_8264ED90;
	// b 0x8239bd10
	return;
loc_8264EF50:
	// addi r10,r3,2
	ctx.r10.s64 = ctx.r3.s64 + 2;
	// addi r8,r15,2
	ctx.r8.s64 = r15.s64 + 2;
	// li r26,11
	r26.s64 = 11;
loc_8264EF5C:
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// add r23,r5,r7
	r23.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lbz r30,3(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r29,4(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r22,r31,r5
	r22.u64 = r31.u64 + ctx.r5.u64;
	// rlwinm r16,r23,3,0,28
	r16.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r28,5(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r21,r30,r31
	r21.u64 = r30.u64 + r31.u64;
	// lbz r25,-2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r23,r23,r16
	r23.u64 = r23.u64 + r16.u64;
	// lbz r27,-1(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// rlwinm r4,r21,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r24,6(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r20,r29,r30
	r20.u64 = r29.u64 + r30.u64;
	// lbz r14,-3(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// add r4,r21,r4
	ctx.r4.u64 = r21.u64 + ctx.r4.u64;
	// add r19,r28,r29
	r19.u64 = r28.u64 + r29.u64;
	// stw r23,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r23.u32);
	// rlwinm r16,r20,3,0,28
	r16.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r23,r19,3,0,28
	r23.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// add r20,r20,r16
	r20.u64 = r20.u64 + r16.u64;
	// add r17,r27,r7
	r17.u64 = r27.u64 + ctx.r7.u64;
	// add r18,r25,r27
	r18.u64 = r25.u64 + r27.u64;
	// add r23,r19,r23
	r23.u64 = r19.u64 + r23.u64;
	// subf r4,r29,r4
	ctx.r4.s64 = ctx.r4.s64 - r29.s64;
	// rlwinm r15,r22,3,0,28
	r15.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// add r22,r22,r15
	r22.u64 = r22.u64 + r15.u64;
	// subf r22,r30,r22
	r22.s64 = r22.s64 - r30.s64;
	// subf r22,r7,r22
	r22.s64 = r22.s64 - ctx.r7.s64;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// lwz r21,-292(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// subf r21,r31,r21
	r21.s64 = r21.s64 - r31.s64;
	// subf r27,r27,r21
	r27.s64 = r21.s64 - r27.s64;
	// subf r21,r28,r20
	r21.s64 = r20.s64 - r28.s64;
	// subf r20,r24,r23
	r20.s64 = r23.s64 - r24.s64;
	// subf r23,r5,r4
	r23.s64 = ctx.r4.s64 - ctx.r5.s64;
	// addi r4,r27,8
	ctx.r4.s64 = r27.s64 + 8;
	// subf r27,r31,r21
	r27.s64 = r21.s64 - r31.s64;
	// subf r31,r30,r20
	r31.s64 = r20.s64 - r30.s64;
	// rlwinm r30,r18,3,0,28
	r30.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r21,r31,8
	r21.s64 = r31.s64 + 8;
	// rlwinm r31,r17,3,0,28
	r31.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r18,r30
	r30.u64 = r18.u64 + r30.u64;
	// add r31,r17,r31
	r31.u64 = r17.u64 + r31.u64;
	// subf r30,r14,r30
	r30.s64 = r30.s64 - r14.s64;
	// subf r31,r5,r31
	r31.s64 = r31.s64 - ctx.r5.s64;
	// subf r5,r7,r30
	ctx.r5.s64 = r30.s64 - ctx.r7.s64;
	// subf r7,r25,r31
	ctx.r7.s64 = r31.s64 - r25.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// srawi r31,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	r31.s64 = r22.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// srawi r30,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	r30.s64 = r23.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// srawi r25,r21,4
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xF) != 0);
	r25.s64 = r21.s32 >> 4;
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r27,r27,r11
	r27.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// lbzx r25,r25,r11
	r25.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// stb r7,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r7.u8);
	// add r7,r24,r28
	ctx.r7.u64 = r24.u64 + r28.u64;
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// lbz r4,7(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// stb r5,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r5.u8);
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stb r31,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r31.u8);
	// stb r30,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, r30.u8);
	// stb r27,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, r27.u8);
	// stb r25,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, r25.u8);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r7,r29,r7
	ctx.r7.s64 = ctx.r7.s64 - r29.s64;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, ctx.r7.u8);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x8264ef5c
	if (!cr6.eq) goto loc_8264EF5C;
	// lwz r8,-296(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r7,28(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// subf r29,r6,r8
	r29.s64 = ctx.r8.s64 - ctx.r6.s64;
	// lwz r10,-304(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r29,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r29.u32);
	// stw r8,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, ctx.r8.u32);
	// li r8,8
	ctx.r8.s64 = 8;
	// stw r8,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r8.u32);
loc_8264F0EC:
	// lwz r8,-292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// add r30,r9,r6
	r30.u64 = ctx.r9.u64 + ctx.r6.u64;
	// subf r25,r9,r29
	r25.s64 = r29.s64 - ctx.r9.s64;
	// add r31,r8,r9
	r31.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r7,r30,2
	ctx.r7.s64 = r30.s64 + 2;
	// subf r28,r9,r31
	r28.s64 = r31.s64 - ctx.r9.s64;
	// stw r30,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r30.u32);
	// subf r22,r9,r30
	r22.s64 = r30.s64 - ctx.r9.s64;
	// addi r5,r31,3
	ctx.r5.s64 = r31.s64 + 3;
	// addi r8,r3,1
	ctx.r8.s64 = ctx.r3.s64 + 1;
	// subf r24,r9,r3
	r24.s64 = ctx.r3.s64 - ctx.r9.s64;
	// stw r28,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r28.u32);
	// subf r28,r3,r31
	r28.s64 = r31.s64 - ctx.r3.s64;
	// subf r23,r3,r29
	r23.s64 = r29.s64 - ctx.r3.s64;
	// li r4,2
	ctx.r4.s64 = 2;
	// stw r28,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r28.u32);
	// subf r28,r30,r29
	r28.s64 = r29.s64 - r30.s64;
	// subf r30,r30,r31
	r30.s64 = r31.s64 - r30.s64;
	// subf r31,r31,r29
	r31.s64 = r29.s64 - r31.s64;
	// stw r28,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r28.u32);
	// stw r30,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r30.u32);
	// stw r31,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r31.u32);
loc_8264F144:
	// lbz r28,0(r9)
	r28.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbzx r31,r9,r25
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + r25.u32);
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r30,8(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r29,9(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// lbz r27,2(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r26,r31,3,0,28
	r26.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// lbz r28,10(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// add r31,r31,r26
	r31.u64 = r31.u64 + r26.u64;
	// lbzx r21,r9,r24
	r21.u64 = PPC_LOAD_U8(ctx.r9.u32 + r24.u32);
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// lbz r20,16(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// rlwinm r27,r30,3,0,28
	r27.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r19,r9,r22
	r19.u64 = PPC_LOAD_U8(ctx.r9.u32 + r22.u32);
	// subf r31,r21,r31
	r31.s64 = r31.s64 - r21.s64;
	// lbz r18,-8(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// lbz r17,17(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// lbz r16,-7(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// subf r27,r20,r30
	r27.s64 = r30.s64 - r20.s64;
	// lbz r15,18(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// subf r30,r19,r31
	r30.s64 = r31.s64 - r19.s64;
	// lbz r14,-6(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// subf r31,r18,r27
	r31.s64 = r27.s64 - r18.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// srawi r27,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r27.s64 = r31.s32 >> 4;
	// rlwinm r31,r29,3,0,28
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// add r26,r29,r31
	r26.u64 = r29.u64 + r31.u64;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// rlwinm r31,r28,3,0,28
	r31.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r29,r27,r11
	r29.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// subf r27,r17,r26
	r27.s64 = r26.s64 - r17.s64;
	// add r31,r28,r31
	r31.u64 = r28.u64 + r31.u64;
	// add r29,r30,r29
	r29.u64 = r30.u64 + r29.u64;
	// subf r30,r16,r27
	r30.s64 = r27.s64 - r16.s64;
	// lwz r27,-272(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// subf r31,r15,r31
	r31.s64 = r31.s64 - r15.s64;
	// addi r26,r30,8
	r26.s64 = r30.s64 + 8;
	// srawi r30,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r30.s64 = r29.s32 >> 1;
	// subf r31,r14,r31
	r31.s64 = r31.s64 - r14.s64;
	// subf r29,r3,r27
	r29.s64 = r27.s64 - ctx.r3.s64;
	// addi r28,r31,8
	r28.s64 = r31.s64 + 8;
	// lwz r31,-296(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stbx r30,r9,r31
	PPC_STORE_U8(ctx.r9.u32 + r31.u32, r30.u8);
	// lbz r30,1(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r31,r8,r23
	r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + r23.u32);
	// lbzx r29,r8,r29
	r29.u64 = PPC_LOAD_U8(ctx.r8.u32 + r29.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r21,0(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// lwz r29,-280(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// subf r31,r21,r31
	r31.s64 = r31.s64 - r21.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// srawi r30,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r30.s64 = r26.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// srawi r30,r28,4
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xF) != 0);
	r30.s64 = r28.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stbx r31,r8,r29
	PPC_STORE_U8(ctx.r8.u32 + r29.u32, r31.u8);
	// lwz r31,-276(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// lbz r29,2(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r31,r7,r31
	r31.u64 = PPC_LOAD_U8(ctx.r7.u32 + r31.u32);
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// rlwinm r29,r31,3,0,28
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// lbz r29,1(r8)
	r29.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// subf r29,r29,r31
	r29.s64 = r31.s64 - r29.s64;
	// lbz r26,0(r7)
	r26.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbz r28,3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// subf r29,r26,r29
	r29.s64 = r29.s64 - r26.s64;
	// lbz r31,11(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// lbz r26,19(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// lbz r28,-5(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lbzx r29,r29,r11
	r29.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lwz r29,-284(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stbx r30,r7,r29
	PPC_STORE_U8(ctx.r7.u32 + r29.u32, r30.u8);
	// lwz r30,-300(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// lbz r29,3(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lbz r21,2(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r20,1(r7)
	r20.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lbzx r30,r5,r30
	r30.u64 = PPC_LOAD_U8(ctx.r5.u32 + r30.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r29,r31,3,0,28
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// subf r31,r26,r31
	r31.s64 = r31.s64 - r26.s64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// addi r29,r31,8
	r29.s64 = r31.s64 + 8;
	// rlwinm r31,r30,3,0,28
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// subf r31,r21,r31
	r31.s64 = r31.s64 - r21.s64;
	// subf r31,r20,r31
	r31.s64 = r31.s64 - r20.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// srawi r30,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r30.s64 = r29.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r31,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r31.u8);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x8264f144
	if (!cr6.eq) goto loc_8264F144;
	// lwz r9,-304(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// lwz r7,-288(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// add r29,r7,r6
	r29.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r8,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r8.u32);
	// stw r29,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r29.u32);
	// bne cr6,0x8264f0ec
	if (!cr6.eq) goto loc_8264F0EC;
	// b 0x8239bd10
	return;
loc_8264F37C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// subf r10,r6,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r6.s64;
	// subf r14,r6,r10
	r14.s64 = ctx.r10.s64 - ctx.r6.s64;
	// stw r9,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r9.u32);
	// addi r10,r14,1
	ctx.r10.s64 = r14.s64 + 1;
	// stw r14,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r14.u32);
	// bne cr6,0x8264f6d0
	if (!cr6.eq) goto loc_8264F6D0;
	// addi r9,r15,2
	ctx.r9.s64 = r15.s64 + 2;
	// li r28,11
	r28.s64 = 11;
loc_8264F3A4:
	// lbz r30,-1(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,1(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// add r25,r30,r8
	r25.u64 = r30.u64 + ctx.r8.u64;
	// lbz r5,2(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r24,r7,r8
	r24.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lbz r3,3(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// rlwinm r18,r25,3,0,28
	r18.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r23,r5,r7
	r23.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lbz r29,5(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r25,r25,r18
	r25.u64 = r25.u64 + r18.u64;
	// lbz r26,-2(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// rlwinm r17,r23,3,0,28
	r17.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r27,6(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r22,r3,r5
	r22.u64 = ctx.r3.u64 + ctx.r5.u64;
	// lbz r14,-3(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// rlwinm r16,r24,3,0,28
	r16.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// add r21,r31,r3
	r21.u64 = r31.u64 + ctx.r3.u64;
	// stw r25,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r25.u32);
	// add r23,r23,r17
	r23.u64 = r23.u64 + r17.u64;
	// add r20,r29,r31
	r20.u64 = r29.u64 + r31.u64;
	// add r16,r24,r16
	r16.u64 = r24.u64 + r16.u64;
	// rlwinm r18,r22,3,0,28
	r18.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r24,r21,3,0,28
	r24.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r25,r20,3,0,28
	r25.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r23,r3,r23
	r23.s64 = r23.s64 - ctx.r3.s64;
	// add r22,r22,r18
	r22.u64 = r22.u64 + r18.u64;
	// add r20,r20,r25
	r20.u64 = r20.u64 + r25.u64;
	// add r21,r21,r24
	r21.u64 = r21.u64 + r24.u64;
	// add r19,r30,r26
	r19.u64 = r30.u64 + r26.u64;
	// subf r18,r5,r16
	r18.s64 = r16.s64 - ctx.r5.s64;
	// subf r22,r31,r22
	r22.s64 = r22.s64 - r31.s64;
	// subf r25,r30,r18
	r25.s64 = r18.s64 - r30.s64;
	// subf r30,r7,r22
	r30.s64 = r22.s64 - ctx.r7.s64;
	// subf r21,r29,r21
	r21.s64 = r21.s64 - r29.s64;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// subf r5,r5,r21
	ctx.r5.s64 = r21.s64 - ctx.r5.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// lwz r17,-300(r1)
	r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// subf r17,r7,r17
	r17.s64 = r17.s64 - ctx.r7.s64;
	// subf r24,r26,r17
	r24.s64 = r17.s64 - r26.s64;
	// subf r26,r8,r23
	r26.s64 = r23.s64 - ctx.r8.s64;
	// subf r23,r27,r20
	r23.s64 = r20.s64 - r27.s64;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// subf r7,r3,r23
	ctx.r7.s64 = r23.s64 - ctx.r3.s64;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// addi r30,r7,8
	r30.s64 = ctx.r7.s64 + 8;
	// rlwinm r7,r19,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r26,r26,8
	r26.s64 = r26.s64 + 8;
	// add r7,r19,r7
	ctx.r7.u64 = r19.u64 + ctx.r7.u64;
	// subf r7,r14,r7
	ctx.r7.s64 = ctx.r7.s64 - r14.s64;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// srawi r7,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	ctx.r7.s64 = r24.s32 >> 4;
	// srawi r25,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r25.s64 = r25.s32 >> 4;
	// srawi r26,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r26.s64 = r26.s32 >> 4;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// lbzx r25,r25,r11
	r25.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// lbzx r26,r26,r11
	r26.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stb r8,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r8.u8);
	// add r8,r27,r29
	ctx.r8.u64 = r27.u64 + r29.u64;
	// stb r7,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r7.u8);
	// stb r25,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r25.u8);
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stb r5,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r5.u8);
	// lbz r5,7(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// stb r26,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, r26.u8);
	// stb r3,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r3.u8);
	// stb r30,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r30.u8);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, ctx.r8.u8);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne cr6,0x8264f3a4
	if (!cr6.eq) goto loc_8264F3A4;
	// addi r9,r4,2
	ctx.r9.s64 = ctx.r4.s64 + 2;
	// addi r10,r15,8
	ctx.r10.s64 = r15.s64 + 8;
	// li r8,8
	ctx.r8.s64 = 8;
loc_8264F518:
	// lbz r4,1(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r7,9(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// lbz r5,10(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r4,11(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// lbz r3,12(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 12);
	// add r4,r4,r31
	ctx.r4.u64 = ctx.r4.u64 + r31.u64;
	// lbz r31,13(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 13);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lbz r30,5(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r30,8(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lbz r26,17(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r25,-7(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// rlwinm r29,r7,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r24,18(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// lbz r23,-6(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// add r29,r7,r29
	r29.u64 = ctx.r7.u64 + r29.u64;
	// lbz r22,19(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// rlwinm r7,r5,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r21,-5(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// subf r29,r26,r29
	r29.s64 = r29.s64 - r26.s64;
	// lbz r20,20(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 20);
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// lbz r19,-4(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + -4);
	// subf r7,r25,r29
	ctx.r7.s64 = r29.s64 - r25.s64;
	// lbz r18,21(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// subf r29,r24,r5
	r29.s64 = ctx.r5.s64 - r24.s64;
	// lbz r17,-3(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// rlwinm r5,r4,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r28,16(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// addi r26,r7,8
	r26.s64 = ctx.r7.s64 + 8;
	// lbz r27,-8(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// subf r7,r23,r29
	ctx.r7.s64 = r29.s64 - r23.s64;
	// add r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 + ctx.r5.u64;
	// addi r4,r7,8
	ctx.r4.s64 = ctx.r7.s64 + 8;
	// subf r7,r22,r5
	ctx.r7.s64 = ctx.r5.s64 - r22.s64;
	// rlwinm r5,r3,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r7,r21,r7
	ctx.r7.s64 = ctx.r7.s64 - r21.s64;
	// add r5,r3,r5
	ctx.r5.u64 = ctx.r3.u64 + ctx.r5.u64;
	// addi r3,r7,8
	ctx.r3.s64 = ctx.r7.s64 + 8;
	// subf r7,r20,r5
	ctx.r7.s64 = ctx.r5.s64 - r20.s64;
	// subf r5,r19,r7
	ctx.r5.s64 = ctx.r7.s64 - r19.s64;
	// rlwinm r7,r31,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r29,r5,8
	r29.s64 = ctx.r5.s64 + 8;
	// add r7,r31,r7
	ctx.r7.u64 = r31.u64 + ctx.r7.u64;
	// subf r7,r18,r7
	ctx.r7.s64 = ctx.r7.s64 - r18.s64;
	// subf r5,r17,r7
	ctx.r5.s64 = ctx.r7.s64 - r17.s64;
	// rlwinm r7,r30,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// subf r7,r28,r7
	ctx.r7.s64 = ctx.r7.s64 - r28.s64;
	// subf r7,r27,r7
	ctx.r7.s64 = ctx.r7.s64 - r27.s64;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// srawi r31,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r31.s64 = r26.s32 >> 4;
	// srawi r4,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 4;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// srawi r30,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r30.s64 = r29.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r7.u8);
	// lbzx r7,r31,r11
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stb r7,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r7.u8);
	// lbzx r7,r4,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// lbz r4,6(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// stb r7,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r7.u8);
	// lbzx r7,r3,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// addi r3,r5,8
	ctx.r3.s64 = ctx.r5.s64 + 8;
	// lbz r5,15(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 15);
	// stb r7,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r7.u8);
	// lbzx r7,r30,r11
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stb r7,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, ctx.r7.u8);
	// lbz r7,14(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 14);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// lbz r4,7(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbz r31,22(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 22);
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// lbz r30,-2(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// lbz r29,23(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 23);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lbz r28,-1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// lbzx r4,r3,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r4,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, ctx.r4.u8);
	// rlwinm r4,r7,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r7,r4
	ctx.r4.u64 = ctx.r7.u64 + ctx.r4.u64;
	// rlwinm r7,r5,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// add r5,r5,r7
	ctx.r5.u64 = ctx.r5.u64 + ctx.r7.u64;
	// subf r7,r30,r4
	ctx.r7.s64 = ctx.r4.s64 - r30.s64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// addi r4,r7,8
	ctx.r4.s64 = ctx.r7.s64 + 8;
	// subf r7,r28,r5
	ctx.r7.s64 = ctx.r5.s64 - r28.s64;
	// srawi r5,r4,4
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r4.s32 >> 4;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, ctx.r5.u8);
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, ctx.r7.u8);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// bne cr6,0x8264f518
	if (!cr6.eq) goto loc_8264F518;
	// b 0x8239bd10
	return;
loc_8264F6D0:
	// addi r8,r15,2
	ctx.r8.s64 = r15.s64 + 2;
	// li r27,11
	r27.s64 = 11;
loc_8264F6D8:
	// lbz r29,-1(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r5,1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// add r24,r7,r29
	r24.u64 = ctx.r7.u64 + r29.u64;
	// lbz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// add r23,r7,r5
	r23.u64 = ctx.r7.u64 + ctx.r5.u64;
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// rlwinm r17,r24,3,0,28
	r17.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r22,r3,r5
	r22.u64 = ctx.r3.u64 + ctx.r5.u64;
	// lbz r28,5(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r24,r24,r17
	r24.u64 = r24.u64 + r17.u64;
	// lbz r25,-2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// rlwinm r16,r22,3,0,28
	r16.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r26,6(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r21,r31,r3
	r21.u64 = r31.u64 + ctx.r3.u64;
	// lbz r4,-3(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// rlwinm r15,r23,3,0,28
	r15.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// add r20,r30,r31
	r20.u64 = r30.u64 + r31.u64;
	// stw r24,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r24.u32);
	// add r22,r22,r16
	r22.u64 = r22.u64 + r16.u64;
	// add r19,r28,r30
	r19.u64 = r28.u64 + r30.u64;
	// add r15,r23,r15
	r15.u64 = r23.u64 + r15.u64;
	// rlwinm r17,r21,3,0,28
	r17.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r23,r20,3,0,28
	r23.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r24,r19,3,0,28
	r24.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r22,r7,r22
	r22.s64 = r22.s64 - ctx.r7.s64;
	// add r21,r21,r17
	r21.u64 = r21.u64 + r17.u64;
	// add r20,r20,r23
	r20.u64 = r20.u64 + r23.u64;
	// add r19,r19,r24
	r19.u64 = r19.u64 + r24.u64;
	// add r18,r29,r25
	r18.u64 = r29.u64 + r25.u64;
	// subf r17,r3,r15
	r17.s64 = r15.s64 - ctx.r3.s64;
	// subf r21,r30,r21
	r21.s64 = r21.s64 - r30.s64;
	// subf r24,r29,r17
	r24.s64 = r17.s64 - r29.s64;
	// subf r29,r5,r21
	r29.s64 = r21.s64 - ctx.r5.s64;
	// subf r20,r28,r20
	r20.s64 = r20.s64 - r28.s64;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// subf r3,r3,r20
	ctx.r3.s64 = r20.s64 - ctx.r3.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// lwz r16,-300(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// subf r16,r5,r16
	r16.s64 = r16.s64 - ctx.r5.s64;
	// subf r23,r25,r16
	r23.s64 = r16.s64 - r25.s64;
	// subf r25,r31,r22
	r25.s64 = r22.s64 - r31.s64;
	// subf r22,r26,r19
	r22.s64 = r19.s64 - r26.s64;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// subf r5,r31,r22
	ctx.r5.s64 = r22.s64 - r31.s64;
	// addi r31,r29,8
	r31.s64 = r29.s64 + 8;
	// addi r29,r5,8
	r29.s64 = ctx.r5.s64 + 8;
	// rlwinm r5,r18,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// add r5,r18,r5
	ctx.r5.u64 = r18.u64 + ctx.r5.u64;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// subf r7,r7,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r7.s64;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// srawi r5,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	ctx.r5.s64 = r23.s32 >> 4;
	// srawi r4,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	ctx.r4.s64 = r24.s32 >> 4;
	// srawi r25,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r25.s64 = r25.s32 >> 4;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// lbzx r4,r4,r11
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r4.u32 + r11.u32);
	// lbzx r25,r25,r11
	r25.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r29,r29,r11
	r29.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// stb r7,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r7.u8);
	// add r7,r26,r28
	ctx.r7.u64 = r26.u64 + r28.u64;
	// stb r4,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r4.u8);
	// lbz r4,7(r10)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// stb r5,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r5.u8);
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stb r25,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r25.u8);
	// stb r31,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, r31.u8);
	// stb r3,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, ctx.r3.u8);
	// stb r29,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, r29.u8);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// subf r7,r4,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r7,r30,r7
	ctx.r7.s64 = ctx.r7.s64 - r30.s64;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// srawi r7,r7,4
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
	// lbzx r7,r7,r11
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + r11.u32);
	// stb r7,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, ctx.r7.u8);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x8264f6d8
	if (!cr6.eq) goto loc_8264F6D8;
	// lwz r8,-284(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// lwz r10,-304(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// subf r30,r6,r8
	r30.s64 = ctx.r8.s64 - ctx.r6.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r30,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r30.u32);
	// lwz r7,28(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r8,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r8.u32);
	// li r8,8
	ctx.r8.s64 = 8;
	// stw r8,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
loc_8264F868:
	// add r31,r9,r6
	r31.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r8,-300(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// subf r26,r9,r30
	r26.s64 = r30.s64 - ctx.r9.s64;
	// subf r29,r14,r31
	r29.s64 = r31.s64 - r14.s64;
	// add r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r7,r31,2
	ctx.r7.s64 = r31.s64 + 2;
	// stw r31,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, r31.u32);
	// subf r23,r9,r31
	r23.s64 = r31.s64 - ctx.r9.s64;
	// addi r5,r3,3
	ctx.r5.s64 = ctx.r3.s64 + 3;
	// stw r29,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r29.u32);
	// subf r29,r14,r3
	r29.s64 = ctx.r3.s64 - r14.s64;
	// subf r22,r9,r3
	r22.s64 = ctx.r3.s64 - ctx.r9.s64;
	// addi r8,r14,1
	ctx.r8.s64 = r14.s64 + 1;
	// subf r25,r9,r14
	r25.s64 = r14.s64 - ctx.r9.s64;
	// subf r24,r14,r30
	r24.s64 = r30.s64 - r14.s64;
	// stw r29,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r29.u32);
	// subf r29,r31,r30
	r29.s64 = r30.s64 - r31.s64;
	// subf r31,r31,r3
	r31.s64 = ctx.r3.s64 - r31.s64;
	// subf r3,r3,r30
	ctx.r3.s64 = r30.s64 - ctx.r3.s64;
	// li r4,2
	ctx.r4.s64 = 2;
	// stw r29,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r29.u32);
	// stw r31,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, r31.u32);
	// stw r3,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r3.u32);
loc_8264F8C4:
	// lbz r29,0(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// lbzx r3,r9,r26
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + r26.u32);
	// lbz r30,0(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r31,8(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// lbz r29,1(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r30,9(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// lbz r28,2(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r27,r3,3,0,28
	r27.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r29,10(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// lbzx r21,r23,r9
	r21.u64 = PPC_LOAD_U8(r23.u32 + ctx.r9.u32);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// lbz r20,16(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// rlwinm r28,r31,3,0,28
	r28.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r19,r25,r9
	r19.u64 = PPC_LOAD_U8(r25.u32 + ctx.r9.u32);
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// lbz r18,-8(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// lbz r17,17(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// lbz r16,-7(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// subf r28,r20,r31
	r28.s64 = r31.s64 - r20.s64;
	// lbz r15,18(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// subf r31,r19,r3
	r31.s64 = ctx.r3.s64 - r19.s64;
	// lbz r14,-6(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// subf r3,r18,r28
	ctx.r3.s64 = r28.s64 - r18.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// srawi r28,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	r28.s64 = ctx.r3.s32 >> 4;
	// rlwinm r3,r30,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r27,r30,r3
	r27.u64 = r30.u64 + ctx.r3.u64;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// rlwinm r3,r29,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r30,r28,r11
	r30.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// subf r28,r17,r27
	r28.s64 = r27.s64 - r17.s64;
	// add r3,r29,r3
	ctx.r3.u64 = r29.u64 + ctx.r3.u64;
	// lwz r29,-284(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// add r30,r31,r30
	r30.u64 = r31.u64 + r30.u64;
	// subf r31,r16,r28
	r31.s64 = r28.s64 - r16.s64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r28,r31,8
	r28.s64 = r31.s64 + 8;
	// srawi r31,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r31.s64 = r30.s32 >> 1;
	// subf r3,r15,r3
	ctx.r3.s64 = ctx.r3.s64 - r15.s64;
	// subf r3,r14,r3
	ctx.r3.s64 = ctx.r3.s64 - r14.s64;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// addi r30,r3,8
	r30.s64 = ctx.r3.s64 + 8;
	// stbx r31,r9,r22
	PPC_STORE_U8(ctx.r9.u32 + r22.u32, r31.u8);
	// lbz r31,1(r9)
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r3,r8,r24
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + r24.u32);
	// lbzx r29,r29,r8
	r29.u64 = PPC_LOAD_U8(r29.u32 + ctx.r8.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r27,0(r8)
	r27.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rlwinm r31,r3,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// subf r3,r29,r3
	ctx.r3.s64 = ctx.r3.s64 - r29.s64;
	// subf r3,r27,r3
	ctx.r3.s64 = ctx.r3.s64 - r27.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// srawi r31,r28,4
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xF) != 0);
	r31.s64 = r28.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// srawi r31,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r31.s64 = r30.s32 >> 4;
	// lwz r30,-276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stbx r3,r8,r30
	PPC_STORE_U8(ctx.r8.u32 + r30.u32, ctx.r3.u8);
	// lwz r3,-280(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lbz r30,2(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbz r28,0(r7)
	r28.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// lbzx r3,r3,r7
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r7.u32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lbz r30,1(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// subf r30,r30,r3
	r30.s64 = ctx.r3.s64 - r30.s64;
	// lbz r29,3(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r3,11(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// subf r30,r28,r30
	r30.s64 = r30.s64 - r28.s64;
	// lbz r28,19(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + r29.u64;
	// lbz r29,-5(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lwz r30,-272(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stbx r31,r30,r7
	PPC_STORE_U8(r30.u32 + ctx.r7.u32, r31.u8);
	// lwz r31,-304(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// lbz r30,3(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lbz r27,2(r8)
	r27.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r21,1(r7)
	r21.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lbzx r31,r31,r5
	r31.u64 = PPC_LOAD_U8(r31.u32 + ctx.r5.u32);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r28,r3
	ctx.r3.s64 = ctx.r3.s64 - r28.s64;
	// subf r3,r29,r3
	ctx.r3.s64 = ctx.r3.s64 - r29.s64;
	// addi r30,r3,8
	r30.s64 = ctx.r3.s64 + 8;
	// rlwinm r3,r31,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// subf r3,r27,r3
	ctx.r3.s64 = ctx.r3.s64 - r27.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// srawi r31,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r31.s64 = r30.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r3.u8);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x8264f8c4
	if (!cr6.eq) goto loc_8264F8C4;
	// lwz r9,-288(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r7,-292(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// lwz r9,-296(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// add r30,r7,r6
	r30.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r14,r9,r6
	r14.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r9,-268(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -268);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r8,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
	// stw r30,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r30.u32);
	// stw r14,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r14.u32);
	// bne cr6,0x8264f868
	if (!cr6.eq) goto loc_8264F868;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8264FAFC"))) PPC_WEAK_FUNC(sub_8264FAFC);
PPC_FUNC_IMPL(__imp__sub_8264FAFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8264FB00"))) PPC_WEAK_FUNC(sub_8264FB00);
PPC_FUNC_IMPL(__imp__sub_8264FB00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// addi r31,r1,-241
	r31.s64 = ctx.r1.s64 + -241;
	// lwz r11,256(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r14,r31,0,0,27
	r14.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r14,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, r14.u32);
	// beq cr6,0x8265013c
	if (cr6.eq) goto loc_8265013C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8264fd80
	if (cr6.eq) goto loc_8264FD80;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264fbf8
	if (!cr6.eq) goto loc_8264FBF8;
	// addi r10,r5,8
	ctx.r10.s64 = ctx.r5.s64 + 8;
	// li r9,8
	ctx.r9.s64 = 8;
loc_8264FB3C:
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r8,-8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r8.u8);
	// lwz r5,-4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// lbz r8,1(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r8.u8);
	// lbz r8,2(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r8,r8,r5
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
	// stb r8,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r8.u8);
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lbz r8,3(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r8.u8);
	// lwz r5,8(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r8,4(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r8.u8);
	// lwz r5,12(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lbz r8,5(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r8.u8);
	// lwz r5,16(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// lbz r8,6(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r8.u8);
	// lbz r8,7(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lwz r5,20(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, ctx.r8.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x8264fb3c
	if (!cr6.eq) goto loc_8264FB3C;
	// b 0x8239bd10
	return;
loc_8264FBF8:
	// addi r9,r5,8
	ctx.r9.s64 = ctx.r5.s64 + 8;
	// li r27,8
	r27.s64 = 8;
loc_8264FC00:
	// addi r8,r4,2
	ctx.r8.s64 = ctx.r4.s64 + 2;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// subf r28,r6,r4
	r28.s64 = ctx.r4.s64 - ctx.r6.s64;
	// li r5,2
	ctx.r5.s64 = 2;
loc_8264FC10:
	// lbz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r31,1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r26,-1(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r31,r31,r3
	r31.u64 = r31.u64 + ctx.r3.u64;
	// lbz r25,2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lwz r30,-8(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// rlwinm r29,r31,3,0,28
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// subf r31,r26,r31
	r31.s64 = r31.s64 - r26.s64;
	// subf r31,r25,r31
	r31.s64 = r31.s64 - r25.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stbx r3,r28,r10
	PPC_STORE_U8(r28.u32 + ctx.r10.u32, ctx.r3.u8);
	// lbz r3,1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r26,0(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r31,r31,r3
	r31.u64 = r31.u64 + ctx.r3.u64;
	// lbz r25,3(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lwz r30,-4(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// rlwinm r29,r31,3,0,28
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// subf r31,r26,r31
	r31.s64 = r31.s64 - r26.s64;
	// subf r31,r25,r31
	r31.s64 = r31.s64 - r25.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r3.u8);
	// lbz r3,2(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r26,1(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// add r31,r31,r3
	r31.u64 = r31.u64 + ctx.r3.u64;
	// lbz r25,4(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lwz r30,0(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r29,r31,3,0,28
	r29.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r29
	r31.u64 = r31.u64 + r29.u64;
	// subf r31,r26,r31
	r31.s64 = r31.s64 - r26.s64;
	// subf r31,r25,r31
	r31.s64 = r31.s64 - r25.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r3.u8);
	// lbz r3,3(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r29,5(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r31,r31,r3
	r31.u64 = r31.u64 + ctx.r3.u64;
	// lbz r26,2(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r29,r31
	r31.s64 = r31.s64 - r29.s64;
	// subf r31,r26,r31
	r31.s64 = r31.s64 - r26.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// lwz r31,4(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// srawi r3,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// stb r3,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r3.u8);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne cr6,0x8264fc10
	if (!cr6.eq) goto loc_8264FC10;
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x8264fc00
	if (!cr6.eq) goto loc_8264FC00;
	// b 0x8239bd10
	return;
loc_8264FD80:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8264ffb0
	if (!cr6.eq) goto loc_8264FFB0;
	// addi r10,r5,8
	ctx.r10.s64 = ctx.r5.s64 + 8;
	// li r8,8
	ctx.r8.s64 = 8;
loc_8264FD90:
	// lbz r31,1(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// addi r9,r6,6
	ctx.r9.s64 = ctx.r6.s64 + 6;
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r30,-1(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbz r29,2(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lwz r3,-8(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r5.u8);
	// lbz r3,1(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r5,2(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r30,3(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r29,0(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lwz r3,-4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r5.u8);
	// lbz r3,2(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r5,3(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lbz r31,4(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r30,1(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r3,r5,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - r31.s64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + r11.u64;
	// lbzx r5,r5,r29
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r29.u32);
	// stb r5,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r5.u8);
	// lbz r3,4(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r5,3(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lbz r30,5(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r29,2(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lwz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r5.u8);
	// lbz r3,4(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r5,5(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r31,3(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rlwinm r3,r5,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lwz r3,8(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - r31.s64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r5.u8);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r5,5(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r29,7(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r30,4(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lwz r3,12(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r5.u8);
	// lbz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r5,7(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lbz r30,8(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r29,5(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lwz r3,16(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// rlwinm r31,r5,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// subf r5,r30,r5
	ctx.r5.s64 = ctx.r5.s64 - r30.s64;
	// subf r5,r29,r5
	ctx.r5.s64 = ctx.r5.s64 - r29.s64;
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// stb r5,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r5.u8);
	// lbz r3,7(r6)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lbz r5,8(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// lbz r30,0(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbz r31,9(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 9);
	// lwz r3,20(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r9,r30,r9
	ctx.r9.s64 = ctx.r9.s64 - r30.s64;
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// srawi r9,r9,4
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// stb r9,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, ctx.r9.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x8264fd90
	if (!cr6.eq) goto loc_8264FD90;
	// b 0x8239bd10
	return;
loc_8264FFB0:
	// addi r9,r5,8
	ctx.r9.s64 = ctx.r5.s64 + 8;
	// li r26,8
	r26.s64 = 8;
loc_8264FFB8:
	// addi r5,r4,3
	ctx.r5.s64 = ctx.r4.s64 + 3;
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// subf r29,r6,r4
	r29.s64 = ctx.r4.s64 - ctx.r6.s64;
	// li r3,2
	ctx.r3.s64 = 2;
loc_8264FFC8:
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r24,1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lwz r28,-8(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// lbz r30,0(r8)
	r30.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r25,-1(r8)
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + -1);
	// add r30,r30,r31
	r30.u64 = r30.u64 + r31.u64;
	// rlwinm r27,r30,3,0,28
	r27.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// subf r30,r25,r30
	r30.s64 = r30.s64 - r25.s64;
	// subf r30,r24,r30
	r30.s64 = r30.s64 - r24.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r31,r31,r28
	r31.u64 = r31.u64 + r28.u64;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// stbx r31,r29,r8
	PPC_STORE_U8(r29.u32 + ctx.r8.u32, r31.u8);
	// lbz r31,1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r25,0(r8)
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r28,r31
	ctx.r8.u64 = r28.u64 + r31.u64;
	// lbz r27,2(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lwz r30,-4(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// rlwinm r28,r8,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// subf r8,r27,r8
	ctx.r8.s64 = ctx.r8.s64 - r27.s64;
	// subf r8,r25,r8
	ctx.r8.s64 = ctx.r8.s64 - r25.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stbx r8,r29,r10
	PPC_STORE_U8(r29.u32 + ctx.r10.u32, ctx.r8.u8);
	// lbz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r31,1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// lbz r27,3(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lwz r25,0(r9)
	r25.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lbzx r8,r8,r25
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r25.u32);
	// stb r8,-1(r5)
	PPC_STORE_U8(ctx.r5.u32 + -1, ctx.r8.u8);
	// lbz r8,3(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r28,4(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r31,r31,r8
	r31.u64 = r31.u64 + ctx.r8.u64;
	// lbz r27,1(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// rlwinm r30,r31,3,0,28
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// subf r31,r28,r31
	r31.s64 = r31.s64 - r28.s64;
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r30,r31,r11
	r30.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lwz r31,4(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r8.u8);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x8264ffc8
	if (!cr6.eq) goto loc_8264FFC8;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x8264ffb8
	if (!cr6.eq) goto loc_8264FFB8;
	// b 0x8239bd10
	return;
loc_8265013C:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x8265081c
	if (cr6.eq) goto loc_8265081C;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x826503a8
	if (!cr6.eq) goto loc_826503A8;
	// subfic r29,r7,-1
	xer.ca = ctx.r7.u32 <= 4294967295;
	r29.s64 = -1 - ctx.r7.s64;
	// subf r10,r7,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r7.s64;
	// subfic r28,r7,1
	xer.ca = ctx.r7.u32 <= 1;
	r28.s64 = 1 - ctx.r7.s64;
	// add r8,r6,r7
	ctx.r8.u64 = ctx.r6.u64 + ctx.r7.u64;
	// subfic r27,r7,2
	xer.ca = ctx.r7.u32 <= 2;
	r27.s64 = 2 - ctx.r7.s64;
	// subfic r26,r7,3
	xer.ca = ctx.r7.u32 <= 3;
	r26.s64 = 3 - ctx.r7.s64;
	// subfic r25,r7,4
	xer.ca = ctx.r7.u32 <= 4;
	r25.s64 = 4 - ctx.r7.s64;
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// subfic r24,r7,5
	xer.ca = ctx.r7.u32 <= 5;
	r24.s64 = 5 - ctx.r7.s64;
	// addi r6,r5,8
	ctx.r6.s64 = ctx.r5.s64 + 8;
	// addi r10,r8,2
	ctx.r10.s64 = ctx.r8.s64 + 2;
	// subfic r23,r7,-2
	xer.ca = ctx.r7.u32 <= 4294967294;
	r23.s64 = -2 - ctx.r7.s64;
	// li r5,8
	ctx.r5.s64 = 8;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
loc_82650184:
	// lbz r31,-2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbzx r3,r23,r10
	ctx.r3.u64 = PPC_LOAD_U8(r23.u32 + ctx.r10.u32);
	// lbz r22,-2(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + -2);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,0(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lwz r31,-8(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + -8);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r3.u8);
	// lbz r31,-1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// lbzx r3,r29,r10
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + ctx.r10.u32);
	// lbz r22,-1(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,1(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lwz r31,-4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r3.u8);
	// lbzx r31,r9,r7
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r7.u32);
	// lbz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r22,2(r8)
	r22.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,0(r9)
	r21.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r3.u8);
	// lbz r31,1(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbzx r3,r28,r10
	ctx.r3.u64 = PPC_LOAD_U8(r28.u32 + ctx.r10.u32);
	// lbz r22,1(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,3(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 3);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r3.u8);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbzx r3,r27,r10
	ctx.r3.u64 = PPC_LOAD_U8(r27.u32 + ctx.r10.u32);
	// lbz r30,2(r9)
	r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r22,4(r8)
	r22.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// rlwinm r31,r3,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lwz r31,8(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// subf r3,r30,r3
	ctx.r3.s64 = ctx.r3.s64 - r30.s64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r3.u8);
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// lbzx r3,r26,r10
	ctx.r3.u64 = PPC_LOAD_U8(r26.u32 + ctx.r10.u32);
	// lbz r22,3(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,5(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// lwz r31,12(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r3.u8);
	// lbz r31,4(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbzx r3,r25,r10
	ctx.r3.u64 = PPC_LOAD_U8(r25.u32 + ctx.r10.u32);
	// lbz r22,4(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,6(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 6);
	// lwz r31,16(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r3.u8);
	// lbz r31,5(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// lbzx r3,r24,r10
	ctx.r3.u64 = PPC_LOAD_U8(r24.u32 + ctx.r10.u32);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// lbz r22,5(r9)
	r22.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r21,7(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 7);
	// lwz r31,20(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r30,r3,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r6,r6,32
	ctx.r6.s64 = ctx.r6.s64 + 32;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// subf r3,r22,r3
	ctx.r3.s64 = ctx.r3.s64 - r22.s64;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// stb r3,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, ctx.r3.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82650184
	if (!cr6.eq) goto loc_82650184;
	// b 0x8239bd10
	return;
loc_826503A8:
	// add r9,r6,r7
	ctx.r9.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r8,r14,2
	ctx.r8.s64 = r14.s64 + 2;
	// subf r10,r7,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r7.s64;
	// li r26,11
	r26.s64 = 11;
	// stw r10,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r10.u32);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r10,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, ctx.r10.u32);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
loc_826503C8:
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// lbz r3,1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// add r23,r3,r6
	r23.u64 = ctx.r3.u64 + ctx.r6.u64;
	// lbz r30,3(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r29,4(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r22,r3,r31
	r22.u64 = ctx.r3.u64 + r31.u64;
	// rlwinm r16,r23,3,0,28
	r16.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r28,5(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r21,r30,r31
	r21.u64 = r30.u64 + r31.u64;
	// lbz r25,-2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r23,r23,r16
	r23.u64 = r23.u64 + r16.u64;
	// lbz r27,-1(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// rlwinm r5,r21,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r24,6(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r20,r29,r30
	r20.u64 = r29.u64 + r30.u64;
	// lbz r14,-3(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// add r19,r28,r29
	r19.u64 = r28.u64 + r29.u64;
	// add r5,r21,r5
	ctx.r5.u64 = r21.u64 + ctx.r5.u64;
	// stw r23,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r23.u32);
	// rlwinm r16,r20,3,0,28
	r16.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r23,r19,3,0,28
	r23.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// add r17,r27,r6
	r17.u64 = r27.u64 + ctx.r6.u64;
	// add r18,r25,r27
	r18.u64 = r25.u64 + r27.u64;
	// rlwinm r15,r22,3,0,28
	r15.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// add r23,r19,r23
	r23.u64 = r19.u64 + r23.u64;
	// subf r5,r3,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r3.s64;
	// add r22,r22,r15
	r22.u64 = r22.u64 + r15.u64;
	// subf r22,r6,r22
	r22.s64 = r22.s64 - ctx.r6.s64;
	// subf r22,r30,r22
	r22.s64 = r22.s64 - r30.s64;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// lwz r21,-296(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// subf r27,r27,r21
	r27.s64 = r21.s64 - r27.s64;
	// add r21,r20,r16
	r21.u64 = r20.u64 + r16.u64;
	// subf r27,r31,r27
	r27.s64 = r27.s64 - r31.s64;
	// subf r20,r24,r23
	r20.s64 = r23.s64 - r24.s64;
	// subf r21,r28,r21
	r21.s64 = r21.s64 - r28.s64;
	// subf r23,r29,r5
	r23.s64 = ctx.r5.s64 - r29.s64;
	// addi r5,r27,8
	ctx.r5.s64 = r27.s64 + 8;
	// subf r27,r31,r21
	r27.s64 = r21.s64 - r31.s64;
	// subf r31,r30,r20
	r31.s64 = r20.s64 - r30.s64;
	// rlwinm r30,r18,3,0,28
	r30.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r21,r31,8
	r21.s64 = r31.s64 + 8;
	// rlwinm r31,r17,3,0,28
	r31.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r18,r30
	r30.u64 = r18.u64 + r30.u64;
	// add r31,r17,r31
	r31.u64 = r17.u64 + r31.u64;
	// subf r30,r14,r30
	r30.s64 = r30.s64 - r14.s64;
	// subf r31,r3,r31
	r31.s64 = r31.s64 - ctx.r3.s64;
	// subf r3,r6,r30
	ctx.r3.s64 = r30.s64 - ctx.r6.s64;
	// subf r6,r25,r31
	ctx.r6.s64 = r31.s64 - r25.s64;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r3,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 4;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// srawi r31,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	r31.s64 = r22.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// srawi r30,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	r30.s64 = r23.s32 >> 4;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// srawi r25,r21,4
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xF) != 0);
	r25.s64 = r21.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r27,r27,r11
	r27.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// lbzx r25,r25,r11
	r25.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// stb r6,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r6.u8);
	// add r6,r24,r28
	ctx.r6.u64 = r24.u64 + r28.u64;
	// stb r5,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r5.u8);
	// lbz r5,7(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stb r3,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r3.u8);
	// rlwinm r3,r6,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stb r31,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r31.u8);
	// stb r30,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, r30.u8);
	// stb r27,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, r27.u8);
	// stb r25,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, r25.u8);
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// subf r6,r29,r6
	ctx.r6.s64 = ctx.r6.s64 - r29.s64;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, ctx.r6.u8);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x826503c8
	if (!cr6.eq) goto loc_826503C8;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r8,8
	ctx.r8.s64 = 8;
	// lwz r28,-300(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// addi r6,r10,8
	ctx.r6.s64 = ctx.r10.s64 + 8;
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// lwz r29,-292(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r8,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
	// stw r28,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r28.u32);
loc_82650554:
	// subf r5,r9,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r9.s64;
	// add r30,r9,r7
	r30.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addi r8,r29,1
	ctx.r8.s64 = r29.s64 + 1;
	// subf r22,r9,r29
	r22.s64 = r29.s64 - ctx.r9.s64;
	// subf r27,r9,r30
	r27.s64 = r30.s64 - ctx.r9.s64;
	// stw r5,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r5.u32);
	// subf r5,r29,r28
	ctx.r5.s64 = r28.s64 - r29.s64;
	// subf r29,r29,r30
	r29.s64 = r30.s64 - r29.s64;
	// stw r30,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r30.u32);
	// addi r3,r4,3
	ctx.r3.s64 = ctx.r4.s64 + 3;
	// subf r23,r9,r28
	r23.s64 = r28.s64 - ctx.r9.s64;
	// li r31,2
	r31.s64 = 2;
	// stw r27,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r27.u32);
	// stw r5,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r5.u32);
	// addi r5,r30,2
	ctx.r5.s64 = r30.s64 + 2;
	// subf r30,r30,r28
	r30.s64 = r28.s64 - r30.s64;
	// stw r29,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r29.u32);
	// stw r30,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r30.u32);
loc_8265059C:
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r29,8(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lbz r27,0(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// lwz r28,-296(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lbzx r30,r23,r9
	r30.u64 = PPC_LOAD_U8(r23.u32 + ctx.r9.u32);
	// lbz r25,1(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// lbz r27,10(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// lbzx r21,r22,r9
	r21.u64 = PPC_LOAD_U8(r22.u32 + ctx.r9.u32);
	// lbzx r19,r28,r9
	r19.u64 = PPC_LOAD_U8(r28.u32 + ctx.r9.u32);
	// rlwinm r24,r30,3,0,28
	r24.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r28,9(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// add r30,r30,r24
	r30.u64 = r30.u64 + r24.u64;
	// lbz r20,16(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// add r28,r28,r25
	r28.u64 = r28.u64 + r25.u64;
	// lbz r25,2(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// subf r30,r21,r30
	r30.s64 = r30.s64 - r21.s64;
	// lbz r18,-8(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// add r27,r27,r25
	r27.u64 = r27.u64 + r25.u64;
	// lbz r17,17(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// rlwinm r25,r29,3,0,28
	r25.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r16,-7(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// lbz r15,18(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// add r29,r29,r25
	r29.u64 = r29.u64 + r25.u64;
	// lwz r26,-8(r6)
	r26.u64 = PPC_LOAD_U32(ctx.r6.u32 + -8);
	// lbz r14,-6(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// subf r25,r20,r29
	r25.s64 = r29.s64 - r20.s64;
	// subf r29,r19,r30
	r29.s64 = r30.s64 - r19.s64;
	// subf r30,r18,r25
	r30.s64 = r25.s64 - r18.s64;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r29,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r29.s64 = r29.s32 >> 4;
	// srawi r25,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r25.s64 = r30.s32 >> 4;
	// rlwinm r30,r28,3,0,28
	r30.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// add r24,r28,r30
	r24.u64 = r28.u64 + r30.u64;
	// lbzx r29,r29,r11
	r29.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// rlwinm r30,r27,3,0,28
	r30.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r28,r25,r11
	r28.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// subf r25,r17,r24
	r25.s64 = r24.s64 - r17.s64;
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// add r28,r29,r28
	r28.u64 = r29.u64 + r28.u64;
	// subf r29,r16,r25
	r29.s64 = r25.s64 - r16.s64;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r25,r29,8
	r25.s64 = r29.s64 + 8;
	// srawi r29,r28,1
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x1) != 0);
	r29.s64 = r28.s32 >> 1;
	// lwz r28,-280(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// subf r30,r15,r30
	r30.s64 = r30.s64 - r15.s64;
	// subf r30,r14,r30
	r30.s64 = r30.s64 - r14.s64;
	// lbzx r29,r29,r11
	r29.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// addi r27,r30,8
	r27.s64 = r30.s64 + 8;
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// lbzx r30,r29,r11
	r30.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// lwz r29,-272(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -272);
	// stbx r30,r29,r9
	PPC_STORE_U8(r29.u32 + ctx.r9.u32, r30.u8);
	// lwz r30,-276(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// lbz r29,1(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r26,r28,r8
	r26.u64 = PPC_LOAD_U8(r28.u32 + ctx.r8.u32);
	// lbz r24,0(r8)
	r24.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbzx r30,r30,r8
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r8.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lwz r29,-4(r6)
	r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	// rlwinm r28,r30,3,0,28
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// subf r30,r26,r30
	r30.s64 = r30.s64 - r26.s64;
	// subf r30,r24,r30
	r30.s64 = r30.s64 - r24.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// srawi r28,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r28.s64 = r25.s32 >> 4;
	// lwz r25,-292(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// srawi r28,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r28.s64 = r27.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbzx r29,r30,r11
	r29.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// subf r30,r25,r4
	r30.s64 = ctx.r4.s64 - r25.s64;
	// stbx r29,r30,r8
	PPC_STORE_U8(r30.u32 + ctx.r8.u32, r29.u8);
	// lwz r30,-284(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbz r29,2(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbz r26,3(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lbz r24,1(r8)
	r24.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbz r21,0(r5)
	r21.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbzx r30,r30,r5
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r5.u32);
	// lbz r20,19(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r29,11(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// lbz r19,-5(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// lbzx r27,r28,r11
	r27.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// rlwinm r26,r30,3,0,28
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r28,0(r6)
	r28.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// subf r30,r24,r30
	r30.s64 = r30.s64 - r24.s64;
	// subf r26,r21,r30
	r26.s64 = r30.s64 - r21.s64;
	// rlwinm r30,r29,3,0,28
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r26,r26,8
	r26.s64 = r26.s64 + 8;
	// add r30,r29,r30
	r30.u64 = r29.u64 + r30.u64;
	// srawi r26,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r26.s64 = r26.s32 >> 4;
	// subf r30,r20,r30
	r30.s64 = r30.s64 - r20.s64;
	// subf r29,r19,r30
	r29.s64 = r30.s64 - r19.s64;
	// lbzx r30,r26,r11
	r30.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// addi r24,r29,8
	r24.s64 = r29.s64 + 8;
	// lwz r26,-304(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// lwz r27,-300(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// lbzx r29,r30,r11
	r29.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// subf r30,r26,r4
	r30.s64 = ctx.r4.s64 - r26.s64;
	// stbx r29,r30,r5
	PPC_STORE_U8(r30.u32 + ctx.r5.u32, r29.u8);
	// subf r30,r4,r27
	r30.s64 = r27.s64 - ctx.r4.s64;
	// lbz r29,3(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lbz r21,2(r8)
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r20,1(r5)
	r20.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lbzx r30,r30,r3
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r3.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lwz r29,4(r6)
	r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// rlwinm r28,r30,3,0,28
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// subf r30,r21,r30
	r30.s64 = r30.s64 - r21.s64;
	// subf r30,r20,r30
	r30.s64 = r30.s64 - r20.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// srawi r28,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	r28.s64 = r24.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stb r30,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r30.u8);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne cr6,0x8265059c
	if (!cr6.eq) goto loc_8265059C;
	// lwz r9,-288(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// add r29,r25,r7
	r29.u64 = r25.u64 + ctx.r7.u64;
	// add r28,r27,r7
	r28.u64 = r27.u64 + ctx.r7.u64;
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r29,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r29.u32);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r28,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, r28.u32);
	// stw r8,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
	// bne cr6,0x82650554
	if (!cr6.eq) goto loc_82650554;
	// b 0x8239bd10
	return;
loc_8265081C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82650bc4
	if (!cr6.eq) goto loc_82650BC4;
	// subf r10,r7,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r7.s64;
	// addi r9,r14,2
	ctx.r9.s64 = r14.s64 + 2;
	// li r27,11
	r27.s64 = 11;
loc_82650830:
	// lbz r8,2(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// lbz r6,3(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// add r24,r6,r8
	r24.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lbz r31,5(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// lbz r30,6(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r23,r3,r6
	r23.u64 = ctx.r3.u64 + ctx.r6.u64;
	// rlwinm r17,r24,3,0,28
	r17.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r29,7(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// add r22,r31,r3
	r22.u64 = r31.u64 + ctx.r3.u64;
	// lbz r26,0(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r24,r24,r17
	r24.u64 = r24.u64 + r17.u64;
	// lbz r28,1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// rlwinm r16,r22,3,0,28
	r16.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r25,8(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r21,r30,r31
	r21.u64 = r30.u64 + r31.u64;
	// lbz r5,-1(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r20,r29,r30
	r20.u64 = r29.u64 + r30.u64;
	// add r22,r22,r16
	r22.u64 = r22.u64 + r16.u64;
	// stw r24,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r24.u32);
	// rlwinm r17,r21,3,0,28
	r17.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r24,r20,3,0,28
	r24.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// add r21,r21,r17
	r21.u64 = r21.u64 + r17.u64;
	// add r24,r20,r24
	r24.u64 = r20.u64 + r24.u64;
	// add r18,r8,r28
	r18.u64 = ctx.r8.u64 + r28.u64;
	// add r19,r26,r28
	r19.u64 = r26.u64 + r28.u64;
	// subf r22,r30,r22
	r22.s64 = r22.s64 - r30.s64;
	// rlwinm r15,r23,3,0,28
	r15.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r20,r25,r24
	r20.s64 = r24.s64 - r25.s64;
	// subf r21,r29,r21
	r21.s64 = r21.s64 - r29.s64;
	// subf r24,r6,r22
	r24.s64 = r22.s64 - ctx.r6.s64;
	// add r23,r23,r15
	r23.u64 = r23.u64 + r15.u64;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// subf r23,r31,r23
	r23.s64 = r23.s64 - r31.s64;
	// subf r23,r8,r23
	r23.s64 = r23.s64 - ctx.r8.s64;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// lwz r16,-304(r1)
	r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// subf r16,r3,r16
	r16.s64 = r16.s64 - ctx.r3.s64;
	// subf r28,r28,r16
	r28.s64 = r16.s64 - r28.s64;
	// addi r22,r28,8
	r22.s64 = r28.s64 + 8;
	// subf r28,r3,r21
	r28.s64 = r21.s64 - ctx.r3.s64;
	// subf r3,r31,r20
	ctx.r3.s64 = r20.s64 - r31.s64;
	// rlwinm r31,r19,3,0,28
	r31.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r21,r3,8
	r21.s64 = ctx.r3.s64 + 8;
	// rlwinm r3,r18,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r19,r31
	r31.u64 = r19.u64 + r31.u64;
	// add r3,r18,r3
	ctx.r3.u64 = r18.u64 + ctx.r3.u64;
	// subf r5,r5,r31
	ctx.r5.s64 = r31.s64 - ctx.r5.s64;
	// subf r3,r6,r3
	ctx.r3.s64 = ctx.r3.s64 - ctx.r6.s64;
	// subf r6,r8,r5
	ctx.r6.s64 = ctx.r5.s64 - ctx.r8.s64;
	// subf r8,r26,r3
	ctx.r8.s64 = ctx.r3.s64 - r26.s64;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// srawi r5,r22,4
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0xF) != 0);
	ctx.r5.s64 = r22.s32 >> 4;
	// srawi r3,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	ctx.r3.s64 = r23.s32 >> 4;
	// srawi r31,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	r31.s64 = r24.s32 >> 4;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// srawi r28,r28,4
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xF) != 0);
	r28.s64 = r28.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// srawi r26,r21,4
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0xF) != 0);
	r26.s64 = r21.s32 >> 4;
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// lbzx r26,r26,r11
	r26.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// stb r8,-1(r9)
	PPC_STORE_U8(ctx.r9.u32 + -1, ctx.r8.u8);
	// add r8,r25,r29
	ctx.r8.u64 = r25.u64 + r29.u64;
	// stb r5,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r5.u8);
	// lbz r5,9(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stb r6,-2(r9)
	PPC_STORE_U8(ctx.r9.u32 + -2, ctx.r6.u8);
	// rlwinm r6,r8,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stb r3,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r3.u8);
	// stb r31,2(r9)
	PPC_STORE_U8(ctx.r9.u32 + 2, r31.u8);
	// stb r28,3(r9)
	PPC_STORE_U8(ctx.r9.u32 + 3, r28.u8);
	// stb r26,4(r9)
	PPC_STORE_U8(ctx.r9.u32 + 4, r26.u8);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r8,r5,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r5.s64;
	// subf r8,r30,r8
	ctx.r8.s64 = ctx.r8.s64 - r30.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// srawi r8,r8,4
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
	// lbzx r8,r8,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r11.u32);
	// stb r8,5(r9)
	PPC_STORE_U8(ctx.r9.u32 + 5, ctx.r8.u8);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne cr6,0x82650830
	if (!cr6.eq) goto loc_82650830;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r8,8
	ctx.r8.s64 = 8;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// addi r10,r14,8
	ctx.r10.s64 = r14.s64 + 8;
loc_826509A8:
	// lbz r3,1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lbz r6,9(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// lbz r5,10(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// lbz r3,11(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbz r31,3(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// lbz r30,4(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbz r31,12(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 12);
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r30,8(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// lbz r25,17(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// lbz r24,-7(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// rlwinm r28,r6,3,0,28
	r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r23,18(r10)
	r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// lbz r22,-6(r10)
	r22.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// add r28,r6,r28
	r28.u64 = ctx.r6.u64 + r28.u64;
	// lbz r21,19(r10)
	r21.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// rlwinm r6,r5,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r20,-5(r10)
	r20.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// subf r28,r25,r28
	r28.s64 = r28.s64 - r25.s64;
	// lbz r19,20(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + 20);
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// lbz r18,-4(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + -4);
	// subf r6,r24,r28
	ctx.r6.s64 = r28.s64 - r24.s64;
	// lbz r27,16(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// subf r5,r23,r5
	ctx.r5.s64 = ctx.r5.s64 - r23.s64;
	// lbz r26,-8(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// addi r28,r6,8
	r28.s64 = ctx.r6.s64 + 8;
	// lwz r29,-8(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	// subf r6,r22,r5
	ctx.r6.s64 = ctx.r5.s64 - r22.s64;
	// addi r5,r6,8
	ctx.r5.s64 = ctx.r6.s64 + 8;
	// rlwinm r6,r3,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// rlwinm r6,r31,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r3,r21,r3
	ctx.r3.s64 = ctx.r3.s64 - r21.s64;
	// add r31,r31,r6
	r31.u64 = r31.u64 + ctx.r6.u64;
	// subf r6,r20,r3
	ctx.r6.s64 = ctx.r3.s64 - r20.s64;
	// subf r3,r19,r31
	ctx.r3.s64 = r31.s64 - r19.s64;
	// addi r31,r6,8
	r31.s64 = ctx.r6.s64 + 8;
	// subf r6,r18,r3
	ctx.r6.s64 = ctx.r3.s64 - r18.s64;
	// addi r3,r6,8
	ctx.r3.s64 = ctx.r6.s64 + 8;
	// rlwinm r6,r30,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r6,r30,r6
	ctx.r6.u64 = r30.u64 + ctx.r6.u64;
	// subf r6,r27,r6
	ctx.r6.s64 = ctx.r6.s64 - r27.s64;
	// subf r6,r26,r6
	ctx.r6.s64 = ctx.r6.s64 - r26.s64;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// srawi r30,r28,4
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0xF) != 0);
	r30.s64 = r28.s32 >> 4;
	// srawi r28,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	r28.s64 = ctx.r5.s32 >> 4;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// srawi r27,r3,4
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	r27.s64 = ctx.r3.s32 >> 4;
	// lbz r3,5(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + r29.u64;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r6.u8);
	// lwz r5,-4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// lbzx r6,r30,r11
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r6.u8);
	// lbzx r6,r28,r11
	ctx.r6.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// lbzx r6,r6,r5
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r5.u32);
	// stb r6,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r6.u8);
	// lbzx r6,r31,r11
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lbz r31,6(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lbz r5,14(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 14);
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r6.u8);
	// lbz r6,13(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 13);
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbz r31,7(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// lbz r3,15(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 15);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// lwz r30,8(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// lbzx r31,r27,r11
	r31.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// lbz r29,21(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// lbz r28,-3(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// rlwinm r30,r6,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r27,22(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 22);
	// lbz r26,-2(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r30,r6,r30
	r30.u64 = ctx.r6.u64 + r30.u64;
	// lbz r25,23(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 23);
	// rlwinm r6,r5,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r24,-1(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// subf r30,r29,r30
	r30.s64 = r30.s64 - r29.s64;
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// subf r6,r28,r30
	ctx.r6.s64 = r30.s64 - r28.s64;
	// subf r5,r27,r5
	ctx.r5.s64 = ctx.r5.s64 - r27.s64;
	// addi r30,r6,8
	r30.s64 = ctx.r6.s64 + 8;
	// subf r6,r26,r5
	ctx.r6.s64 = ctx.r5.s64 - r26.s64;
	// stb r31,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, r31.u8);
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lwz r31,12(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// rlwinm r5,r3,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r29,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	r29.s64 = ctx.r6.s32 >> 4;
	// add r5,r3,r5
	ctx.r5.u64 = ctx.r3.u64 + ctx.r5.u64;
	// lbzx r6,r30,r11
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// add r3,r6,r31
	ctx.r3.u64 = ctx.r6.u64 + r31.u64;
	// subf r6,r25,r5
	ctx.r6.s64 = ctx.r5.s64 - r25.s64;
	// subf r6,r24,r6
	ctx.r6.s64 = ctx.r6.s64 - r24.s64;
	// lbzx r5,r3,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r3,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 4;
	// stb r5,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r5.u8);
	// lwz r5,16(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lbzx r6,r29,r11
	ctx.r6.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r6.u8);
	// lwz r5,20(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// lbzx r6,r3,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, ctx.r6.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x826509a8
	if (!cr6.eq) goto loc_826509A8;
	// b 0x8239bd10
	return;
loc_82650BC4:
	// add r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r8,r14,2
	ctx.r8.s64 = r14.s64 + 2;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// li r26,11
	r26.s64 = 11;
	// subf r10,r7,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r16,r7,r10
	r16.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r10,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r10.u32);
	// addi r10,r16,1
	ctx.r10.s64 = r16.s64 + 1;
	// stw r16,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r16.u32);
loc_82650BE8:
	// lbz r3,1(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r28,-1(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// add r22,r6,r3
	r22.u64 = ctx.r6.u64 + ctx.r3.u64;
	// lbz r30,3(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r23,r28,r6
	r23.u64 = r28.u64 + ctx.r6.u64;
	// lbz r31,2(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r5,r22,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r29,4(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// rlwinm r15,r23,3,0,28
	r15.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r27,5(r10)
	r27.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
	// add r20,r30,r31
	r20.u64 = r30.u64 + r31.u64;
	// lbz r24,-2(r10)
	r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + -2);
	// add r15,r23,r15
	r15.u64 = r23.u64 + r15.u64;
	// lbz r25,6(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rlwinm r23,r20,3,0,28
	r23.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r14,-3(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -3);
	// stw r5,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, ctx.r5.u32);
	// add r21,r31,r3
	r21.u64 = r31.u64 + ctx.r3.u64;
	// add r19,r29,r30
	r19.u64 = r29.u64 + r30.u64;
	// rlwinm r5,r21,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 3) & 0xFFFFFFF8;
	// add r18,r27,r29
	r18.u64 = r27.u64 + r29.u64;
	// stw r23,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r23.u32);
	// add r17,r28,r24
	r17.u64 = r28.u64 + r24.u64;
	// add r5,r21,r5
	ctx.r5.u64 = r21.u64 + ctx.r5.u64;
	// subf r24,r24,r15
	r24.s64 = r15.s64 - r24.s64;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// lwz r23,-304(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r23,r22,r23
	r23.u64 = r22.u64 + r23.u64;
	// rlwinm r22,r19,3,0,28
	r22.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r21,-284(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// add r22,r19,r22
	r22.u64 = r19.u64 + r22.u64;
	// add r21,r20,r21
	r21.u64 = r20.u64 + r21.u64;
	// stw r23,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r23.u32);
	// rlwinm r23,r18,3,0,28
	r23.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r21,r29,r21
	r21.s64 = r21.s64 - r29.s64;
	// add r19,r18,r23
	r19.u64 = r18.u64 + r23.u64;
	// subf r22,r27,r22
	r22.s64 = r22.s64 - r27.s64;
	// lwz r15,-304(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// subf r20,r28,r15
	r20.s64 = r15.s64 - r28.s64;
	// subf r28,r3,r24
	r28.s64 = r24.s64 - ctx.r3.s64;
	// subf r24,r30,r5
	r24.s64 = ctx.r5.s64 - r30.s64;
	// subf r23,r31,r20
	r23.s64 = r20.s64 - r31.s64;
	// subf r5,r25,r19
	ctx.r5.s64 = r19.s64 - r25.s64;
	// addi r20,r28,8
	r20.s64 = r28.s64 + 8;
	// subf r28,r3,r21
	r28.s64 = r21.s64 - ctx.r3.s64;
	// subf r3,r30,r5
	ctx.r3.s64 = ctx.r5.s64 - r30.s64;
	// subf r31,r31,r22
	r31.s64 = r22.s64 - r31.s64;
	// addi r30,r3,8
	r30.s64 = ctx.r3.s64 + 8;
	// rlwinm r3,r17,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r23,r23,8
	r23.s64 = r23.s64 + 8;
	// add r3,r17,r3
	ctx.r3.u64 = r17.u64 + ctx.r3.u64;
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// subf r3,r14,r3
	ctx.r3.s64 = ctx.r3.s64 - r14.s64;
	// addi r5,r28,8
	ctx.r5.s64 = r28.s64 + 8;
	// subf r6,r6,r3
	ctx.r6.s64 = ctx.r3.s64 - ctx.r6.s64;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// srawi r3,r20,4
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0xF) != 0);
	ctx.r3.s64 = r20.s32 >> 4;
	// srawi r28,r23,4
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0xF) != 0);
	r28.s64 = r23.s32 >> 4;
	// srawi r24,r24,4
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0xF) != 0);
	r24.s64 = r24.s32 >> 4;
	// srawi r5,r5,4
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 4;
	// srawi r31,r31,4
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xF) != 0);
	r31.s64 = r31.s32 >> 4;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbzx r3,r3,r11
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// lbzx r24,r24,r11
	r24.u64 = PPC_LOAD_U8(r24.u32 + r11.u32);
	// lbzx r5,r5,r11
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r11.u32);
	// lbzx r31,r31,r11
	r31.u64 = PPC_LOAD_U8(r31.u32 + r11.u32);
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stb r6,-2(r8)
	PPC_STORE_U8(ctx.r8.u32 + -2, ctx.r6.u8);
	// add r6,r25,r27
	ctx.r6.u64 = r25.u64 + r27.u64;
	// stb r3,-1(r8)
	PPC_STORE_U8(ctx.r8.u32 + -1, ctx.r3.u8);
	// stb r5,2(r8)
	PPC_STORE_U8(ctx.r8.u32 + 2, ctx.r5.u8);
	// lbz r5,7(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 7);
	// stb r28,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, r28.u8);
	// stb r24,1(r8)
	PPC_STORE_U8(ctx.r8.u32 + 1, r24.u8);
	// stb r31,3(r8)
	PPC_STORE_U8(ctx.r8.u32 + 3, r31.u8);
	// stb r30,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, r30.u8);
	// rlwinm r3,r6,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// subf r6,r5,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r5.s64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// subf r6,r29,r6
	ctx.r6.s64 = ctx.r6.s64 - r29.s64;
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
	// srawi r6,r6,4
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// lbzx r6,r6,r11
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// stb r6,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, ctx.r6.u8);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x82650be8
	if (!cr6.eq) goto loc_82650BE8;
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r8,8
	ctx.r8.s64 = 8;
	// lwz r29,-280(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// addi r6,r10,8
	ctx.r6.s64 = ctx.r10.s64 + 8;
	// lwz r10,-288(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -288);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r8,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r8.u32);
	// stw r29,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r29.u32);
loc_82650D80:
	// add r30,r9,r7
	r30.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addi r3,r4,3
	ctx.r3.s64 = ctx.r4.s64 + 3;
	// subf r28,r9,r30
	r28.s64 = r30.s64 - ctx.r9.s64;
	// addi r5,r30,2
	ctx.r5.s64 = r30.s64 + 2;
	// addi r8,r16,1
	ctx.r8.s64 = r16.s64 + 1;
	// stw r30,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, r30.u32);
	// subf r24,r9,r29
	r24.s64 = r29.s64 - ctx.r9.s64;
	// subf r23,r9,r16
	r23.s64 = r16.s64 - ctx.r9.s64;
	// stw r28,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r28.u32);
	// subf r28,r16,r30
	r28.s64 = r30.s64 - r16.s64;
	// subf r30,r30,r29
	r30.s64 = r29.s64 - r30.s64;
	// subf r22,r9,r4
	r22.s64 = ctx.r4.s64 - ctx.r9.s64;
	// subf r21,r16,r29
	r21.s64 = r29.s64 - r16.s64;
	// subf r20,r16,r4
	r20.s64 = ctx.r4.s64 - r16.s64;
	// li r31,2
	r31.s64 = 2;
	// stw r28,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, r28.u32);
	// stw r30,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, r30.u32);
loc_82650DC4:
	// lbz r27,0(r9)
	r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// lbzx r30,r24,r9
	r30.u64 = PPC_LOAD_U8(r24.u32 + ctx.r9.u32);
	// lbz r28,0(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r29,8(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 8);
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// lwz r27,-304(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// lbz r26,1(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lbz r28,9(r10)
	r28.u64 = PPC_LOAD_U8(ctx.r10.u32 + 9);
	// rlwinm r25,r30,3,0,28
	r25.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r18,16(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 16);
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// lbzx r17,r23,r9
	r17.u64 = PPC_LOAD_U8(r23.u32 + ctx.r9.u32);
	// rlwinm r26,r29,3,0,28
	r26.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lbzx r19,r27,r9
	r19.u64 = PPC_LOAD_U8(r27.u32 + ctx.r9.u32);
	// add r30,r30,r25
	r30.u64 = r30.u64 + r25.u64;
	// lbz r16,-8(r10)
	r16.u64 = PPC_LOAD_U8(ctx.r10.u32 + -8);
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// lbz r15,17(r10)
	r15.u64 = PPC_LOAD_U8(ctx.r10.u32 + 17);
	// subf r30,r19,r30
	r30.s64 = r30.s64 - r19.s64;
	// lbz r14,-7(r10)
	r14.u64 = PPC_LOAD_U8(ctx.r10.u32 + -7);
	// subf r26,r18,r29
	r26.s64 = r29.s64 - r18.s64;
	// lwz r27,-8(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + -8);
	// subf r29,r17,r30
	r29.s64 = r30.s64 - r17.s64;
	// subf r30,r16,r26
	r30.s64 = r26.s64 - r16.s64;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r26,r29,4
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0xF) != 0);
	r26.s64 = r29.s32 >> 4;
	// srawi r25,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r25.s64 = r30.s32 >> 4;
	// rlwinm r29,r28,3,0,28
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 3) & 0xFFFFFFF8;
	// add r28,r28,r29
	r28.u64 = r28.u64 + r29.u64;
	// lbzx r30,r26,r11
	r30.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// lbzx r29,r25,r11
	r29.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// subf r28,r15,r28
	r28.s64 = r28.s64 - r15.s64;
	// add r29,r30,r29
	r29.u64 = r30.u64 + r29.u64;
	// subf r30,r14,r28
	r30.s64 = r28.s64 - r14.s64;
	// lwz r28,-284(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + -284);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r26,r30,8
	r26.s64 = r30.s64 + 8;
	// srawi r30,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	r30.s64 = r29.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stbx r30,r22,r9
	PPC_STORE_U8(r22.u32 + ctx.r9.u32, r30.u8);
	// lbz r29,1(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lbzx r30,r21,r8
	r30.u64 = PPC_LOAD_U8(r21.u32 + ctx.r8.u32);
	// lbzx r27,r28,r8
	r27.u64 = PPC_LOAD_U8(r28.u32 + ctx.r8.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r25,0(r8)
	r25.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lwz r29,-4(r6)
	r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + -4);
	// rlwinm r28,r30,3,0,28
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// subf r30,r27,r30
	r30.s64 = r30.s64 - r27.s64;
	// subf r30,r25,r30
	r30.s64 = r30.s64 - r25.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// srawi r28,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r28.s64 = r26.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stbx r30,r20,r8
	PPC_STORE_U8(r20.u32 + ctx.r8.u32, r30.u8);
	// lwz r30,-280(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -280);
	// lbz r29,2(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lbz r28,1(r8)
	r28.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// lbz r27,0(r5)
	r27.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// lbzx r30,r30,r5
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r5.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// rlwinm r29,r30,3,0,28
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// subf r30,r28,r30
	r30.s64 = r30.s64 - r28.s64;
	// subf r30,r27,r30
	r30.s64 = r30.s64 - r27.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// lbzx r27,r30,r11
	r27.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbz r30,10(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 10);
	// lbz r29,2(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lbz r26,3(r10)
	r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbz r29,11(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 11);
	// lbz r25,18(r10)
	r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 18);
	// add r29,r29,r26
	r29.u64 = r29.u64 + r26.u64;
	// lbz r19,-6(r10)
	r19.u64 = PPC_LOAD_U8(ctx.r10.u32 + -6);
	// rlwinm r26,r30,3,0,28
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r18,19(r10)
	r18.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// lbz r17,-5(r10)
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// lwz r28,0(r6)
	r28.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// subf r30,r25,r30
	r30.s64 = r30.s64 - r25.s64;
	// subf r26,r19,r30
	r26.s64 = r30.s64 - r19.s64;
	// rlwinm r30,r29,3,0,28
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r26,r26,8
	r26.s64 = r26.s64 + 8;
	// add r30,r29,r30
	r30.u64 = r29.u64 + r30.u64;
	// srawi r26,r26,4
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0xF) != 0);
	r26.s64 = r26.s32 >> 4;
	// subf r30,r18,r30
	r30.s64 = r30.s64 - r18.s64;
	// subf r29,r17,r30
	r29.s64 = r30.s64 - r17.s64;
	// lbzx r30,r26,r11
	r30.u64 = PPC_LOAD_U8(r26.u32 + r11.u32);
	// addi r25,r29,8
	r25.s64 = r29.s64 + 8;
	// lwz r26,-276(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + -276);
	// add r30,r27,r30
	r30.u64 = r27.u64 + r30.u64;
	// lwz r27,-292(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + -292);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// lbzx r29,r30,r11
	r29.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// subf r30,r26,r4
	r30.s64 = ctx.r4.s64 - r26.s64;
	// stbx r29,r30,r5
	PPC_STORE_U8(r30.u32 + ctx.r5.u32, r29.u8);
	// subf r30,r4,r27
	r30.s64 = r27.s64 - ctx.r4.s64;
	// lbz r29,3(r9)
	r29.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lbz r19,2(r8)
	r19.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lbz r18,1(r5)
	r18.u64 = PPC_LOAD_U8(ctx.r5.u32 + 1);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lbzx r30,r30,r3
	r30.u64 = PPC_LOAD_U8(r30.u32 + ctx.r3.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lwz r29,4(r6)
	r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// rlwinm r28,r30,3,0,28
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// subf r30,r19,r30
	r30.s64 = r30.s64 - r19.s64;
	// subf r30,r18,r30
	r30.s64 = r30.s64 - r18.s64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// srawi r28,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r28.s64 = r25.s32 >> 4;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// lbzx r28,r28,r11
	r28.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// add r30,r30,r28
	r30.u64 = r30.u64 + r28.u64;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// srawi r30,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r30.s64 = r30.s32 >> 1;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// lbzx r30,r30,r11
	r30.u64 = PPC_LOAD_U8(r30.u32 + r11.u32);
	// stb r30,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r30.u8);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne cr6,0x82650dc4
	if (!cr6.eq) goto loc_82650DC4;
	// lwz r9,-300(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// add r29,r27,r7
	r29.u64 = r27.u64 + ctx.r7.u64;
	// lwz r5,-296(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// add r16,r5,r7
	r16.u64 = ctx.r5.u64 + ctx.r7.u64;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r29,-292(r1)
	PPC_STORE_U32(ctx.r1.u32 + -292, r29.u32);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r8,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r8.u32);
	// stw r16,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r16.u32);
	// bne cr6,0x82650d80
	if (!cr6.eq) goto loc_82650D80;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82651038"))) PPC_WEAK_FUNC(sub_82651038);
PPC_FUNC_IMPL(__imp__sub_82651038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r9,r6,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r6,0(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r7,r7,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// srawi r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
	// srawi r10,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 2;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r11,-16
	cr6.compare<int32_t>(r11.s32, -16, xer);
	// bge cr6,0x82651078
	if (!cr6.lt) goto loc_82651078;
	// li r11,-16
	r11.s64 = -16;
	// b 0x8265108c
	goto loc_8265108C;
loc_82651078:
	// lwz r30,136(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// rlwinm r30,r30,4,0,27
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// ble cr6,0x82651090
	if (!cr6.gt) goto loc_82651090;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_8265108C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82651090:
	// cmpwi cr6,r10,-16
	cr6.compare<int32_t>(ctx.r10.s32, -16, xer);
	// bge cr6,0x826510a4
	if (!cr6.lt) goto loc_826510A4;
	// li r10,-16
	ctx.r10.s64 = -16;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826510c8
	goto loc_826510C8;
loc_826510A4:
	// lwz r31,140(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// rlwinm r31,r31,4,0,27
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpw cr6,r10,r31
	cr6.compare<int32_t>(ctx.r10.s32, r31.s32, xer);
	// ble cr6,0x826510c0
	if (!cr6.gt) goto loc_826510C0;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826510c8
	goto loc_826510C8;
loc_826510C0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826510f0
	if (cr6.eq) goto loc_826510F0;
loc_826510C8:
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r8,30
	ctx.r8.u64 = ctx.r8.u32 & 0x3;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_826510F0:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826510FC"))) PPC_WEAK_FUNC(sub_826510FC);
PPC_FUNC_IMPL(__imp__sub_826510FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82651100"))) PPC_WEAK_FUNC(sub_82651100);
PPC_FUNC_IMPL(__imp__sub_82651100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// rlwinm r30,r6,4,0,27
	r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r6,0(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r31,0(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r29,r7,4,0,27
	r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r7,r6,0,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	// srawi r9,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r9.s64 = r31.s32 >> 2;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r7,136(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 136);
	// srawi r10,r6,2
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 2;
	// lwz r28,140(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r7,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// add r8,r10,r29
	ctx.r8.u64 = ctx.r10.u64 + r29.u64;
	// rlwinm r7,r28,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// beq cr6,0x8265115c
	if (cr6.eq) goto loc_8265115C;
	// li r10,-17
	ctx.r10.s64 = -17;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// b 0x82651160
	goto loc_82651160;
loc_8265115C:
	// li r10,-18
	ctx.r10.s64 = -18;
loc_82651160:
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// bge cr6,0x82651170
	if (!cr6.lt) goto loc_82651170;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// b 0x8265117c
	goto loc_8265117C;
loc_82651170:
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// ble cr6,0x82651180
	if (!cr6.gt) goto loc_82651180;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_8265117C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82651180:
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// bge cr6,0x82651194
	if (!cr6.lt) goto loc_82651194;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826511b0
	goto loc_826511B0;
loc_82651194:
	// cmpw cr6,r8,r7
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, xer);
	// ble cr6,0x826511a8
	if (!cr6.gt) goto loc_826511A8;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x826511b0
	goto loc_826511B0;
loc_826511A8:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x826511d8
	if (cr6.eq) goto loc_826511D8;
loc_826511B0:
	// subf r11,r30,r9
	r11.s64 = ctx.r9.s64 - r30.s64;
	// subf r10,r29,r8
	ctx.r10.s64 = ctx.r8.s64 - r29.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r8,r31,30
	ctx.r8.u64 = r31.u32 & 0x3;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_826511D8:
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_826511DC"))) PPC_WEAK_FUNC(sub_826511DC);
PPC_FUNC_IMPL(__imp__sub_826511DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826511E0"))) PPC_WEAK_FUNC(sub_826511E0);
PPC_FUNC_IMPL(__imp__sub_826511E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce8
	// mullw r11,r5,r7
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r7.s32);
	// lwz r31,100(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r27,r4,5,0,26
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r26,r5,5,0,26
	r26.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x82651434
	if (!cr6.eq) goto loc_82651434;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82651228
	if (cr6.eq) goto loc_82651228;
	// rlwinm r31,r11,1,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r30,r31,r9
	r30.u64 = PPC_LOAD_U16(r31.u32 + ctx.r9.u32);
	// lhzx r31,r31,r10
	r31.u64 = PPC_LOAD_U16(r31.u32 + ctx.r10.u32);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// extsh r28,r31
	r28.s64 = r31.s16;
	// b 0x82651284
	goto loc_82651284;
loc_82651228:
	// lwz r31,136(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x8265127c
	if (!cr6.eq) goto loc_8265127C;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x8265127c
	if (!cr6.gt) goto loc_8265127C;
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82651244:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// lhzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
loc_82651258:
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x82651444
	if (!cr6.eq) goto loc_82651444;
loc_82651260:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x8239bd38
	return;
loc_8265127C:
	// li r28,0
	r28.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_82651284:
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r31,r11,1,0,30
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r29,r31,r9
	r29.u64 = PPC_LOAD_U16(r31.u32 + ctx.r9.u32);
	// lhzx r25,r31,r10
	r25.u64 = PPC_LOAD_U16(r31.u32 + ctx.r10.u32);
	// extsh r31,r29
	r31.s64 = r29.s16;
	// extsh r29,r25
	r29.s64 = r25.s16;
	// blt cr6,0x82651300
	if (cr6.lt) goto loc_82651300;
	// beq cr6,0x826512d8
	if (cr6.eq) goto loc_826512D8;
	// cmplwi cr6,r6,3
	cr6.compare<uint32_t>(ctx.r6.u32, 3, xer);
	// bge cr6,0x82651344
	if (!cr6.lt) goto loc_82651344;
	// addi r5,r7,-1
	ctx.r5.s64 = ctx.r7.s64 + -1;
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// li r5,1
	ctx.r5.s64 = 1;
	// blt cr6,0x826512c8
	if (cr6.lt) goto loc_826512C8;
	// li r5,0
	ctx.r5.s64 = 0;
loc_826512C8:
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x82651344
	goto loc_82651344;
loc_826512D8:
	// addi r5,r7,-2
	ctx.r5.s64 = ctx.r7.s64 + -2;
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// li r5,1
	ctx.r5.s64 = 1;
	// blt cr6,0x826512ec
	if (cr6.lt) goto loc_826512EC;
	// li r5,0
	ctx.r5.s64 = 0;
loc_826512EC:
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x82651344
	goto loc_82651344;
loc_82651300:
	// lwz r3,3364(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3364);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82651314
	if (!cr6.eq) goto loc_82651314;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82651340
	if (cr6.eq) goto loc_82651340;
loc_82651314:
	// xor r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 ^ ctx.r5.u64;
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82651334
	if (cr6.eq) goto loc_82651334;
	// addi r5,r7,-1
	ctx.r5.s64 = ctx.r7.s64 + -1;
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// li r5,0
	ctx.r5.s64 = 0;
	// blt cr6,0x82651338
	if (cr6.lt) goto loc_82651338;
loc_82651334:
	// li r5,1
	ctx.r5.s64 = 1;
loc_82651338:
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
loc_82651340:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82651344:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r31,-16384
	ctx.r5.s64 = r31.s64 + -16384;
	// addi r4,r30,-16384
	ctx.r4.s64 = r30.s64 + -16384;
	// cntlzw r5,r5
	ctx.r5.u64 = ctx.r5.u32 == 0 ? 32 : __builtin_clz(ctx.r5.u32);
	// cntlzw r3,r4
	ctx.r3.u64 = ctx.r4.u32 == 0 ? 32 : __builtin_clz(ctx.r4.u32);
	// lhzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// rlwinm r4,r5,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x1;
	// lhzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + ctx.r10.u32);
	// rlwinm r5,r3,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 27) & 0x1;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r9,r11,-16384
	ctx.r9.s64 = r11.s64 + -16384;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bgt cr6,0x82651260
	if (cr6.gt) goto loc_82651260;
	// bne cr6,0x826513c8
	if (!cr6.eq) goto loc_826513C8;
	// cmpwi cr6,r31,16384
	cr6.compare<int32_t>(r31.s32, 16384, xer);
	// bne cr6,0x826513a4
	if (!cr6.eq) goto loc_826513A4;
	// li r29,0
	r29.s64 = 0;
	// li r31,0
	r31.s64 = 0;
	// b 0x826513c8
	goto loc_826513C8;
loc_826513A4:
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826513b8
	if (!cr6.eq) goto loc_826513B8;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// b 0x826513c8
	goto loc_826513C8;
loc_826513B8:
	// cmpwi cr6,r30,16384
	cr6.compare<int32_t>(r30.s32, 16384, xer);
	// bne cr6,0x826513c8
	if (!cr6.eq) goto loc_826513C8;
	// li r28,0
	r28.s64 = 0;
	// li r30,0
	r30.s64 = 0;
loc_826513C8:
	// subf r9,r31,r11
	ctx.r9.s64 = r11.s64 - r31.s64;
	// subf r4,r30,r11
	ctx.r4.s64 = r11.s64 - r30.s64;
	// subf r3,r31,r30
	ctx.r3.s64 = r30.s64 - r31.s64;
	// subf r5,r29,r10
	ctx.r5.s64 = ctx.r10.s64 - r29.s64;
	// subf r25,r28,r10
	r25.s64 = ctx.r10.s64 - r28.s64;
	// xor r4,r4,r9
	ctx.r4.u64 = ctx.r4.u64 ^ ctx.r9.u64;
	// subf r24,r29,r28
	r24.s64 = r28.s64 - r29.s64;
	// xor r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r9.u64;
	// xor r25,r25,r5
	r25.u64 = r25.u64 ^ ctx.r5.u64;
	// srawi r9,r4,31
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 31;
	// xor r24,r24,r5
	r24.u64 = r24.u64 ^ ctx.r5.u64;
	// srawi r5,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 31;
	// srawi r4,r25,31
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r25.s32 >> 31;
	// srawi r3,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r24.s32 >> 31;
	// or r25,r9,r5
	r25.u64 = ctx.r9.u64 | ctx.r5.u64;
	// or r24,r4,r3
	r24.u64 = ctx.r4.u64 | ctx.r3.u64;
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// andc r9,r28,r24
	ctx.r9.u64 = r28.u64 & ~r24.u64;
	// and r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 & ctx.r10.u64;
	// andc r30,r30,r25
	r30.u64 = r30.u64 & ~r25.u64;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
	// or r11,r30,r11
	r11.u64 = r30.u64 | r11.u64;
	// and r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 & r31.u64;
	// and r9,r3,r29
	ctx.r9.u64 = ctx.r3.u64 & r29.u64;
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// b 0x82651258
	goto loc_82651258;
loc_82651434:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bgt cr6,0x82651244
	if (cr6.gt) goto loc_82651244;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r11,0
	r11.s64 = 0;
loc_82651444:
	// cmpwi cr6,r6,1
	cr6.compare<int32_t>(ctx.r6.s32, 1, xer);
	// add r9,r11,r27
	ctx.r9.u64 = r11.u64 + r27.u64;
	// add r5,r10,r26
	ctx.r5.u64 = ctx.r10.u64 + r26.u64;
	// li r6,-60
	ctx.r6.s64 = -60;
	// beq cr6,0x8265145c
	if (cr6.eq) goto loc_8265145C;
	// li r6,-28
	ctx.r6.s64 = -28;
loc_8265145C:
	// rlwinm r7,r7,5,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// bge cr6,0x8265147c
	if (!cr6.lt) goto loc_8265147C;
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// b 0x82651488
	goto loc_82651488;
loc_8265147C:
	// cmpw cr6,r9,r7
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r7.s32, xer);
	// ble cr6,0x8265148c
	if (!cr6.gt) goto loc_8265148C;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
loc_82651488:
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_8265148C:
	// cmpw cr6,r5,r6
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r6.s32, xer);
	// bge cr6,0x8265149c
	if (!cr6.lt) goto loc_8265149C;
	// subf r9,r5,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r5.s64;
	// b 0x826514a8
	goto loc_826514A8;
loc_8265149C:
	// cmpw cr6,r5,r8
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r8.s32, xer);
	// ble cr6,0x826514ac
	if (!cr6.gt) goto loc_826514AC;
	// subf r9,r5,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r5.s64;
loc_826514A8:
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_826514AC:
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_826514C4"))) PPC_WEAK_FUNC(sub_826514C4);
PPC_FUNC_IMPL(__imp__sub_826514C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826514C8"))) PPC_WEAK_FUNC(sub_826514C8);
PPC_FUNC_IMPL(__imp__sub_826514C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce0
	// mullw r11,r5,r6
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r6.s32);
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r25,r5,6,0,25
	r25.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// rlwinm r26,r4,6,0,25
	r26.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 6) & 0xFFFFFFC0;
	// li r24,0
	r24.s64 = 0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x826516b8
	if (!cr6.eq) goto loc_826516B8;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82651518
	if (cr6.eq) goto loc_82651518;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r3,r5,r8
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r8.u32);
	// lhzx r5,r5,r9
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r9.u32);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// extsh r28,r5
	r28.s64 = ctx.r5.s16;
	// b 0x8265156c
	goto loc_8265156C;
loc_82651518:
	// lwz r5,136(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// bne cr6,0x82651564
	if (!cr6.eq) goto loc_82651564;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_8265152C:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r8.u32);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lhzx r11,r11,r9
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_82651548:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826516d0
	if (!cr6.eq) goto loc_826516D0;
	// stw r24,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r24.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r24,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r24.u32);
	// b 0x8239bd30
	return;
loc_82651564:
	// mr r28,r24
	r28.u64 = r24.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
loc_8265156C:
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// addi r5,r6,-1
	ctx.r5.s64 = ctx.r6.s64 + -1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpw cr6,r4,r5
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r5.s32, xer);
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r4,r5,r8
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r8.u32);
	// lhzx r31,r5,r9
	r31.u64 = PPC_LOAD_U16(ctx.r5.u32 + ctx.r9.u32);
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// extsh r29,r31
	r29.s64 = r31.s16;
	// li r4,1
	ctx.r4.s64 = 1;
	// blt cr6,0x8265159c
	if (cr6.lt) goto loc_8265159C;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
loc_8265159C:
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r31,r5,-16384
	r31.s64 = ctx.r5.s64 + -16384;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// cntlzw r4,r31
	ctx.r4.u64 = r31.u32 == 0 ? 32 : __builtin_clz(r31.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r30,r3,-16384
	r30.s64 = ctx.r3.s64 + -16384;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r4,27,31,31
	r31.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// cntlzw r4,r30
	ctx.r4.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r4,r4,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// lhzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + ctx.r8.u32);
	// lhzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + ctx.r9.u32);
	// extsh r11,r8
	r11.s64 = ctx.r8.s16;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// addi r8,r11,-16384
	ctx.r8.s64 = r11.s64 + -16384;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// ble cr6,0x82651608
	if (!cr6.gt) goto loc_82651608;
	// li r11,16384
	r11.s64 = 16384;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r24,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r24.u32);
	// stw r24,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r24.u32);
	// b 0x8239bd30
	return;
loc_82651608:
	// bne cr6,0x82651644
	if (!cr6.eq) goto loc_82651644;
	// cmpwi cr6,r5,16384
	cr6.compare<int32_t>(ctx.r5.s32, 16384, xer);
	// bne cr6,0x82651620
	if (!cr6.eq) goto loc_82651620;
	// mr r29,r24
	r29.u64 = r24.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// b 0x82651644
	goto loc_82651644;
loc_82651620:
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x82651634
	if (!cr6.eq) goto loc_82651634;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// mr r11,r24
	r11.u64 = r24.u64;
	// b 0x82651644
	goto loc_82651644;
loc_82651634:
	// cmpwi cr6,r3,16384
	cr6.compare<int32_t>(ctx.r3.s32, 16384, xer);
	// bne cr6,0x82651644
	if (!cr6.eq) goto loc_82651644;
	// mr r28,r24
	r28.u64 = r24.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
loc_82651644:
	// subf r8,r5,r11
	ctx.r8.s64 = r11.s64 - ctx.r5.s64;
	// subf r31,r3,r11
	r31.s64 = r11.s64 - ctx.r3.s64;
	// subf r30,r5,r3
	r30.s64 = ctx.r3.s64 - ctx.r5.s64;
	// subf r4,r29,r9
	ctx.r4.s64 = ctx.r9.s64 - r29.s64;
	// subf r23,r28,r9
	r23.s64 = ctx.r9.s64 - r28.s64;
	// xor r31,r31,r8
	r31.u64 = r31.u64 ^ ctx.r8.u64;
	// subf r22,r29,r28
	r22.s64 = r28.s64 - r29.s64;
	// xor r30,r30,r8
	r30.u64 = r30.u64 ^ ctx.r8.u64;
	// xor r23,r23,r4
	r23.u64 = r23.u64 ^ ctx.r4.u64;
	// srawi r8,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r31.s32 >> 31;
	// xor r22,r22,r4
	r22.u64 = r22.u64 ^ ctx.r4.u64;
	// srawi r4,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r30.s32 >> 31;
	// srawi r31,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r23.s32 >> 31;
	// srawi r30,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r22.s32 >> 31;
	// or r23,r8,r4
	r23.u64 = ctx.r8.u64 | ctx.r4.u64;
	// or r22,r31,r30
	r22.u64 = r31.u64 | r30.u64;
	// and r11,r8,r11
	r11.u64 = ctx.r8.u64 & r11.u64;
	// andc r8,r28,r22
	ctx.r8.u64 = r28.u64 & ~r22.u64;
	// and r9,r31,r9
	ctx.r9.u64 = r31.u64 & ctx.r9.u64;
	// andc r3,r3,r23
	ctx.r3.u64 = ctx.r3.u64 & ~r23.u64;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r11,r3,r11
	r11.u64 = ctx.r3.u64 | r11.u64;
	// and r5,r4,r5
	ctx.r5.u64 = ctx.r4.u64 & ctx.r5.u64;
	// and r8,r30,r29
	ctx.r8.u64 = r30.u64 & r29.u64;
	// or r11,r11,r5
	r11.u64 = r11.u64 | ctx.r5.u64;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r9,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r9.u32);
	// b 0x82651548
	goto loc_82651548;
loc_826516B8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bgt cr6,0x8265152c
	if (cr6.gt) goto loc_8265152C;
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r24,0
	r24.s64 = 0;
	// stw r24,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r24.u32);
	// stw r24,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r24.u32);
loc_826516D0:
	// rlwinm r11,r6,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r8,r7,6,0,25
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r6,0(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r7,r11,-4
	ctx.r7.s64 = r11.s64 + -4;
	// add r11,r9,r26
	r11.u64 = ctx.r9.u64 + r26.u64;
	// add r6,r6,r25
	ctx.r6.u64 = ctx.r6.u64 + r25.u64;
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// cmpwi cr6,r11,-60
	cr6.compare<int32_t>(r11.s32, -60, xer);
	// bge cr6,0x82651704
	if (!cr6.lt) goto loc_82651704;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// addi r11,r11,-60
	r11.s64 = r11.s64 + -60;
	// b 0x82651714
	goto loc_82651714;
loc_82651704:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// ble cr6,0x82651718
	if (!cr6.gt) goto loc_82651718;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
loc_82651714:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_82651718:
	// cmpwi cr6,r6,-60
	cr6.compare<int32_t>(ctx.r6.s32, -60, xer);
	// bge cr6,0x82651738
	if (!cr6.lt) goto loc_82651738;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// addi r11,r11,-60
	r11.s64 = r11.s64 + -60;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// b 0x8239bd30
	return;
loc_82651738:
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// ble cr6,0x82651750
	if (!cr6.gt) goto loc_82651750;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_82651750:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82651758"))) PPC_WEAK_FUNC(sub_82651758);
PPC_FUNC_IMPL(__imp__sub_82651758) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// addi r3,r10,1
	ctx.r3.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r31,136(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// addi r28,r10,1
	r28.s64 = ctx.r10.s64 + 1;
	// lwz r9,140(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 140);
	// mullw r10,r31,r3
	ctx.r10.s64 = int64_t(r31.s32) * int64_t(ctx.r3.s32);
	// lwz r11,1772(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1772);
	// lwz r8,1776(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 1776);
	// stw r25,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, r25.u32);
	// stw r23,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, r23.u32);
	// rlwinm r21,r9,3,0,28
	r21.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r22,r31,3,0,28
	r22.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r10,r28
	ctx.r9.u64 = ctx.r10.u64 + r28.u64;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r21,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r21.u32);
	// stw r22,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, r22.u32);
	// beq cr6,0x826517d8
	if (cr6.eq) goto loc_826517D8;
	// lhzx r11,r10,r11
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r28,r11
	r28.s64 = r11.s16;
	// cmpwi cr6,r28,16384
	cr6.compare<int32_t>(r28.s32, 16384, xer);
	// beq cr6,0x82651878
	if (cr6.eq) goto loc_82651878;
	// lhzx r11,r10,r8
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// extsh r3,r11
	ctx.r3.s64 = r11.s16;
	// b 0x82651d74
	goto loc_82651D74;
loc_826517D8:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lhzx r6,r10,r11
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// lhzx r4,r10,r8
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// rlwinm r7,r31,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// extsh r5,r6
	ctx.r5.s64 = ctx.r6.s16;
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// addi r29,r5,-16384
	r29.s64 = ctx.r5.s64 + -16384;
	// lhzx r6,r10,r11
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhzx r30,r10,r8
	r30.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// extsh r6,r30
	ctx.r6.s64 = r30.s16;
	// cntlzw r30,r29
	r30.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// rlwinm r29,r30,27,31,31
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x1;
	// lhzx r27,r10,r11
	r27.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// rlwinm r30,r7,1,0,30
	r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r26,r10,r8
	r26.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r8.u32);
	// extsh r10,r27
	ctx.r10.s64 = r27.s16;
	// extsh r7,r26
	ctx.r7.s64 = r26.s16;
	// addi r26,r10,-16384
	r26.s64 = ctx.r10.s64 + -16384;
	// addi r27,r9,-16384
	r27.s64 = ctx.r9.s64 + -16384;
	// lhzx r11,r30,r11
	r11.u64 = PPC_LOAD_U16(r30.u32 + r11.u32);
	// lhzx r8,r30,r8
	ctx.r8.u64 = PPC_LOAD_U16(r30.u32 + ctx.r8.u32);
	// cntlzw r30,r26
	r30.u64 = r26.u32 == 0 ? 32 : __builtin_clz(r26.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// rlwinm r26,r30,27,31,31
	r26.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x1;
	// addi r30,r11,-16384
	r30.s64 = r11.s64 + -16384;
	// cntlzw r27,r27
	r27.u64 = r27.u32 == 0 ? 32 : __builtin_clz(r27.u32);
	// cntlzw r30,r30
	r30.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r27,r27,27,31,31
	r27.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 27) & 0x1;
	// rlwinm r30,r30,27,31,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 27) & 0x1;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// add r30,r30,r26
	r30.u64 = r30.u64 + r26.u64;
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// ble cr6,0x82651954
	if (!cr6.gt) goto loc_82651954;
loc_82651878:
	// mullw r11,r31,r25
	r11.s64 = int64_t(r31.s32) * int64_t(r25.s32);
	// lwz r8,1780(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 1780);
	// add r9,r11,r23
	ctx.r9.u64 = r11.u64 + r23.u64;
	// li r11,16384
	r11.s64 = 16384;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r8,r10
	PPC_STORE_U16(ctx.r8.u32 + ctx.r10.u32, r11.u16);
	// lwz r8,1784(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 1784);
	// sthx r11,r8,r10
	PPC_STORE_U16(ctx.r8.u32 + ctx.r10.u32, r11.u16);
	// lwz r10,14772(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 14772);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8265194c
	if (!cr6.gt) goto loc_8265194C;
	// lwz r10,284(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 284);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8265194c
	if (!cr6.eq) goto loc_8265194C;
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,3048(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 3048);
	// li r31,0
	r31.s64 = 0;
	// sthx r31,r10,r9
	PPC_STORE_U16(ctx.r10.u32 + ctx.r9.u32, r31.u16);
	// lwz r9,3048(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 3048);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sth r31,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r31.u16);
	// lwz r10,15472(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 15472);
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// blt cr6,0x8265194c
	if (cr6.lt) goto loc_8265194C;
	// lwz r10,136(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// lwz r7,15268(r24)
	ctx.r7.u64 = PPC_LOAD_U32(r24.u32 + 15268);
	// mullw r9,r10,r25
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r23
	ctx.r10.u64 = ctx.r8.u64 + r23.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r11,r10,r7
	PPC_STORE_U16(ctx.r10.u32 + ctx.r7.u32, r11.u16);
	// lwz r8,15268(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 15268);
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// sth r11,2(r8)
	PPC_STORE_U16(ctx.r8.u32 + 2, r11.u16);
	// lwz r8,15268(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 15268);
	// sthx r11,r9,r8
	PPC_STORE_U16(ctx.r9.u32 + ctx.r8.u32, r11.u16);
	// lwz r8,15268(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 15268);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r11,2(r8)
	PPC_STORE_U16(ctx.r8.u32 + 2, r11.u16);
	// lwz r8,15272(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 15272);
	// sthx r11,r10,r8
	PPC_STORE_U16(ctx.r10.u32 + ctx.r8.u32, r11.u16);
	// lwz r8,15272(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 15272);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// sth r11,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r11.u16);
	// lwz r10,15272(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 15272);
	// sthx r11,r9,r10
	PPC_STORE_U16(ctx.r9.u32 + ctx.r10.u32, r11.u16);
	// lwz r10,15272(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 15272);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sth r11,2(r10)
	PPC_STORE_U16(ctx.r10.u32 + 2, r11.u16);
loc_8265194C:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8239bd10
	return;
loc_82651954:
	// cmpwi cr6,r30,1
	cr6.compare<int32_t>(r30.s32, 1, xer);
	// bne cr6,0x82651b2c
	if (!cr6.eq) goto loc_82651B2C;
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826519d0
	if (!cr6.eq) goto loc_826519D0;
	// subf r11,r5,r10
	r11.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r31,r5,r9
	r31.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r8,r4,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r30,r6,r7
	r30.s64 = ctx.r7.s64 - ctx.r6.s64;
	// xor r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 ^ r11.u64;
	// subf r29,r4,r6
	r29.s64 = ctx.r6.s64 - ctx.r4.s64;
	// xor r31,r31,r11
	r31.u64 = r31.u64 ^ r11.u64;
	// xor r30,r30,r8
	r30.u64 = r30.u64 ^ ctx.r8.u64;
	// srawi r11,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = ctx.r3.s32 >> 31;
	// xor r29,r29,r8
	r29.u64 = r29.u64 ^ ctx.r8.u64;
	// srawi r8,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r31.s32 >> 31;
	// srawi r3,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// or r30,r11,r8
	r30.u64 = r11.u64 | ctx.r8.u64;
	// or r29,r3,r31
	r29.u64 = ctx.r3.u64 | r31.u64;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// andc r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 & ~r30.u64;
	// and r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 & ctx.r7.u64;
	// andc r10,r6,r29
	ctx.r10.u64 = ctx.r6.u64 & ~r29.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// and r9,r8,r5
	ctx.r9.u64 = ctx.r8.u64 & ctx.r5.u64;
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// and r8,r31,r4
	ctx.r8.u64 = r31.u64 & ctx.r4.u64;
	// or r28,r11,r9
	r28.u64 = r11.u64 | ctx.r9.u64;
	// or r3,r10,r8
	ctx.r3.u64 = ctx.r10.u64 | ctx.r8.u64;
	// b 0x82651d74
	goto loc_82651D74;
loc_826519D0:
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// bne cr6,0x82651a44
	if (!cr6.eq) goto loc_82651A44;
	// subf r10,r5,r11
	ctx.r10.s64 = r11.s64 - ctx.r5.s64;
	// subf r3,r9,r11
	ctx.r3.s64 = r11.s64 - ctx.r9.s64;
	// subf r31,r5,r9
	r31.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// subf r30,r6,r8
	r30.s64 = ctx.r8.s64 - ctx.r6.s64;
	// xor r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r10.u64;
	// subf r29,r4,r6
	r29.s64 = ctx.r6.s64 - ctx.r4.s64;
	// xor r31,r31,r10
	r31.u64 = r31.u64 ^ ctx.r10.u64;
	// xor r30,r30,r7
	r30.u64 = r30.u64 ^ ctx.r7.u64;
	// srawi r10,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 31;
	// xor r29,r29,r7
	r29.u64 = r29.u64 ^ ctx.r7.u64;
	// srawi r7,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = r31.s32 >> 31;
	// srawi r3,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// or r30,r10,r7
	r30.u64 = ctx.r10.u64 | ctx.r7.u64;
	// or r29,r3,r31
	r29.u64 = ctx.r3.u64 | r31.u64;
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// and r8,r3,r8
	ctx.r8.u64 = ctx.r3.u64 & ctx.r8.u64;
	// andc r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 & ~r30.u64;
	// andc r10,r6,r29
	ctx.r10.u64 = ctx.r6.u64 & ~r29.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// or r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 | ctx.r8.u64;
	// and r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 & ctx.r5.u64;
	// and r8,r31,r4
	ctx.r8.u64 = r31.u64 & ctx.r4.u64;
	// or r28,r11,r9
	r28.u64 = r11.u64 | ctx.r9.u64;
	// or r3,r10,r8
	ctx.r3.u64 = ctx.r10.u64 | ctx.r8.u64;
	// b 0x82651d74
	goto loc_82651D74;
loc_82651A44:
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// bne cr6,0x82651ab8
	if (!cr6.eq) goto loc_82651AB8;
	// subf r9,r5,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// subf r31,r5,r11
	r31.s64 = r11.s64 - ctx.r5.s64;
	// subf r6,r4,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r4.s64;
	// subf r30,r8,r7
	r30.s64 = ctx.r7.s64 - ctx.r8.s64;
	// xor r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r9.u64;
	// subf r29,r4,r8
	r29.s64 = ctx.r8.s64 - ctx.r4.s64;
	// xor r31,r31,r9
	r31.u64 = r31.u64 ^ ctx.r9.u64;
	// xor r30,r30,r6
	r30.u64 = r30.u64 ^ ctx.r6.u64;
	// srawi r9,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 31;
	// xor r29,r29,r6
	r29.u64 = r29.u64 ^ ctx.r6.u64;
	// srawi r6,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = r31.s32 >> 31;
	// srawi r3,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// or r30,r9,r6
	r30.u64 = ctx.r9.u64 | ctx.r6.u64;
	// or r29,r3,r31
	r29.u64 = ctx.r3.u64 | r31.u64;
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// andc r9,r8,r29
	ctx.r9.u64 = ctx.r8.u64 & ~r29.u64;
	// and r8,r3,r7
	ctx.r8.u64 = ctx.r3.u64 & ctx.r7.u64;
	// andc r11,r11,r30
	r11.u64 = r11.u64 & ~r30.u64;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// and r10,r6,r5
	ctx.r10.u64 = ctx.r6.u64 & ctx.r5.u64;
	// and r8,r31,r4
	ctx.r8.u64 = r31.u64 & ctx.r4.u64;
	// or r28,r11,r10
	r28.u64 = r11.u64 | ctx.r10.u64;
	// or r3,r9,r8
	ctx.r3.u64 = ctx.r9.u64 | ctx.r8.u64;
	// b 0x82651d74
	goto loc_82651D74;
loc_82651AB8:
	// cmpwi cr6,r5,16384
	cr6.compare<int32_t>(ctx.r5.s32, 16384, xer);
	// bne cr6,0x82651d74
	if (!cr6.eq) goto loc_82651D74;
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// subf r3,r9,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r31,r11,r9
	r31.s64 = ctx.r9.s64 - r11.s64;
	// subf r4,r8,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subf r30,r6,r7
	r30.s64 = ctx.r7.s64 - ctx.r6.s64;
	// subf r29,r8,r6
	r29.s64 = ctx.r6.s64 - ctx.r8.s64;
	// xor r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 ^ ctx.r5.u64;
	// xor r31,r31,r5
	r31.u64 = r31.u64 ^ ctx.r5.u64;
	// xor r30,r30,r4
	r30.u64 = r30.u64 ^ ctx.r4.u64;
	// srawi r5,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r3.s32 >> 31;
	// xor r29,r29,r4
	r29.u64 = r29.u64 ^ ctx.r4.u64;
	// srawi r4,r31,31
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r31.s32 >> 31;
	// srawi r3,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r30.s32 >> 31;
	// srawi r31,r29,31
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r29.s32 >> 31;
	// or r30,r5,r4
	r30.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r29,r3,r31
	r29.u64 = ctx.r3.u64 | r31.u64;
	// andc r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 & ~r30.u64;
	// and r11,r4,r11
	r11.u64 = ctx.r4.u64 & r11.u64;
	// and r8,r31,r8
	ctx.r8.u64 = r31.u64 & ctx.r8.u64;
	// andc r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 & ~r29.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// or r9,r6,r8
	ctx.r9.u64 = ctx.r6.u64 | ctx.r8.u64;
	// and r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 & ctx.r10.u64;
	// and r8,r3,r7
	ctx.r8.u64 = ctx.r3.u64 & ctx.r7.u64;
	// or r28,r11,r10
	r28.u64 = r11.u64 | ctx.r10.u64;
	// or r3,r9,r8
	ctx.r3.u64 = ctx.r9.u64 | ctx.r8.u64;
	// b 0x82651d74
	goto loc_82651D74;
loc_82651B2C:
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// bne cr6,0x82651b8c
	if (!cr6.eq) goto loc_82651B8C;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82651b4c
	if (cr6.eq) goto loc_82651B4C;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
loc_82651B4C:
	// cmpwi cr6,r10,16384
	cr6.compare<int32_t>(ctx.r10.s32, 16384, xer);
	// beq cr6,0x82651b5c
	if (cr6.eq) goto loc_82651B5C;
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r31,r7,r31
	r31.u64 = ctx.r7.u64 + r31.u64;
loc_82651B5C:
	// cmpwi cr6,r9,16384
	cr6.compare<int32_t>(ctx.r9.s32, 16384, xer);
	// beq cr6,0x82651b6c
	if (cr6.eq) goto loc_82651B6C;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r31,r6,r31
	r31.u64 = ctx.r6.u64 + r31.u64;
loc_82651B6C:
	// cmpwi cr6,r5,16384
	cr6.compare<int32_t>(ctx.r5.s32, 16384, xer);
	// beq cr6,0x82651b7c
	if (cr6.eq) goto loc_82651B7C;
	// add r3,r5,r3
	ctx.r3.u64 = ctx.r5.u64 + ctx.r3.u64;
	// add r31,r4,r31
	r31.u64 = ctx.r4.u64 + r31.u64;
loc_82651B7C:
	// srawi r11,r3,1
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	r11.s64 = ctx.r3.s32 >> 1;
	// addze r28,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r28.s64 = temp.s64;
	// srawi r11,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r11.s64 = r31.s32 >> 1;
	// b 0x82651d70
	goto loc_82651D70;
loc_82651B8C:
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// subf r30,r10,r9
	r30.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r28,r9,r11
	r28.s64 = r11.s64 - ctx.r9.s64;
	// srawi r31,r3,31
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = ctx.r3.s32 >> 31;
	// subf r27,r10,r11
	r27.s64 = r11.s64 - ctx.r10.s64;
	// srawi r29,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r30.s32 >> 31;
	// subf r30,r9,r10
	r30.s64 = ctx.r10.s64 - ctx.r9.s64;
	// srawi r3,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	ctx.r3.s64 = r28.s32 >> 31;
	// subf r28,r11,r9
	r28.s64 = ctx.r9.s64 - r11.s64;
	// srawi r27,r27,31
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x7FFFFFFF) != 0);
	r27.s64 = r27.s32 >> 31;
	// srawi r26,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	r26.s64 = r30.s32 >> 31;
	// srawi r25,r28,31
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x7FFFFFFF) != 0);
	r25.s64 = r28.s32 >> 31;
	// not r28,r31
	r28.u64 = ~r31.u64;
	// not r3,r3
	ctx.r3.u64 = ~ctx.r3.u64;
	// not r26,r26
	r26.u64 = ~r26.u64;
	// not r25,r25
	r25.u64 = ~r25.u64;
	// not r30,r29
	r30.u64 = ~r29.u64;
	// not r27,r27
	r27.u64 = ~r27.u64;
	// xor r16,r31,r3
	r16.u64 = r31.u64 ^ ctx.r3.u64;
	// xor r29,r29,r3
	r29.u64 = r29.u64 ^ ctx.r3.u64;
	// and r14,r26,r3
	r14.u64 = r26.u64 & ctx.r3.u64;
	// and r15,r28,r25
	r15.u64 = r28.u64 & r25.u64;
	// xor r17,r31,r30
	r17.u64 = r31.u64 ^ r30.u64;
	// and r3,r27,r3
	ctx.r3.u64 = r27.u64 & ctx.r3.u64;
	// and r28,r26,r28
	r28.u64 = r26.u64 & r28.u64;
	// and r31,r27,r30
	r31.u64 = r27.u64 & r30.u64;
	// and r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 & r11.u64;
	// and r28,r28,r10
	r28.u64 = r28.u64 & ctx.r10.u64;
	// and r15,r15,r11
	r15.u64 = r15.u64 & r11.u64;
	// and r31,r31,r10
	r31.u64 = r31.u64 & ctx.r10.u64;
	// and r30,r25,r30
	r30.u64 = r25.u64 & r30.u64;
	// or r3,r28,r3
	ctx.r3.u64 = r28.u64 | ctx.r3.u64;
	// and r11,r16,r11
	r11.u64 = r16.u64 & r11.u64;
	// and r28,r17,r10
	r28.u64 = r17.u64 & ctx.r10.u64;
	// or r31,r15,r31
	r31.u64 = r15.u64 | r31.u64;
	// and r15,r14,r9
	r15.u64 = r14.u64 & ctx.r9.u64;
	// and r30,r30,r9
	r30.u64 = r30.u64 & ctx.r9.u64;
	// or r11,r28,r11
	r11.u64 = r28.u64 | r11.u64;
	// and r9,r29,r9
	ctx.r9.u64 = r29.u64 & ctx.r9.u64;
	// or r31,r31,r15
	r31.u64 = r31.u64 | r15.u64;
	// or r10,r3,r30
	ctx.r10.u64 = ctx.r3.u64 | r30.u64;
	// or r3,r11,r9
	ctx.r3.u64 = r11.u64 | ctx.r9.u64;
	// subf r11,r31,r5
	r11.s64 = ctx.r5.s64 - r31.s64;
	// subf r9,r5,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r30,r10,r31
	r30.s64 = r31.s64 - ctx.r10.s64;
	// srawi r11,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 31;
	// srawi r9,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 31;
	// srawi r30,r30,31
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r30.s32 >> 31;
	// eqv r9,r9,r11
	// eqv r11,r30,r11
	// subf r23,r8,r7
	r23.s64 = ctx.r7.s64 - ctx.r8.s64;
	// or r30,r9,r11
	r30.u64 = ctx.r9.u64 | r11.u64;
	// and r11,r31,r11
	r11.u64 = r31.u64 & r11.u64;
	// andc r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 & ~r30.u64;
	// and r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 & ctx.r5.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// subf r22,r7,r6
	r22.s64 = ctx.r6.s64 - ctx.r7.s64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// subf r21,r6,r8
	r21.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// subf r20,r7,r8
	r20.s64 = ctx.r8.s64 - ctx.r7.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subf r19,r6,r7
	r19.s64 = ctx.r7.s64 - ctx.r6.s64;
	// addze r28,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r28.s64 = temp.s64;
	// srawi r10,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r23.s32 >> 31;
	// srawi r5,r22,31
	xer.ca = (r22.s32 < 0) & ((r22.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r22.s32 >> 31;
	// srawi r11,r21,31
	xer.ca = (r21.s32 < 0) & ((r21.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = r21.s32 >> 31;
	// not r9,r5
	ctx.r9.u64 = ~ctx.r5.u64;
	// subf r18,r8,r6
	r18.s64 = ctx.r6.s64 - ctx.r8.s64;
	// srawi r31,r20,31
	xer.ca = (r20.s32 < 0) & ((r20.u32 & 0x7FFFFFFF) != 0);
	r31.s64 = r20.s32 >> 31;
	// srawi r29,r19,31
	xer.ca = (r19.s32 < 0) & ((r19.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r19.s32 >> 31;
	// not r11,r11
	r11.u64 = ~r11.u64;
	// xor r27,r10,r9
	r27.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// srawi r30,r18,31
	xer.ca = (r18.s32 < 0) & ((r18.u32 & 0x7FFFFFFF) != 0);
	r30.s64 = r18.s32 >> 31;
	// not r3,r10
	ctx.r3.u64 = ~ctx.r10.u64;
	// not r31,r31
	r31.u64 = ~r31.u64;
	// not r30,r30
	r30.u64 = ~r30.u64;
	// not r29,r29
	r29.u64 = ~r29.u64;
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// xor r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 ^ r11.u64;
	// and r27,r27,r7
	r27.u64 = r27.u64 & ctx.r7.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// lwz r21,-176(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	// and r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 & ctx.r6.u64;
	// lwz r22,-172(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	// or r10,r27,r10
	ctx.r10.u64 = r27.u64 | ctx.r10.u64;
	// lwz r25,-168(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	// and r27,r31,r9
	r27.u64 = r31.u64 & ctx.r9.u64;
	// lwz r23,-164(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + -164);
	// or r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 | ctx.r5.u64;
	// and r10,r3,r30
	ctx.r10.u64 = ctx.r3.u64 & r30.u64;
	// and r26,r29,r11
	r26.u64 = r29.u64 & r11.u64;
	// and r11,r31,r11
	r11.u64 = r31.u64 & r11.u64;
	// and r3,r29,r3
	ctx.r3.u64 = r29.u64 & ctx.r3.u64;
	// and r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ctx.r8.u64;
	// and r27,r27,r7
	r27.u64 = r27.u64 & ctx.r7.u64;
	// and r11,r11,r8
	r11.u64 = r11.u64 & ctx.r8.u64;
	// and r9,r30,r9
	ctx.r9.u64 = r30.u64 & ctx.r9.u64;
	// and r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 & ctx.r7.u64;
	// or r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 | r27.u64;
	// and r27,r26,r6
	r27.u64 = r26.u64 & ctx.r6.u64;
	// or r11,r7,r11
	r11.u64 = ctx.r7.u64 | r11.u64;
	// and r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 & ctx.r6.u64;
	// or r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 | r27.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// subf r9,r10,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r8,r4,r11
	ctx.r8.s64 = r11.s64 - ctx.r4.s64;
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// srawi r9,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 31;
	// srawi r8,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 31;
	// srawi r7,r7,31
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 31;
	// eqv r8,r8,r9
	// eqv r9,r7,r9
	// or r7,r8,r9
	ctx.r7.u64 = ctx.r8.u64 | ctx.r9.u64;
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// andc r11,r11,r7
	r11.u64 = r11.u64 & ~ctx.r7.u64;
	// and r9,r8,r4
	ctx.r9.u64 = ctx.r8.u64 & ctx.r4.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
loc_82651D70:
	// addze r3,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r3.s64 = temp.s64;
loc_82651D74:
	// lwz r11,136(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 136);
	// lwz r10,14772(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 14772);
	// mullw r11,r25,r11
	r11.s64 = int64_t(r25.s32) * int64_t(r11.s32);
	// add r6,r11,r23
	ctx.r6.u64 = r11.u64 + r23.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82651e2c
	if (!cr6.gt) goto loc_82651E2C;
	// lwz r11,15472(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15472);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82651e10
	if (!cr6.eq) goto loc_82651E10;
	// srawi r10,r28,2
	xer.ca = (r28.s32 < 0) & ((r28.u32 & 0x3) != 0);
	ctx.r10.s64 = r28.s32 >> 2;
	// rlwinm r11,r23,3,0,28
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r9,r3,2
	xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r3.s32 >> 2;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r10,r25,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82651dd0
	if (!cr6.lt) goto loc_82651DD0;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r11,r28
	ctx.r7.s64 = r28.s64 - r11.s64;
	// b 0x82651de4
	goto loc_82651DE4;
loc_82651DD0:
	// cmpw cr6,r11,r22
	cr6.compare<int32_t>(r11.s32, r22.s32, xer);
	// ble cr6,0x82651de4
	if (!cr6.gt) goto loc_82651DE4;
	// subf r11,r11,r22
	r11.s64 = r22.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r11,r28
	ctx.r7.u64 = r11.u64 + r28.u64;
loc_82651DE4:
	// cmpwi cr6,r10,-8
	cr6.compare<int32_t>(ctx.r10.s32, -8, xer);
	// bge cr6,0x82651dfc
	if (!cr6.lt) goto loc_82651DFC;
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r11,r3
	ctx.r8.s64 = ctx.r3.s64 - r11.s64;
	// b 0x82651e10
	goto loc_82651E10;
loc_82651DFC:
	// cmpw cr6,r10,r21
	cr6.compare<int32_t>(ctx.r10.s32, r21.s32, xer);
	// ble cr6,0x82651e10
	if (!cr6.gt) goto loc_82651E10;
	// subf r11,r10,r21
	r11.s64 = r21.s64 - ctx.r10.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r3
	ctx.r8.u64 = r11.u64 + ctx.r3.u64;
loc_82651E10:
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// lwz r8,3048(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 3048);
	// rlwinm r11,r6,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// sthx r7,r11,r8
	PPC_STORE_U16(r11.u32 + ctx.r8.u32, ctx.r7.u16);
	// lwz r10,3048(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 3048);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r9,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r9.u16);
loc_82651E2C:
	// lis r11,-32138
	r11.s64 = -2106195968;
	// lwz r10,1792(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1792);
	// rlwinm r9,r28,2,28,29
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xC;
	// addi r11,r11,15904
	r11.s64 = r11.s64 + 15904;
	// rlwinm r8,r3,2,28,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xC;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r10,r9,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// lwzx r11,r8,r11
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// srawi r9,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// srawi r10,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r10.s64 = r11.s32 >> 1;
	// beq cr6,0x82651ea0
	if (cr6.eq) goto loc_82651EA0;
	// clrlwi r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82651e80
	if (cr6.eq) goto loc_82651E80;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82651e7c
	if (!cr6.gt) goto loc_82651E7C;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// b 0x82651e80
	goto loc_82651E80;
loc_82651E7C:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_82651E80:
	// clrlwi r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82651ea0
	if (cr6.eq) goto loc_82651EA0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82651e9c
	if (!cr6.gt) goto loc_82651E9C;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// b 0x82651ea0
	goto loc_82651EA0;
loc_82651E9C:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82651EA0:
	// lwz r11,15472(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 15472);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82651f1c
	if (!cr6.eq) goto loc_82651F1C;
	// srawi r8,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 2;
	// rlwinm r11,r23,3,0,28
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r7,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 2;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r8,r25,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// bge cr6,0x82651edc
	if (!cr6.lt) goto loc_82651EDC;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// b 0x82651ef0
	goto loc_82651EF0;
loc_82651EDC:
	// cmpw cr6,r11,r22
	cr6.compare<int32_t>(r11.s32, r22.s32, xer);
	// ble cr6,0x82651ef0
	if (!cr6.gt) goto loc_82651EF0;
	// subf r11,r11,r22
	r11.s64 = r22.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_82651EF0:
	// cmpwi cr6,r8,-8
	cr6.compare<int32_t>(ctx.r8.s32, -8, xer);
	// bge cr6,0x82651f08
	if (!cr6.lt) goto loc_82651F08;
	// addi r11,r8,8
	r11.s64 = ctx.r8.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// b 0x82651f1c
	goto loc_82651F1C;
loc_82651F08:
	// cmpw cr6,r8,r21
	cr6.compare<int32_t>(ctx.r8.s32, r21.s32, xer);
	// ble cr6,0x82651f1c
	if (!cr6.gt) goto loc_82651F1C;
	// subf r11,r8,r21
	r11.s64 = r21.s64 - ctx.r8.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_82651F1C:
	// lwz r8,1780(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 1780);
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// li r3,0
	ctx.r3.s64 = 0;
	// sthx r9,r8,r11
	PPC_STORE_U16(ctx.r8.u32 + r11.u32, ctx.r9.u16);
	// lwz r9,1784(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 1784);
	// sthx r10,r9,r11
	PPC_STORE_U16(ctx.r9.u32 + r11.u32, ctx.r10.u16);
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82651F38"))) PPC_WEAK_FUNC(sub_82651F38);
PPC_FUNC_IMPL(__imp__sub_82651F38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcfc
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r30,r10,3,0,28
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r31,0(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// beq cr6,0x82651f70
	if (cr6.eq) goto loc_82651F70;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// li r3,-9
	ctx.r3.s64 = -9;
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// b 0x82651f78
	goto loc_82651F78;
loc_82651F70:
	// li r3,-8
	ctx.r3.s64 = -8;
	// rlwinm r29,r10,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
loc_82651F78:
	// cmpwi cr6,r31,16384
	cr6.compare<int32_t>(r31.s32, 16384, xer);
	// beq cr6,0x82651ff8
	if (cr6.eq) goto loc_82651FF8;
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r10,r31,2
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x3) != 0);
	ctx.r10.s64 = r31.s32 >> 2;
	// rlwinm r8,r5,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r9,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r9.s64 = r11.s32 >> 2;
	// cmpwi cr6,r10,-8
	cr6.compare<int32_t>(ctx.r10.s32, -8, xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// bge cr6,0x82651fb0
	if (!cr6.lt) goto loc_82651FB0;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r31,r10,r31
	r31.s64 = r31.s64 - ctx.r10.s64;
	// b 0x82651fc4
	goto loc_82651FC4;
loc_82651FB0:
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// ble cr6,0x82651fc4
	if (!cr6.gt) goto loc_82651FC4;
	// subf r10,r10,r30
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
loc_82651FC4:
	// cmpw cr6,r9,r3
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r3.s32, xer);
	// bge cr6,0x82651fe4
	if (!cr6.lt) goto loc_82651FE4;
	// subf r10,r9,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r9.s64;
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// b 0x8239bd4c
	return;
loc_82651FE4:
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// ble cr6,0x82651ff8
	if (!cr6.gt) goto loc_82651FF8;
	// subf r10,r9,r29
	ctx.r10.s64 = r29.s64 - ctx.r9.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_82651FF8:
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r31.u32);
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82652004"))) PPC_WEAK_FUNC(sub_82652004);
PPC_FUNC_IMPL(__imp__sub_82652004) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82652008"))) PPC_WEAK_FUNC(sub_82652008);
PPC_FUNC_IMPL(__imp__sub_82652008) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// rlwinm r3,r9,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rlwinm r31,r8,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// beq cr6,0x826520ac
	if (cr6.eq) goto loc_826520AC;
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
	// rlwinm r5,r5,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// bge cr6,0x82652060
	if (!cr6.lt) goto loc_82652060;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// b 0x82652074
	goto loc_82652074;
loc_82652060:
	// cmpw cr6,r11,r3
	cr6.compare<int32_t>(r11.s32, ctx.r3.s32, xer);
	// ble cr6,0x82652074
	if (!cr6.gt) goto loc_82652074;
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
loc_82652074:
	// cmpwi cr6,r8,-8
	cr6.compare<int32_t>(ctx.r8.s32, -8, xer);
	// bge cr6,0x82652098
	if (!cr6.lt) goto loc_82652098;
	// addi r11,r8,8
	r11.s64 = ctx.r8.s64 + 8;
	// stw r9,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r9.u32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82652098:
	// cmpw cr6,r8,r31
	cr6.compare<int32_t>(ctx.r8.s32, r31.s32, xer);
	// ble cr6,0x826520ac
	if (!cr6.gt) goto loc_826520AC;
	// subf r11,r8,r31
	r11.s64 = r31.s64 - ctx.r8.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
loc_826520AC:
	// stw r9,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r9.u32);
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826520BC"))) PPC_WEAK_FUNC(sub_826520BC);
PPC_FUNC_IMPL(__imp__sub_826520BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826520C0"))) PPC_WEAK_FUNC(sub_826520C0);
PPC_FUNC_IMPL(__imp__sub_826520C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// srawi r30,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r30.s64 = ctx.r9.s32 >> 2;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// clrlwi r7,r8,30
	ctx.r7.u64 = ctx.r8.u32 & 0x3;
	// srawi r31,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r31.s64 = ctx.r8.s32 >> 2;
	// mullw r8,r30,r4
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r4.s32);
	// add r31,r8,r31
	r31.u64 = ctx.r8.u64 + r31.u64;
	// clrlwi r8,r9,30
	ctx.r8.u64 = ctx.r9.u32 & 0x3;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// add r3,r31,r3
	ctx.r3.u64 = r31.u64 + ctx.r3.u64;
	// bne cr6,0x82652188
	if (!cr6.eq) goto loc_82652188;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82652188
	if (!cr6.eq) goto loc_82652188;
	// ld r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// std r9,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r9.u64);
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// ldx r11,r11,r4
	r11.u64 = PPC_LOAD_U64(r11.u32 + ctx.r4.u32);
	// stdx r11,r10,r6
	PPC_STORE_U64(ctx.r10.u32 + ctx.r6.u32, r11.u64);
	// b 0x826521a8
	goto loc_826521A8;
loc_82652188:
	// lwz r9,3904(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 3904);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8265219c
	if (!cr6.eq) goto loc_8265219C;
	// lwz r11,3140(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3140);
	// b 0x826521a0
	goto loc_826521A0;
loc_8265219C:
	// lwz r11,3144(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 3144);
loc_826521A0:
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826521A8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826521C0"))) PPC_WEAK_FUNC(sub_826521C0);
PPC_FUNC_IMPL(__imp__sub_826521C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcc0
	// lis r11,-32244
	r11.s64 = -2113142784;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,30936
	r11.s64 = r11.s64 + 30936;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r16,r10,r11
	r16.u64 = ctx.r10.u64 + r11.u64;
	// add r24,r7,r11
	r24.u64 = ctx.r7.u64 + r11.u64;
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r11,r3
	r31.u64 = r11.u64 + ctx.r3.u64;
	// li r11,8
	r11.s64 = 8;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r24,-328(r1)
	PPC_STORE_U32(ctx.r1.u32 + -328, r24.u32);
	// add r26,r8,r3
	r26.u64 = ctx.r8.u64 + ctx.r3.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r10,r3
	r30.u64 = ctx.r10.u64 + ctx.r3.u64;
	// stw r31,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r31.u32);
	// stw r11,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r11.u32);
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r4,r11
	r29.u64 = ctx.r4.u64 + r11.u64;
	// stw r26,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r26.u32);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, r30.u32);
	// add r25,r7,r5
	r25.u64 = ctx.r7.u64 + ctx.r5.u64;
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// rlwinm r8,r29,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// add r27,r7,r3
	r27.u64 = ctx.r7.u64 + ctx.r3.u64;
	// stw r25,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r25.u32);
	// add r28,r8,r3
	r28.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r29,r10,r3
	r29.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r7,r11,r3
	ctx.r7.u64 = r11.u64 + ctx.r3.u64;
	// stw r27,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, r27.u32);
	// stw r28,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r28.u32);
	// stw r29,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r29.u32);
	// stw r7,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r7.u32);
	// b 0x8265226c
	goto loc_8265226C;
loc_82652268:
	// lwz r24,-328(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + -328);
loc_8265226C:
	// addi r8,r4,-1
	ctx.r8.s64 = ctx.r4.s64 + -1;
	// stw r25,-316(r1)
	PPC_STORE_U32(ctx.r1.u32 + -316, r25.u32);
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// lbz r18,1(r26)
	r18.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// add r21,r8,r3
	r21.u64 = ctx.r8.u64 + ctx.r3.u64;
	// lhz r11,2(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 2);
	// add r5,r25,r6
	ctx.r5.u64 = r25.u64 + ctx.r6.u64;
	// lbz r20,1(r28)
	r20.u64 = PPC_LOAD_U8(r28.u32 + 1);
	// lhz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lbz r17,1(r3)
	r17.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// add r24,r3,r4
	r24.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lbz r14,1(r7)
	r14.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lbz r25,1(r21)
	r25.u64 = PPC_LOAD_U8(r21.u32 + 1);
	// li r21,2
	r21.s64 = 2;
	// lbz r19,1(r27)
	r19.u64 = PPC_LOAD_U8(r27.u32 + 1);
	// addi r8,r1,-284
	ctx.r8.s64 = ctx.r1.s64 + -284;
	// lbz r23,0(r3)
	r23.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r28,0(r28)
	r28.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// lbz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// stw r21,-308(r1)
	PPC_STORE_U32(ctx.r1.u32 + -308, r21.u32);
	// lbz r21,1(r29)
	r21.u64 = PPC_LOAD_U8(r29.u32 + 1);
	// lbz r27,0(r27)
	r27.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// stw r18,-296(r1)
	PPC_STORE_U32(ctx.r1.u32 + -296, r18.u32);
	// mullw r18,r17,r11
	r18.s64 = int64_t(r17.s32) * int64_t(r11.s32);
	// stw r20,-320(r1)
	PPC_STORE_U32(ctx.r1.u32 + -320, r20.u32);
	// lbz r26,0(r26)
	r26.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// stw r21,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r21.u32);
	// stw r27,-312(r1)
	PPC_STORE_U32(ctx.r1.u32 + -312, r27.u32);
	// stw r19,-304(r1)
	PPC_STORE_U32(ctx.r1.u32 + -304, r19.u32);
	// stw r5,-300(r1)
	PPC_STORE_U32(ctx.r1.u32 + -300, ctx.r5.u32);
	// stw r26,-324(r1)
	PPC_STORE_U32(ctx.r1.u32 + -324, r26.u32);
	// lbz r5,1(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// mullw r17,r23,r10
	r17.s64 = int64_t(r23.s32) * int64_t(ctx.r10.s32);
	// lbz r30,0(r30)
	r30.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lbz r22,1(r31)
	r22.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// lbz r29,0(r29)
	r29.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lbz r31,0(r31)
	r31.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// lbz r24,1(r24)
	r24.u64 = PPC_LOAD_U8(r24.u32 + 1);
	// mullw r23,r7,r10
	r23.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// mullw r19,r25,r10
	r19.s64 = int64_t(r25.s32) * int64_t(ctx.r10.s32);
	// mullw r25,r30,r10
	r25.s64 = int64_t(r30.s32) * int64_t(ctx.r10.s32);
	// mullw r26,r5,r11
	r26.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r7,-368(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// stw r28,-368(r1)
	PPC_STORE_U32(ctx.r1.u32 + -368, r28.u32);
	// lwz r5,-312(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -312);
	// mullw r28,r7,r11
	r28.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// lwz r7,-320(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -320);
	// mullw r30,r7,r11
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r27,r29,r10
	r27.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// mullw r21,r31,r10
	r21.s64 = int64_t(r31.s32) * int64_t(ctx.r10.s32);
	// mullw r31,r5,r10
	r31.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r5,-296(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -296);
	// lwz r7,-368(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -368);
	// mullw r29,r7,r10
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// lwz r7,-304(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -304);
	// mullw r20,r24,r11
	r20.s64 = int64_t(r24.s32) * int64_t(r11.s32);
	// mullw r22,r22,r11
	r22.s64 = int64_t(r22.s32) * int64_t(r11.s32);
	// mullw r24,r14,r11
	r24.s64 = int64_t(r14.s32) * int64_t(r11.s32);
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r11,r5,r11
	r11.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r5,-324(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -324);
	// mullw r10,r5,r10
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// add r5,r18,r17
	ctx.r5.u64 = r18.u64 + r17.u64;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r5,-288(r1)
	PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r5.u32);
	// add r5,r20,r19
	ctx.r5.u64 = r20.u64 + r19.u64;
	// stw r7,-260(r1)
	PPC_STORE_U32(ctx.r1.u32 + -260, ctx.r7.u32);
	// stw r5,-284(r1)
	PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r5.u32);
	// add r5,r22,r21
	ctx.r5.u64 = r22.u64 + r21.u64;
	// stw r5,-280(r1)
	PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r5.u32);
	// add r5,r24,r23
	ctx.r5.u64 = r24.u64 + r23.u64;
	// stw r5,-276(r1)
	PPC_STORE_U32(ctx.r1.u32 + -276, ctx.r5.u32);
	// add r5,r26,r25
	ctx.r5.u64 = r26.u64 + r25.u64;
	// stw r5,-272(r1)
	PPC_STORE_U32(ctx.r1.u32 + -272, ctx.r5.u32);
	// add r5,r28,r27
	ctx.r5.u64 = r28.u64 + r27.u64;
	// stw r5,-268(r1)
	PPC_STORE_U32(ctx.r1.u32 + -268, ctx.r5.u32);
	// add r5,r30,r29
	ctx.r5.u64 = r30.u64 + r29.u64;
	// stw r5,-264(r1)
	PPC_STORE_U32(ctx.r1.u32 + -264, ctx.r5.u32);
	// lwz r5,-316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -316);
	// lwz r31,-308(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + -308);
	// lwz r30,-300(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + -300);
	// stw r11,-256(r1)
	PPC_STORE_U32(ctx.r1.u32 + -256, r11.u32);
loc_826523C0:
	// lhz r11,0(r16)
	r11.u64 = PPC_LOAD_U16(r16.u32 + 0);
	// lhz r7,2(r16)
	ctx.r7.u64 = PPC_LOAD_U16(r16.u32 + 2);
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lwz r29,-4(r8)
	r29.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// mullw r11,r29,r11
	r11.s64 = int64_t(r29.s32) * int64_t(r11.s32);
	// mullw r7,r7,r10
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82652400
	if (!cr6.lt) goto loc_82652400;
	// li r11,0
	r11.s64 = 0;
	// b 0x8265240c
	goto loc_8265240C;
loc_82652400:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x8265240c
	if (!cr6.gt) goto loc_8265240C;
	// li r11,255
	r11.s64 = 255;
loc_8265240C:
	// stb r11,0(r15)
	PPC_STORE_U8(r15.u32 + 0, r11.u8);
	// lhz r11,2(r16)
	r11.u64 = PPC_LOAD_U16(r16.u32 + 2);
	// lhz r29,0(r16)
	r29.u64 = PPC_LOAD_U16(r16.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r11,r11,r7
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// mullw r10,r29,r10
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265244c
	if (!cr6.lt) goto loc_8265244C;
	// li r11,0
	r11.s64 = 0;
	// b 0x82652458
	goto loc_82652458;
loc_8265244C:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x82652458
	if (!cr6.gt) goto loc_82652458;
	// li r11,255
	r11.s64 = 255;
loc_82652458:
	// stbx r11,r15,r6
	PPC_STORE_U8(r15.u32 + ctx.r6.u32, r11.u8);
	// lhz r11,2(r16)
	r11.u64 = PPC_LOAD_U16(r16.u32 + 2);
	// lhz r29,0(r16)
	r29.u64 = PPC_LOAD_U16(r16.u32 + 0);
	// lwz r10,8(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// mullw r7,r29,r7
	ctx.r7.s64 = int64_t(r29.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82652498
	if (!cr6.lt) goto loc_82652498;
	// li r11,0
	r11.s64 = 0;
	// b 0x826524a4
	goto loc_826524A4;
loc_82652498:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x826524a4
	if (!cr6.gt) goto loc_826524A4;
	// li r11,255
	r11.s64 = 255;
loc_826524A4:
	// stb r11,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r11.u8);
	// lhz r11,2(r16)
	r11.u64 = PPC_LOAD_U16(r16.u32 + 2);
	// lhz r29,0(r16)
	r29.u64 = PPC_LOAD_U16(r16.u32 + 0);
	// lwz r7,12(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r29,r29
	r29.s64 = r29.s16;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// mullw r10,r29,r10
	ctx.r10.s64 = int64_t(r29.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// srawi r11,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	r11.s64 = r11.s32 >> 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x826524e4
	if (!cr6.lt) goto loc_826524E4;
	// li r11,0
	r11.s64 = 0;
	// b 0x826524f0
	goto loc_826524F0;
loc_826524E4:
	// cmpwi cr6,r11,255
	cr6.compare<int32_t>(r11.s32, 255, xer);
	// ble cr6,0x826524f0
	if (!cr6.gt) goto loc_826524F0;
	// li r11,255
	r11.s64 = 255;
loc_826524F0:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// add r15,r15,r11
	r15.u64 = r15.u64 + r11.u64;
	// add r5,r11,r5
	ctx.r5.u64 = r11.u64 + ctx.r5.u64;
	// stb r10,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r10.u8);
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x826523c0
	if (!cr6.eq) goto loc_826523C0;
	// lwz r10,-336(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -336);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// lwz r11,-332(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -332);
	// addi r31,r10,1
	r31.s64 = ctx.r10.s64 + 1;
	// lwz r10,-344(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -344);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// lwz r10,-352(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -352);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// lwz r10,-364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -364);
	// stw r31,-336(r1)
	PPC_STORE_U32(ctx.r1.u32 + -336, r31.u32);
	// addi r29,r10,1
	r29.s64 = ctx.r10.s64 + 1;
	// lwz r10,-360(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -360);
	// stw r11,-332(r1)
	PPC_STORE_U32(ctx.r1.u32 + -332, r11.u32);
	// addi r28,r10,1
	r28.s64 = ctx.r10.s64 + 1;
	// lwz r10,-356(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -356);
	// stw r7,-344(r1)
	PPC_STORE_U32(ctx.r1.u32 + -344, ctx.r7.u32);
	// addi r27,r10,1
	r27.s64 = ctx.r10.s64 + 1;
	// lwz r10,-348(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -348);
	// stw r30,-352(r1)
	PPC_STORE_U32(ctx.r1.u32 + -352, r30.u32);
	// addi r26,r10,1
	r26.s64 = ctx.r10.s64 + 1;
	// stw r29,-364(r1)
	PPC_STORE_U32(ctx.r1.u32 + -364, r29.u32);
	// stw r28,-360(r1)
	PPC_STORE_U32(ctx.r1.u32 + -360, r28.u32);
	// stw r27,-356(r1)
	PPC_STORE_U32(ctx.r1.u32 + -356, r27.u32);
	// stw r26,-348(r1)
	PPC_STORE_U32(ctx.r1.u32 + -348, r26.u32);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// lwz r10,-340(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -340);
	// addi r25,r10,1
	r25.s64 = ctx.r10.s64 + 1;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// stw r25,-340(r1)
	PPC_STORE_U32(ctx.r1.u32 + -340, r25.u32);
	// bne cr6,0x82652268
	if (!cr6.eq) goto loc_82652268;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826525A0"))) PPC_WEAK_FUNC(sub_826525A0);
PPC_FUNC_IMPL(__imp__sub_826525A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcc0
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,44(r1)
	PPC_STORE_U32(ctx.r1.u32 + 44, ctx.r6.u32);
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r21,r5,1
	r21.s64 = ctx.r5.s64 + 1;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// subfic r29,r11,1
	xer.ca = r11.u32 <= 1;
	r29.s64 = 1 - r11.s64;
	// add r5,r11,r3
	ctx.r5.u64 = r11.u64 + ctx.r3.u64;
	// subfic r28,r11,2
	xer.ca = r11.u32 <= 2;
	r28.s64 = 2 - r11.s64;
	// subfic r27,r11,3
	xer.ca = r11.u32 <= 3;
	r27.s64 = 3 - r11.s64;
	// addi r22,r3,1
	r22.s64 = ctx.r3.s64 + 1;
	// addi r18,r4,-1
	r18.s64 = ctx.r4.s64 + -1;
	// li r19,8
	r19.s64 = 8;
	// subfic r24,r4,-1
	xer.ca = ctx.r4.u32 <= 4294967295;
	r24.s64 = -1 - ctx.r4.s64;
	// addi r20,r5,3
	r20.s64 = ctx.r5.s64 + 3;
	// subf r23,r11,r10
	r23.s64 = ctx.r10.s64 - r11.s64;
	// subf r26,r4,r10
	r26.s64 = ctx.r10.s64 - ctx.r4.s64;
loc_826525E8:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r30,r20
	r30.u64 = r20.u64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// add r11,r18,r22
	r11.u64 = r18.u64 + r22.u64;
	// li r25,2
	r25.s64 = 2;
loc_826525FC:
	// lhz r6,6(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lbzx r31,r23,r11
	r31.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r16,r5
	r16.s64 = ctx.r5.s16;
	// lbzx r17,r10,r24
	r17.u64 = PPC_LOAD_U8(ctx.r10.u32 + r24.u32);
	// mullw r5,r31,r6
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lhz r15,2(r7)
	r15.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r6,-1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// lhz r14,4(r7)
	r14.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// stw r31,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r31.u32);
	// mullw r31,r17,r16
	r31.s64 = int64_t(r17.s32) * int64_t(r16.s32);
	// extsh r17,r15
	r17.s64 = r15.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// mullw r31,r6,r17
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// extsh r16,r14
	r16.s64 = r14.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lwz r6,-160(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// mullw r31,r6,r16
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r16.s32);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// sraw r5,r6,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r5.s64 = ctx.r6.s32 >> temp.u32;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x82652668
	if (!cr6.lt) goto loc_82652668;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x82652674
	goto loc_82652674;
loc_82652668:
	// cmpwi cr6,r5,255
	cr6.compare<int32_t>(ctx.r5.s32, 255, xer);
	// ble cr6,0x82652674
	if (!cr6.gt) goto loc_82652674;
	// li r5,255
	ctx.r5.s64 = 255;
loc_82652674:
	// stb r5,-1(r3)
	PPC_STORE_U8(ctx.r3.u32 + -1, ctx.r5.u8);
	// lhz r6,6(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lbzx r31,r10,r26
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + r26.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r16,r5
	r16.s64 = ctx.r5.s16;
	// lbzx r17,r29,r11
	r17.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// mullw r5,r31,r6
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lhz r15,4(r7)
	r15.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbz r6,1(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lhz r14,2(r7)
	r14.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// stw r31,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r31.u32);
	// mullw r31,r17,r16
	r31.s64 = int64_t(r17.s32) * int64_t(r16.s32);
	// extsh r17,r15
	r17.s64 = r15.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// mullw r31,r6,r17
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// extsh r16,r14
	r16.s64 = r14.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lwz r6,-160(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// mullw r31,r16,r6
	r31.s64 = int64_t(r16.s32) * int64_t(ctx.r6.s32);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// sraw r5,r6,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r5.s64 = ctx.r6.s32 >> temp.u32;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x826526e4
	if (!cr6.lt) goto loc_826526E4;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x826526f0
	goto loc_826526F0;
loc_826526E4:
	// cmpwi cr6,r5,255
	cr6.compare<int32_t>(ctx.r5.s32, 255, xer);
	// ble cr6,0x826526f0
	if (!cr6.gt) goto loc_826526F0;
	// li r5,255
	ctx.r5.s64 = 255;
loc_826526F0:
	// stb r5,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r5.u8);
	// lhz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lhz r5,4(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbzx r31,r28,r11
	r31.u64 = PPC_LOAD_U8(r28.u32 + r11.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r16,r5
	r16.s64 = ctx.r5.s16;
	// lbz r17,2(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mullw r5,r31,r6
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lbz r31,-1(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + -1);
	// lhz r15,2(r7)
	r15.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r6,1(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// lhz r14,6(r7)
	r14.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// stw r31,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r31.u32);
	// mullw r31,r17,r16
	r31.s64 = int64_t(r17.s32) * int64_t(r16.s32);
	// extsh r17,r15
	r17.s64 = r15.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// mullw r31,r6,r17
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// extsh r16,r14
	r16.s64 = r14.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lwz r6,-160(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// mullw r31,r6,r16
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r16.s32);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// sraw r5,r6,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r5.s64 = ctx.r6.s32 >> temp.u32;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x82652760
	if (!cr6.lt) goto loc_82652760;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x8265276c
	goto loc_8265276C;
loc_82652760:
	// cmpwi cr6,r5,255
	cr6.compare<int32_t>(ctx.r5.s32, 255, xer);
	// ble cr6,0x8265276c
	if (!cr6.gt) goto loc_8265276C;
	// li r5,255
	ctx.r5.s64 = 255;
loc_8265276C:
	// stb r5,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r5.u8);
	// lhz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lhz r5,4(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbzx r31,r27,r11
	r31.u64 = PPC_LOAD_U8(r27.u32 + r11.u32);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// extsh r16,r5
	r16.s64 = ctx.r5.s16;
	// lbz r17,3(r11)
	r17.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// mullw r5,r31,r6
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(ctx.r6.s32);
	// lbz r31,0(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// lhz r15,2(r7)
	r15.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r6,2(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// lhz r14,6(r7)
	r14.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// stw r31,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r31.u32);
	// mullw r31,r17,r16
	r31.s64 = int64_t(r17.s32) * int64_t(r16.s32);
	// extsh r17,r15
	r17.s64 = r15.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// mullw r31,r6,r17
	r31.s64 = int64_t(ctx.r6.s32) * int64_t(r17.s32);
	// extsh r16,r14
	r16.s64 = r14.s16;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lwz r6,-160(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -160);
	// mullw r31,r16,r6
	r31.s64 = int64_t(r16.s32) * int64_t(ctx.r6.s32);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r5,r9
	ctx.r6.u64 = ctx.r5.u64 + ctx.r9.u64;
	// sraw r5,r6,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r5.s64 = ctx.r6.s32 >> temp.u32;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x826527dc
	if (!cr6.lt) goto loc_826527DC;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x826527e8
	goto loc_826527E8;
loc_826527DC:
	// cmpwi cr6,r5,255
	cr6.compare<int32_t>(ctx.r5.s32, 255, xer);
	// ble cr6,0x826527e8
	if (!cr6.gt) goto loc_826527E8;
	// li r5,255
	ctx.r5.s64 = 255;
loc_826527E8:
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// stb r5,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r5.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x826525fc
	if (!cr6.eq) goto loc_826525FC;
	// lwz r11,44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 44);
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// add r22,r22,r4
	r22.u64 = r22.u64 + ctx.r4.u64;
	// add r21,r21,r11
	r21.u64 = r21.u64 + r11.u64;
	// add r20,r20,r4
	r20.u64 = r20.u64 + ctx.r4.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x826525e8
	if (!cr6.eq) goto loc_826525E8;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82652828"))) PPC_WEAK_FUNC(sub_82652828);
PPC_FUNC_IMPL(__imp__sub_82652828) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bce4
	// addi r27,r3,1
	r27.s64 = ctx.r3.s64 + 1;
	// addi r28,r5,1
	r28.s64 = ctx.r5.s64 + 1;
	// li r26,8
	r26.s64 = 8;
loc_8265283C:
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
	// li r29,2
	r29.s64 = 2;
loc_82652848:
	// lhz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// lhz r5,4(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbz r3,-2(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// mullw r10,r3,r10
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r10.s32);
	// lhz r25,6(r7)
	r25.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r3,2(r7)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// mullw r5,r5,r31
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(r31.s32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// lbz r5,1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r31,r25
	r31.s64 = r25.s16;
	// lbz r25,-1(r11)
	r25.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// extsh r3,r3
	ctx.r3.s64 = ctx.r3.s16;
	// mullw r5,r31,r5
	ctx.r5.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// mullw r5,r3,r25
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(r25.s32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sraw r10,r10,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x826528ac
	if (!cr6.lt) goto loc_826528AC;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826528b8
	goto loc_826528B8;
loc_826528AC:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x826528b8
	if (!cr6.gt) goto loc_826528B8;
	// li r10,255
	ctx.r10.s64 = 255;
loc_826528B8:
	// stb r10,-1(r30)
	PPC_STORE_U8(r30.u32 + -1, ctx.r10.u8);
	// lhz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbz r3,1(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r5,6(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r31,0(r7)
	r31.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// lhz r3,2(r7)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r24,0(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r25,r5
	r25.s64 = ctx.r5.s16;
	// lbz r5,-1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r23,r3
	r23.s64 = ctx.r3.s16;
	// mullw r3,r31,r5
	ctx.r3.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// lbz r31,2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// mullw r31,r25,r31
	r31.s64 = int64_t(r25.s32) * int64_t(r31.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// mullw r5,r23,r24
	ctx.r5.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sraw r10,r10,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82652920
	if (!cr6.lt) goto loc_82652920;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8265292c
	goto loc_8265292C;
loc_82652920:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x8265292c
	if (!cr6.gt) goto loc_8265292C;
	// li r10,255
	ctx.r10.s64 = 255;
loc_8265292C:
	// stb r10,0(r30)
	PPC_STORE_U8(r30.u32 + 0, ctx.r10.u8);
	// lhz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbz r3,2(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r5,6(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r31,0(r7)
	r31.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// lhz r3,2(r7)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r24,1(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r25,r5
	r25.s64 = ctx.r5.s16;
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r23,r3
	r23.s64 = ctx.r3.s16;
	// mullw r3,r31,r5
	ctx.r3.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// lbz r31,3(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// mullw r31,r31,r25
	r31.s64 = int64_t(r31.s32) * int64_t(r25.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// mullw r5,r23,r24
	ctx.r5.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sraw r10,r10,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82652994
	if (!cr6.lt) goto loc_82652994;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x826529a0
	goto loc_826529A0;
loc_82652994:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x826529a0
	if (!cr6.gt) goto loc_826529A0;
	// li r10,255
	ctx.r10.s64 = 255;
loc_826529A0:
	// stb r10,1(r30)
	PPC_STORE_U8(r30.u32 + 1, ctx.r10.u8);
	// lhz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lbz r3,3(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r5,6(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r31,0(r7)
	r31.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// mullw r10,r10,r3
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// lhz r3,2(r7)
	ctx.r3.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lbz r24,2(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// extsh r25,r5
	r25.s64 = ctx.r5.s16;
	// lbz r5,1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// extsh r31,r31
	r31.s64 = r31.s16;
	// extsh r23,r3
	r23.s64 = ctx.r3.s16;
	// mullw r3,r31,r5
	ctx.r3.s64 = int64_t(r31.s32) * int64_t(ctx.r5.s32);
	// lbz r31,4(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// mullw r31,r25,r31
	r31.s64 = int64_t(r25.s32) * int64_t(r31.s32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// mullw r5,r23,r24
	ctx.r5.s64 = int64_t(r23.s32) * int64_t(r24.s32);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sraw r10,r10,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r10.s64 = ctx.r10.s32 >> temp.u32;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82652a08
	if (!cr6.lt) goto loc_82652A08;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82652a14
	goto loc_82652A14;
loc_82652A08:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82652a14
	if (!cr6.gt) goto loc_82652A14;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82652A14:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// stb r10,2(r30)
	PPC_STORE_U8(r30.u32 + 2, ctx.r10.u8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82652848
	if (!cr6.eq) goto loc_82652848;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// add r28,r28,r6
	r28.u64 = r28.u64 + ctx.r6.u64;
	// add r27,r27,r4
	r27.u64 = r27.u64 + ctx.r4.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x8265283c
	if (!cr6.eq) goto loc_8265283C;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82652A44"))) PPC_WEAK_FUNC(sub_82652A44);
PPC_FUNC_IMPL(__imp__sub_82652A44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82652A48"))) PPC_WEAK_FUNC(sub_82652A48);
PPC_FUNC_IMPL(__imp__sub_82652A48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bccc
	// stwu r1,-704(r1)
	ea = -704 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lhz r30,6(r8)
	r30.u64 = PPC_LOAD_U16(ctx.r8.u32 + 6);
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r29,4(r8)
	r29.u64 = PPC_LOAD_U16(ctx.r8.u32 + 4);
	// add r31,r3,r4
	r31.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lhz r26,2(r8)
	r26.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
	// extsh r28,r30
	r28.s64 = r30.s16;
	// lhz r25,0(r8)
	r25.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// subf r8,r4,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r4.s64;
	// extsh r27,r29
	r27.s64 = r29.s16;
	// addi r22,r8,-1
	r22.s64 = ctx.r8.s64 + -1;
	// rlwinm r8,r4,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// extsh r26,r26
	r26.s64 = r26.s16;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// extsh r25,r25
	r25.s64 = r25.s16;
	// addi r21,r31,-1
	r21.s64 = r31.s64 + -1;
	// addi r20,r1,16
	r20.s64 = ctx.r1.s64 + 16;
	// li r19,8
	r19.s64 = 8;
	// subf r24,r11,r4
	r24.s64 = ctx.r4.s64 - r11.s64;
	// subf r23,r11,r8
	r23.s64 = ctx.r8.s64 - r11.s64;
loc_82652AA0:
	// mr r31,r20
	r31.u64 = r20.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r11,r21
	r11.u64 = r21.u64;
	// li r8,11
	ctx.r8.s64 = 11;
loc_82652AB0:
	// lbzx r30,r24,r11
	r30.u64 = PPC_LOAD_U8(r24.u32 + r11.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbzx r29,r23,r11
	r29.u64 = PPC_LOAD_U8(r23.u32 + r11.u32);
	// mullw r30,r30,r26
	r30.s64 = int64_t(r30.s32) * int64_t(r26.s32);
	// lbz r18,0(r11)
	r18.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r17,0(r3)
	r17.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// mullw r29,r29,r28
	r29.s64 = int64_t(r29.s32) * int64_t(r28.s32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// mullw r29,r18,r27
	r29.s64 = int64_t(r18.s32) * int64_t(r27.s32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// mullw r29,r17,r25
	r29.s64 = int64_t(r17.s32) * int64_t(r25.s32);
	// add r30,r30,r29
	r30.u64 = r30.u64 + r29.u64;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// add r30,r30,r10
	r30.u64 = r30.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// sraw r30,r30,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (r30.s32 < 0) & (((r30.s32 >> temp.u32) << temp.u32) != r30.s32);
	r30.s64 = r30.s32 >> temp.u32;
	// sth r30,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r30.u16);
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// bne cr6,0x82652ab0
	if (!cr6.eq) goto loc_82652AB0;
	// addi r19,r19,-1
	r19.s64 = r19.s64 + -1;
	// add r22,r22,r4
	r22.u64 = r22.u64 + ctx.r4.u64;
	// add r21,r21,r4
	r21.u64 = r21.u64 + ctx.r4.u64;
	// addi r20,r20,70
	r20.s64 = r20.s64 + 70;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x82652aa0
	if (!cr6.eq) goto loc_82652AA0;
	// lwz r3,788(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// addi r27,r5,1
	r27.s64 = ctx.r5.s64 + 1;
	// addi r26,r1,20
	r26.s64 = ctx.r1.s64 + 20;
	// li r25,8
	r25.s64 = 8;
loc_82652B28:
	// mr r31,r27
	r31.u64 = r27.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// li r28,2
	r28.s64 = 2;
loc_82652B34:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r5,r10
	ctx.r5.s64 = ctx.r10.s16;
	// lhz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r10,r9
	ctx.r10.s64 = ctx.r9.s16;
	// lhz r9,2(r7)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lhz r30,-4(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// extsh r24,r9
	r24.s64 = ctx.r9.s16;
	// lhz r9,6(r7)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// extsh r23,r9
	r23.s64 = ctx.r9.s16;
	// lhz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r8,r9
	ctx.r8.s64 = ctx.r9.s16;
	// extsh r9,r30
	ctx.r9.s64 = r30.s16;
	// mullw r30,r4,r5
	r30.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// mullw r9,r9,r29
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r29.s32);
	// mullw r29,r24,r8
	r29.s64 = int64_t(r24.s32) * int64_t(ctx.r8.s32);
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// mullw r4,r23,r10
	ctx.r4.s64 = int64_t(r23.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// srawi r9,r9,7
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7F) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x82652ba8
	if (!cr6.lt) goto loc_82652BA8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x82652bb4
	goto loc_82652BB4;
loc_82652BA8:
	// cmpwi cr6,r9,255
	cr6.compare<int32_t>(ctx.r9.s32, 255, xer);
	// ble cr6,0x82652bb4
	if (!cr6.gt) goto loc_82652BB4;
	// li r9,255
	ctx.r9.s64 = 255;
loc_82652BB4:
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r9,r4
	ctx.r9.s64 = ctx.r4.s16;
	// stb r30,-1(r31)
	PPC_STORE_U8(r31.u32 + -1, r30.u8);
	// lhz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// lhz r30,0(r7)
	r30.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// lhz r29,2(r7)
	r29.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lhz r24,6(r7)
	r24.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// mullw r4,r4,r10
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// mullw r8,r30,r8
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r8.s32);
	// extsh r30,r29
	r30.s64 = r29.s16;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// mullw r4,r30,r5
	ctx.r4.s64 = int64_t(r30.s32) * int64_t(ctx.r5.s32);
	// extsh r29,r24
	r29.s64 = r24.s16;
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r4,r29,r9
	ctx.r4.s64 = int64_t(r29.s32) * int64_t(ctx.r9.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82652c18
	if (!cr6.lt) goto loc_82652C18;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82652c24
	goto loc_82652C24;
loc_82652C18:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82652c24
	if (!cr6.gt) goto loc_82652C24;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82652C24:
	// stb r8,0(r31)
	PPC_STORE_U8(r31.u32 + 0, ctx.r8.u8);
	// lhz r8,2(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// lhz r30,0(r7)
	r30.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lhz r29,4(r7)
	r29.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// extsh r30,r30
	r30.s64 = r30.s16;
	// lhz r4,6(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// mullw r8,r8,r10
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// lhz r24,6(r7)
	r24.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// mullw r5,r30,r5
	ctx.r5.s64 = int64_t(r30.s32) * int64_t(ctx.r5.s32);
	// extsh r30,r29
	r30.s64 = r29.s16;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// mullw r5,r30,r9
	ctx.r5.s64 = int64_t(r30.s32) * int64_t(ctx.r9.s32);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// extsh r29,r24
	r29.s64 = r24.s16;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// mullw r5,r29,r4
	ctx.r5.s64 = int64_t(r29.s32) * int64_t(ctx.r4.s32);
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// srawi r8,r8,7
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bge cr6,0x82652c84
	if (!cr6.lt) goto loc_82652C84;
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x82652c90
	goto loc_82652C90;
loc_82652C84:
	// cmpwi cr6,r8,255
	cr6.compare<int32_t>(ctx.r8.s32, 255, xer);
	// ble cr6,0x82652c90
	if (!cr6.gt) goto loc_82652C90;
	// li r8,255
	ctx.r8.s64 = 255;
loc_82652C90:
	// stb r8,1(r31)
	PPC_STORE_U8(r31.u32 + 1, ctx.r8.u8);
	// lhz r8,6(r7)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// lhz r5,2(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// extsh r29,r8
	r29.s64 = ctx.r8.s16;
	// lhz r24,0(r7)
	r24.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// lhz r30,4(r7)
	r30.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// mullw r5,r8,r9
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// extsh r9,r24
	ctx.r9.s64 = r24.s16;
	// extsh r30,r30
	r30.s64 = r30.s16;
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lhz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// mullw r8,r30,r4
	ctx.r8.s64 = int64_t(r30.s32) * int64_t(ctx.r4.s32);
	// mullw r10,r10,r29
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r29.s32);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82652cf0
	if (!cr6.lt) goto loc_82652CF0;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82652cfc
	goto loc_82652CFC;
loc_82652CF0:
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// ble cr6,0x82652cfc
	if (!cr6.gt) goto loc_82652CFC;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82652CFC:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// stb r10,2(r31)
	PPC_STORE_U8(r31.u32 + 2, ctx.r10.u8);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82652b34
	if (!cr6.eq) goto loc_82652B34;
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// add r27,r27,r6
	r27.u64 = r27.u64 + ctx.r6.u64;
	// addi r26,r26,70
	r26.s64 = r26.s64 + 70;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x82652b28
	if (!cr6.eq) goto loc_82652B28;
	// addi r1,r1,704
	ctx.r1.s64 = ctx.r1.s64 + 704;
	// b 0x8239bd1c
	return;
}

__attribute__((alias("__imp__sub_82652D30"))) PPC_WEAK_FUNC(sub_82652D30);
PPC_FUNC_IMPL(__imp__sub_82652D30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82652D40:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82652d40
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652D40;
	// add r11,r5,r6
	r11.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r10,r3,r4
	ctx.r10.u64 = ctx.r3.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652D6C:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652d6c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652D6C;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652D98:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652d98
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652D98;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652DC4:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652dc4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652DC4;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652DF0:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652df0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652DF0;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652E1C:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652e1c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652E1C;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82652E48:
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// bdnz 0x82652e48
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652E48;
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// li r8,8
	ctx.r8.s64 = 8;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_82652E70:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// bdnz 0x82652e70
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82652E70;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82652E88"))) PPC_WEAK_FUNC(sub_82652E88);
PPC_FUNC_IMPL(__imp__sub_82652E88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// addi r10,r10,15920
	ctx.r10.s64 = ctx.r10.s64 + 15920;
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r8,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82652f04
	if (!cr6.eq) goto loc_82652F04;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82652ed4
	if (!cr6.eq) goto loc_82652ED4;
	// bl 0x82652d30
	sub_82652D30(ctx, base);
	// b 0x82652f80
	goto loc_82652F80;
loc_82652ED4:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r8,4
	ctx.r8.s64 = 4;
	// beq cr6,0x82652ee4
	if (cr6.eq) goto loc_82652EE4;
	// li r8,6
	ctx.r8.s64 = 6;
loc_82652EE4:
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r11,r8,-1
	r11.s64 = ctx.r8.s64 + -1;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// slw r11,r9,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// bl 0x826525a0
	sub_826525A0(ctx, base);
	// b 0x82652f80
	goto loc_82652F80;
loc_82652F04:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82652f34
	if (!cr6.eq) goto loc_82652F34;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r8,4
	ctx.r8.s64 = 4;
	// beq cr6,0x82652f1c
	if (cr6.eq) goto loc_82652F1C;
	// li r8,6
	ctx.r8.s64 = 6;
loc_82652F1C:
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r8,-1
	r11.s64 = ctx.r8.s64 + -1;
	// slw r11,r10,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// subf r9,r31,r11
	ctx.r9.s64 = r11.s64 - r31.s64;
	// bl 0x82652828
	sub_82652828(ctx, base);
	// b 0x82652f80
	goto loc_82652F80;
loc_82652F34:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r9,4
	ctx.r9.s64 = 4;
	// beq cr6,0x82652f44
	if (cr6.eq) goto loc_82652F44;
	// li r9,6
	ctx.r9.s64 = 6;
loc_82652F44:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// li r11,4
	r11.s64 = 4;
	// beq cr6,0x82652f54
	if (cr6.eq) goto loc_82652F54;
	// li r11,6
	r11.s64 = 6;
loc_82652F54:
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// li r30,1
	r30.s64 = 1;
	// addi r9,r11,-7
	ctx.r9.s64 = r11.s64 + -7;
	// subfic r11,r31,64
	xer.ca = r31.u32 <= 64;
	r11.s64 = 64 - r31.s64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// slw r11,r30,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// bl 0x82652a48
	sub_82652A48(ctx, base);
loc_82652F80:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82652F98"))) PPC_WEAK_FUNC(sub_82652F98);
PPC_FUNC_IMPL(__imp__sub_82652F98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf8
	// li r29,8
	r29.s64 = 8;
loc_82652FA4:
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// lhz r9,2(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r7,4(r5)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r5.u32 + 4);
	// extsh r30,r9
	r30.s64 = ctx.r9.s16;
	// lhz r28,6(r5)
	r28.u64 = PPC_LOAD_U16(ctx.r5.u32 + 6);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r31,1(r4)
	r31.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// lbz r8,2(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// extsh r9,r28
	ctx.r9.s64 = r28.s16;
	// lbz r10,3(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x82652ffc
	if (!cr6.gt) goto loc_82652FFC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82652ffc
	if (cr6.lt) goto loc_82652FFC;
	// li r11,255
	r11.s64 = 255;
loc_82652FFC:
	// cmplwi cr6,r31,255
	cr6.compare<uint32_t>(r31.u32, 255, xer);
	// ble cr6,0x82653014
	if (!cr6.gt) goto loc_82653014;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// li r31,0
	r31.s64 = 0;
	// blt cr6,0x82653014
	if (cr6.lt) goto loc_82653014;
	// li r31,255
	r31.s64 = 255;
loc_82653014:
	// cmplwi cr6,r8,255
	cr6.compare<uint32_t>(ctx.r8.u32, 255, xer);
	// ble cr6,0x8265302c
	if (!cr6.gt) goto loc_8265302C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// li r8,0
	ctx.r8.s64 = 0;
	// blt cr6,0x8265302c
	if (cr6.lt) goto loc_8265302C;
	// li r8,255
	ctx.r8.s64 = 255;
loc_8265302C:
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x82653044
	if (!cr6.gt) goto loc_82653044;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// blt cr6,0x82653044
	if (cr6.lt) goto loc_82653044;
	// li r10,255
	ctx.r10.s64 = 255;
loc_82653044:
	// stb r10,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r10.u8);
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// stb r31,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, r31.u8);
	// stb r8,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r8.u8);
	// lhz r10,8(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 8);
	// lbz r11,4(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// lhz r9,10(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 10);
	// lhz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 12);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lhz r10,14(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 14);
	// extsh r31,r9
	r31.s64 = ctx.r9.s16;
	// lbz r7,5(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// lbz r9,6(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// lbz r11,7(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 7);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r30,255
	cr6.compare<uint32_t>(r30.u32, 255, xer);
	// ble cr6,0x826530ac
	if (!cr6.gt) goto loc_826530AC;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r30,0
	r30.s64 = 0;
	// blt cr6,0x826530ac
	if (cr6.lt) goto loc_826530AC;
	// li r30,255
	r30.s64 = 255;
loc_826530AC:
	// cmplwi cr6,r7,255
	cr6.compare<uint32_t>(ctx.r7.u32, 255, xer);
	// ble cr6,0x826530c4
	if (!cr6.gt) goto loc_826530C4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// li r7,0
	ctx.r7.s64 = 0;
	// blt cr6,0x826530c4
	if (cr6.lt) goto loc_826530C4;
	// li r7,255
	ctx.r7.s64 = 255;
loc_826530C4:
	// cmplwi cr6,r9,255
	cr6.compare<uint32_t>(ctx.r9.u32, 255, xer);
	// ble cr6,0x826530dc
	if (!cr6.gt) goto loc_826530DC;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// blt cr6,0x826530dc
	if (cr6.lt) goto loc_826530DC;
	// li r9,255
	ctx.r9.s64 = 255;
loc_826530DC:
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x826530f4
	if (!cr6.gt) goto loc_826530F4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x826530f4
	if (cr6.lt) goto loc_826530F4;
	// li r11,255
	r11.s64 = 255;
loc_826530F4:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// stb r30,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, r30.u8);
	// stb r7,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r7.u8);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// stb r9,6(r3)
	PPC_STORE_U8(ctx.r3.u32 + 6, ctx.r9.u8);
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// stb r11,7(r3)
	PPC_STORE_U8(ctx.r3.u32 + 7, r11.u8);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// bne cr6,0x82652fa4
	if (!cr6.eq) goto loc_82652FA4;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_82653120"))) PPC_WEAK_FUNC(sub_82653120);
PPC_FUNC_IMPL(__imp__sub_82653120) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r4,8
	ctx.r4.s64 = 8;
loc_82653124:
	// lhz r11,0(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// lhz r10,2(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
	// lhz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 4);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r8,6(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 6);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// extsh r8,r8
	ctx.r8.s64 = ctx.r8.s16;
	// addi r7,r10,128
	ctx.r7.s64 = ctx.r10.s64 + 128;
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// addi r10,r8,128
	ctx.r10.s64 = ctx.r8.s64 + 128;
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x8265316c
	if (!cr6.gt) goto loc_8265316C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x8265316c
	if (cr6.lt) goto loc_8265316C;
	// li r11,255
	r11.s64 = 255;
loc_8265316C:
	// cmplwi cr6,r7,255
	cr6.compare<uint32_t>(ctx.r7.u32, 255, xer);
	// ble cr6,0x82653184
	if (!cr6.gt) goto loc_82653184;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// li r7,0
	ctx.r7.s64 = 0;
	// blt cr6,0x82653184
	if (cr6.lt) goto loc_82653184;
	// li r7,255
	ctx.r7.s64 = 255;
loc_82653184:
	// cmplwi cr6,r9,255
	cr6.compare<uint32_t>(ctx.r9.u32, 255, xer);
	// ble cr6,0x8265319c
	if (!cr6.gt) goto loc_8265319C;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// blt cr6,0x8265319c
	if (cr6.lt) goto loc_8265319C;
	// li r9,255
	ctx.r9.s64 = 255;
loc_8265319C:
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x826531b4
	if (!cr6.gt) goto loc_826531B4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// blt cr6,0x826531b4
	if (cr6.lt) goto loc_826531B4;
	// li r10,255
	ctx.r10.s64 = 255;
loc_826531B4:
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// stb r10,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r10.u8);
	// stb r7,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r7.u8);
	// stb r9,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r9.u8);
	// lhz r11,8(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 8);
	// lhz r10,10(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 10);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// lhz r9,12(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 12);
	// lhz r7,14(r5)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r5.u32 + 14);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// addi r8,r11,128
	ctx.r8.s64 = r11.s64 + 128;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// addi r7,r10,128
	ctx.r7.s64 = ctx.r10.s64 + 128;
	// addi r10,r11,128
	ctx.r10.s64 = r11.s64 + 128;
	// addi r11,r9,128
	r11.s64 = ctx.r9.s64 + 128;
	// cmplwi cr6,r8,255
	cr6.compare<uint32_t>(ctx.r8.u32, 255, xer);
	// ble cr6,0x8265320c
	if (!cr6.gt) goto loc_8265320C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// li r8,0
	ctx.r8.s64 = 0;
	// blt cr6,0x8265320c
	if (cr6.lt) goto loc_8265320C;
	// li r8,255
	ctx.r8.s64 = 255;
loc_8265320C:
	// cmplwi cr6,r7,255
	cr6.compare<uint32_t>(ctx.r7.u32, 255, xer);
	// ble cr6,0x82653224
	if (!cr6.gt) goto loc_82653224;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// li r7,0
	ctx.r7.s64 = 0;
	// blt cr6,0x82653224
	if (cr6.lt) goto loc_82653224;
	// li r7,255
	ctx.r7.s64 = 255;
loc_82653224:
	// cmplwi cr6,r10,255
	cr6.compare<uint32_t>(ctx.r10.u32, 255, xer);
	// ble cr6,0x8265323c
	if (!cr6.gt) goto loc_8265323C;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,0
	ctx.r10.s64 = 0;
	// blt cr6,0x8265323c
	if (cr6.lt) goto loc_8265323C;
	// li r10,255
	ctx.r10.s64 = 255;
loc_8265323C:
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x82653254
	if (!cr6.gt) goto loc_82653254;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,0
	r11.s64 = 0;
	// blt cr6,0x82653254
	if (cr6.lt) goto loc_82653254;
	// li r11,255
	r11.s64 = 255;
loc_82653254:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// stb r8,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r8.u8);
	// stb r7,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r7.u8);
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// stb r10,6(r3)
	PPC_STORE_U8(ctx.r3.u32 + 6, ctx.r10.u8);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stb r11,7(r3)
	PPC_STORE_U8(ctx.r3.u32 + 7, r11.u8);
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// bne cr6,0x82653124
	if (!cr6.eq) goto loc_82653124;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265327C"))) PPC_WEAK_FUNC(sub_8265327C);
PPC_FUNC_IMPL(__imp__sub_8265327C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82653280"))) PPC_WEAK_FUNC(sub_82653280);
PPC_FUNC_IMPL(__imp__sub_82653280) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// addi r24,r24,8
	r24.s64 = r24.s64 + 8;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// addi r6,r25,8
	ctx.r6.s64 = r25.s64 + 8;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// rlwinm r10,r31,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r11,r30,3,0,28
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r24,r10,r24
	r24.u64 = ctx.r10.u64 + r24.u64;
	// add r25,r11,r25
	r25.u64 = r11.u64 + r25.u64;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// addi r6,r25,8
	ctx.r6.s64 = r25.s64 + 8;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r4,r24,-8
	ctx.r4.s64 = r24.s64 + -8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x826520c0
	sub_826520C0(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82653338"))) PPC_WEAK_FUNC(sub_82653338);
PPC_FUNC_IMPL(__imp__sub_82653338) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// stw r8,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r8.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cntlzw r11,r15
	r11.u64 = r15.u32 == 0 ? 32 : __builtin_clz(r15.u32);
	// mr r18,r5
	r18.u64 = ctx.r5.u64;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// mr r16,r6
	r16.u64 = ctx.r6.u64;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// lwz r8,1780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 1780);
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// add r10,r11,r4
	ctx.r10.u64 = r11.u64 + ctx.r4.u64;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// rlwinm r17,r11,3,0,28
	r17.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r21,r11,1,0,30
	r21.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r17,1,0,30
	r20.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r14,r11,r8
	r14.u64 = r11.u64 + ctx.r8.u64;
	// mullw r11,r21,r10
	r11.s64 = int64_t(r21.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r24,r11,r9
	r24.u64 = r11.u64 + ctx.r9.u64;
	// bne cr6,0x826534c0
	if (!cr6.eq) goto loc_826534C0;
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x82653440
	if (!cr6.gt) goto loc_82653440;
	// rlwinm r11,r21,1,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// li r30,0
	r30.s64 = 0;
	// mr r29,r24
	r29.u64 = r24.u64;
	// add r27,r11,r24
	r27.u64 = r11.u64 + r24.u64;
loc_826533C0:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x82653428
	if (cr6.eq) goto loc_82653428;
	// lhz r11,0(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826533f8
	if (!cr6.eq) goto loc_826533F8;
	// lhz r11,-2(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826533f8
	if (!cr6.eq) goto loc_826533F8;
	// lwz r11,2980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2980);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// lwz r10,3200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3200);
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826533F8:
	// lhz r11,0(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 0);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x82653428
	if (!cr6.eq) goto loc_82653428;
	// lhz r11,-2(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x82653428
	if (!cr6.eq) goto loc_82653428;
	// lwz r11,2988(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2988);
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// lwz r10,3200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3200);
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82653428:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r27,r27,2
	r27.s64 = r27.s64 + 2;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpw cr6,r26,r21
	cr6.compare<int32_t>(r26.s32, r21.s32, xer);
	// blt cr6,0x826533c0
	if (cr6.lt) goto loc_826533C0;
loc_82653440:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826534c0
	if (!cr6.gt) goto loc_826534C0;
	// li r30,0
	r30.s64 = 0;
	// mr r29,r14
	r29.u64 = r14.u64;
loc_82653458:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826534a8
	if (cr6.eq) goto loc_826534A8;
	// lhz r11,0(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 0);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826534a8
	if (!cr6.eq) goto loc_826534A8;
	// lhz r11,-2(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + -2);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// bne cr6,0x826534a8
	if (!cr6.eq) goto loc_826534A8;
	// lwz r11,2992(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// lwz r11,3200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3200);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,3000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// lwz r10,3200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 3200);
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826534A8:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// blt cr6,0x82653458
	if (cr6.lt) goto loc_82653458;
loc_826534C0:
	// li r26,0
	r26.s64 = 0;
	// cmpwi cr6,r21,0
	cr6.compare<int32_t>(r21.s32, 0, xer);
	// ble cr6,0x826535f4
	if (!cr6.gt) goto loc_826535F4;
	// rlwinm r11,r21,1,0,30
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// cntlzw r10,r15
	ctx.r10.u64 = r15.u32 == 0 ? 32 : __builtin_clz(r15.u32);
	// li r30,0
	r30.s64 = 0;
	// rlwinm r19,r10,27,31,31
	r19.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// mr r25,r18
	r25.u64 = r18.u64;
	// add r23,r11,r24
	r23.u64 = r11.u64 + r24.u64;
	// subf r22,r11,r24
	r22.s64 = r24.s64 - r11.s64;
loc_826534E8:
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82653504
	if (!cr6.eq) goto loc_82653504;
	// lhz r11,0(r22)
	r11.u64 = PPC_LOAD_U16(r22.u32 + 0);
	// li r8,1
	ctx.r8.s64 = 1;
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// beq cr6,0x82653508
	if (cr6.eq) goto loc_82653508;
loc_82653504:
	// li r8,0
	ctx.r8.s64 = 0;
loc_82653508:
	// mr r27,r19
	r27.u64 = r19.u64;
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x82653540
	if (cr6.eq) goto loc_82653540;
	// lhz r11,0(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// lhz r9,0(r23)
	ctx.r9.u64 = PPC_LOAD_U16(r23.u32 + 0);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// extsh r11,r9
	r11.s64 = ctx.r9.s16;
	// addi r10,r10,-16384
	ctx.r10.s64 = ctx.r10.s64 + -16384;
	// addi r11,r11,-16384
	r11.s64 = r11.s64 + -16384;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r29,r10,27,31,31
	r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// rlwinm r27,r11,27,31,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_82653540:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x82653550
	if (!cr6.eq) goto loc_82653550;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82653590
	if (cr6.eq) goto loc_82653590;
loc_82653550:
	// lwz r6,3204(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 3204);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,2984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2984);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// lwz r4,2980(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 2980);
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// add r4,r30,r4
	ctx.r4.u64 = r30.u64 + ctx.r4.u64;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82653598
	if (!cr6.eq) goto loc_82653598;
loc_82653590:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x826535d4
	if (cr6.eq) goto loc_826535D4;
loc_82653598:
	// lwz r11,2980(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2980);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// lwz r6,2988(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 2988);
	// add r11,r7,r26
	r11.u64 = ctx.r7.u64 + r26.u64;
	// lwz r29,3204(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3204);
	// add r4,r6,r30
	ctx.r4.u64 = ctx.r6.u64 + r30.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// add r6,r11,r18
	ctx.r6.u64 = r11.u64 + r18.u64;
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826535D4:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r22,r22,2
	r22.s64 = r22.s64 + 2;
	// addi r24,r24,2
	r24.s64 = r24.s64 + 2;
	// addi r23,r23,2
	r23.s64 = r23.s64 + 2;
	// addi r25,r25,8
	r25.s64 = r25.s64 + 8;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpw cr6,r26,r21
	cr6.compare<int32_t>(r26.s32, r21.s32, xer);
	// blt cr6,0x826534e8
	if (cr6.lt) goto loc_826534E8;
loc_826535F4:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r25,0
	r25.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826536ec
	if (!cr6.gt) goto loc_826536EC;
	// li r30,0
	r30.s64 = 0;
	// mr r24,r14
	r24.u64 = r14.u64;
	// subf r26,r28,r16
	r26.s64 = r16.s64 - r28.s64;
loc_82653610:
	// lwz r11,300(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82653638
	if (!cr6.eq) goto loc_82653638;
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// li r27,1
	r27.s64 = 1;
	// subf r11,r11,r25
	r11.s64 = r25.s64 - r11.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r14
	r11.u64 = PPC_LOAD_U16(r11.u32 + r14.u32);
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// beq cr6,0x8265363c
	if (cr6.eq) goto loc_8265363C;
loc_82653638:
	// li r27,0
	r27.s64 = 0;
loc_8265363C:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// bne cr6,0x82653654
	if (!cr6.eq) goto loc_82653654;
	// lhz r11,0(r24)
	r11.u64 = PPC_LOAD_U16(r24.u32 + 0);
	// li r29,1
	r29.s64 = 1;
	// cmplwi cr6,r11,16384
	cr6.compare<uint32_t>(r11.u32, 16384, xer);
	// beq cr6,0x82653658
	if (cr6.eq) goto loc_82653658;
loc_82653654:
	// li r29,0
	r29.s64 = 0;
loc_82653658:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x82653668
	if (!cr6.eq) goto loc_82653668;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x826536d0
	if (cr6.eq) goto loc_826536D0;
loc_82653668:
	// lwz r7,2992(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,2996(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2996);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// add r4,r30,r7
	ctx.r4.u64 = r30.u64 + ctx.r7.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// lwz r11,3204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3204);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// add r6,r28,r26
	ctx.r6.u64 = r28.u64 + r26.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r4,3000(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// lwz r11,3004(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3004);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 208);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r29,3204(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 3204);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// add r4,r30,r4
	ctx.r4.u64 = r30.u64 + ctx.r4.u64;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// mtctr r29
	ctr.u64 = r29.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_826536D0:
	// lwz r11,136(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 136);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r24,r24,2
	r24.s64 = r24.s64 + 2;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x82653610
	if (cr6.lt) goto loc_82653610;
loc_826536EC:
	// lwz r10,2988(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 2988);
	// lwz r11,2984(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2984);
	// lwz r9,2996(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 2996);
	// lwz r8,3004(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 3004);
	// stw r10,2984(r31)
	PPC_STORE_U32(r31.u32 + 2984, ctx.r10.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,2992(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 2992);
	// stw r9,2992(r31)
	PPC_STORE_U32(r31.u32 + 2992, ctx.r9.u32);
	// stw r10,2988(r31)
	PPC_STORE_U32(r31.u32 + 2988, ctx.r10.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,3000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3000);
	// stw r8,3000(r31)
	PPC_STORE_U32(r31.u32 + 3000, ctx.r8.u32);
	// stw r10,2996(r31)
	PPC_STORE_U32(r31.u32 + 2996, ctx.r10.u32);
	// stw r11,3004(r31)
	PPC_STORE_U32(r31.u32 + 3004, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8265372C"))) PPC_WEAK_FUNC(sub_8265372C);
PPC_FUNC_IMPL(__imp__sub_8265372C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82653730"))) PPC_WEAK_FUNC(sub_82653730);
PPC_FUNC_IMPL(__imp__sub_82653730) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcf4
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x8265379c
	if (!cr6.gt) goto loc_8265379C;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
loc_82653744:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82653784
	if (!cr6.gt) goto loc_82653784;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// subf r29,r5,r3
	r29.s64 = ctx.r3.s64 - ctx.r5.s64;
	// subf r28,r5,r7
	r28.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_8265375C:
	// lbzx r31,r29,r11
	r31.u64 = PPC_LOAD_U8(r29.u32 + r11.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// srawi r31,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	r31.s64 = r31.s32 >> 1;
	// stbx r31,r28,r11
	PPC_STORE_U8(r28.u32 + r11.u32, r31.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x8265375c
	if (!cr6.eq) goto loc_8265375C;
loc_82653784:
	// addi r27,r27,-1
	r27.s64 = r27.s64 + -1;
	// add r3,r3,r4
	ctx.r3.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82653744
	if (!cr6.eq) goto loc_82653744;
loc_8265379C:
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_826537A0"))) PPC_WEAK_FUNC(sub_826537A0);
PPC_FUNC_IMPL(__imp__sub_826537A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bcfc
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// lwz r6,140(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r31,14776(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14776);
	// lwz r29,3392(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3392);
	// rlwinm r6,r6,6,0,25
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// addi r30,r11,-4
	r30.s64 = r11.s64 + -4;
	// mullw r11,r31,r29
	r11.s64 = int64_t(r31.s32) * int64_t(r29.s32);
	// addi r29,r6,-4
	r29.s64 = ctx.r6.s64 + -4;
	// addi r6,r11,-256
	ctx.r6.s64 = r11.s64 + -256;
	// mullw r31,r11,r4
	r31.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// mullw r4,r6,r4
	ctx.r4.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r4.s32);
	// beq cr6,0x82653824
	if (cr6.eq) goto loc_82653824;
	// addi r31,r31,255
	r31.s64 = r31.s64 + 255;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// srawi r31,r31,9
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1FF) != 0);
	r31.s64 = r31.s32 >> 9;
	// srawi r11,r11,9
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FF) != 0);
	r11.s64 = r11.s32 >> 9;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r31,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r31.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// mullw r11,r6,r5
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// addi r6,r4,255
	ctx.r6.s64 = ctx.r4.s64 + 255;
	// addi r11,r11,255
	r11.s64 = r11.s64 + 255;
	// srawi r6,r6,9
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1FF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 9;
	// srawi r11,r11,9
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FF) != 0);
	r11.s64 = r11.s32 >> 9;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x82653850
	goto loc_82653850;
loc_82653824:
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r31,r31,8
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0xFF) != 0);
	r31.s64 = r31.s32 >> 8;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// stw r31,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r31.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// mullw r11,r6,r5
	r11.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r5.s32);
	// addi r6,r4,128
	ctx.r6.s64 = ctx.r4.s64 + 128;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// srawi r6,r6,8
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xFF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 8;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
loc_82653850:
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stw r6,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r6.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,19976(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82653918
	if (!cr6.eq) goto loc_82653918;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r11,r7,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r5,r8,6,0,25
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r8,r6,r11
	ctx.r8.u64 = ctx.r6.u64 + r11.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// cmpwi cr6,r8,-60
	cr6.compare<int32_t>(ctx.r8.s32, -60, xer);
	// bge cr6,0x82653894
	if (!cr6.lt) goto loc_82653894;
	// subfic r8,r11,-60
	xer.ca = r11.u32 <= 4294967236;
	ctx.r8.s64 = -60 - r11.s64;
	// b 0x826538a0
	goto loc_826538A0;
loc_82653894:
	// cmpw cr6,r8,r30
	cr6.compare<int32_t>(ctx.r8.s32, r30.s32, xer);
	// ble cr6,0x826538a4
	if (!cr6.gt) goto loc_826538A4;
	// subf r8,r11,r30
	ctx.r8.s64 = r30.s64 - r11.s64;
loc_826538A0:
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
loc_826538A4:
	// cmpwi cr6,r7,-60
	cr6.compare<int32_t>(ctx.r7.s32, -60, xer);
	// bge cr6,0x826538b4
	if (!cr6.lt) goto loc_826538B4;
	// subfic r9,r5,-60
	xer.ca = ctx.r5.u32 <= 4294967236;
	ctx.r9.s64 = -60 - ctx.r5.s64;
	// b 0x826538c0
	goto loc_826538C0;
loc_826538B4:
	// cmpw cr6,r7,r29
	cr6.compare<int32_t>(ctx.r7.s32, r29.s32, xer);
	// ble cr6,0x826538c4
	if (!cr6.gt) goto loc_826538C4;
	// subf r9,r5,r29
	ctx.r9.s64 = r29.s64 - ctx.r5.s64;
loc_826538C0:
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
loc_826538C4:
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmpwi cr6,r10,-60
	cr6.compare<int32_t>(ctx.r10.s32, -60, xer);
	// bge cr6,0x826538e4
	if (!cr6.lt) goto loc_826538E4;
	// subfic r11,r11,-60
	xer.ca = r11.u32 <= 4294967236;
	r11.s64 = -60 - r11.s64;
	// b 0x826538f0
	goto loc_826538F0;
loc_826538E4:
	// cmpw cr6,r10,r30
	cr6.compare<int32_t>(ctx.r10.s32, r30.s32, xer);
	// ble cr6,0x826538f4
	if (!cr6.gt) goto loc_826538F4;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
loc_826538F0:
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
loc_826538F4:
	// cmpwi cr6,r9,-60
	cr6.compare<int32_t>(ctx.r9.s32, -60, xer);
	// bge cr6,0x82653908
	if (!cr6.lt) goto loc_82653908;
	// subfic r11,r5,-60
	xer.ca = ctx.r5.u32 <= 4294967236;
	r11.s64 = -60 - ctx.r5.s64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// b 0x8239bd4c
	return;
loc_82653908:
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// ble cr6,0x82653918
	if (!cr6.gt) goto loc_82653918;
	// subf r11,r5,r29
	r11.s64 = r29.s64 - ctx.r5.s64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82653918:
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8265391C"))) PPC_WEAK_FUNC(sub_8265391C);
PPC_FUNC_IMPL(__imp__sub_8265391C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82653920"))) PPC_WEAK_FUNC(sub_82653920);
PPC_FUNC_IMPL(__imp__sub_82653920) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// rlwinm r9,r11,2,28,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xC;
	// addi r10,r10,15904
	ctx.r10.s64 = ctx.r10.s64 + 15904;
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm r9,r11,2,28,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// lwz r11,1792(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1792);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8265398c
	if (cr6.eq) goto loc_8265398C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82653984
	if (!cr6.gt) goto loc_82653984;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x82653988
	goto loc_82653988;
loc_82653984:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82653988:
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
loc_8265398C:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x826539b0
	if (!cr6.gt) goto loc_826539B0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
loc_826539B0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826539BC"))) PPC_WEAK_FUNC(sub_826539BC);
PPC_FUNC_IMPL(__imp__sub_826539BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826539C0"))) PPC_WEAK_FUNC(sub_826539C0);
PPC_FUNC_IMPL(__imp__sub_826539C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,2
	r30.s64 = 2;
	// addi r29,r31,15920
	r29.s64 = r31.s64 + 15920;
loc_826539D8:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8266cea8
	sub_8266CEA8(ctx, base);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r29,r29,1888
	r29.s64 = r29.s64 + 1888;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826539d8
	if (!cr6.eq) goto loc_826539D8;
	// lis r11,-32126
	r11.s64 = -2105409536;
	// lwz r11,-25068(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -25068);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82653a1c
	if (!cr6.eq) goto loc_82653A1C;
	// lis r10,-32155
	ctx.r10.s64 = -2107310080;
	// lis r11,-32155
	r11.s64 = -2107310080;
	// addi r10,r10,11912
	ctx.r10.s64 = ctx.r10.s64 + 11912;
	// addi r11,r11,12184
	r11.s64 = r11.s64 + 12184;
	// stw r10,3140(r31)
	PPC_STORE_U32(r31.u32 + 3140, ctx.r10.u32);
	// stw r11,3148(r31)
	PPC_STORE_U32(r31.u32 + 3148, r11.u32);
loc_82653A1C:
	// lis r6,-32155
	ctx.r6.s64 = -2107310080;
	// lis r7,-32155
	ctx.r7.s64 = -2107310080;
	// lis r8,-32155
	ctx.r8.s64 = -2107310080;
	// lis r9,-32155
	ctx.r9.s64 = -2107310080;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r6,r6,8640
	ctx.r6.s64 = ctx.r6.s64 + 8640;
	// addi r7,r7,12184
	ctx.r7.s64 = ctx.r7.s64 + 12184;
	// addi r8,r8,12576
	ctx.r8.s64 = ctx.r8.s64 + 12576;
	// addi r9,r9,14128
	ctx.r9.s64 = ctx.r9.s64 + 14128;
	// addi r10,r10,4824
	ctx.r10.s64 = ctx.r10.s64 + 4824;
	// addi r11,r11,5456
	r11.s64 = r11.s64 + 5456;
	// stw r6,3144(r31)
	PPC_STORE_U32(r31.u32 + 3144, ctx.r6.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r7,3148(r31)
	PPC_STORE_U32(r31.u32 + 3148, ctx.r7.u32);
	// stw r8,3152(r31)
	PPC_STORE_U32(r31.u32 + 3152, ctx.r8.u32);
	// stw r9,3208(r31)
	PPC_STORE_U32(r31.u32 + 3208, ctx.r9.u32);
	// stw r10,3200(r31)
	PPC_STORE_U32(r31.u32 + 3200, ctx.r10.u32);
	// stw r11,3204(r31)
	PPC_STORE_U32(r31.u32 + 3204, r11.u32);
	// bl 0x826751a8
	sub_826751A8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82653A74"))) PPC_WEAK_FUNC(sub_82653A74);
PPC_FUNC_IMPL(__imp__sub_82653A74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82653A78"))) PPC_WEAK_FUNC(sub_82653A78);
PPC_FUNC_IMPL(__imp__sub_82653A78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r24,156(r27)
	r24.u64 = PPC_LOAD_U32(r27.u32 + 156);
	// lwz r23,160(r27)
	r23.u64 = PPC_LOAD_U32(r27.u32 + 160);
	// bl 0x826035b8
	sub_826035B8(ctx, base);
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r25,1
	r25.s64 = 1;
	// li r26,0
	r26.s64 = 0;
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82653b10
	if (!cr6.lt) goto loc_82653B10;
loc_82653AB8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653b10
	if (cr6.eq) goto loc_82653B10;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653b00
	if (!cr0.lt) goto loc_82653B00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653B00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653ab8
	if (cr6.gt) goto loc_82653AB8;
loc_82653B10:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653b4c
	if (!cr0.lt) goto loc_82653B4C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653B4C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x82653c44
	if (cr6.eq) goto loc_82653C44;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,4
	r30.s64 = 4;
	// stw r25,21216(r27)
	PPC_STORE_U32(r27.u32 + 21216, r25.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// stw r25,21212(r27)
	PPC_STORE_U32(r27.u32 + 21212, r25.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bge cr6,0x82653bd0
	if (!cr6.lt) goto loc_82653BD0;
loc_82653B78:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653bd0
	if (cr6.eq) goto loc_82653BD0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653bc0
	if (!cr0.lt) goto loc_82653BC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653BC0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653b78
	if (cr6.gt) goto loc_82653B78;
loc_82653BD0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653c0c
	if (!cr0.lt) goto loc_82653C0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653C0C:
	// cmpwi cr6,r30,8
	cr6.compare<int32_t>(r30.s32, 8, xer);
	// bgt cr6,0x82653c20
	if (cr6.gt) goto loc_82653C20;
	// stw r30,21224(r27)
	PPC_STORE_U32(r27.u32 + 21224, r30.u32);
	// stw r30,21220(r27)
	PPC_STORE_U32(r27.u32 + 21220, r30.u32);
	// b 0x82653c4c
	goto loc_82653C4C;
loc_82653C20:
	// addi r10,r30,-8
	ctx.r10.s64 = r30.s64 + -8;
	// addi r11,r10,2
	r11.s64 = ctx.r10.s64 + 2;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// stw r10,21220(r27)
	PPC_STORE_U32(r27.u32 + 21220, ctx.r10.u32);
	// stw r11,21224(r27)
	PPC_STORE_U32(r27.u32 + 21224, r11.u32);
	// blt cr6,0x82653c3c
	if (cr6.lt) goto loc_82653C3C;
	// li r11,8
	r11.s64 = 8;
loc_82653C3C:
	// stw r11,21224(r27)
	PPC_STORE_U32(r27.u32 + 21224, r11.u32);
	// b 0x82653c4c
	goto loc_82653C4C;
loc_82653C44:
	// stw r26,21216(r27)
	PPC_STORE_U32(r27.u32 + 21216, r26.u32);
	// stw r26,21212(r27)
	PPC_STORE_U32(r27.u32 + 21212, r26.u32);
loc_82653C4C:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bge cr6,0x82653cc0
	if (!cr6.lt) goto loc_82653CC0;
loc_82653C68:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653cc0
	if (cr6.eq) goto loc_82653CC0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653cb0
	if (!cr0.lt) goto loc_82653CB0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653CB0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653c68
	if (cr6.gt) goto loc_82653C68;
loc_82653CC0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653cfc
	if (!cr0.lt) goto loc_82653CFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653CFC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82653d18
	if (!cr6.eq) goto loc_82653D18;
	// lwz r11,21352(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 21352);
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21356);
	// stw r11,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r11.u32);
	// stw r10,160(r27)
	PPC_STORE_U32(r27.u32 + 160, ctx.r10.u32);
	// b 0x82654044
	goto loc_82654044;
loc_82653D18:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82653d8c
	if (!cr6.lt) goto loc_82653D8C;
loc_82653D34:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653d8c
	if (cr6.eq) goto loc_82653D8C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653d7c
	if (!cr0.lt) goto loc_82653D7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653D7C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653d34
	if (cr6.gt) goto loc_82653D34;
loc_82653D8C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	r28.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653dc8
	if (!cr0.lt) goto loc_82653DC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653DC8:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,2
	r30.s64 = 2;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82653e3c
	if (!cr6.lt) goto loc_82653E3C;
loc_82653DE4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653e3c
	if (cr6.eq) goto loc_82653E3C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653e2c
	if (!cr0.lt) goto loc_82653E2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653E2C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653de4
	if (cr6.gt) goto loc_82653DE4;
loc_82653E3C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653e78
	if (!cr0.lt) goto loc_82653E78;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653E78:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82654024
	if (!cr6.eq) goto loc_82654024;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82654024
	if (!cr6.eq) goto loc_82654024;
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// li r30,12
	r30.s64 = 12;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x82653efc
	if (!cr6.lt) goto loc_82653EFC;
loc_82653EA4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653efc
	if (cr6.eq) goto loc_82653EFC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653eec
	if (!cr0.lt) goto loc_82653EEC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653EEC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653ea4
	if (cr6.gt) goto loc_82653EA4;
loc_82653EFC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653f38
	if (!cr0.lt) goto loc_82653F38;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653F38:
	// lwz r31,84(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 84);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// li r30,12
	r30.s64 = 12;
	// rlwinm r28,r11,1,0,30
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bge cr6,0x82653fb4
	if (!cr6.lt) goto loc_82653FB4;
loc_82653F5C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82653fb4
	if (cr6.eq) goto loc_82653FB4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82653fa4
	if (!cr0.lt) goto loc_82653FA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653FA4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82653f5c
	if (cr6.gt) goto loc_82653F5C;
loc_82653FB4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82653ff0
	if (!cr0.lt) goto loc_82653FF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82653FF0:
	// lwz r10,21352(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21352);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// bgt cr6,0x82654018
	if (cr6.gt) goto loc_82654018;
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 21356);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x82654018
	if (cr6.gt) goto loc_82654018;
	// stw r28,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r28.u32);
	// b 0x82654040
	goto loc_82654040;
loc_82654018:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82654024:
	// addi r11,r28,5400
	r11.s64 = r28.s64 + 5400;
	// addi r10,r30,5404
	ctx.r10.s64 = r30.s64 + 5404;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// stw r11,156(r27)
	PPC_STORE_U32(r27.u32 + 156, r11.u32);
	// lwzx r11,r10,r27
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r27.u32);
loc_82654040:
	// stw r11,160(r27)
	PPC_STORE_U32(r27.u32 + 160, r11.u32);
loc_82654044:
	// lwz r4,156(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + 156);
	// cmpw cr6,r4,r24
	cr6.compare<int32_t>(ctx.r4.s32, r24.s32, xer);
	// bne cr6,0x82654064
	if (!cr6.eq) goto loc_82654064;
	// lwz r11,160(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 160);
	// cmpw cr6,r11,r23
	cr6.compare<int32_t>(r11.s32, r23.s32, xer);
	// bne cr6,0x82654064
	if (!cr6.eq) goto loc_82654064;
	// stw r26,21184(r27)
	PPC_STORE_U32(r27.u32 + 21184, r26.u32);
	// b 0x82654068
	goto loc_82654068;
loc_82654064:
	// stw r25,21184(r27)
	PPC_STORE_U32(r27.u32 + 21184, r25.u32);
loc_82654068:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r5,160(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 160);
	// bl 0x82606a70
	sub_82606A70(ctx, base);
	// stw r26,21360(r27)
	PPC_STORE_U32(r27.u32 + 21360, r26.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82654080"))) PPC_WEAK_FUNC(sub_82654080);
PPC_FUNC_IMPL(__imp__sub_82654080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lwz r9,3356(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// lwz r11,212(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x82654098
	if (!cr6.eq) goto loc_82654098;
	// stw r11,15224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15224, r11.u32);
	// blr 
	return;
loc_82654098:
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// bne cr6,0x826540c4
	if (!cr6.eq) goto loc_826540C4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r11,15228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15228, r11.u32);
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,15224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15224, ctx.r10.u32);
	// blr 
	return;
loc_826540C4:
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r11,15232(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15232, r11.u32);
	// stw r11,15228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15228, r11.u32);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// stw r11,15224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15224, r11.u32);
	// beq cr6,0x8265411c
	if (cr6.eq) goto loc_8265411C;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// beq cr6,0x82654110
	if (cr6.eq) goto loc_82654110;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// bne cr6,0x82654124
	if (!cr6.eq) goto loc_82654124;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,15228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15228, r11.u32);
	// stw r11,15232(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15232, r11.u32);
	// b 0x82654120
	goto loc_82654120;
loc_82654110:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,15228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15228, r11.u32);
	// b 0x82654120
	goto loc_82654120;
loc_8265411C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82654120:
	// stw r11,15224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15224, r11.u32);
loc_82654124:
	// lwz r11,15224(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15224);
	// lwz r10,15228(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15228);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,15232(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15232);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,15224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15224, r11.u32);
	// add r11,r9,r10
	r11.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,15228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15228, ctx.r10.u32);
	// stw r11,15232(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15232, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82654154"))) PPC_WEAK_FUNC(sub_82654154);
PPC_FUNC_IMPL(__imp__sub_82654154) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82654158"))) PPC_WEAK_FUNC(sub_82654158);
PPC_FUNC_IMPL(__imp__sub_82654158) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r31,84(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 84);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r29.u32);
	// extsh r30,r10
	r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654264
	if (cr6.lt) goto loc_82654264;
	// clrlwi r10,r30,28
	ctx.r10.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x8265425c
	if (!cr6.lt) goto loc_8265425C;
loc_826541C4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826541f0
	if (cr6.lt) goto loc_826541F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826541c4
	if (cr6.eq) goto loc_826541C4;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826542a8
	goto loc_826542A8;
loc_826541F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8265425C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x826542a8
	goto loc_826542A8;
loc_82654264:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_82654274:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654274
	if (cr6.lt) goto loc_82654274;
loc_826542A8:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// mr r26,r30
	r26.u64 = r30.u64;
	// cmpwi cr6,r27,8
	cr6.compare<int32_t>(r27.s32, 8, xer);
	// bne cr6,0x826542c8
	if (!cr6.eq) goto loc_826542C8;
	// addi r26,r30,1
	r26.s64 = r30.s64 + 1;
	// cmpwi cr6,r26,37
	cr6.compare<int32_t>(r26.s32, 37, xer);
	// blt cr6,0x826543e8
	if (cr6.lt) goto loc_826543E8;
	// addi r26,r26,-37
	r26.s64 = r26.s64 + -37;
loc_826542C8:
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
loc_826542CC:
	// rlwinm r24,r11,0,30,28
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFB;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x82654730
	if (cr6.eq) goto loc_82654730;
	// cmpwi cr6,r26,35
	cr6.compare<int32_t>(r26.s32, 35, xer);
	// beq cr6,0x82654574
	if (cr6.eq) goto loc_82654574;
	// cmpwi cr6,r26,36
	cr6.compare<int32_t>(r26.s32, 36, xer);
	// beq cr6,0x82654560
	if (cr6.eq) goto loc_82654560;
	// lwz r11,1972(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1972);
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r27,76(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// bl 0x8263a9e0
	sub_8263A9E0(ctx, base);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x82654310
	if (cr6.eq) goto loc_82654310;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq cr6,0x82654314
	if (cr6.eq) goto loc_82654314;
loc_82654310:
	// li r10,0
	ctx.r10.s64 = 0;
loc_82654314:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// rlwinm r28,r3,2,0,29
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r25,r11,-17696
	r25.s64 = r11.s64 + -17696;
	// lwzx r11,r28,r25
	r11.u64 = PPC_LOAD_U32(r28.u32 + r25.u32);
	// subf r30,r10,r11
	r30.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x826543f0
	if (!cr6.gt) goto loc_826543F0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x826543f0
	if (cr6.eq) goto loc_826543F0;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826543a4
	if (!cr6.gt) goto loc_826543A4;
loc_8265434C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826543a4
	if (cr6.eq) goto loc_826543A4;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82654394
	if (!cr0.lt) goto loc_82654394;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654394:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8265434c
	if (cr6.gt) goto loc_8265434C;
loc_826543A4:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826543e0
	if (!cr0.lt) goto loc_826543E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826543E0:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// b 0x826543f4
	goto loc_826543F4;
loc_826543E8:
	// rlwinm r11,r11,0,29,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF7;
	// b 0x826542cc
	goto loc_826542CC;
loc_826543F0:
	// li r10,0
	ctx.r10.s64 = 0;
loc_826543F4:
	// addi r9,r25,24
	ctx.r9.s64 = r25.s64 + 24;
	// clrlwi r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwzx r9,r28,r9
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + ctx.r9.u32);
	// rlwinm r8,r11,16,0,15
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r9,r11,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// rlwinm r10,r10,15,0,16
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0xFFFF8000;
	// xor r11,r10,r9
	r11.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r11,r24,0,16,31
	r11.u64 = (__builtin_rotateleft32(r24.u32, 0) & 0xFFFF) | (r11.u64 & 0xFFFFFFFFFFFF0000);
	// subf r28,r8,r11
	r28.s64 = r11.s64 - ctx.r8.s64;
	// rlwimi r28,r11,0,16,31
	r28.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFF) | (r28.u64 & 0xFFFFFFFFFFFF0000);
	// bl 0x8263a9f8
	sub_8263A9F8(ctx, base);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// beq cr6,0x82654448
	if (cr6.eq) goto loc_82654448;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// li r11,1
	r11.s64 = 1;
	// beq cr6,0x8265444c
	if (cr6.eq) goto loc_8265444C;
loc_82654448:
	// li r11,0
	r11.s64 = 0;
loc_8265444C:
	// rlwinm r27,r3,2,0,29
	r27.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r27,r25
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + r25.u32);
	// subf. r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// ble 0x82654514
	if (!cr0.gt) goto loc_82654514;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x82654514
	if (cr6.eq) goto loc_82654514;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826544d0
	if (!cr6.gt) goto loc_826544D0;
loc_82654478:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826544d0
	if (cr6.eq) goto loc_826544D0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826544c0
	if (!cr0.lt) goto loc_826544C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826544C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82654478
	if (cr6.gt) goto loc_82654478;
loc_826544D0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8265450c
	if (!cr0.lt) goto loc_8265450C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265450C:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// b 0x82654518
	goto loc_82654518;
loc_82654514:
	// li r10,0
	ctx.r10.s64 = 0;
loc_82654518:
	// addi r9,r25,24
	ctx.r9.s64 = r25.s64 + 24;
	// clrlwi r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	// neg r11,r11
	r11.s64 = -r11.s64;
	// lwzx r9,r27,r9
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + ctx.r9.u32);
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// xor r11,r10,r9
	r11.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r11,r28,0,28,15
	r11.u64 = (__builtin_rotateleft32(r28.u32, 0) & 0xFFFFFFFFFFFF000F) | (r11.u64 & 0xFFF0);
	// extsh r10,r11
	ctx.r10.s64 = r11.s16;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// rlwimi r10,r11,0,28,15
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFFFFFF000F) | (ctx.r10.u64 & 0xFFF0);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82654560:
	// andi. r11,r24,11
	r11.u64 = r24.u64 & 11;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ori r11,r11,4
	r11.u64 = r11.u64 | 4;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82654574:
	// lwz r11,1972(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1972);
	// li r29,0
	r29.s64 = 0;
	// lwz r10,408(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 408);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r28,76(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// subf r30,r28,r10
	r30.s64 = ctx.r10.s64 - r28.s64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826545a0
	if (!cr6.eq) goto loc_826545A0;
	// li r11,0
	r11.s64 = 0;
	// b 0x82654640
	goto loc_82654640;
loc_826545A0:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82654600
	if (!cr6.gt) goto loc_82654600;
loc_826545A8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82654600
	if (cr6.eq) goto loc_82654600;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826545f0
	if (!cr0.lt) goto loc_826545F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826545F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x826545a8
	if (cr6.gt) goto loc_826545A8;
loc_82654600:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8265463c
	if (!cr0.lt) goto loc_8265463C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265463C:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82654640:
	// lwz r9,412(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + 412);
	// rlwimi r24,r11,16,0,15
	r24.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (r24.u64 & 0xFFFFFFFF0000FFFF);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,0
	r29.s64 = 0;
	// subf r30,r28,r9
	r30.s64 = ctx.r9.s64 - r28.s64;
	// mr r28,r24
	r28.u64 = r24.u64;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8265467c
	if (!cr6.eq) goto loc_8265467C;
	// li r11,0
	r11.s64 = 0;
	// rlwimi r28,r11,4,16,27
	r28.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0xFFF0) | (r28.u64 & 0xFFFFFFFFFFFF000F);
	// mr r11,r28
	r11.u64 = r28.u64;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_8265467C:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826546dc
	if (!cr6.gt) goto loc_826546DC;
loc_82654684:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826546dc
	if (cr6.eq) goto loc_826546DC;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r9,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826546cc
	if (!cr0.lt) goto loc_826546CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826546CC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82654684
	if (cr6.gt) goto loc_82654684;
loc_826546DC:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82654718
	if (!cr0.lt) goto loc_82654718;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654718:
	// mr r11,r30
	r11.u64 = r30.u64;
	// rlwimi r28,r11,4,16,27
	r28.u64 = (__builtin_rotateleft32(r11.u32, 4) & 0xFFF0) | (r28.u64 & 0xFFFFFFFFFFFF000F);
	// mr r11,r28
	r11.u64 = r28.u64;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82654730:
	// clrlwi r11,r24,28
	r11.u64 = r24.u32 & 0xF;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82654740"))) PPC_WEAK_FUNC(sub_82654740);
PPC_FUNC_IMPL(__imp__sub_82654740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r28,1772(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// rlwinm r27,r10,1,0,30
	r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r25,1776(r3)
	r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mullw r10,r27,r11
	ctx.r10.s64 = int64_t(r27.s32) * int64_t(r11.s32);
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// li r26,0
	r26.s64 = 0;
	// add r24,r10,r29
	r24.u64 = ctx.r10.u64 + r29.u64;
	// bne cr6,0x826547a8
	if (!cr6.eq) goto loc_826547A8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826547a4
	if (cr6.eq) goto loc_826547A4;
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// lwz r10,21264(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21264);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826547a8
	if (cr6.eq) goto loc_826547A8;
loc_826547A4:
	// li r26,1
	r26.s64 = 1;
loc_826547A8:
	// lwz r10,19980(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19980);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x826547dc
	if (cr6.eq) goto loc_826547DC;
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r7,1776(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1776);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r6,1772(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// bl 0x8261cca8
	sub_8261CCA8(ctx, base);
	// b 0x82654808
	goto loc_82654808;
loc_826547DC:
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x826511e0
	sub_826511E0(ctx, base);
loc_82654808:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8265490c
	if (cr6.eq) goto loc_8265490C;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bne cr6,0x8265490c
	if (!cr6.eq) goto loc_8265490C;
	// subf r11,r27,r24
	r11.s64 = r24.s64 - r27.s64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r10,r24,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r28
	ctx.r5.u64 = ctx.r10.u64 + r28.u64;
	// lhzx r6,r11,r28
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// lhzx r7,r11,r25
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + r25.u32);
	// lhz r11,-2(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + -2);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x8265486c
	if (!cr6.eq) goto loc_8265486C;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// srawi r5,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = r11.s32 >> 31;
	// srawi r4,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 31;
	// xor r11,r11,r5
	r11.u64 = r11.u64 ^ ctx.r5.u64;
	// xor r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r4.u64;
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// b 0x82654898
	goto loc_82654898;
loc_8265486C:
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + r25.u64;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// lhz r10,-2(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// srawi r5,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 31;
	// srawi r4,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r4.s64 = r11.s32 >> 31;
	// xor r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r5.u64;
	// xor r3,r11,r4
	ctx.r3.u64 = r11.u64 ^ ctx.r4.u64;
	// subf r11,r5,r10
	r11.s64 = ctx.r10.s64 - ctx.r5.s64;
	// subf r10,r4,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r4.s64;
loc_82654898:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// ble cr6,0x826548b0
	if (!cr6.gt) goto loc_826548B0;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
loc_826548B0:
	// extsh r11,r6
	r11.s64 = ctx.r6.s16;
	// cmpwi cr6,r11,16384
	cr6.compare<int32_t>(r11.s32, 16384, xer);
	// bne cr6,0x826548d8
	if (!cr6.eq) goto loc_826548D8;
	// srawi r11,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	r11.s64 = ctx.r8.s32 >> 31;
	// srawi r10,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 31;
	// xor r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 ^ r11.u64;
	// xor r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r10.u64;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// b 0x826548fc
	goto loc_826548FC;
loc_826548D8:
	// extsh r10,r7
	ctx.r10.s64 = ctx.r7.s16;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
loc_826548FC:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// bgt cr6,0x82654910
	if (cr6.gt) goto loc_82654910;
loc_8265490C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82654910:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239bd38
	return;
}

__attribute__((alias("__imp__sub_82654918"))) PPC_WEAK_FUNC(sub_82654918);
PPC_FUNC_IMPL(__imp__sub_82654918) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// mr r17,r6
	r17.u64 = ctx.r6.u64;
	// mr r16,r8
	r16.u64 = ctx.r8.u64;
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwz r30,0(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r27,4(r26)
	r27.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// lwz r25,28(r26)
	r25.u64 = PPC_LOAD_U32(r26.u32 + 28);
	// lwz r24,32(r26)
	r24.u64 = PPC_LOAD_U32(r26.u32 + 32);
	// lwz r19,312(r22)
	r19.u64 = PPC_LOAD_U32(r22.u32 + 312);
	// lwz r18,316(r22)
	r18.u64 = PPC_LOAD_U32(r22.u32 + 316);
	// lwz r31,84(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// beq cr6,0x82654b14
	if (cr6.eq) goto loc_82654B14;
	// lbz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654a50
	if (cr6.lt) goto loc_82654A50;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x82654a48
	if (!cr6.lt) goto loc_82654A48;
loc_826549B0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826549dc
	if (cr6.lt) goto loc_826549DC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826549b0
	if (cr6.eq) goto loc_826549B0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82654a94
	goto loc_82654A94;
loc_826549DC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82654A48:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82654a94
	goto loc_82654A94;
loc_82654A50:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_82654A60:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654a60
	if (cr6.lt) goto loc_82654A60;
loc_82654A94:
	// mr r21,r30
	r21.u64 = r30.u64;
	// cmplw cr6,r30,r27
	cr6.compare<uint32_t>(r30.u32, r27.u32, xer);
	// bne cr6,0x82654aac
	if (!cr6.eq) goto loc_82654AAC;
loc_82654AA0:
	// li r3,-1
	ctx.r3.s64 = -1;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
loc_82654AAC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x82654ad8
	if (!cr0.lt) goto loc_82654AD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654AD8:
	// lbzx r11,r30,r25
	r11.u64 = PPC_LOAD_U8(r30.u32 + r25.u32);
	// neg r9,r29
	ctx.r9.s64 = -r29.s64;
	// lbzx r27,r30,r24
	r27.u64 = PPC_LOAD_U8(r30.u32 + r24.u32);
	// cmplw cr6,r30,r23
	cr6.compare<uint32_t>(r30.u32, r23.u32, xer);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// blt cr6,0x82654af8
	if (cr6.lt) goto loc_82654AF8;
	// lwz r10,16(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// b 0x82654afc
	goto loc_82654AFC;
loc_82654AF8:
	// lwz r10,12(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 12);
loc_82654AFC:
	// lbzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + r27.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// xor r11,r11,r9
	r11.u64 = r11.u64 ^ ctx.r9.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
	// b 0x82655128
	goto loc_82655128;
loc_82654B14:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r29,r8,0
	r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x82654b40
	if (!cr0.lt) goto loc_82654B40;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654B40:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82654cf0
	if (cr6.eq) goto loc_82654CF0;
	// lbz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U8(r30.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// lwz r28,0(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654c34
	if (cr6.lt) goto loc_82654C34;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r7,r11,r9
	ctx.r7.s64 = ctx.r9.s64 - r11.s64;
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// bge cr6,0x82654c2c
	if (!cr6.lt) goto loc_82654C2C;
loc_82654B94:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82654bc0
	if (cr6.lt) goto loc_82654BC0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82654b94
	if (cr6.eq) goto loc_82654B94;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82654c78
	goto loc_82654C78;
loc_82654BC0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82654C2C:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x82654c78
	goto loc_82654C78;
loc_82654C34:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_82654C44:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82654c44
	if (cr6.lt) goto loc_82654C44;
loc_82654C78:
	// mr r21,r30
	r21.u64 = r30.u64;
	// cmplw cr6,r30,r27
	cr6.compare<uint32_t>(r30.u32, r27.u32, xer);
	// beq cr6,0x82654aa0
	if (cr6.eq) goto loc_82654AA0;
	// lbzx r9,r30,r25
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + r25.u32);
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// extsb r29,r9
	r29.s64 = ctx.r9.s8;
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lbzx r28,r30,r24
	r28.u64 = PPC_LOAD_U8(r30.u32 + r24.u32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r27,r11,0
	r27.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x82654cbc
	if (!cr0.lt) goto loc_82654CBC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654CBC:
	// lwz r10,1932(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 1932);
	// neg r9,r27
	ctx.r9.s64 = -r27.s64;
	// cmplw cr6,r30,r23
	cr6.compare<uint32_t>(r30.u32, r23.u32, xer);
	// blt cr6,0x82654cd4
	if (cr6.lt) goto loc_82654CD4;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 24);
	// b 0x82654cd8
	goto loc_82654CD8;
loc_82654CD4:
	// lwz r11,20(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 20);
loc_82654CD8:
	// lbzx r11,r11,r29
	r11.u64 = PPC_LOAD_U8(r11.u32 + r29.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r27,r11,r28
	r27.u64 = r11.u64 + r28.u64;
	// xor r11,r29,r9
	r11.u64 = r29.u64 ^ ctx.r9.u64;
	// subf r10,r9,r11
	ctx.r10.s64 = r11.s64 - ctx.r9.s64;
	// b 0x82655128
	goto loc_82655128;
loc_82654CF0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x82654d1c
	if (!cr0.lt) goto loc_82654D1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654D1C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r21,r23
	r21.u64 = r23.u64;
	// bne cr6,0x82654d2c
	if (!cr6.eq) goto loc_82654D2C;
	// li r21,0
	r21.s64 = 0;
loc_82654D2C:
	// lwz r11,15472(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 15472);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// blt cr6,0x82654fc8
	if (cr6.lt) goto loc_82654FC8;
	// lwz r11,1944(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1944);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82654d54
	if (cr6.eq) goto loc_82654D54;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x82649548
	sub_82649548(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,1944(r22)
	PPC_STORE_U32(r22.u32 + 1944, r11.u32);
loc_82654D54:
	// lwz r30,84(r22)
	r30.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// li r28,0
	r28.s64 = 0;
	// lwz r29,1952(r22)
	r29.u64 = PPC_LOAD_U32(r22.u32 + 1952);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82654d78
	if (!cr6.eq) goto loc_82654D78;
	// li r27,0
	r27.s64 = 0;
	// b 0x82654e18
	goto loc_82654E18;
loc_82654D78:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x82654dd8
	if (!cr6.gt) goto loc_82654DD8;
loc_82654D80:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82654dd8
	if (cr6.eq) goto loc_82654DD8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x82654dc8
	if (!cr0.lt) goto loc_82654DC8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654DC8:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x82654d80
	if (cr6.gt) goto loc_82654D80;
loc_82654DD8:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x82654e14
	if (!cr0.lt) goto loc_82654E14;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654E14:
	// mr r27,r29
	r27.u64 = r29.u64;
loc_82654E18:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// bge 0x82654e44
	if (!cr0.lt) goto loc_82654E44;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654E44:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r30,84(r22)
	r30.u64 = PPC_LOAD_U32(r22.u32 + 84);
	// lwz r29,1948(r22)
	r29.u64 = PPC_LOAD_U32(r22.u32 + 1948);
	// li r28,0
	r28.s64 = 0;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x82654f14
	if (cr6.eq) goto loc_82654F14;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82654e70
	if (!cr6.eq) goto loc_82654E70;
	// neg r10,r29
	ctx.r10.s64 = -r29.s64;
	// b 0x82655128
	goto loc_82655128;
loc_82654E70:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x82654ed0
	if (!cr6.gt) goto loc_82654ED0;
loc_82654E78:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82654ed0
	if (cr6.eq) goto loc_82654ED0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x82654ec0
	if (!cr0.lt) goto loc_82654EC0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654EC0:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x82654e78
	if (cr6.gt) goto loc_82654E78;
loc_82654ED0:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x82654f0c
	if (!cr0.lt) goto loc_82654F0C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654F0C:
	// neg r10,r29
	ctx.r10.s64 = -r29.s64;
	// b 0x82655128
	goto loc_82655128;
loc_82654F14:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82654f24
	if (!cr6.eq) goto loc_82654F24;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82655128
	goto loc_82655128;
loc_82654F24:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x82654f84
	if (!cr6.gt) goto loc_82654F84;
loc_82654F2C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82654f84
	if (cr6.eq) goto loc_82654F84;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x82654f74
	if (!cr0.lt) goto loc_82654F74;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654F74:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x82654f2c
	if (cr6.gt) goto loc_82654F2C;
loc_82654F84:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r8.u64);
	// bge 0x82654fc0
	if (!cr0.lt) goto loc_82654FC0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82654FC0:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// b 0x82655128
	goto loc_82655128;
loc_82654FC8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,6
	r30.s64 = 6;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bge cr6,0x82655038
	if (!cr6.lt) goto loc_82655038;
loc_82654FE0:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82655038
	if (cr6.eq) goto loc_82655038;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82655028
	if (!cr0.lt) goto loc_82655028;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655028:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82654fe0
	if (cr6.gt) goto loc_82654FE0;
loc_82655038:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82655074
	if (!cr0.lt) goto loc_82655074;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655074:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r27,r30
	r27.u64 = r30.u64;
	// li r30,8
	r30.s64 = 8;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x826550e8
	if (!cr6.lt) goto loc_826550E8;
loc_82655090:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826550e8
	if (cr6.eq) goto loc_826550E8;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826550d8
	if (!cr0.lt) goto loc_826550D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826550D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82655090
	if (cr6.gt) goto loc_82655090;
loc_826550E8:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82655124
	if (!cr0.lt) goto loc_82655124;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655124:
	// extsb r10,r30
	ctx.r10.s64 = r30.s8;
loc_82655128:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82654aa0
	if (!cr6.eq) goto loc_82654AA0;
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// stw r11,0(r16)
	PPC_STORE_U32(r16.u32 + 0, r11.u32);
	// bge cr6,0x82654aa0
	if (!cr6.lt) goto loc_82654AA0;
	// lbzx r11,r11,r17
	r11.u64 = PPC_LOAD_U8(r11.u32 + r17.u32);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// blt cr6,0x82655168
	if (cr6.lt) goto loc_82655168;
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8265517c
	if (!cr6.eq) goto loc_8265517C;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
loc_82655168:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r11,r20
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r20.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// sthx r10,r11,r20
	PPC_STORE_U16(r11.u32 + r20.u32, ctx.r10.u16);
	// b 0x826551a0
	goto loc_826551A0;
loc_8265517C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r9,1760(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 1760);
	// mullw r10,r10,r19
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r19.s32);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x82655198
	if (!cr6.gt) goto loc_82655198;
	// add r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 + r18.u64;
	// b 0x8265519c
	goto loc_8265519C;
loc_82655198:
	// subf r10,r18,r10
	ctx.r10.s64 = ctx.r10.s64 - r18.s64;
loc_8265519C:
	// stwx r10,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r10.u32);
loc_826551A0:
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r16)
	PPC_STORE_U32(r16.u32 + 0, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239bd18
	return;
}

__attribute__((alias("__imp__sub_826551B8"))) PPC_WEAK_FUNC(sub_826551B8);
PPC_FUNC_IMPL(__imp__sub_826551B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r21,0(r4)
	r21.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// li r28,1
	r28.s64 = 1;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lwz r11,8(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 8);
	// lwz r25,312(r26)
	r25.u64 = PPC_LOAD_U32(r26.u32 + 312);
	// lwz r24,316(r26)
	r24.u64 = PPC_LOAD_U32(r26.u32 + 316);
	// addi r18,r11,1
	r18.s64 = r11.s64 + 1;
	// lwz r31,84(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 84);
	// lwz r16,0(r21)
	r16.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// lwz r20,28(r21)
	r20.u64 = PPC_LOAD_U32(r21.u32 + 28);
	// lwz r19,32(r21)
	r19.u64 = PPC_LOAD_U32(r21.u32 + 32);
	// lwz r17,4(r21)
	r17.u64 = PPC_LOAD_U32(r21.u32 + 4);
	// beq cr6,0x826554c0
	if (cr6.eq) goto loc_826554C0;
	// lis r11,0
	r11.s64 = 0;
	// li r15,64
	r15.s64 = 64;
	// ori r23,r11,32768
	r23.u64 = r11.u64 | 32768;
	// li r14,0
	r14.s64 = 0;
	// b 0x8265521c
	goto loc_8265521C;
loc_82655218:
	// lwz r28,80(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_8265521C:
	// lbz r4,8(r16)
	ctx.r4.u64 = PPC_LOAD_U8(r16.u32 + 8);
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subfic r10,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r16)
	r29.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82655318
	if (cr6.lt) goto loc_82655318;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// sradi r9,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r9.s64 = r11.s64 >> 63;
	// addic. r10,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r10.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicr r11,r11,1,62
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// extsw r29,r9
	r29.s64 = ctx.r9.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// bge 0x82655310
	if (!cr0.lt) goto loc_82655310;
loc_82655278:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826552a4
	if (cr6.lt) goto loc_826552A4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x82655278
	if (cr6.eq) goto loc_82655278;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8265536c
	goto loc_8265536C;
loc_826552A4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_82655310:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8265536c
	goto loc_8265536C;
loc_82655318:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_82655320:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r23
	r11.u64 = r30.u64 + r23.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82655320
	if (cr6.lt) goto loc_82655320;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// sradi r11,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	r11.s64 = r11.s64 >> 63;
	// extsw r29,r11
	r29.s64 = r11.s32;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
loc_8265536C:
	// clrlwi r5,r30,24
	ctx.r5.u64 = r30.u32 & 0xFF;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// lbzx r11,r10,r20
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + r20.u32);
	// lbzx r4,r10,r19
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + r19.u32);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// add r6,r4,r28
	ctx.r6.u64 = ctx.r4.u64 + r28.u64;
	// xor r11,r11,r29
	r11.u64 = r11.u64 ^ r29.u64;
	// subf r9,r29,r11
	ctx.r9.s64 = r11.s64 - r29.s64;
	// lbzx r11,r6,r22
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + r22.u32);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bge cr6,0x826553ac
	if (!cr6.lt) goto loc_826553AC;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r8,r27
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + r27.u32);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// sthx r7,r8,r27
	PPC_STORE_U16(ctx.r8.u32 + r27.u32, ctx.r7.u16);
	// b 0x826553f8
	goto loc_826553F8;
loc_826553AC:
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne cr6,0x826553d4
	if (!cr6.eq) goto loc_826553D4;
	// srawi r8,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	ctx.r8.s64 = r11.s32 >> 3;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r7,r8,r27
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + r27.u32);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// sthx r7,r8,r27
	PPC_STORE_U16(ctx.r8.u32 + r27.u32, ctx.r7.u16);
	// b 0x826553f8
	goto loc_826553F8;
loc_826553D4:
	// lwz r7,1760(r26)
	ctx.r7.u64 = PPC_LOAD_U32(r26.u32 + 1760);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// mullw r8,r9,r25
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x826553f0
	if (!cr6.gt) goto loc_826553F0;
	// add r8,r8,r24
	ctx.r8.u64 = ctx.r8.u64 + r24.u64;
	// b 0x826553f4
	goto loc_826553F4;
loc_826553F0:
	// subf r8,r24,r8
	ctx.r8.s64 = ctx.r8.s64 - r24.s64;
loc_826553F4:
	// stwx r8,r7,r3
	PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, ctx.r8.u32);
loc_826553F8:
	// subfc r8,r18,r10
	xer.ca = ctx.r10.u32 >= r18.u32;
	ctx.r8.s64 = ctx.r10.s64 - r18.s64;
	// subf r7,r10,r17
	ctx.r7.s64 = r17.s64 - ctx.r10.s64;
	// addi r28,r6,1
	r28.s64 = ctx.r6.s64 + 1;
	// subfe r10,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// cntlzw r8,r7
	ctx.r8.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// subfc r7,r15,r28
	xer.ca = r28.u32 >= r15.u32;
	ctx.r7.s64 = r28.s64 - r15.s64;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// subfe r10,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r7.u64 + ctx.r7.u64 + xer.ca;
	xer.ca = temp.u8;
	// or r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 | ctx.r8.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x8265521c
	if (cr6.eq) goto loc_8265521C;
	// clrlwi r10,r5,24
	ctx.r10.u64 = ctx.r5.u32 & 0xFF;
	// cmpw cr6,r10,r17
	cr6.compare<int32_t>(ctx.r10.s32, r17.s32, xer);
	// bne cr6,0x826554c0
	if (!cr6.eq) goto loc_826554C0;
	// subf r10,r4,r28
	ctx.r10.s64 = r28.s64 - ctx.r4.s64;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// bge cr6,0x82655464
	if (!cr6.lt) goto loc_82655464;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r27.u32);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// sthx r10,r11,r27
	PPC_STORE_U16(r11.u32 + r27.u32, ctx.r10.u16);
	// b 0x82655498
	goto loc_82655498;
loc_82655464:
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8265548c
	if (!cr6.eq) goto loc_8265548C;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r27.u32);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// sthx r10,r11,r27
	PPC_STORE_U16(r11.u32 + r27.u32, ctx.r10.u16);
	// b 0x82655498
	goto loc_82655498;
loc_8265548C:
	// lwz r10,1760(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 1760);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r14,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r14.u32);
loc_82655498:
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// neg r7,r29
	ctx.r7.s64 = -r29.s64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82654918
	sub_82654918(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplw cr6,r11,r18
	cr6.compare<uint32_t>(r11.u32, r18.u32, xer);
	// blt cr6,0x82655218
	if (cr6.lt) goto loc_82655218;
loc_826554C0:
	// li r11,1
	r11.s64 = 1;
loc_826554C4:
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// lwz r4,1760(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 1760);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r11,5,0,26
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// lhzx r10,r10,r27
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r27.u32);
	// lhzx r11,r9,r27
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r27.u32);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// extsh r9,r11
	ctx.r9.s64 = r11.s16;
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// mullw r5,r10,r25
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(r25.s32);
	// srawi r7,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s32 >> 31;
	// subfic r10,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r10.s64;
	// mullw r6,r9,r25
	ctx.r6.s64 = int64_t(ctx.r9.s32) * int64_t(r25.s32);
	// subfe r30,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	r30.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfic r10,r9,0
	xer.ca = ctx.r9.u32 <= 0;
	ctx.r10.s64 = 0 - ctx.r9.s64;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// xor r10,r8,r24
	ctx.r10.u64 = ctx.r8.u64 ^ r24.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// and r8,r10,r30
	ctx.r8.u64 = ctx.r10.u64 & r30.u64;
	// xor r10,r7,r24
	ctx.r10.u64 = ctx.r7.u64 ^ r24.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stwx r8,r3,r4
	PPC_STORE_U32(ctx.r3.u32 + ctx.r4.u32, ctx.r8.u32);
	// lwz r8,1760(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + 1760);
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
	// stwx r10,r31,r8
	PPC_STORE_U32(r31.u32 + ctx.r8.u32, ctx.r10.u32);
	// blt cr6,0x826554c4
	if (cr6.lt) goto loc_826554C4;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_82655554"))) PPC_WEAK_FUNC(sub_82655554);
PPC_FUNC_IMPL(__imp__sub_82655554) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82655558"))) PPC_WEAK_FUNC(sub_82655558);
PPC_FUNC_IMPL(__imp__sub_82655558) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// lwz r30,0(r5)
	r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// lbz r4,8(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 8);
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// clrldi r10,r11,32
	ctx.r10.u64 = r11.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r30
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + r30.u32);
	// extsh r25,r10
	r25.s64 = ctx.r10.s16;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x82655664
	if (cr6.lt) goto loc_82655664;
	// clrlwi r10,r25,28
	ctx.r10.u64 = r25.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// sld r11,r11,r10
	r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// subf r11,r10,r9
	r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge cr6,0x8265565c
	if (!cr6.lt) goto loc_8265565C;
loc_826555C4:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x826555f0
	if (cr6.lt) goto loc_826555F0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x826555c4
	if (cr6.eq) goto loc_826555C4;
	// srawi r25,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r25.s64 = r25.s32 >> 4;
	// b 0x826556a8
	goto loc_826556A8;
loc_826555F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8265565C:
	// srawi r25,r25,4
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0xF) != 0);
	r25.s64 = r25.s32 >> 4;
	// b 0x826556a8
	goto loc_826556A8;
loc_82655664:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r28,r11,32768
	r28.u64 = r11.u64 | 32768;
loc_82655674:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r29,r11,r25
	r29.u64 = r11.u64 + r25.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r29,r28
	r11.u64 = r29.u64 + r28.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r30
	r11.u64 = PPC_LOAD_U16(r11.u32 + r30.u32);
	// extsh r25,r11
	r25.s64 = r11.s16;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// blt cr6,0x82655674
	if (cr6.lt) goto loc_82655674;
loc_826556A8:
	// lwz r31,84(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82655940
	if (!cr6.eq) goto loc_82655940;
	// cmpw cr6,r25,r27
	cr6.compare<int32_t>(r25.s32, r27.s32, xer);
	// beq cr6,0x82655950
	if (cr6.eq) goto loc_82655950;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x82655924
	if (cr6.eq) goto loc_82655924;
	// lwz r11,14756(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 14756);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826558dc
	if (cr6.eq) goto loc_826558DC;
	// cmpwi cr6,r26,2
	cr6.compare<int32_t>(r26.s32, 2, xer);
	// beq cr6,0x82655804
	if (cr6.eq) goto loc_82655804;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// beq cr6,0x8265572c
	if (cr6.eq) goto loc_8265572C;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x82655710
	if (!cr0.lt) goto loc_82655710;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655710:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// neg r11,r25
	r11.s64 = -r25.s64;
	// bne cr6,0x82655918
	if (!cr6.eq) goto loc_82655918;
	// lwz r10,1760(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// mr r11,r25
	r11.u64 = r25.u64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82655930
	goto loc_82655930;
loc_8265572C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,2
	r30.s64 = 2;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x8265579c
	if (!cr6.lt) goto loc_8265579C;
loc_82655744:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8265579c
	if (cr6.eq) goto loc_8265579C;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8265578c
	if (!cr0.lt) goto loc_8265578C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265578C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82655744
	if (cr6.gt) goto loc_82655744;
loc_8265579C:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826557d8
	if (!cr0.lt) goto loc_826557D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826557D8:
	// rlwinm r11,r30,31,1,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r25,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r9,r30,31
	ctx.r9.u64 = r30.u32 & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// beq cr6,0x826558d0
	if (cr6.eq) goto loc_826558D0;
	// lwz r10,1760(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// neg r11,r11
	r11.s64 = -r11.s64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82655930
	goto loc_82655930;
loc_82655804:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bge cr6,0x82655874
	if (!cr6.lt) goto loc_82655874;
loc_8265581C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82655874
	if (cr6.eq) goto loc_82655874;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x82655864
	if (!cr0.lt) goto loc_82655864;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655864:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8265581c
	if (cr6.gt) goto loc_8265581C;
loc_82655874:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x826558b0
	if (!cr0.lt) goto loc_826558B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826558B0:
	// rlwinm r11,r30,31,1,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r25,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r9,r30,31
	ctx.r9.u64 = r30.u32 & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// addi r11,r11,-3
	r11.s64 = r11.s64 + -3;
	// beq cr6,0x826558d0
	if (cr6.eq) goto loc_826558D0;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_826558D0:
	// lwz r10,1760(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82655930
	goto loc_82655930;
loc_826558DC:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r30,r8,0
	r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bge 0x82655908
	if (!cr0.lt) goto loc_82655908;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655908:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// neg r11,r25
	r11.s64 = -r25.s64;
	// bne cr6,0x82655918
	if (!cr6.eq) goto loc_82655918;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_82655918:
	// lwz r10,1760(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82655930
	goto loc_82655930;
loc_82655924:
	// lwz r11,1760(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_82655930:
	// lwz r11,84(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 84);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82655a4c
	if (cr6.eq) goto loc_82655A4C;
loc_82655940:
	// li r11,4
	r11.s64 = 4;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
loc_82655950:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r26,4
	cr6.compare<int32_t>(r26.s32, 4, xer);
	// bgt cr6,0x82655970
	if (cr6.gt) goto loc_82655970;
	// lwz r10,14756(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 14756);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82655970
	if (cr6.eq) goto loc_82655970;
	// srawi r11,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r11.s64 = r26.s32 >> 1;
	// subfic r11,r11,3
	xer.ca = r11.u32 <= 3;
	r11.s64 = 3 - r11.s64;
loc_82655970:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r11,9
	r30.s64 = r11.s64 + 9;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x82655990
	if (!cr6.eq) goto loc_82655990;
	// li r11,0
	r11.s64 = 0;
	// b 0x82655a30
	goto loc_82655A30;
loc_82655990:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x826559f0
	if (!cr6.gt) goto loc_826559F0;
loc_82655998:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x826559f0
	if (cr6.eq) goto loc_826559F0;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x826559e0
	if (!cr0.lt) goto loc_826559E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_826559E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x82655998
	if (cr6.gt) goto loc_82655998;
loc_826559F0:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x82655a2c
	if (!cr0.lt) goto loc_82655A2C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_82655A2C:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82655A30:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82655a44
	if (cr6.eq) goto loc_82655A44;
	// neg r11,r11
	r11.s64 = -r11.s64;
loc_82655A44:
	// lwz r10,1760(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_82655A4C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82655A5C"))) PPC_WEAK_FUNC(sub_82655A5C);
PPC_FUNC_IMPL(__imp__sub_82655A5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82655A60"))) PPC_WEAK_FUNC(sub_82655A60);
PPC_FUNC_IMPL(__imp__sub_82655A60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,272(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// beq cr6,0x82655aa8
	if (cr6.eq) goto loc_82655AA8;
	// lwz r11,21264(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21264);
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82655aa8
	if (!cr6.eq) goto loc_82655AA8;
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
loc_82655AA8:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82655ab4
	if (cr6.eq) goto loc_82655AB4;
	// addi r31,r4,-8
	r31.s64 = ctx.r4.s64 + -8;
loc_82655AB4:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82655af0
	if (cr6.eq) goto loc_82655AF0;
	// lwz r11,21264(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21264);
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r9
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82655af0
	if (!cr6.eq) goto loc_82655AF0;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82655af0
	if (cr6.eq) goto loc_82655AF0;
	// lwz r11,136(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// addi r8,r11,-8
	ctx.r8.s64 = r11.s64 + -8;
loc_82655AF0:
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lbz r8,3(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 3);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rlwinm r7,r11,16,16,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF;
	// rlwinm r5,r11,24,8,31
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 24) & 0xFFFFFF;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// rlwinm r8,r11,8,24,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFF;
	// clrlwi r6,r7,24
	ctx.r6.u64 = ctx.r7.u32 & 0xFF;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// bne cr6,0x82655b24
	if (!cr6.eq) goto loc_82655B24;
	// lbz r7,1(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// b 0x82655b28
	goto loc_82655B28;
loc_82655B24:
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
loc_82655B28:
	// lbz r11,3(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 3);
	// clrlwi r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	// xor r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r7.u64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// beq cr6,0x82655b48
	if (cr6.eq) goto loc_82655B48;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82655B48:
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// lbz r8,1(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// clrlwi r10,r6,24
	ctx.r10.u64 = ctx.r6.u32 & 0xFF;
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// bne cr6,0x82655b6c
	if (!cr6.eq) goto loc_82655B6C;
	// lbz r9,3(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// b 0x82655b70
	goto loc_82655B70;
loc_82655B6C:
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82655B70:
	// clrlwi r8,r5,24
	ctx.r8.u64 = ctx.r5.u32 & 0xFF;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// xor r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 ^ ctx.r9.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// clrlwi r8,r9,24
	ctx.r8.u64 = ctx.r9.u32 & 0xFF;
	// beq cr6,0x82655b90
	if (cr6.eq) goto loc_82655B90;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_82655B90:
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// rlwimi r9,r11,8,0,23
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 8) & 0xFFFFFF00) | (ctx.r9.u64 & 0xFFFFFFFF000000FF);
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// stw r11,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82655BBC"))) PPC_WEAK_FUNC(sub_82655BBC);
PPC_FUNC_IMPL(__imp__sub_82655BBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82655BC0"))) PPC_WEAK_FUNC(sub_82655BC0);
PPC_FUNC_IMPL(__imp__sub_82655BC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82655bfc
	if (cr6.eq) goto loc_82655BFC;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82655bfc
	if (!cr6.eq) goto loc_82655BFC;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82655BFC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82655C10"))) PPC_WEAK_FUNC(sub_82655C10);
PPC_FUNC_IMPL(__imp__sub_82655C10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82655c3c
	if (cr6.eq) goto loc_82655C3C;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82655C3C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82655C50"))) PPC_WEAK_FUNC(sub_82655C50);
PPC_FUNC_IMPL(__imp__sub_82655C50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// li r11,1
	r11.s64 = 1;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r25,r10,26,6,31
	r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FFFFFF;
	// slw r31,r11,r27
	r31.u64 = r27.u8 & 0x20 ? 0 : (r11.u32 << (r27.u8 & 0x3F));
	// rlwinm r11,r25,1,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// rlwinm r5,r30,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// cmplwi cr6,r27,16
	cr6.compare<uint32_t>(r27.u32, 16, xer);
	// blt cr6,0x82655ca4
	if (cr6.lt) goto loc_82655CA4;
loc_82655C98:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
loc_82655CA4:
	// lis r11,0
	r11.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r6,r11,32768
	ctx.r6.u64 = r11.u64 | 32768;
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// subf r5,r6,r31
	ctx.r5.s64 = r31.s64 - ctx.r6.s64;
	// subf r30,r6,r30
	r30.s64 = r30.s64 - ctx.r6.s64;
	// ble cr6,0x82655e08
	if (!cr6.gt) goto loc_82655E08;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r31,r29,4
	r31.s64 = r29.s64 + 4;
loc_82655CC8:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// clrlwi r11,r10,26
	r11.u64 = ctx.r10.u32 & 0x3F;
	// rlwinm r8,r10,26,6,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x3FFFFFF;
	// cmplwi cr6,r11,26
	cr6.compare<uint32_t>(r11.u32, 26, xer);
	// bgt cr6,0x82655c98
	if (cr6.gt) goto loc_82655C98;
	// srw. r10,r8,r11
	ctx.r10.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x82655c98
	if (!cr0.eq) goto loc_82655C98;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bgt cr6,0x82655d58
	if (cr6.gt) goto loc_82655D58;
	// subf r9,r11,r27
	ctx.r9.s64 = r27.s64 - r11.s64;
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// slw r10,r8,r9
	ctx.r10.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// slw r7,r7,r9
	ctx.r7.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r9.u8 & 0x3F));
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// beq cr6,0x82655d0c
	if (cr6.eq) goto loc_82655D0C;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_82655D0C:
	// cmplwi cr6,r9,2048
	cr6.compare<uint32_t>(ctx.r9.u32, 2048, xer);
	// bge cr6,0x82655c98
	if (!cr6.lt) goto loc_82655C98;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// beq cr6,0x82655d24
	if (cr6.eq) goto loc_82655D24;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_82655D24:
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// add r11,r9,r28
	r11.u64 = ctx.r9.u64 + r28.u64;
loc_82655D34:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82655c98
	if (!cr6.eq) goto loc_82655C98;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sth r8,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r8.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// bne cr6,0x82655d34
	if (!cr6.eq) goto loc_82655D34;
	// b 0x82655df4
	goto loc_82655DF4;
loc_82655D58:
	// subf r10,r27,r11
	ctx.r10.s64 = r11.s64 - r27.s64;
	// srw r11,r8,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
loc_82655D60:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r11,r28
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r9,r9
	ctx.r9.s64 = ctx.r9.s16;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt cr6,0x82655c98
	if (cr6.gt) goto loc_82655C98;
	// bne cr6,0x82655d8c
	if (!cr6.eq) goto loc_82655D8C;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmpw cr6,r5,r30
	cr6.compare<int32_t>(ctx.r5.s32, r30.s32, xer);
	// sthx r9,r11,r28
	PPC_STORE_U16(r11.u32 + r28.u32, ctx.r9.u16);
	// bgt cr6,0x82655c98
	if (cr6.gt) goto loc_82655C98;
loc_82655D8C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// srw r9,r8,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82655db0
	if (cr6.eq) goto loc_82655DB0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82655DB0:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82655d60
	if (!cr6.eq) goto loc_82655D60;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r10,r28
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + r28.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82655c98
	if (!cr6.eq) goto loc_82655C98;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// beq cr6,0x82655dd8
	if (cr6.eq) goto loc_82655DD8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_82655DD8:
	// cmplwi cr6,r11,32768
	cr6.compare<uint32_t>(r11.u32, 32768, xer);
	// bge cr6,0x82655c98
	if (!cr6.lt) goto loc_82655C98;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// beq cr6,0x82655df0
	if (cr6.eq) goto loc_82655DF0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
loc_82655DF0:
	// sthx r11,r10,r28
	PPC_STORE_U16(ctx.r10.u32 + r28.u32, r11.u16);
loc_82655DF4:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmpw cr6,r4,r25
	cr6.compare<int32_t>(ctx.r4.s32, r25.s32, xer);
	// blt cr6,0x82655cc8
	if (cr6.lt) goto loc_82655CC8;
loc_82655E08:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd3c
	return;
}

__attribute__((alias("__imp__sub_82655E14"))) PPC_WEAK_FUNC(sub_82655E14);
PPC_FUNC_IMPL(__imp__sub_82655E14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82655E18"))) PPC_WEAK_FUNC(sub_82655E18);
PPC_FUNC_IMPL(__imp__sub_82655E18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r11,r11,26,6,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82655e6c
	if (!cr6.gt) goto loc_82655E6C;
	// addi r9,r29,4
	ctx.r9.s64 = r29.s64 + 4;
loc_82655E48:
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// clrlwi r10,r10,26
	ctx.r10.u64 = ctx.r10.u32 & 0x3F;
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bge cr6,0x82655e5c
	if (!cr6.lt) goto loc_82655E5C;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_82655E5C:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82655e48
	if (!cr6.eq) goto loc_82655E48;
loc_82655E6C:
	// clrlwi r31,r8,24
	r31.u64 = ctx.r8.u32 & 0xFF;
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x82655e84
	if (!cr6.gt) goto loc_82655E84;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
loc_82655E84:
	// cmplwi cr6,r11,128
	cr6.compare<uint32_t>(r11.u32, 128, xer);
	// blt cr6,0x82655e94
	if (cr6.lt) goto loc_82655E94;
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
loc_82655E94:
	// lwz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// stw r5,4(r27)
	PPC_STORE_U32(r27.u32 + 4, ctx.r5.u32);
	// stb r31,8(r27)
	PPC_STORE_U8(r27.u32 + 8, r31.u8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82655eb4
	if (cr6.eq) goto loc_82655EB4;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_82655EB4:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// clrlwi r31,r31,24
	r31.u64 = r31.u32 & 0xFF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// beq cr6,0x82655ef0
	if (cr6.eq) goto loc_82655EF0;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r10,r11,27,5,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFE;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// slw r9,r9,r31
	ctx.r9.u64 = r31.u8 & 0x20 ? 0 : (ctx.r9.u32 << (r31.u8 & 0x3F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r10,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r10.u32);
	// b 0x82655f10
	goto loc_82655F10;
loc_82655EF0:
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwinm r11,r11,27,5,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFE;
	// li r4,2
	ctx.r4.s64 = 2;
	// slw r10,r10,r31
	ctx.r10.u64 = r31.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r31.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_82655F10:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82655f3c
	if (cr6.eq) goto loc_82655F3C;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82655c50
	sub_82655C50(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
loc_82655F3C:
	// li r3,5
	ctx.r3.s64 = 5;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82655F48"))) PPC_WEAK_FUNC(sub_82655F48);
PPC_FUNC_IMPL(__imp__sub_82655F48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// stw r4,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r4.u32);
	// stb r30,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r30.u8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82655f80
	if (cr6.eq) goto loc_82655F80;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82655F80:
	// clrlwi r30,r30,24
	r30.u64 = r30.u32 & 0xFF;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwinm r11,r11,27,5,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFE;
	// li r4,2
	ctx.r4.s64 = 2;
	// slw r10,r10,r30
	ctx.r10.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x82655fcc
	if (cr6.eq) goto loc_82655FCC;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82655c50
	sub_82655C50(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_82655FCC:
	// li r3,5
	ctx.r3.s64 = 5;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_82655FD8"))) PPC_WEAK_FUNC(sub_82655FD8);
PPC_FUNC_IMPL(__imp__sub_82655FD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// vspltish v13,2
	// addi r10,r4,112
	ctx.r10.s64 = ctx.r4.s64 + 112;
	// vspltish v0,3
	// lvx128 v3,r0,r4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r4,16
	r11.s64 = ctx.r4.s64 + 16;
	// vspltish v11,4
	// addi r9,r4,64
	ctx.r9.s64 = ctx.r4.s64 + 64;
	// vspltish v1,1
	// vslh v29,v3,v13
	// addi r7,r4,48
	ctx.r7.s64 = ctx.r4.s64 + 48;
	// vslh v3,v3,v0
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v31,v8,v11
	// addi r8,r4,32
	ctx.r8.s64 = ctx.r4.s64 + 32;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v30,v8,v0
	// vadduhm v4,v10,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvx128 v2,r0,r9
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v3,v3,v29
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v29.u16)));
	// addi r11,r4,80
	r11.s64 = ctx.r4.s64 + 80;
	// vslh v28,v2,v13
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v27,v31,v30
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v30.u16)));
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v2,v2,v0
	// addi r10,r4,96
	ctx.r10.s64 = ctx.r4.s64 + 96;
	// vadduhm v30,v3,v11
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vspltish v12,6
	// vslh v3,v4,v11
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vadduhm v25,v10,v10
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v2,v2,v28
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v28,v10,v13
	// lvx128 v5,r0,r10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubuhm v29,v3,v4
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vslh v21,v10,v0
	// vslh v10,v8,v13
	// vslh v8,v9,v0
	// vsubuhm v24,v29,v27
	_mm_store_si128((__m128i*)v24.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vslh v27,v9,v13
	// vadduhm v3,v9,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v23,v9,v9
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v9,v28,v25
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v25,v21,v28
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v21,v8,v27
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v8,v3,v11
	// vadduhm v28,v27,v23
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v23.u16)));
	// vadduhm v23,v31,v10
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v4,v4,v13
	// vslh v26,v6,v11
	// vslh v10,v3,v13
	// vslh v22,v6,v13
	// vsubuhm v3,v8,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vsubuhm v8,v29,v9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vadduhm v31,v25,v4
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubuhm v29,v4,v23
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vor v4,v10,v10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vadduhm v27,v26,v22
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v22.u16)));
	// vor v9,v10,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v3,v3
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vsubuhm v3,v30,v2
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vsubuhm v4,v4,v27
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vslh v25,v7,v11
	// vsubuhm v27,v10,v28
	_mm_store_si128((__m128i*)v27.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vadduhm v28,v7,v7
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v4,v30,v2
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v2,v31,v27
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vslh v31,v7,v13
	// vslh v7,v6,v0
	// vslh v11,v5,v11
	// vadduhm v6,v5,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v9,v9,v21
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)v21.u16)));
	// vadduhm v7,v26,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v5,v5,v13
	// vadduhm v31,v31,v28
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vsubuhm v9,v24,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsubuhm v10,v10,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vadduhm v7,v5,v6
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubuhm v11,v31,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vadduhm v10,v29,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v7,v7,v25
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)v25.u16)));
	// lis r11,-32243
	r11.s64 = -2113077248;
	// vadduhm v6,v3,v11
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// li r10,16
	ctx.r10.s64 = 16;
	// vsubuhm v11,v3,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// addi r11,r11,2832
	r11.s64 = r11.s64 + 2832;
	// vor v5,v7,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v3,v7,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v7,v6,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vadduhm v31,v11,v8
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v6,v4,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsubuhm v5,v4,v3
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v4,v7,v9
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v29,v11,v8
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vadduhm v3,v6,v2
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vadduhm v30,v5,v10
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubuhm v5,v5,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsrah v4,v4,v0
	// vsrah v3,v3,v0
	// vsubuhm v2,v6,v2
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vsubuhm v11,v7,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsrah v8,v31,v0
	// vsrah v7,v30,v0
	// vmrglh v9,v3,v4
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vmrghh v10,v3,v4
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vsrah v6,v5,v0
	// vsrah v4,v2,v0
	// vsrah v5,v29,v0
	// vsrah v11,v11,v0
	// vmrghh v3,v8,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vmrglh v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghh v2,v6,v5
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghh v31,v11,v4
	_mm_store_si128((__m128i*)v31.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v11,v11,v4
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vmrglh v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghw v4,v10,v3
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v10.u32)));
	// vmrglw v3,v10,v3
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v10.u32)));
	// vspltish v5,8
	// vmrghw v10,v2,v31
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrglw v2,v2,v31
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghw v29,v6,v11
	_mm_store_si128((__m128i*)v29.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrglw v28,v6,v11
	_mm_store_si128((__m128i*)v28.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// lvx128 v6,r10,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglw v31,v9,v8
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vslh v27,v5,v13
	// vmrghw v30,v9,v8
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vperm v5,v4,v10,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v11,v4,v10,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v9,v3,v2,v6
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v10,v3,v2,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v4,v30,v29,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v3,v31,v28,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vslh v24,v11,v13
	// vperm v7,v31,v28,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v23,v9,v9
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vperm v8,v30,v29,v6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v6,v5,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v5,v5,v13
	// vslh v2,v4,v13
	// vadduhm v25,v7,v7
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v26,v8,v13
	// vadduhm v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v5,v4,v4
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v31,v8,v8
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v30,v11,v11
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v4,v27,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v11,v7
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v28,v2,v5
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v2,v7,v0
	// vslh v27,v7,v13
	// vadduhm v5,v8,v9
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v7,v6,v0
	// vadduhm v20,v26,v31
	_mm_store_si128((__m128i*)v20.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v29,v9,v0
	// vadduhm v22,v6,v6
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubuhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v27,v2,v27
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v21,v5,v5
	_mm_store_si128((__m128i*)v21.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v26,v30,v11
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v24,v24,v30
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vadduhm v2,v2,v25
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)v25.u16)));
	// addi r11,r3,16
	r11.s64 = ctx.r3.s64 + 16;
	// vor v25,v7,v7
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// addi r8,r3,64
	ctx.r8.s64 = ctx.r3.s64 + 64;
	// vor v11,v22,v22
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v22.u8));
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
	// vsubuhm v7,v7,v27
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)v27.u8)));
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// vadduhm v23,v29,v23
	_mm_store_si128((__m128i*)v23.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v23.u16)));
	// addi r7,r3,80
	ctx.r7.s64 = ctx.r3.s64 + 80;
	// vsubuhm v30,v25,v26
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)v26.u8)));
	// addi r6,r3,96
	ctx.r6.s64 = ctx.r3.s64 + 96;
	// vor v27,v11,v11
	_mm_store_si128((__m128i*)v27.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v26,v11,v11
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v21,v21
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v21.u8));
	// vslh v22,v3,v13
	// vadduhm v27,v27,v24
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubuhm v26,v26,v2
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// vor v24,v11,v11
	_mm_store_si128((__m128i*)v24.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vadduhm v11,v11,v20
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)v20.u16)));
	// vslh v2,v5,v0
	// vadduhm v8,v31,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsubuhm v25,v7,v11
	_mm_store_si128((__m128i*)v25.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vadduhm v7,v4,v28
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v11,v3,v10
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubuhm v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vadduhm v28,v10,v10
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubuhm v2,v2,v5
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vslh v11,v11,v0
	// vslh v0,v10,v0
	// vadduhm v28,v28,v10
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v10,v22,v3
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v22.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v28,v0,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v0,v9,v13
	// vsrah v9,v5,v1
	// vsubuhm v13,v11,v28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v28.u8)));
	// vadduhm v31,v29,v0
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubuhm v0,v24,v23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vadduhm v3,v30,v0
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubuhm v0,v11,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsubuhm v11,v2,v8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsubuhm v10,v2,v31
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vsrah v8,v6,v1
	// vadduhm v5,v27,v11
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v2,v26,v10
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v11,v7,v0
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vsubuhm v0,v7,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vadduhm v7,v5,v9
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v10,v4,v13
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vadduhm v6,v25,v8
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v25.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v9,v2,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v13,v4,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vadduhm v8,v3,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v3,v10,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v4,v0,v9
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vadduhm v2,v13,v8
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsubuhm v13,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsrah v8,v3,v12
	// vadduhm v5,v11,v7
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsrah v0,v0,v12
	// vsubuhm v10,v10,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v11,v11,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v9,v5,v12
	// addi r11,r3,112
	r11.s64 = ctx.r3.s64 + 112;
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v6,v2,v12
	// vsrah v5,v4,v12
	// vsrah v13,v13,v12
	// vsrah v10,v10,v12
	// stvx v9,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v0,v11,v12
	// stvx v6,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v5,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265639C"))) PPC_WEAK_FUNC(sub_8265639C);
PPC_FUNC_IMPL(__imp__sub_8265639C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826563A0"))) PPC_WEAK_FUNC(sub_826563A0);
PPC_FUNC_IMPL(__imp__sub_826563A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	uint32_t ea{};
	// addi r11,r5,128
	r11.s64 = ctx.r5.s64 + 128;
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,32
	ctx.r9.s64 = ctx.r5.s64 + 32;
	// vspltish v13,2
	// addi r8,r5,64
	ctx.r8.s64 = ctx.r5.s64 + 64;
	// vspltish v0,3
	// addi r7,r5,96
	ctx.r7.s64 = ctx.r5.s64 + 96;
	// vspltish v11,4
	// addi r10,r5,144
	ctx.r10.s64 = ctx.r5.s64 + 144;
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,160
	r11.s64 = ctx.r5.s64 + 160;
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,224
	ctx.r9.s64 = ctx.r5.s64 + 224;
	// lvx128 v7,r0,r8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r5,48
	ctx.r8.s64 = ctx.r5.s64 + 48;
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r5,176
	ctx.r7.s64 = ctx.r5.s64 + 176;
	// lvx128 v4,r0,r10
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r5,192
	ctx.r10.s64 = ctx.r5.s64 + 192;
	// lvx128 v5,r0,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v31,r0,r9
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r5,80
	ctx.r9.s64 = ctx.r5.s64 + 80;
	// lvx128 v28,r0,r8
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkswss v4,v9,v4
	// lvx128 v26,r0,r7
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r5,112
	ctx.r6.s64 = ctx.r5.s64 + 112;
	// vpkswss v9,v5,v26
	// lvx128 v3,r0,r10
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r5,240
	r11.s64 = ctx.r5.s64 + 240;
	// vpkswss v1,v10,v1
	// lvx128 v29,r0,r9
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkswss v10,v8,v28
	// addi r10,r5,208
	ctx.r10.s64 = ctx.r5.s64 + 208;
	// vpkswss v8,v7,v29
	// vslh v24,v4,v0
	// vslh v29,v4,v13
	// lvx128 v25,r0,r6
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v27,r0,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpkswss v7,v6,v25
	// vpkswss v5,v31,v27
	// vslh v31,v10,v13
	// vadduhm v28,v10,v10
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// lvx128 v30,r0,r10
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vslh v23,v10,v0
	// vpkswss v6,v3,v30
	// vslh v27,v1,v13
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// vadduhm v4,v10,v5
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vspltish v2,1
	// vadduhm v10,v24,v29
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vspltish v12,6
	// vadduhm v29,v31,v28
	_mm_store_si128((__m128i*)v29.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v1,v1,v0
	// vslh v30,v5,v11
	// vslh v26,v5,v0
	// vslh v28,v4,v11
	// vadduhm v24,v23,v31
	_mm_store_si128((__m128i*)v24.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v31,v4,v13
	// vadduhm v1,v1,v27
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v27.u16)));
	// vadduhm v27,v30,v26
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vsubuhm v4,v28,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vadduhm v3,v9,v7
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v25,v5,v13
	// vslh v5,v9,v13
	// vsubuhm v28,v4,v27
	_mm_store_si128((__m128i*)v28.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vadduhm v27,v24,v31
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v24.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v26,v30,v25
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vslh v24,v3,v11
	// vslh v30,v3,v13
	// vsubuhm v29,v4,v29
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)v29.u8)));
	// vsubuhm v31,v31,v26
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)v26.u8)));
	// vsubuhm v3,v24,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vadduhm v1,v11,v1
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v1.u16)));
	// vadduhm v26,v9,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v4,v7,v11
	// vslh v25,v7,v13
	// vslh v24,v7,v0
	// vor v23,v30,v30
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)v30.u8));
	// vslh v9,v9,v0
	// lis r11,-32243
	r11.s64 = -2113077248;
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// li r10,-16
	ctx.r10.s64 = -16;
	// vadduhm v3,v1,v10
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// addi r11,r11,2880
	r11.s64 = r11.s64 + 2880;
	// vsubuhm v10,v1,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vadduhm v9,v9,v5
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vadduhm v1,v5,v26
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v5,v4,v25
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vor v22,v30,v30
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)v30.u8));
	// vadduhm v4,v4,v24
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vadduhm v30,v8,v8
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vslh v21,v8,v13
	// vadduhm v9,v23,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v23.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v4,v7,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vslh v23,v6,v11
	// vadduhm v30,v21,v30
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v21.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsubuhm v5,v22,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v4,v31,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubuhm v9,v28,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vsubuhm v31,v30,v23
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v23.u8)));
	// vadduhm v5,v29,v5
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vor v28,v6,v6
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v29,v6,v6
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v30,v6,v6
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vor v6,v31,v31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)v31.u8));
	// vsubuhm v7,v7,v1
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v31,v29,v28
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vslh v30,v30,v13
	// vslh v8,v8,v11
	// vsubuhm v11,v10,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v7,v27,v7
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v1,v30,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v31,v10,v6
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vadduhm v6,v11,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsubuhm v5,v11,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v10,v1,v8
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vor v11,v31,v31
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v31.u8));
	// vsrah v6,v6,v0
	// vsrah v5,v5,v0
	// vor v1,v10,v10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vadduhm v10,v3,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubuhm v30,v11,v9
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vadduhm v31,v11,v9
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v8,v3,v1
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vadduhm v9,v10,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vsubuhm v1,v10,v7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsrah v11,v31,v0
	// vadduhm v3,v8,v4
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubuhm v4,v8,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vsrah v9,v9,v0
	// vsrah v10,v30,v0
	// vsrah v8,v3,v0
	// vsrah v7,v4,v0
	// vsrah v4,v1,v0
	// vmrghh v3,v9,v11
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrglh v11,v9,v11
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vmrghh v1,v6,v8
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vmrghh v31,v7,v5
	_mm_store_si128((__m128i*)v31.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vmrghh v30,v10,v4
	_mm_store_si128((__m128i*)v30.u16, _mm_unpackhi_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrglh v9,v6,v8
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// lvx128 v6,r10,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglh v8,v7,v5
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglh v10,v10,v4
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_unpacklo_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vmrghw v4,v3,v1
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrglw v3,v3,v1
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrghw v1,v31,v30
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v31.u32)));
	// vmrglw v31,v31,v30
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)v31.u32)));
	// vmrghw v30,v11,v9
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vmrghw v28,v8,v10
	_mm_store_si128((__m128i*)v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), _mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmrglw v27,v8,v10
	_mm_store_si128((__m128i*)v27.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), _mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmrglw v29,v11,v9
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vperm v5,v4,v1,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v11,v4,v1,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v10,v3,v31,v6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vperm v8,v3,v31,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vspltish v1,8
	// vperm v9,v29,v27,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vperm v4,v30,v28,v6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v31,v5,v5
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vperm v7,v30,v28,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vslh v5,v5,v13
	// vadduhm v3,v11,v11
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vslh v1,v1,v13
	// vadduhm v28,v4,v4
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v31,v5,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vperm v5,v29,v27,v6
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v6,v11,v9
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v4,v4,v13
	// vadduhm v27,v3,v11
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vslh v30,v11,v13
	// vadduhm v1,v1,v31
	_mm_store_si128((__m128i*)ctx.v1.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vslh v31,v9,v0
	// vslh v29,v9,v13
	// vslh v11,v6,v0
	// vadduhm v4,v4,v28
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vadduhm v28,v30,v3
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v3.u16)));
	// vadduhm v30,v31,v29
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v29.u16)));
	// vsubuhm v11,v11,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v9,v9,v9
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v3,v6,v6
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vslh v24,v8,v13
	// vsubuhm v30,v11,v30
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vsubuhm v29,v11,v27
	_mm_store_si128((__m128i*)v29.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vadduhm v11,v7,v8
	_mm_store_si128((__m128i*)ctx.v11.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v27,v31,v9
	_mm_store_si128((__m128i*)v27.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vslh v31,v8,v0
	// vadduhm v25,v8,v8
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v8,v5,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v5.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vslh v26,v7,v13
	// vadduhm v9,v7,v7
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vslh v23,v11,v0
	// vadduhm v28,v3,v28
	_mm_store_si128((__m128i*)v28.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)v28.u16)));
	// vsubuhm v27,v3,v27
	_mm_store_si128((__m128i*)v27.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)v27.u8)));
	// vadduhm v3,v1,v4
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vsubuhm v4,v1,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v4.u8)));
	// vslh v1,v8,v0
	// vadduhm v8,v11,v11
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vadduhm v26,v26,v9
	_mm_store_si128((__m128i*)v26.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v26.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vsubuhm v23,v23,v11
	_mm_store_si128((__m128i*)v23.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vadduhm v22,v9,v7
	_mm_store_si128((__m128i*)v22.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v25,v31,v25
	_mm_store_si128((__m128i*)v25.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v25.u16)));
	// vadduhm v7,v8,v26
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vor v9,v23,v23
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v23.u8));
	// vadduhm v31,v31,v24
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)v24.u16)));
	// vsubuhm v26,v8,v25
	_mm_store_si128((__m128i*)v26.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)v25.u8)));
	// vsubuhm v8,v30,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubuhm v30,v9,v22
	_mm_store_si128((__m128i*)v30.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v22.u8)));
	// vsubuhm v31,v9,v31
	_mm_store_si128((__m128i*)v31.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)v31.u8)));
	// vslh v13,v5,v13
	// vslh v0,v10,v0
	// vadduhm v9,v28,v30
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v28.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vadduhm v30,v10,v10
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v7,v29,v26
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v29.u16), _mm_load_si128((__m128i*)v26.u16)));
	// vadduhm v31,v27,v31
	_mm_store_si128((__m128i*)v31.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v27.u16), _mm_load_si128((__m128i*)v31.u16)));
	// vadduhm v30,v30,v10
	_mm_store_si128((__m128i*)v30.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v30.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v10,v13,v5
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsrah v13,v6,v2
	// vadduhm v5,v0,v30
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)v30.u16)));
	// vsrah v0,v11,v2
	// vsubuhm v11,v1,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vadduhm v8,v8,v13
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vsubuhm v10,v1,v5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v13,v7,v13
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vadduhm v7,v3,v11
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v3.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsubuhm v6,v3,v11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vsubuhm v5,v4,v10
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vadduhm v9,v9,v0
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vadduhm v10,v4,v10
	_mm_store_si128((__m128i*)ctx.v10.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v4.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vadduhm v0,v31,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)v31.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vor v11,v5,v5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vadduhm v5,v7,v9
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v9.u16)));
	// vadduhm v3,v10,v13
	_mm_store_si128((__m128i*)ctx.v3.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v10.u16), _mm_load_si128((__m128i*)ctx.v13.u16)));
	// vor v1,v0,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vadduhm v4,v11,v8
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v11.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsrah v5,v5,v12
	// vsrah v3,v3,v12
	// vsrah v4,v4,v12
	// vpkshus v5,v5,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16)));
	// vpkshus v4,v4,v4
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16)));
	// stvlx v5,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v5.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v3,v3,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16)));
	// vadduhm v0,v6,v0
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v0.u16)));
	// vor v30,v13,v13
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vsubuhm v13,v6,v1
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v1.u8)));
	// vor v5,v11,v11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// stvlx v4,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v4.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vsrah v0,v0,v12
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vsrah v13,v13,v12
	// vsubuhm v10,v10,v30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)v30.u8)));
	// vsubuhm v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vpkshus v0,v0,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v0.s16), _mm_load_si128((__m128i*)ctx.v0.s16)));
	// vpkshus v13,v13,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v13.s16), _mm_load_si128((__m128i*)ctx.v13.s16)));
	// stvlx v3,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v3.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vsrah v11,v10,v12
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vsubuhm v10,v5,v8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// vsrah v10,v10,v12
	// vsrah v12,v9,v12
	// stvlx v0,r9,r11
	ea = ctx.r9.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16)));
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stvlx v13,r10,r11
	ea = ctx.r10.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16)));
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stvlx v0,r9,r11
	ea = ctx.r9.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// vpkshus v0,v12,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_packus_epi16(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16)));
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stvlx v13,r9,r11
	ea = ctx.r9.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// stvlx v0,r9,r11
	ea = ctx.r9.u32 + r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265683C"))) PPC_WEAK_FUNC(sub_8265683C);
PPC_FUNC_IMPL(__imp__sub_8265683C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82656840"))) PPC_WEAK_FUNC(sub_82656840);
PPC_FUNC_IMPL(__imp__sub_82656840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// rlwinm r11,r6,6,0,25
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// li r9,24
	ctx.r9.s64 = 24;
	// vspltish v8,1
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// vspltish v13,6
	// li r8,8
	ctx.r8.s64 = 8;
	// vspltisw v0,3
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x3)));
	// li r11,32
	r11.s64 = 32;
	// li r7,32
	ctx.r7.s64 = 32;
	// stw r10,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r10.u32);
	// lvrx v12,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,16
	ctx.r9.s64 = 16;
	// lvlx v11,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v11,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,40
	r11.s64 = 40;
	// lvlx v9,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,24
	ctx.r9.s64 = 24;
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,48
	ctx.r8.s64 = 48;
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// lvrx v9,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,56
	r11.s64 = 56;
	// vupkhsh v12,v12
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v12.s16), _mm_load_si128((__m128i*)ctx.v12.s16))));
	// lvlx v7,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,40
	ctx.r9.s64 = 40;
	// vor v9,v7,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vupkhsh v11,v11
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16))));
	// lvrx v7,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,64
	ctx.r8.s64 = 64;
	// vor v5,v6,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v7,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,48
	r11.s64 = 48;
	// vcfsx v12,v12,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v12.u32)));
	// lvlx v6,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,72
	ctx.r9.s64 = 72;
	// vor v4,v6,v7
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vcfsx v31,v11,0
	_mm_store_ps(v31.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// lvrx v7,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r8,56
	ctx.r8.s64 = 56;
	// vupkhsh v30,v5
	_mm_store_si128((__m128i*)v30.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v5.s16), _mm_load_si128((__m128i*)ctx.v5.s16))));
	// lvlx v6,r5,r11
	temp.u32 = ctx.r5.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r11,-32243
	r11.s64 = -2113077248;
	// vor v3,v6,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v7,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r11,3088
	r11.s64 = r11.s64 + 3088;
	// vupkhsh v29,v4
	_mm_store_si128((__m128i*)v29.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v4.s16), _mm_load_si128((__m128i*)ctx.v4.s16))));
	// li r9,-48
	ctx.r9.s64 = -48;
	// lvlx v6,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v6,v6,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vupkhsh v10,v10
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v10.s16), _mm_load_si128((__m128i*)ctx.v10.s16))));
	// vupkhsh v28,v3
	_mm_store_si128((__m128i*)v28.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v3.s16), _mm_load_si128((__m128i*)ctx.v3.s16))));
	// vcfsx v26,v30,0
	_mm_store_ps(v26.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v30.u32)));
	// vupkhsh v9,v9
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16))));
	// lvx128 v7,r9,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-112
	ctx.r9.s64 = -112;
	// vupkhsh v11,v6
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16))));
	// vcfsx v25,v10,0
	_mm_store_ps(v25.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vcfsx v10,v9,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vcfsx v9,v29,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v29.u32)));
	// vcfsx v30,v28,0
	_mm_store_ps(v30.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)v28.u32)));
	// lvx128 v6,r9,r11
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-176
	ctx.r9.s64 = -176;
	// vcfsx v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// lvx128 v5,r9,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-160
	ctx.r9.s64 = -160;
	// lvx128 v4,r9,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-192
	ctx.r9.s64 = -192;
	// vmulfp128 v24,v4,v12
	_mm_store_ps(v24.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v12.f32)));
	// lvx128 v3,r9,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-96
	ctx.r9.s64 = -96;
	// lvx128 v27,r9,r11
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-144
	ctx.r9.s64 = -144;
	// lvx128 v2,r9,r11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-128
	ctx.r9.s64 = -128;
	// lvx128 v1,r9,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v29,v12,v11
	_mm_store_ps(v29.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v11.f32)));
	// li r9,-80
	ctx.r9.s64 = -80;
	// vmulfp128 v23,v5,v11
	_mm_store_ps(v23.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v22,v1,v11
	_mm_store_ps(v22.f32, _mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v11,v26,v3
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v3.f32)));
	// vmaddfp v3,v25,v3,v27
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v25.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(v27.f32)));
	// vmulfp128 v1,v1,v10
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v4,v4,v9
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v28,v7,v29
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(v29.f32)));
	// vmulfp128 v29,v6,v29
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(v29.f32)));
	// vsubfp v27,v28,v23
	_mm_store_ps(v27.f32, _mm_sub_ps(_mm_load_ps(v28.f32), _mm_load_ps(v23.f32)));
	// vor v23,v11,v11
	_mm_store_si128((__m128i*)v23.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vsubfp v28,v28,v24
	_mm_store_ps(v28.f32, _mm_sub_ps(_mm_load_ps(v28.f32), _mm_load_ps(v24.f32)));
	// vmaddfp v26,v2,v12,v29
	_mm_store_ps(v26.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(v29.f32)));
	// vsubfp v24,v29,v22
	_mm_store_ps(v24.f32, _mm_sub_ps(_mm_load_ps(v29.f32), _mm_load_ps(v22.f32)));
	// lvx128 v29,r9,r11
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-16
	ctx.r9.s64 = -16;
	// vmulfp128 v29,v29,v30
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(v29.f32), _mm_load_ps(v30.f32)));
	// vor v22,v11,v11
	_mm_store_si128((__m128i*)v22.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vaddfp v11,v9,v10
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	// vaddfp v23,v3,v23
	_mm_store_ps(v23.f32, _mm_add_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v23.f32)));
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-32
	ctx.r9.s64 = -32;
	// vmulfp128 v21,v12,v31
	_mm_store_ps(v21.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v31.f32)));
	// vsubfp v3,v3,v22
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v22.f32)));
	// lvx128 v25,r9,r11
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v25,v25,v31
	_mm_store_ps(v25.f32, _mm_mul_ps(_mm_load_ps(v25.f32), _mm_load_ps(v31.f32)));
	// vsubfp v31,v21,v29
	_mm_store_ps(v31.f32, _mm_sub_ps(_mm_load_ps(v21.f32), _mm_load_ps(v29.f32)));
	// vmulfp128 v29,v5,v10
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v10,v6,v11
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v11,v7,v11
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v11.f32)));
	// vmaddfp v12,v12,v30,v25
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v30.f32)), _mm_load_ps(v25.f32)));
	// vaddfp v7,v3,v31
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v31.f32)));
	// vsubfp v3,v3,v31
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v31.f32)));
	// vor v30,v12,v12
	_mm_store_si128((__m128i*)v30.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v25,v12,v12
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v11,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vmaddfp v11,v2,v9,v10
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vaddfp v10,v28,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v10.f32)));
	// vor v9,v12,v12
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vaddfp v9,v24,v9
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_load_ps(v24.f32), _mm_load_ps(ctx.v9.f32)));
	// vsubfp v5,v27,v11
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(v27.f32), _mm_load_ps(ctx.v11.f32)));
	// vsubfp v11,v10,v1
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v1.f32)));
	// vor v10,v12,v12
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v12,v23,v23
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v23.u8));
	// vaddfp v10,v26,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v10.f32)));
	// vsubfp v6,v9,v29
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(v29.f32)));
	// vsubfp v9,v12,v25
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v25.f32)));
	// vctsxs v5,v5,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v11,v11,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v11.f32)));
	// vsubfp v10,v10,v4
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v4.f32)));
	// vaddfp v4,v12,v30
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v30.f32)));
	// vctsxs v9,v9,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vctsxs v12,v10,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v10,v7,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v7,v6,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vctsxs v6,v4,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v4,v3,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vaddsws v2,v10,v5
	// vaddsws v1,v9,v7
	// vsubsws v9,v9,v7
	temp.s64 = int64_t(ctx.v9.s32[0]) - int64_t(ctx.v7.s32[0]);
	ctx.v9.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[1]) - int64_t(ctx.v7.s32[1]);
	ctx.v9.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[2]) - int64_t(ctx.v7.s32[2]);
	ctx.v9.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[3]) - int64_t(ctx.v7.s32[3]);
	ctx.v9.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsubsws v7,v6,v12
	temp.s64 = int64_t(ctx.v6.s32[0]) - int64_t(ctx.v12.s32[0]);
	ctx.v7.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v6.s32[1]) - int64_t(ctx.v12.s32[1]);
	ctx.v7.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v6.s32[2]) - int64_t(ctx.v12.s32[2]);
	ctx.v7.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v6.s32[3]) - int64_t(ctx.v12.s32[3]);
	ctx.v7.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v3,v6,v12
	// vsubsws v10,v10,v5
	temp.s64 = int64_t(ctx.v10.s32[0]) - int64_t(ctx.v5.s32[0]);
	ctx.v10.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[1]) - int64_t(ctx.v5.s32[1]);
	ctx.v10.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[2]) - int64_t(ctx.v5.s32[2]);
	ctx.v10.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[3]) - int64_t(ctx.v5.s32[3]);
	ctx.v10.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v12,v9,v0
	ctx.v12.s32[0] = ctx.v9.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v12.s32[1] = ctx.v9.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v12.s32[2] = ctx.v9.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v12.s32[3] = ctx.v9.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v9,v7,v0
	ctx.v9.s32[0] = ctx.v7.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v9.s32[1] = ctx.v7.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v9.s32[2] = ctx.v7.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v9.s32[3] = ctx.v7.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vaddsws v7,v4,v11
	// vsubsws v6,v4,v11
	temp.s64 = int64_t(ctx.v4.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v6.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v6.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v6.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v6.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v3,v3,v0
	ctx.v3.s32[0] = ctx.v3.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v3.s32[1] = ctx.v3.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v3.s32[2] = ctx.v3.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v3.s32[3] = ctx.v3.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v2,v2,v0
	ctx.v2.s32[0] = ctx.v2.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v2.s32[1] = ctx.v2.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v2.s32[2] = ctx.v2.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v2.s32[3] = ctx.v2.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v11,v7,v0
	ctx.v11.s32[0] = ctx.v7.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v11.s32[1] = ctx.v7.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v11.s32[2] = ctx.v7.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v11.s32[3] = ctx.v7.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v1,v1,v0
	ctx.v1.s32[0] = ctx.v1.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v1.s32[1] = ctx.v1.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v1.s32[2] = ctx.v1.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v1.s32[3] = ctx.v1.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v10,v10,v0
	ctx.v10.s32[0] = ctx.v10.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v10.s32[1] = ctx.v10.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v10.s32[2] = ctx.v10.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v10.s32[3] = ctx.v10.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v0,v6,v0
	ctx.v0.s32[0] = ctx.v6.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v6.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v6.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v6.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vmrghw v7,v3,v11
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrglw v11,v3,v11
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrghw v4,v12,v10
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), _mm_load_si128((__m128i*)ctx.v12.u32)));
	// vmrghw v6,v2,v1
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrglw v5,v2,v1
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghw v3,v0,v9
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// vmrglw v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), _mm_load_si128((__m128i*)ctx.v12.u32)));
	// vmrglw v0,v0,v9
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// li r9,32
	ctx.r9.s64 = 32;
	// vmrghw v2,v7,v6
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), _mm_load_si128((__m128i*)ctx.v7.u32)));
	// addi r8,r10,48
	ctx.r8.s64 = ctx.r10.s64 + 48;
	// vmrglw v6,v7,v6
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), _mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmrglw v10,v4,v3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v4.u32)));
	// vmrghw v7,v12,v0
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v12.u32)));
	// vmrglw v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v12.u32)));
	// vmrghw v1,v11,v5
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v5.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vcfsx v9,v10,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v10.u32)));
	// vmrglw v5,v11,v5
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v5.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vcfsx v10,v7,0
	_mm_store_ps(ctx.v10.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vmrghw v11,v4,v3
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v4.u32)));
	// vcfsx v7,v0,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)));
	// vcfsx v0,v2,0
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v2.u32)));
	// vcfsx v12,v1,0
	_mm_store_ps(ctx.v12.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v1.u32)));
	// vcfsx v11,v11,0
	_mm_store_ps(ctx.v11.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v11.u32)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// vsubfp v4,v0,v12
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)));
	// vaddfp v2,v12,v0
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v0,r9,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,96
	ctx.r9.s64 = 96;
	// vsubfp v3,v11,v10
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v10.f32)));
	// vaddfp v1,v10,v11
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v11.f32)));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v27,v10,v7
	_mm_store_ps(v27.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v7.f32)));
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,128
	ctx.r9.s64 = 128;
	// lvx128 v11,r9,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmaddfp v31,v12,v4,v0
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v30,v11,v5
	_mm_store_ps(v30.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v5.f32)));
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// vmulfp128 v29,v11,v7
	_mm_store_ps(v29.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v7.f32)));
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// vmulfp128 v5,v10,v5
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v28,v11,v6
	_mm_store_ps(v28.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v26,v11,v9
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v9.f32)));
	// vctsxs v11,v4,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vmaddfp v4,v12,v2,v0
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(ctx.v0.f32)));
	// vctsxs v2,v2,0
	_mm_store_si128((__m128i*)ctx.v2.s32, _mm_vctsxs(_mm_load_ps(ctx.v2.f32)));
	// vmaddfp v7,v10,v6,v30
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(v30.f32)));
	// vmaddfp v10,v10,v9,v29
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(v29.f32)));
	// vsubfp v9,v28,v5
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(v28.f32), _mm_load_ps(ctx.v5.f32)));
	// vmaddfp v5,v12,v3,v0
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v0,v12,v1,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v1.f32)), _mm_load_ps(ctx.v0.f32)));
	// vctsxs v12,v31,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(v31.f32)));
	// vsubfp v6,v26,v27
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(v26.f32), _mm_load_ps(v27.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v10,v10,0
	_mm_store_si128((__m128i*)ctx.v10.s32, _mm_vctsxs(_mm_load_ps(ctx.v10.f32)));
	// vctsxs v9,v9,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vctsxs v31,v5,0
	_mm_store_si128((__m128i*)v31.s32, _mm_vctsxs(_mm_load_ps(ctx.v5.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vctsxs v5,v4,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v4,v3,0
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_vctsxs(_mm_load_ps(ctx.v3.f32)));
	// vctsxs v6,v6,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vpkswss v12,v12,v31
	// vpkswss v0,v5,v0
	// vctsxs v5,v1,0
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_vctsxs(_mm_load_ps(ctx.v1.f32)));
	// vpkswss v11,v11,v4
	// vsrah v11,v11,v8
	// vadduhm v12,v12,v11
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vpkswss v11,v7,v10
	// vpkswss v10,v9,v6
	// vpkswss v5,v2,v5
	// vsrah v5,v5,v8
	// vadduhm v8,v12,v10
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_load_si128((__m128i*)ctx.v10.u16)));
	// vsubuhm v12,v12,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vadduhm v0,v0,v5
	_mm_store_si128((__m128i*)ctx.v0.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vsrah v10,v8,v13
	// vsrah v12,v12,v13
	// vadduhm v9,v0,v11
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v0.u16), _mm_load_si128((__m128i*)ctx.v11.u16)));
	// vsubuhm v0,v0,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsrah v11,v9,v13
	// vsrah v0,v0,v13
	// stvx v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82656C08"))) PPC_WEAK_FUNC(sub_82656C08);
PPC_FUNC_IMPL(__imp__sub_82656C08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	uint32_t ea{};
	// rlwinm r11,r6,3,0,28
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltish v9,4
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// vspltish v3,1
	// li r10,16
	ctx.r10.s64 = 16;
	// vspltish v11,3
	// li r9,48
	ctx.r9.s64 = 48;
	// vspltisw v0,1
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x1)));
	// li r8,32
	ctx.r8.s64 = 32;
	// vspltisw v13,6
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_set1_epi32(int(0x6)));
	// stw r7,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r7.u32);
	// li r7,48
	ctx.r7.s64 = 48;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v8,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r10,32
	ctx.r10.s64 = 32;
	// lvrx v6,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,16
	ctx.r9.s64 = 16;
	// vor v7,v7,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v8,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,64
	ctx.r8.s64 = 64;
	// vor v6,v8,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// lvlx v4,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltisw v12,2
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_set1_epi32(int(0x2)));
	// lvrx v8,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vspltisw v10,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_set1_epi32(int(0x3)));
	// lvlx v5,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v8,v5,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvrx v5,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v5,v4,v5
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v4,v7,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubuhm v7,v7,v6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vadduhm v6,v8,v5
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v5.u16)));
	// vslh v2,v4,v9
	// vslh v1,v7,v9
	// vadduhm v4,v2,v4
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v2.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v2,v1,v7
	_mm_store_si128((__m128i*)ctx.v2.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v1.u16), _mm_load_si128((__m128i*)ctx.v7.u16)));
	// vadduhm v7,v9,v4
	_mm_store_si128((__m128i*)ctx.v7.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vslh v4,v6,v3
	// vspltish v3,5
	// vslh v6,v6,v11
	// vadduhm v9,v9,v2
	_mm_store_si128((__m128i*)ctx.v9.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v2.u16)));
	// vslh v5,v5,v3
	// vadduhm v6,v6,v4
	_mm_store_si128((__m128i*)ctx.v6.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vspltish v4,2
	// vslh v4,v8,v4
	// vslh v8,v8,v11
	// vadduhm v8,v8,v4
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v8.u16), _mm_load_si128((__m128i*)ctx.v4.u16)));
	// vadduhm v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v6.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vsubuhm v6,v6,v5
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8)));
	// vadduhm v5,v7,v8
	_mm_store_si128((__m128i*)ctx.v5.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v7.u16), _mm_load_si128((__m128i*)ctx.v8.u16)));
	// vadduhm v4,v9,v6
	_mm_store_si128((__m128i*)ctx.v4.u16, _mm_add_epi16(_mm_load_si128((__m128i*)ctx.v9.u16), _mm_load_si128((__m128i*)ctx.v6.u16)));
	// vsubuhm v6,v9,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vsubuhm v3,v7,v8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_sub_epi16(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsrah v9,v5,v11
	// vsrah v8,v4,v11
	// vsrah v7,v6,v11
	// vsrah v11,v3,v11
	// vupkhsh v6,v9
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16))));
	// vupkhsh v5,v8
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16))));
	// vupkhsh v4,v7
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16))));
	// vupkhsh v3,v11
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v11.s16), _mm_load_si128((__m128i*)ctx.v11.s16))));
	// vor v2,v11,v11
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v11,v6,v6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v6.u8));
	// vupklsh v6,v9
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v9.s16)));
	// vor v9,v5,v5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v5.u8));
	// vupklsh v5,v8
	_mm_store_si128((__m128i*)ctx.v5.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v8.s16)));
	// vor v8,v4,v4
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// vupklsh v4,v7
	_mm_store_si128((__m128i*)ctx.v4.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v7.s16)));
	// vor v7,v3,v3
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// vupklsh v3,v2
	_mm_store_si128((__m128i*)ctx.v3.s32, _mm_cvtepi16_epi32(_mm_load_si128((__m128i*)ctx.v2.s16)));
	// vmrghw v2,v11,v8
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vmrghw v1,v9,v7
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrglw v29,v9,v7
	_mm_store_si128((__m128i*)v29.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrglw v9,v6,v4
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrglw v11,v11,v8
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vspltisw v7,5
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_set1_epi32(int(0x5)));
	// vmrglw v31,v5,v3
	_mm_store_si128((__m128i*)v31.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrghw v4,v6,v4
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrglw v8,v2,v1
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghw v2,v2,v1
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vslw v1,v0,v7
	ctx.v1.u32[0] = ctx.v0.u32[0] << (ctx.v7.u8[0] & 0x1F);
	ctx.v1.u32[1] = ctx.v0.u32[1] << (ctx.v7.u8[4] & 0x1F);
	ctx.v1.u32[2] = ctx.v0.u32[2] << (ctx.v7.u8[8] & 0x1F);
	ctx.v1.u32[3] = ctx.v0.u32[3] << (ctx.v7.u8[12] & 0x1F);
	// vmrghw v30,v5,v3
	_mm_store_si128((__m128i*)v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v5.u32)));
	// vmrghw v6,v11,v29
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v29.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vmrglw v5,v11,v29
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v29.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vslw v29,v8,v0
	v29.u32[0] = ctx.v8.u32[0] << (ctx.v0.u8[0] & 0x1F);
	v29.u32[1] = ctx.v8.u32[1] << (ctx.v0.u8[4] & 0x1F);
	v29.u32[2] = ctx.v8.u32[2] << (ctx.v0.u8[8] & 0x1F);
	v29.u32[3] = ctx.v8.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vslw v11,v2,v0
	ctx.v11.u32[0] = ctx.v2.u32[0] << (ctx.v0.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v2.u32[1] << (ctx.v0.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v2.u32[2] << (ctx.v0.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v2.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vmrglw v7,v9,v31
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vslw v2,v2,v12
	ctx.v2.u32[0] = ctx.v2.u32[0] << (ctx.v12.u8[0] & 0x1F);
	ctx.v2.u32[1] = ctx.v2.u32[1] << (ctx.v12.u8[4] & 0x1F);
	ctx.v2.u32[2] = ctx.v2.u32[2] << (ctx.v12.u8[8] & 0x1F);
	ctx.v2.u32[3] = ctx.v2.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vmrghw v3,v9,v31
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v31.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrghw v9,v4,v30
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)ctx.v4.u32)));
	// vslw v21,v8,v12
	v21.u32[0] = ctx.v8.u32[0] << (ctx.v12.u8[0] & 0x1F);
	v21.u32[1] = ctx.v8.u32[1] << (ctx.v12.u8[4] & 0x1F);
	v21.u32[2] = ctx.v8.u32[2] << (ctx.v12.u8[8] & 0x1F);
	v21.u32[3] = ctx.v8.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vmrglw v4,v4,v30
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)v30.u32), _mm_load_si128((__m128i*)ctx.v4.u32)));
	// vslw v27,v5,v10
	v27.u32[0] = ctx.v5.u32[0] << (ctx.v10.u8[0] & 0x1F);
	v27.u32[1] = ctx.v5.u32[1] << (ctx.v10.u8[4] & 0x1F);
	v27.u32[2] = ctx.v5.u32[2] << (ctx.v10.u8[8] & 0x1F);
	v27.u32[3] = ctx.v5.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vslw v30,v7,v10
	v30.u32[0] = ctx.v7.u32[0] << (ctx.v10.u8[0] & 0x1F);
	v30.u32[1] = ctx.v7.u32[1] << (ctx.v10.u8[4] & 0x1F);
	v30.u32[2] = ctx.v7.u32[2] << (ctx.v10.u8[8] & 0x1F);
	v30.u32[3] = ctx.v7.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vaddsws v11,v2,v11
	// vslw v24,v7,v12
	v24.u32[0] = ctx.v7.u32[0] << (ctx.v12.u8[0] & 0x1F);
	v24.u32[1] = ctx.v7.u32[1] << (ctx.v12.u8[4] & 0x1F);
	v24.u32[2] = ctx.v7.u32[2] << (ctx.v12.u8[8] & 0x1F);
	v24.u32[3] = ctx.v7.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vslw v20,v7,v0
	v20.u32[0] = ctx.v7.u32[0] << (ctx.v0.u8[0] & 0x1F);
	v20.u32[1] = ctx.v7.u32[1] << (ctx.v0.u8[4] & 0x1F);
	v20.u32[2] = ctx.v7.u32[2] << (ctx.v0.u8[8] & 0x1F);
	v20.u32[3] = ctx.v7.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vslw v23,v4,v12
	v23.u32[0] = ctx.v4.u32[0] << (ctx.v12.u8[0] & 0x1F);
	v23.u32[1] = ctx.v4.u32[1] << (ctx.v12.u8[4] & 0x1F);
	v23.u32[2] = ctx.v4.u32[2] << (ctx.v12.u8[8] & 0x1F);
	v23.u32[3] = ctx.v4.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vaddsws v2,v11,v1
	// vslw v1,v9,v0
	ctx.v1.u32[0] = ctx.v9.u32[0] << (ctx.v0.u8[0] & 0x1F);
	ctx.v1.u32[1] = ctx.v9.u32[1] << (ctx.v0.u8[4] & 0x1F);
	ctx.v1.u32[2] = ctx.v9.u32[2] << (ctx.v0.u8[8] & 0x1F);
	ctx.v1.u32[3] = ctx.v9.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vslw v9,v9,v12
	ctx.v9.u32[0] = ctx.v9.u32[0] << (ctx.v12.u8[0] & 0x1F);
	ctx.v9.u32[1] = ctx.v9.u32[1] << (ctx.v12.u8[4] & 0x1F);
	ctx.v9.u32[2] = ctx.v9.u32[2] << (ctx.v12.u8[8] & 0x1F);
	ctx.v9.u32[3] = ctx.v9.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vaddsws v11,v8,v7
	// vaddsws v7,v30,v24
	// vaddsws v24,v29,v8
	// vaddsws v1,v9,v1
	// vslw v9,v11,v10
	ctx.v9.u32[0] = ctx.v11.u32[0] << (ctx.v10.u8[0] & 0x1F);
	ctx.v9.u32[1] = ctx.v11.u32[1] << (ctx.v10.u8[4] & 0x1F);
	ctx.v9.u32[2] = ctx.v11.u32[2] << (ctx.v10.u8[8] & 0x1F);
	ctx.v9.u32[3] = ctx.v11.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vaddsws v28,v11,v11
	// vaddsws v29,v21,v29
	// vslw v22,v5,v0
	v22.u32[0] = ctx.v5.u32[0] << (ctx.v0.u8[0] & 0x1F);
	v22.u32[1] = ctx.v5.u32[1] << (ctx.v0.u8[4] & 0x1F);
	v22.u32[2] = ctx.v5.u32[2] << (ctx.v0.u8[8] & 0x1F);
	v22.u32[3] = ctx.v5.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vsubsws v31,v9,v11
	temp.s64 = int64_t(ctx.v9.s32[0]) - int64_t(ctx.v11.s32[0]);
	v31.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[1]) - int64_t(ctx.v11.s32[1]);
	v31.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[2]) - int64_t(ctx.v11.s32[2]);
	v31.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[3]) - int64_t(ctx.v11.s32[3]);
	v31.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v9,v4,v5
	// vslw v5,v5,v12
	ctx.v5.u32[0] = ctx.v5.u32[0] << (ctx.v12.u8[0] & 0x1F);
	ctx.v5.u32[1] = ctx.v5.u32[1] << (ctx.v12.u8[4] & 0x1F);
	ctx.v5.u32[2] = ctx.v5.u32[2] << (ctx.v12.u8[8] & 0x1F);
	ctx.v5.u32[3] = ctx.v5.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vslw v12,v3,v12
	ctx.v12.u32[0] = ctx.v3.u32[0] << (ctx.v12.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v3.u32[1] << (ctx.v12.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v3.u32[2] << (ctx.v12.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v3.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vor v26,v31,v31
	_mm_store_si128((__m128i*)v26.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v25,v31,v31
	_mm_store_si128((__m128i*)v25.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v31,v28,v28
	_mm_store_si128((__m128i*)v31.u8, _mm_load_si128((__m128i*)v28.u8));
	// vslw v28,v4,v0
	v28.u32[0] = ctx.v4.u32[0] << (ctx.v0.u8[0] & 0x1F);
	v28.u32[1] = ctx.v4.u32[1] << (ctx.v0.u8[4] & 0x1F);
	v28.u32[2] = ctx.v4.u32[2] << (ctx.v0.u8[8] & 0x1F);
	v28.u32[3] = ctx.v4.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vsubsws v7,v26,v7
	temp.s64 = int64_t(v26.s32[0]) - int64_t(ctx.v7.s32[0]);
	ctx.v7.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v26.s32[1]) - int64_t(ctx.v7.s32[1]);
	ctx.v7.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v26.s32[2]) - int64_t(ctx.v7.s32[2]);
	ctx.v7.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v26.s32[3]) - int64_t(ctx.v7.s32[3]);
	ctx.v7.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v8,v9,v9
	// vor v19,v31,v31
	_mm_store_si128((__m128i*)v19.u8, _mm_load_si128((__m128i*)v31.u8));
	// vor v18,v31,v31
	_mm_store_si128((__m128i*)v18.u8, _mm_load_si128((__m128i*)v31.u8));
	// vsubsws v31,v25,v24
	temp.s64 = int64_t(v25.s32[0]) - int64_t(v24.s32[0]);
	v31.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v25.s32[1]) - int64_t(v24.s32[1]);
	v31.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v25.s32[2]) - int64_t(v24.s32[2]);
	v31.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v25.s32[3]) - int64_t(v24.s32[3]);
	v31.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v26,v23,v28
	// vaddsws v25,v30,v20
	// vslw v23,v9,v10
	v23.u32[0] = ctx.v9.u32[0] << (ctx.v10.u8[0] & 0x1F);
	v23.u32[1] = ctx.v9.u32[1] << (ctx.v10.u8[4] & 0x1F);
	v23.u32[2] = ctx.v9.u32[2] << (ctx.v10.u8[8] & 0x1F);
	v23.u32[3] = ctx.v9.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vaddsws v30,v19,v29
	// vaddsws v24,v27,v22
	// vsubsws v29,v18,v25
	temp.s64 = int64_t(v18.s32[0]) - int64_t(v25.s32[0]);
	v29.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v18.s32[1]) - int64_t(v25.s32[1]);
	v29.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v18.s32[2]) - int64_t(v25.s32[2]);
	v29.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v18.s32[3]) - int64_t(v25.s32[3]);
	v29.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsubsws v25,v23,v9
	temp.s64 = int64_t(v23.s32[0]) - int64_t(ctx.v9.s32[0]);
	v25.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v23.s32[1]) - int64_t(ctx.v9.s32[1]);
	v25.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v23.s32[2]) - int64_t(ctx.v9.s32[2]);
	v25.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v23.s32[3]) - int64_t(ctx.v9.s32[3]);
	v25.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v23,v28,v4
	// vor v4,v8,v8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v28,v8,v8
	_mm_store_si128((__m128i*)v28.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v8,v25,v25
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)v25.u8));
	// vaddsws v5,v27,v5
	// vaddsws v4,v4,v26
	// vsubsws v26,v28,v24
	temp.s64 = int64_t(v28.s32[0]) - int64_t(v24.s32[0]);
	v26.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v28.s32[1]) - int64_t(v24.s32[1]);
	v26.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v28.s32[2]) - int64_t(v24.s32[2]);
	v26.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(v28.s32[3]) - int64_t(v24.s32[3]);
	v26.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsubsws v5,v8,v5
	temp.s64 = int64_t(ctx.v8.s32[0]) - int64_t(ctx.v5.s32[0]);
	ctx.v5.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[1]) - int64_t(ctx.v5.s32[1]);
	ctx.v5.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[2]) - int64_t(ctx.v5.s32[2]);
	ctx.v5.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[3]) - int64_t(ctx.v5.s32[3]);
	ctx.v5.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsubsws v28,v7,v4
	temp.s64 = int64_t(ctx.v7.s32[0]) - int64_t(ctx.v4.s32[0]);
	v28.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[1]) - int64_t(ctx.v4.s32[1]);
	v28.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[2]) - int64_t(ctx.v4.s32[2]);
	v28.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[3]) - int64_t(ctx.v4.s32[3]);
	v28.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v7,v3,v6
	// vaddsws v31,v31,v26
	// vsubsws v4,v2,v1
	temp.s64 = int64_t(ctx.v2.s32[0]) - int64_t(ctx.v1.s32[0]);
	ctx.v4.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[1]) - int64_t(ctx.v1.s32[1]);
	ctx.v4.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[2]) - int64_t(ctx.v1.s32[2]);
	ctx.v4.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[3]) - int64_t(ctx.v1.s32[3]);
	ctx.v4.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v3,v12,v3
	// vslw v26,v7,v10
	v26.u32[0] = ctx.v7.u32[0] << (ctx.v10.u8[0] & 0x1F);
	v26.u32[1] = ctx.v7.u32[1] << (ctx.v10.u8[4] & 0x1F);
	v26.u32[2] = ctx.v7.u32[2] << (ctx.v10.u8[8] & 0x1F);
	v26.u32[3] = ctx.v7.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vaddsws v7,v2,v1
	// vslw v1,v6,v0
	ctx.v1.u32[0] = ctx.v6.u32[0] << (ctx.v0.u8[0] & 0x1F);
	ctx.v1.u32[1] = ctx.v6.u32[1] << (ctx.v0.u8[4] & 0x1F);
	ctx.v1.u32[2] = ctx.v6.u32[2] << (ctx.v0.u8[8] & 0x1F);
	ctx.v1.u32[3] = ctx.v6.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// vslw v10,v6,v10
	ctx.v10.u32[0] = ctx.v6.u32[0] << (ctx.v10.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v6.u32[1] << (ctx.v10.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v6.u32[2] << (ctx.v10.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v6.u32[3] << (ctx.v10.u8[12] & 0x1F);
	// vor v2,v26,v26
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v26.u8));
	// vsraw v12,v9,v0
	ctx.v12.s32[0] = ctx.v9.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v12.s32[1] = ctx.v9.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v12.s32[2] = ctx.v9.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v12.s32[3] = ctx.v9.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vaddsws v1,v1,v6
	// vsubsws v6,v8,v23
	temp.s64 = int64_t(ctx.v8.s32[0]) - int64_t(v23.s32[0]);
	ctx.v6.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[1]) - int64_t(v23.s32[1]);
	ctx.v6.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[2]) - int64_t(v23.s32[2]);
	ctx.v6.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[3]) - int64_t(v23.s32[3]);
	ctx.v6.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v0,v11,v0
	ctx.v0.s32[0] = ctx.v11.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v11.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v11.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v11.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsubsws v11,v2,v3
	temp.s64 = int64_t(ctx.v2.s32[0]) - int64_t(ctx.v3.s32[0]);
	ctx.v11.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[1]) - int64_t(ctx.v3.s32[1]);
	ctx.v11.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[2]) - int64_t(ctx.v3.s32[2]);
	ctx.v11.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[3]) - int64_t(ctx.v3.s32[3]);
	ctx.v11.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v10,v10,v1
	// vaddsws v8,v30,v6
	// vaddsws v6,v29,v5
	// vsubsws v10,v2,v10
	temp.s64 = int64_t(ctx.v2.s32[0]) - int64_t(ctx.v10.s32[0]);
	ctx.v10.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[1]) - int64_t(ctx.v10.s32[1]);
	ctx.v10.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[2]) - int64_t(ctx.v10.s32[2]);
	ctx.v10.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v2.s32[3]) - int64_t(ctx.v10.s32[3]);
	ctx.v10.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v9,v8,v12
	// vaddsws v12,v6,v12
	// vaddsws v8,v7,v11
	// vsubsws v11,v7,v11
	temp.s64 = int64_t(ctx.v7.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v11.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v11.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v11.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v11.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// li r10,4
	ctx.r10.s64 = 4;
	// vaddsws v5,v8,v9
	// li r8,4
	ctx.r8.s64 = 4;
	// vsubsws v7,v4,v10
	temp.s64 = int64_t(ctx.v4.s32[0]) - int64_t(ctx.v10.s32[0]);
	ctx.v7.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[1]) - int64_t(ctx.v10.s32[1]);
	ctx.v7.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[2]) - int64_t(ctx.v10.s32[2]);
	ctx.v7.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v4.s32[3]) - int64_t(ctx.v10.s32[3]);
	ctx.v7.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// li r7,4
	ctx.r7.s64 = 4;
	// vaddsws v10,v4,v10
	// li r6,4
	ctx.r6.s64 = 4;
	// vsubsws v3,v11,v12
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v12.s32[0]);
	ctx.v3.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v12.s32[1]);
	ctx.v3.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v12.s32[2]);
	ctx.v3.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v12.s32[3]);
	ctx.v3.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// li r5,4
	ctx.r5.s64 = 4;
	// vaddsws v4,v11,v12
	// li r4,4
	ctx.r4.s64 = 4;
	// vsraw v12,v5,v13
	ctx.v12.s32[0] = ctx.v5.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v12.s32[1] = ctx.v5.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v12.s32[2] = ctx.v5.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v12.s32[3] = ctx.v5.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// li r3,4
	ctx.r3.s64 = 4;
	// vaddsws v6,v28,v0
	// vaddsws v0,v31,v0
	// vsubsws v9,v8,v9
	temp.s64 = int64_t(ctx.v8.s32[0]) - int64_t(ctx.v9.s32[0]);
	ctx.v9.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[1]) - int64_t(ctx.v9.s32[1]);
	ctx.v9.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[2]) - int64_t(ctx.v9.s32[2]);
	ctx.v9.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v8.s32[3]) - int64_t(ctx.v9.s32[3]);
	ctx.v9.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vpkswss v12,v12,v12
	// vsraw v5,v3,v13
	ctx.v5.s32[0] = ctx.v3.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v5.s32[1] = ctx.v3.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v5.s32[2] = ctx.v3.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v5.s32[3] = ctx.v3.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vaddsws v8,v7,v6
	// vsraw v11,v4,v13
	ctx.v11.s32[0] = ctx.v4.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v11.s32[1] = ctx.v4.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v11.s32[2] = ctx.v4.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v11.s32[3] = ctx.v4.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsubsws v3,v10,v0
	temp.s64 = int64_t(ctx.v10.s32[0]) - int64_t(ctx.v0.s32[0]);
	ctx.v3.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[1]) - int64_t(ctx.v0.s32[1]);
	ctx.v3.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[2]) - int64_t(ctx.v0.s32[2]);
	ctx.v3.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[3]) - int64_t(ctx.v0.s32[3]);
	ctx.v3.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vaddsws v4,v10,v0
	// vsraw v0,v8,v13
	ctx.v0.s32[0] = ctx.v8.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v8.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v8.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v8.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsubsws v7,v7,v6
	temp.s64 = int64_t(ctx.v7.s32[0]) - int64_t(ctx.v6.s32[0]);
	ctx.v7.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[1]) - int64_t(ctx.v6.s32[1]);
	ctx.v7.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[2]) - int64_t(ctx.v6.s32[2]);
	ctx.v7.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v7.s32[3]) - int64_t(ctx.v6.s32[3]);
	ctx.v7.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vpkswss v11,v11,v11
	// stvewx v12,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// vsraw v10,v4,v13
	ctx.v10.s32[0] = ctx.v4.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v10.s32[1] = ctx.v4.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v10.s32[2] = ctx.v4.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v10.s32[3] = ctx.v4.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vpkswss v0,v0,v0
	// vsraw v8,v3,v13
	ctx.v8.s32[0] = ctx.v3.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v8.s32[1] = ctx.v3.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v8.s32[2] = ctx.v3.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v8.s32[3] = ctx.v3.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsraw v9,v9,v13
	ctx.v9.s32[0] = ctx.v9.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v9.s32[1] = ctx.v9.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v9.s32[2] = ctx.v9.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v9.s32[3] = ctx.v9.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsraw v13,v7,v13
	ctx.v13.s32[0] = ctx.v7.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v13.s32[1] = ctx.v7.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v13.s32[2] = ctx.v7.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v13.s32[3] = ctx.v7.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vpkswss v7,v5,v5
	// stvewx v12,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// vor v6,v0,v0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vor v5,v0,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vpkswss v10,v10,v10
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// vpkswss v8,v8,v8
	// vpkswss v0,v13,v13
	// vpkswss v13,v9,v9
	// stvewx v6,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v5,r9,r8
	ea = (ctx.r9.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v5.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v10,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v10,r9,r7
	ea = (ctx.r9.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v11,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v11,r9,r6
	ea = (ctx.r9.u32 + ctx.r6.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v7,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v7,r9,r5
	ea = (ctx.r9.u32 + ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v8,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v8,r9,r4
	ea = (ctx.r9.u32 + ctx.r4.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stvewx v0,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r9,r3
	ea = (ctx.r9.u32 + ctx.r3.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r10,4
	ctx.r10.s64 = 4;
	// stvewx v13,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82656FC4"))) PPC_WEAK_FUNC(sub_82656FC4);
PPC_FUNC_IMPL(__imp__sub_82656FC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82656FC8"))) PPC_WEAK_FUNC(sub_82656FC8);
PPC_FUNC_IMPL(__imp__sub_82656FC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32243
	r11.s64 = -2113077248;
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// li r9,-64
	ctx.r9.s64 = -64;
	// vspltisw v0,3
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x3)));
	// addi r11,r11,3344
	r11.s64 = r11.s64 + 3344;
	// vspltisw v10,1
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_set1_epi32(int(0x1)));
	// rlwinm r10,r6,2,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x8;
	// vspltisw v13,6
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_set1_epi32(int(0x6)));
	// li r8,32
	ctx.r8.s64 = 32;
	// li r7,16
	ctx.r7.s64 = 16;
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// clrlwi r9,r6,31
	ctx.r9.u64 = ctx.r6.u32 & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lvx128 v11,r8,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,16
	ctx.r9.s64 = 16;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// stw r10,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r10.u32);
	// lvrx v9,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,24
	ctx.r9.s64 = 24;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v7,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v8,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,8
	ctx.r8.s64 = 8;
	// vor v8,v8,v7
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// li r7,40
	ctx.r7.s64 = 40;
	// vupkhsh v9,v9
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v9.s16), _mm_load_si128((__m128i*)ctx.v9.s16))));
	// lvrx v7,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v5,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,-16
	ctx.r9.s64 = -16;
	// vupkhsh v8,v8
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v8.s16), _mm_load_si128((__m128i*)ctx.v8.s16))));
	// lvlx v6,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// lvrx v6,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vcfsx v9,v9,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vor v6,v5,v6
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v6.u8)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vupkhsh v7,v7
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v7.s16), _mm_load_si128((__m128i*)ctx.v7.s16))));
	// vupkhsh v6,v6
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_cvtepi16_epi32(_mm_unpackhi_epi64(_mm_load_si128((__m128i*)ctx.v6.s16), _mm_load_si128((__m128i*)ctx.v6.s16))));
	// vcfsx v7,v7,0
	_mm_store_ps(ctx.v7.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// vcfsx v6,v6,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v6.u32)));
	// vsubfp v5,v9,v8
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v8.f32)));
	// vaddfp v9,v9,v8
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v8.f32)));
	// vaddfp v3,v7,v6
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v6.f32)));
	// vmaddfp v8,v11,v5,v12
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v12.f32)));
	// lvx128 v5,r9,r11
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmaddfp v9,v11,v9,v12
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// li r9,48
	ctx.r9.s64 = 48;
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,16
	ctx.r9.s64 = 16;
	// vmulfp128 v6,v12,v6
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v6.f32)));
	// lvx128 v4,r9,r11
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,-32
	ctx.r9.s64 = -32;
	// vctsxs v11,v9,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vctsxs v9,v8,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v8,v5,v3
	_mm_store_ps(ctx.v8.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v3.f32)));
	// vmaddfp v7,v4,v7,v8
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v8.f32)));
	// vsubfp v6,v8,v6
	_mm_store_ps(ctx.v6.f32, _mm_sub_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v6.f32)));
	// vctsxs v7,v7,0
	_mm_store_si128((__m128i*)ctx.v7.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vctsxs v8,v6,0
	_mm_store_si128((__m128i*)ctx.v8.s32, _mm_vctsxs(_mm_load_ps(ctx.v6.f32)));
	// vaddsws v6,v11,v7
	// vsubsws v7,v11,v7
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v7.s32[0]);
	ctx.v7.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v7.s32[1]);
	ctx.v7.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v7.s32[2]);
	ctx.v7.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v7.s32[3]);
	ctx.v7.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v11,v6,v0
	ctx.v11.s32[0] = ctx.v6.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v11.s32[1] = ctx.v6.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v11.s32[2] = ctx.v6.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v11.s32[3] = ctx.v6.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vaddsws v6,v9,v8
	// vsubsws v8,v9,v8
	temp.s64 = int64_t(ctx.v9.s32[0]) - int64_t(ctx.v8.s32[0]);
	ctx.v8.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[1]) - int64_t(ctx.v8.s32[1]);
	ctx.v8.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[2]) - int64_t(ctx.v8.s32[2]);
	ctx.v8.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v9.s32[3]) - int64_t(ctx.v8.s32[3]);
	ctx.v8.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v7,v7,v0
	ctx.v7.s32[0] = ctx.v7.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v7.s32[1] = ctx.v7.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v7.s32[2] = ctx.v7.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v7.s32[3] = ctx.v7.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v9,v6,v0
	ctx.v9.s32[0] = ctx.v6.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v9.s32[1] = ctx.v6.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v9.s32[2] = ctx.v6.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v9.s32[3] = ctx.v6.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vsraw v0,v8,v0
	ctx.v0.s32[0] = ctx.v8.s32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v8.s32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v8.s32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v8.s32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vmrghw v8,v9,v7
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrghw v6,v11,v0
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vmrglw v0,v11,v0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), _mm_load_si128((__m128i*)ctx.v11.u32)));
	// vmrglw v11,v9,v7
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrghw v9,v6,v8
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrglw v8,v6,v8
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v6.u32)));
	// vmrghw v7,v0,v11
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// vmrglw v5,v0,v11
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// vcfsx v9,v9,0
	_mm_store_ps(ctx.v9.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v9.u32)));
	// vcfsx v8,v8,0
	_mm_store_ps(ctx.v8.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vcfsx v6,v7,0
	_mm_store_ps(ctx.v6.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v7.u32)));
	// lvx128 v7,r9,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v5,v5,0
	_mm_store_ps(ctx.v5.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v5.u32)));
	// li r9,-48
	ctx.r9.s64 = -48;
	// vsubfp v4,v9,v6
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v6.f32)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v9,v6,v9
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v9.f32)));
	// li r8,4
	ctx.r8.s64 = 4;
	// li r7,4
	ctx.r7.s64 = 4;
	// lvx128 v0,r9,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,4
	r11.s64 = 4;
	// vmulfp128 v3,v0,v8
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32)));
	// li r9,4
	ctx.r9.s64 = 4;
	// vmulfp128 v0,v0,v5
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v6,v11,v5
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v5.f32)));
	// vmaddfp v0,v11,v8,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v8,v7,v4,v12
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v7,v7,v9,v12
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)));
	// vctsxs v12,v9,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(ctx.v9.f32)));
	// vsubfp v11,v3,v6
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v6.f32)));
	// vctsxs v6,v4,0
	_mm_store_si128((__m128i*)ctx.v6.s32, _mm_vctsxs(_mm_load_ps(ctx.v4.f32)));
	// vctsxs v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vctsxs v9,v7,0
	_mm_store_si128((__m128i*)ctx.v9.s32, _mm_vctsxs(_mm_load_ps(ctx.v7.f32)));
	// vsraw v5,v12,v10
	ctx.v5.s32[0] = ctx.v12.s32[0] >> (ctx.v10.u8[0] & 0x1F);
	ctx.v5.s32[1] = ctx.v12.s32[1] >> (ctx.v10.u8[4] & 0x1F);
	ctx.v5.s32[2] = ctx.v12.s32[2] >> (ctx.v10.u8[8] & 0x1F);
	ctx.v5.s32[3] = ctx.v12.s32[3] >> (ctx.v10.u8[12] & 0x1F);
	// vctsxs v12,v11,0
	_mm_store_si128((__m128i*)ctx.v12.s32, _mm_vctsxs(_mm_load_ps(ctx.v11.f32)));
	// vctsxs v11,v8,0
	_mm_store_si128((__m128i*)ctx.v11.s32, _mm_vctsxs(_mm_load_ps(ctx.v8.f32)));
	// vsraw v10,v6,v10
	ctx.v10.s32[0] = ctx.v6.s32[0] >> (ctx.v10.u8[0] & 0x1F);
	ctx.v10.s32[1] = ctx.v6.s32[1] >> (ctx.v10.u8[4] & 0x1F);
	ctx.v10.s32[2] = ctx.v6.s32[2] >> (ctx.v10.u8[8] & 0x1F);
	ctx.v10.s32[3] = ctx.v6.s32[3] >> (ctx.v10.u8[12] & 0x1F);
	// vaddsws v11,v11,v10
	// vaddsws v10,v9,v5
	// vaddsws v9,v11,v12
	// vaddsws v8,v10,v0
	// vsubsws v11,v11,v12
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v12.s32[0]);
	ctx.v11.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v12.s32[1]);
	ctx.v11.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v12.s32[2]);
	ctx.v11.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v12.s32[3]);
	ctx.v11.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsubsws v0,v10,v0
	temp.s64 = int64_t(ctx.v10.s32[0]) - int64_t(ctx.v0.s32[0]);
	ctx.v0.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[1]) - int64_t(ctx.v0.s32[1]);
	ctx.v0.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[2]) - int64_t(ctx.v0.s32[2]);
	ctx.v0.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v10.s32[3]) - int64_t(ctx.v0.s32[3]);
	ctx.v0.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vsraw v9,v9,v13
	ctx.v9.s32[0] = ctx.v9.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v9.s32[1] = ctx.v9.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v9.s32[2] = ctx.v9.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v9.s32[3] = ctx.v9.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsraw v8,v8,v13
	ctx.v8.s32[0] = ctx.v8.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v8.s32[1] = ctx.v8.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v8.s32[2] = ctx.v8.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v8.s32[3] = ctx.v8.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsraw v11,v11,v13
	ctx.v11.s32[0] = ctx.v11.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v11.s32[1] = ctx.v11.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v11.s32[2] = ctx.v11.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v11.s32[3] = ctx.v11.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vsraw v0,v0,v13
	ctx.v0.s32[0] = ctx.v0.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v0.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v0.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v0.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vpkswss v12,v9,v9
	// vpkswss v8,v8,v8
	// vpkswss v13,v11,v11
	// vpkswss v0,v0,v0
	// stvewx v8,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// stvewx v8,r10,r11
	ea = (ctx.r10.u32 + r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// stvewx v12,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v12,r10,r9
	ea = (ctx.r10.u32 + ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
	// addi r10,r11,32
	ctx.r10.s64 = r11.s64 + 32;
	// addi r11,r11,48
	r11.s64 = r11.s64 + 48;
	// stvewx v13,r0,r10
	ea = (ctx.r10.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v13,r10,r8
	ea = (ctx.r10.u32 + ctx.r8.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r0,r11
	ea = (r11.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v0,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826571E0"))) PPC_WEAK_FUNC(sub_826571E0);
PPC_FUNC_IMPL(__imp__sub_826571E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r9,r4,32
	ctx.r9.s64 = ctx.r4.s64 + 32;
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// addi r8,r4,48
	ctx.r8.s64 = ctx.r4.s64 + 48;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// sth r11,-32(r1)
	PPC_STORE_U16(ctx.r1.u32 + -32, r11.u16);
	// addi r7,r1,-48
	ctx.r7.s64 = ctx.r1.s64 + -48;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// neg r7,r11
	ctx.r7.s64 = -r11.s64;
	// vspltw v0,v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// sth r7,-16(r1)
	PPC_STORE_U16(ctx.r1.u32 + -16, ctx.r7.u16);
	// stvx v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vcfsx v0,v0,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v13,v13,0
	_mm_store_si128((__m128i*)ctx.v13.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u16), _mm_set1_epi16(short(0xF0E))));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// stvx v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsplth v12,v12,0
	_mm_store_si128((__m128i*)ctx.v12.u16, _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u16), _mm_set1_epi16(short(0xF0E))));
	// stvx v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// stvx v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcfsx v13,v13,0
	_mm_store_ps(ctx.v13.f32, _mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v13.u32)));
	// stvx v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265725C"))) PPC_WEAK_FUNC(sub_8265725C);
PPC_FUNC_IMPL(__imp__sub_8265725C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82657260"))) PPC_WEAK_FUNC(sub_82657260);
PPC_FUNC_IMPL(__imp__sub_82657260) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,16
	r11.s64 = 16;
	// li r31,0
	r31.s64 = 0;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r9,8
	ctx.r9.s64 = 8;
	// li r23,1
	r23.s64 = 1;
	// li r29,2
	r29.s64 = 2;
	// stb r11,208(r1)
	PPC_STORE_U8(ctx.r1.u32 + 208, r11.u8);
	// li r30,5
	r30.s64 = 5;
	// stb r31,209(r1)
	PPC_STORE_U8(ctx.r1.u32 + 209, r31.u8);
	// li r3,6
	ctx.r3.s64 = 6;
	// stb r11,210(r1)
	PPC_STORE_U8(ctx.r1.u32 + 210, r11.u8);
	// li r4,9
	ctx.r4.s64 = 9;
	// stb r11,212(r1)
	PPC_STORE_U8(ctx.r1.u32 + 212, r11.u8);
	// li r5,10
	ctx.r5.s64 = 10;
	// stb r23,211(r1)
	PPC_STORE_U8(ctx.r1.u32 + 211, r23.u8);
	// li r10,12
	ctx.r10.s64 = 12;
	// stb r29,213(r1)
	PPC_STORE_U8(ctx.r1.u32 + 213, r29.u8);
	// li r7,14
	ctx.r7.s64 = 14;
	// stb r11,214(r1)
	PPC_STORE_U8(ctx.r1.u32 + 214, r11.u8);
	// li r24,3
	r24.s64 = 3;
	// stb r11,216(r1)
	PPC_STORE_U8(ctx.r1.u32 + 216, r11.u8);
	// li r25,7
	r25.s64 = 7;
	// stb r8,217(r1)
	PPC_STORE_U8(ctx.r1.u32 + 217, ctx.r8.u8);
	// li r26,11
	r26.s64 = 11;
	// stb r11,218(r1)
	PPC_STORE_U8(ctx.r1.u32 + 218, r11.u8);
	// li r6,13
	ctx.r6.s64 = 13;
	// stb r30,219(r1)
	PPC_STORE_U8(ctx.r1.u32 + 219, r30.u8);
	// li r27,15
	r27.s64 = 15;
	// stb r11,220(r1)
	PPC_STORE_U8(ctx.r1.u32 + 220, r11.u8);
	// li r28,17
	r28.s64 = 17;
	// stb r24,215(r1)
	PPC_STORE_U8(ctx.r1.u32 + 215, r24.u8);
	// li r17,20
	r17.s64 = 20;
	// stb r3,221(r1)
	PPC_STORE_U8(ctx.r1.u32 + 221, ctx.r3.u8);
	// li r18,21
	r18.s64 = 21;
	// stb r11,222(r1)
	PPC_STORE_U8(ctx.r1.u32 + 222, r11.u8);
	// li r19,24
	r19.s64 = 24;
	// stb r25,223(r1)
	PPC_STORE_U8(ctx.r1.u32 + 223, r25.u8);
	// stb r11,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, r11.u8);
	// li r20,25
	r20.s64 = 25;
	// stb r9,113(r1)
	PPC_STORE_U8(ctx.r1.u32 + 113, ctx.r9.u8);
	// stb r11,114(r1)
	PPC_STORE_U8(ctx.r1.u32 + 114, r11.u8);
	// stb r4,115(r1)
	PPC_STORE_U8(ctx.r1.u32 + 115, ctx.r4.u8);
	// stb r11,116(r1)
	PPC_STORE_U8(ctx.r1.u32 + 116, r11.u8);
	// stb r5,117(r1)
	PPC_STORE_U8(ctx.r1.u32 + 117, ctx.r5.u8);
	// stb r11,118(r1)
	PPC_STORE_U8(ctx.r1.u32 + 118, r11.u8);
	// stb r26,119(r1)
	PPC_STORE_U8(ctx.r1.u32 + 119, r26.u8);
	// stb r11,120(r1)
	PPC_STORE_U8(ctx.r1.u32 + 120, r11.u8);
	// stb r10,121(r1)
	PPC_STORE_U8(ctx.r1.u32 + 121, ctx.r10.u8);
	// stb r11,122(r1)
	PPC_STORE_U8(ctx.r1.u32 + 122, r11.u8);
	// stb r6,123(r1)
	PPC_STORE_U8(ctx.r1.u32 + 123, ctx.r6.u8);
	// stb r11,124(r1)
	PPC_STORE_U8(ctx.r1.u32 + 124, r11.u8);
	// stb r7,125(r1)
	PPC_STORE_U8(ctx.r1.u32 + 125, ctx.r7.u8);
	// stb r11,126(r1)
	PPC_STORE_U8(ctx.r1.u32 + 126, r11.u8);
	// stb r27,127(r1)
	PPC_STORE_U8(ctx.r1.u32 + 127, r27.u8);
	// stb r11,176(r1)
	PPC_STORE_U8(ctx.r1.u32 + 176, r11.u8);
	// stb r31,177(r1)
	PPC_STORE_U8(ctx.r1.u32 + 177, r31.u8);
	// stb r11,178(r1)
	PPC_STORE_U8(ctx.r1.u32 + 178, r11.u8);
	// stb r29,179(r1)
	PPC_STORE_U8(ctx.r1.u32 + 179, r29.u8);
	// stb r11,180(r1)
	PPC_STORE_U8(ctx.r1.u32 + 180, r11.u8);
	// stb r8,181(r1)
	PPC_STORE_U8(ctx.r1.u32 + 181, ctx.r8.u8);
	// stb r11,182(r1)
	PPC_STORE_U8(ctx.r1.u32 + 182, r11.u8);
	// stb r3,183(r1)
	PPC_STORE_U8(ctx.r1.u32 + 183, ctx.r3.u8);
	// stb r11,184(r1)
	PPC_STORE_U8(ctx.r1.u32 + 184, r11.u8);
	// stb r9,185(r1)
	PPC_STORE_U8(ctx.r1.u32 + 185, ctx.r9.u8);
	// stb r11,186(r1)
	PPC_STORE_U8(ctx.r1.u32 + 186, r11.u8);
	// stb r5,187(r1)
	PPC_STORE_U8(ctx.r1.u32 + 187, ctx.r5.u8);
	// stb r11,188(r1)
	PPC_STORE_U8(ctx.r1.u32 + 188, r11.u8);
	// stb r10,189(r1)
	PPC_STORE_U8(ctx.r1.u32 + 189, ctx.r10.u8);
	// stb r11,190(r1)
	PPC_STORE_U8(ctx.r1.u32 + 190, r11.u8);
	// stb r7,191(r1)
	PPC_STORE_U8(ctx.r1.u32 + 191, ctx.r7.u8);
	// stb r31,144(r1)
	PPC_STORE_U8(ctx.r1.u32 + 144, r31.u8);
	// stb r23,145(r1)
	PPC_STORE_U8(ctx.r1.u32 + 145, r23.u8);
	// stb r11,146(r1)
	PPC_STORE_U8(ctx.r1.u32 + 146, r11.u8);
	// stb r28,147(r1)
	PPC_STORE_U8(ctx.r1.u32 + 147, r28.u8);
	// stb r8,148(r1)
	PPC_STORE_U8(ctx.r1.u32 + 148, ctx.r8.u8);
	// stb r30,149(r1)
	PPC_STORE_U8(ctx.r1.u32 + 149, r30.u8);
	// stb r17,150(r1)
	PPC_STORE_U8(ctx.r1.u32 + 150, r17.u8);
	// stb r18,151(r1)
	PPC_STORE_U8(ctx.r1.u32 + 151, r18.u8);
	// stb r9,152(r1)
	PPC_STORE_U8(ctx.r1.u32 + 152, ctx.r9.u8);
	// stb r4,153(r1)
	PPC_STORE_U8(ctx.r1.u32 + 153, ctx.r4.u8);
	// stb r19,154(r1)
	PPC_STORE_U8(ctx.r1.u32 + 154, r19.u8);
	// li r15,18
	r15.s64 = 18;
	// stb r11,94(r1)
	PPC_STORE_U8(ctx.r1.u32 + 94, r11.u8);
	// stb r11,104(r1)
	PPC_STORE_U8(ctx.r1.u32 + 104, r11.u8);
	// li r21,28
	r21.s64 = 28;
	// stb r11,136(r1)
	PPC_STORE_U8(ctx.r1.u32 + 136, r11.u8);
	// li r11,18
	r11.s64 = 18;
	// stb r8,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, ctx.r8.u8);
	// li r22,29
	r22.s64 = 29;
	// stb r30,83(r1)
	PPC_STORE_U8(ctx.r1.u32 + 83, r30.u8);
	// li r14,22
	r14.s64 = 22;
	// stb r15,106(r1)
	PPC_STORE_U8(ctx.r1.u32 + 106, r15.u8);
	// li r15,19
	r15.s64 = 19;
	// stb r3,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, ctx.r3.u8);
	// lis r16,-32126
	r16.s64 = -2105409536;
	// stb r8,100(r1)
	PPC_STORE_U8(ctx.r1.u32 + 100, ctx.r8.u8);
	// stb r30,101(r1)
	PPC_STORE_U8(ctx.r1.u32 + 101, r30.u8);
	// addi r16,r16,-23440
	r16.s64 = r16.s64 + -23440;
	// stb r3,102(r1)
	PPC_STORE_U8(ctx.r1.u32 + 102, ctx.r3.u8);
	// stb r15,107(r1)
	PPC_STORE_U8(ctx.r1.u32 + 107, r15.u8);
	// li r15,23
	r15.s64 = 23;
	// stb r8,130(r1)
	PPC_STORE_U8(ctx.r1.u32 + 130, ctx.r8.u8);
	// li r8,30
	ctx.r8.s64 = 30;
	// stb r30,131(r1)
	PPC_STORE_U8(ctx.r1.u32 + 131, r30.u8);
	// li r30,26
	r30.s64 = 26;
	// stb r3,162(r1)
	PPC_STORE_U8(ctx.r1.u32 + 162, ctx.r3.u8);
	// li r3,27
	ctx.r3.s64 = 27;
	// stb r11,168(r1)
	PPC_STORE_U8(ctx.r1.u32 + 168, r11.u8);
	// li r11,19
	r11.s64 = 19;
	// stb r20,155(r1)
	PPC_STORE_U8(ctx.r1.u32 + 155, r20.u8);
	// stb r10,156(r1)
	PPC_STORE_U8(ctx.r1.u32 + 156, ctx.r10.u8);
	// stb r6,157(r1)
	PPC_STORE_U8(ctx.r1.u32 + 157, ctx.r6.u8);
	// stb r21,158(r1)
	PPC_STORE_U8(ctx.r1.u32 + 158, r21.u8);
	// stb r22,159(r1)
	PPC_STORE_U8(ctx.r1.u32 + 159, r22.u8);
	// stb r29,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r29.u8);
	// stb r24,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r24.u8);
	// stb r25,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, r25.u8);
	// stb r9,86(r1)
	PPC_STORE_U8(ctx.r1.u32 + 86, ctx.r9.u8);
	// stb r4,87(r1)
	PPC_STORE_U8(ctx.r1.u32 + 87, ctx.r4.u8);
	// stb r5,88(r1)
	PPC_STORE_U8(ctx.r1.u32 + 88, ctx.r5.u8);
	// stb r26,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, r26.u8);
	// stb r10,90(r1)
	PPC_STORE_U8(ctx.r1.u32 + 90, ctx.r10.u8);
	// stb r6,91(r1)
	PPC_STORE_U8(ctx.r1.u32 + 91, ctx.r6.u8);
	// stb r7,92(r1)
	PPC_STORE_U8(ctx.r1.u32 + 92, ctx.r7.u8);
	// stb r27,93(r1)
	PPC_STORE_U8(ctx.r1.u32 + 93, r27.u8);
	// stb r28,95(r1)
	PPC_STORE_U8(ctx.r1.u32 + 95, r28.u8);
	// stb r31,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r31.u8);
	// stb r23,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, r23.u8);
	// stb r29,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, r29.u8);
	// stb r24,99(r1)
	PPC_STORE_U8(ctx.r1.u32 + 99, r24.u8);
	// stb r25,103(r1)
	PPC_STORE_U8(ctx.r1.u32 + 103, r25.u8);
	// stb r28,105(r1)
	PPC_STORE_U8(ctx.r1.u32 + 105, r28.u8);
	// stb r17,108(r1)
	PPC_STORE_U8(ctx.r1.u32 + 108, r17.u8);
	// stb r18,109(r1)
	PPC_STORE_U8(ctx.r1.u32 + 109, r18.u8);
	// stb r14,110(r1)
	PPC_STORE_U8(ctx.r1.u32 + 110, r14.u8);
	// stb r15,111(r1)
	PPC_STORE_U8(ctx.r1.u32 + 111, r15.u8);
	// stb r31,128(r1)
	PPC_STORE_U8(ctx.r1.u32 + 128, r31.u8);
	// stb r23,129(r1)
	PPC_STORE_U8(ctx.r1.u32 + 129, r23.u8);
	// stb r9,132(r1)
	PPC_STORE_U8(ctx.r1.u32 + 132, ctx.r9.u8);
	// stb r4,133(r1)
	PPC_STORE_U8(ctx.r1.u32 + 133, ctx.r4.u8);
	// stb r10,134(r1)
	PPC_STORE_U8(ctx.r1.u32 + 134, ctx.r10.u8);
	// stb r6,135(r1)
	PPC_STORE_U8(ctx.r1.u32 + 135, ctx.r6.u8);
	// stb r28,137(r1)
	PPC_STORE_U8(ctx.r1.u32 + 137, r28.u8);
	// stb r17,138(r1)
	PPC_STORE_U8(ctx.r1.u32 + 138, r17.u8);
	// stb r18,139(r1)
	PPC_STORE_U8(ctx.r1.u32 + 139, r18.u8);
	// stb r19,140(r1)
	PPC_STORE_U8(ctx.r1.u32 + 140, r19.u8);
	// stb r20,141(r1)
	PPC_STORE_U8(ctx.r1.u32 + 141, r20.u8);
	// stb r21,142(r1)
	PPC_STORE_U8(ctx.r1.u32 + 142, r21.u8);
	// stb r22,143(r1)
	PPC_STORE_U8(ctx.r1.u32 + 143, r22.u8);
	// stb r29,160(r1)
	PPC_STORE_U8(ctx.r1.u32 + 160, r29.u8);
	// stb r24,161(r1)
	PPC_STORE_U8(ctx.r1.u32 + 161, r24.u8);
	// stb r25,163(r1)
	PPC_STORE_U8(ctx.r1.u32 + 163, r25.u8);
	// stb r5,164(r1)
	PPC_STORE_U8(ctx.r1.u32 + 164, ctx.r5.u8);
	// stb r26,165(r1)
	PPC_STORE_U8(ctx.r1.u32 + 165, r26.u8);
	// stb r7,166(r1)
	PPC_STORE_U8(ctx.r1.u32 + 166, ctx.r7.u8);
	// stb r27,167(r1)
	PPC_STORE_U8(ctx.r1.u32 + 167, r27.u8);
	// stb r11,169(r1)
	PPC_STORE_U8(ctx.r1.u32 + 169, r11.u8);
	// stb r14,170(r1)
	PPC_STORE_U8(ctx.r1.u32 + 170, r14.u8);
	// stb r15,171(r1)
	PPC_STORE_U8(ctx.r1.u32 + 171, r15.u8);
	// stb r30,172(r1)
	PPC_STORE_U8(ctx.r1.u32 + 172, r30.u8);
	// stb r3,173(r1)
	PPC_STORE_U8(ctx.r1.u32 + 173, ctx.r3.u8);
	// stb r8,174(r1)
	PPC_STORE_U8(ctx.r1.u32 + 174, ctx.r8.u8);
	// li r11,31
	r11.s64 = 31;
	// stb r3,203(r1)
	PPC_STORE_U8(ctx.r1.u32 + 203, ctx.r3.u8);
	// stb r20,201(r1)
	PPC_STORE_U8(ctx.r1.u32 + 201, r20.u8);
	// lis r20,-32126
	r20.s64 = -2105409536;
	// stb r21,204(r1)
	PPC_STORE_U8(ctx.r1.u32 + 204, r21.u8);
	// lis r21,-32126
	r21.s64 = -2105409536;
	// stb r22,205(r1)
	PPC_STORE_U8(ctx.r1.u32 + 205, r22.u8);
	// lis r22,-32126
	r22.s64 = -2105409536;
	// lis r23,-32126
	r23.s64 = -2105409536;
	// stb r26,195(r1)
	PPC_STORE_U8(ctx.r1.u32 + 195, r26.u8);
	// stb r11,175(r1)
	PPC_STORE_U8(ctx.r1.u32 + 175, r11.u8);
	// lis r24,-32126
	r24.s64 = -2105409536;
	// stb r11,207(r1)
	PPC_STORE_U8(ctx.r1.u32 + 207, r11.u8);
	// li r11,255
	r11.s64 = 255;
	// lis r25,-32126
	r25.s64 = -2105409536;
	// stb r27,199(r1)
	PPC_STORE_U8(ctx.r1.u32 + 199, r27.u8);
	// lis r26,-32126
	r26.s64 = -2105409536;
	// stb r10,196(r1)
	PPC_STORE_U8(ctx.r1.u32 + 196, ctx.r10.u8);
	// lis r27,-32126
	r27.s64 = -2105409536;
	// stb r30,202(r1)
	PPC_STORE_U8(ctx.r1.u32 + 202, r30.u8);
	// lis r28,-32126
	r28.s64 = -2105409536;
	// stb r4,193(r1)
	PPC_STORE_U8(ctx.r1.u32 + 193, ctx.r4.u8);
	// stb r11,226(r1)
	PPC_STORE_U8(ctx.r1.u32 + 226, r11.u8);
	// lis r29,-32126
	r29.s64 = -2105409536;
	// stb r11,227(r1)
	PPC_STORE_U8(ctx.r1.u32 + 227, r11.u8);
	// lis r10,-32126
	ctx.r10.s64 = -2105409536;
	// stb r11,228(r1)
	PPC_STORE_U8(ctx.r1.u32 + 228, r11.u8);
	// lis r30,-32126
	r30.s64 = -2105409536;
	// stb r11,229(r1)
	PPC_STORE_U8(ctx.r1.u32 + 229, r11.u8);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// stb r11,230(r1)
	PPC_STORE_U8(ctx.r1.u32 + 230, r11.u8);
	// stb r11,231(r1)
	PPC_STORE_U8(ctx.r1.u32 + 231, r11.u8);
	// stb r11,232(r1)
	PPC_STORE_U8(ctx.r1.u32 + 232, r11.u8);
	// stb r11,233(r1)
	PPC_STORE_U8(ctx.r1.u32 + 233, r11.u8);
	// stb r11,234(r1)
	PPC_STORE_U8(ctx.r1.u32 + 234, r11.u8);
	// stb r11,235(r1)
	PPC_STORE_U8(ctx.r1.u32 + 235, r11.u8);
	// stb r11,236(r1)
	PPC_STORE_U8(ctx.r1.u32 + 236, r11.u8);
	// stb r11,237(r1)
	PPC_STORE_U8(ctx.r1.u32 + 237, r11.u8);
	// stb r11,238(r1)
	PPC_STORE_U8(ctx.r1.u32 + 238, r11.u8);
	// stb r11,239(r1)
	PPC_STORE_U8(ctx.r1.u32 + 239, r11.u8);
	// addi r11,r16,15
	r11.s64 = r16.s64 + 15;
	// stb r5,194(r1)
	PPC_STORE_U8(ctx.r1.u32 + 194, ctx.r5.u8);
	// li r5,16
	ctx.r5.s64 = 16;
	// rlwinm r11,r11,0,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF0;
	// stb r9,192(r1)
	PPC_STORE_U8(ctx.r1.u32 + 192, ctx.r9.u8);
	// stb r6,197(r1)
	PPC_STORE_U8(ctx.r1.u32 + 197, ctx.r6.u8);
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stb r7,198(r1)
	PPC_STORE_U8(ctx.r1.u32 + 198, ctx.r7.u8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stb r19,200(r1)
	PPC_STORE_U8(ctx.r1.u32 + 200, r19.u8);
	// stb r8,206(r1)
	PPC_STORE_U8(ctx.r1.u32 + 206, ctx.r8.u8);
	// stb r31,224(r1)
	PPC_STORE_U8(ctx.r1.u32 + 224, r31.u8);
	// stb r31,225(r1)
	PPC_STORE_U8(ctx.r1.u32 + 225, r31.u8);
	// stw r3,3360(r10)
	PPC_STORE_U32(ctx.r10.u32 + 3360, ctx.r3.u32);
	// stw r11,3368(r20)
	PPC_STORE_U32(r20.u32 + 3368, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3340(r21)
	PPC_STORE_U32(r21.u32 + 3340, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3344(r22)
	PPC_STORE_U32(r22.u32 + 3344, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3376(r23)
	PPC_STORE_U32(r23.u32 + 3376, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3356(r24)
	PPC_STORE_U32(r24.u32 + 3356, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3332(r25)
	PPC_STORE_U32(r25.u32 + 3332, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3348(r26)
	PPC_STORE_U32(r26.u32 + 3348, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3336(r27)
	PPC_STORE_U32(r27.u32 + 3336, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3352(r28)
	PPC_STORE_U32(r28.u32 + 3352, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3372(r29)
	PPC_STORE_U32(r29.u32 + 3372, r11.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r11,3364(r30)
	PPC_STORE_U32(r30.u32 + 3364, r11.u32);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r3,3368(r20)
	ctx.r3.u64 = PPC_LOAD_U32(r20.u32 + 3368);
	// li r5,16
	ctx.r5.s64 = 16;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r3,3340(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 3340);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r5,16
	ctx.r5.s64 = 16;
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3344(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 3344);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3376(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + 3376);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3356(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 3356);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3332(r25)
	ctx.r3.u64 = PPC_LOAD_U32(r25.u32 + 3332);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3348(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 3348);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,3336(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 3336);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// lwz r11,3372(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 3372);
	// lis r10,128
	ctx.r10.s64 = 8388608;
	// lis r9,128
	ctx.r9.s64 = 8388608;
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// ori r9,r9,128
	ctx.r9.u64 = ctx.r9.u64 | 128;
	// std r31,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r31.u64);
	// rldimi r10,r10,32,0
	ctx.r10.u64 = (__builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r10.u64 & 0xFFFFFFFF);
	// std r31,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r31.u64);
	// rldimi r9,r9,32,0
	ctx.r9.u64 = (__builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r9.u64 & 0xFFFFFFFF);
	// lwz r11,3364(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 3364);
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// li r5,16
	ctx.r5.s64 = 16;
	// std r10,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r10.u64);
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// lwz r3,3352(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 3352);
	// bl 0x8239ce50
	sub_8239CE50(ctx, base);
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_826576EC"))) PPC_WEAK_FUNC(sub_826576EC);
PPC_FUNC_IMPL(__imp__sub_826576EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826576F0"))) PPC_WEAK_FUNC(sub_826576F0);
PPC_FUNC_IMPL(__imp__sub_826576F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r6,-32158
	ctx.r6.s64 = -2107506688;
	// lis r7,-32158
	ctx.r7.s64 = -2107506688;
	// lis r8,-32158
	ctx.r8.s64 = -2107506688;
	// lis r9,-32158
	ctx.r9.s64 = -2107506688;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lwz r5,20056(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 20056);
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r6,r6,20024
	ctx.r6.s64 = ctx.r6.s64 + 20024;
	// addi r7,r7,20704
	ctx.r7.s64 = ctx.r7.s64 + 20704;
	// addi r8,r8,19136
	ctx.r8.s64 = ctx.r8.s64 + 19136;
	// addi r9,r9,15208
	ctx.r9.s64 = ctx.r9.s64 + 15208;
	// addi r10,r10,-1360
	ctx.r10.s64 = ctx.r10.s64 + -1360;
	// addi r11,r11,-520
	r11.s64 = r11.s64 + -520;
	// stw r6,3148(r31)
	PPC_STORE_U32(r31.u32 + 3148, ctx.r6.u32);
	// stw r7,3152(r31)
	PPC_STORE_U32(r31.u32 + 3152, ctx.r7.u32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// stw r8,3144(r31)
	PPC_STORE_U32(r31.u32 + 3144, ctx.r8.u32);
	// stw r9,3140(r31)
	PPC_STORE_U32(r31.u32 + 3140, ctx.r9.u32);
	// stw r10,15856(r31)
	PPC_STORE_U32(r31.u32 + 15856, ctx.r10.u32);
	// stw r11,15852(r31)
	PPC_STORE_U32(r31.u32 + 15852, r11.u32);
	// beq cr6,0x82657770
	if (cr6.eq) goto loc_82657770;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r10,r10,-3496
	ctx.r10.s64 = ctx.r10.s64 + -3496;
	// addi r11,r11,-2904
	r11.s64 = r11.s64 + -2904;
	// stw r10,15856(r31)
	PPC_STORE_U32(r31.u32 + 15856, ctx.r10.u32);
	// stw r11,15852(r31)
	PPC_STORE_U32(r31.u32 + 15852, r11.u32);
loc_82657770:
	// lis r11,-32153
	r11.s64 = -2107179008;
	// addi r11,r11,-11368
	r11.s64 = r11.s64 + -11368;
	// stw r11,3120(r31)
	PPC_STORE_U32(r31.u32 + 3120, r11.u32);
	// bl 0x82657260
	sub_82657260(ctx, base);
	// lwz r11,3956(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 3956);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x826577a4
	if (cr6.eq) goto loc_826577A4;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r10,r10,2040
	ctx.r10.s64 = ctx.r10.s64 + 2040;
	// addi r11,r11,3176
	r11.s64 = r11.s64 + 3176;
	// stw r10,15864(r31)
	PPC_STORE_U32(r31.u32 + 15864, ctx.r10.u32);
	// stw r11,15868(r31)
	PPC_STORE_U32(r31.u32 + 15868, r11.u32);
loc_826577A4:
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	r11.s64 = -2107572224;
	// addi r10,r10,4824
	ctx.r10.s64 = ctx.r10.s64 + 4824;
	// addi r11,r11,5456
	r11.s64 = r11.s64 + 5456;
	// stw r10,3200(r31)
	PPC_STORE_U32(r31.u32 + 3200, ctx.r10.u32);
	// stw r11,3204(r31)
	PPC_STORE_U32(r31.u32 + 3204, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_826577D0"))) PPC_WEAK_FUNC(sub_826577D0);
PPC_FUNC_IMPL(__imp__sub_826577D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplwi cr6,r5,1024
	cr6.compare<uint32_t>(ctx.r5.u32, 1024, xer);
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// blt cr6,0x826577f4
	if (cr6.lt) goto loc_826577F4;
	// li r27,1024
	r27.s64 = 1024;
loc_826577F4:
	// neg r11,r30
	r11.s64 = -r30.s64;
	// clrlwi r28,r11,25
	r28.u64 = r11.u32 & 0x7F;
	// cmplw cr6,r5,r28
	cr6.compare<uint32_t>(ctx.r5.u32, r28.u32, xer);
	// bge cr6,0x82657808
	if (!cr6.lt) goto loc_82657808;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_82657808:
	// subf r29,r28,r5
	r29.s64 = ctx.r5.s64 - r28.s64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8265782c
	if (cr6.eq) goto loc_8265782C;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
	// add r31,r28,r31
	r31.u64 = r28.u64 + r31.u64;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
loc_8265782C:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82657848
	if (cr6.eq) goto loc_82657848;
loc_82657838:
	// dcbt r11,r31
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// blt cr6,0x82657838
	if (cr6.lt) goto loc_82657838;
loc_82657848:
	// cmplwi cr6,r29,512
	cr6.compare<uint32_t>(r29.u32, 512, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// blt cr6,0x82657858
	if (cr6.lt) goto loc_82657858;
	// li r11,512
	r11.s64 = 512;
loc_82657858:
	// rlwinm r10,r11,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82657878
	if (cr6.eq) goto loc_82657878;
loc_82657868:
	// dcbzl r11,r30
	memset(base + ((r11.u32 + r30.u32) & ~127), 0, 128);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82657868
	if (cr6.lt) goto loc_82657868;
loc_82657878:
	// clrlwi r11,r31,28
	r11.u64 = r31.u32 & 0xF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82657974
	if (!cr6.eq) goto loc_82657974;
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// blt cr6,0x82657938
	if (cr6.lt) goto loc_82657938;
loc_8265788C:
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// ble cr6,0x8265789c
	if (!cr6.gt) goto loc_8265789C;
	// li r11,1024
	r11.s64 = 1024;
	// dcbt r11,r31
loc_8265789C:
	// cmplwi cr6,r29,640
	cr6.compare<uint32_t>(r29.u32, 640, xer);
	// ble cr6,0x826578ac
	if (!cr6.gt) goto loc_826578AC;
	// li r11,512
	r11.s64 = 512;
	// dcbzl r11,r30
	memset(base + ((r11.u32 + r30.u32) & ~127), 0, 128);
loc_826578AC:
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r31,32
	ctx.r10.s64 = r31.s64 + 32;
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r31,48
	ctx.r9.s64 = r31.s64 + 48;
	// addi r8,r31,64
	ctx.r8.s64 = r31.s64 + 64;
	// addi r7,r31,80
	ctx.r7.s64 = r31.s64 + 80;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r31,96
	ctx.r6.s64 = r31.s64 + 96;
	// addi r11,r31,112
	r11.s64 = r31.s64 + 112;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// lvx128 v9,r0,r7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r30,80
	ctx.r7.s64 = r30.s64 + 80;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r30,96
	ctx.r6.s64 = r30.s64 + 96;
	// addi r11,r30,16
	r11.s64 = r30.s64 + 16;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r30,112
	ctx.r5.s64 = r30.s64 + 112;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r29,-128
	r29.s64 = r29.s64 + -128;
	// stvx v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// stvx v9,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// stvx v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge cr6,0x8265788c
	if (!cr6.lt) goto loc_8265788C;
loc_82657938:
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// blt cr6,0x82657ab0
	if (cr6.lt) goto loc_82657AB0;
	// rlwinm r28,r29,28,4,31
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r5,r28,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
	// rlwinm r11,r28,4,0,27
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
loc_82657960:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82657960
	if (!cr6.eq) goto loc_82657960;
	// b 0x82657ab0
	goto loc_82657AB0;
loc_82657974:
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// blt cr6,0x82657a70
	if (cr6.lt) goto loc_82657A70;
loc_8265797C:
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// ble cr6,0x8265798c
	if (!cr6.gt) goto loc_8265798C;
	// li r11,1024
	r11.s64 = 1024;
	// dcbt r11,r31
loc_8265798C:
	// cmplwi cr6,r29,640
	cr6.compare<uint32_t>(r29.u32, 640, xer);
	// ble cr6,0x8265799c
	if (!cr6.gt) goto loc_8265799C;
	// li r11,512
	r11.s64 = 512;
	// dcbzl r11,r30
	memset(base + ((r11.u32 + r30.u32) & ~127), 0, 128);
loc_8265799C:
	// li r11,16
	r11.s64 = 16;
	// lvlx v13,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,16
	ctx.r9.s64 = 16;
	// li r10,32
	ctx.r10.s64 = 32;
	// addi r8,r30,48
	ctx.r8.s64 = r30.s64 + 48;
	// addi r7,r30,80
	ctx.r7.s64 = r30.s64 + 80;
	// lvrx v0,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,48
	r11.s64 = 48;
	// lvlx v12,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v13,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,64
	ctx.r9.s64 = 64;
	// lvlx v11,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r10,80
	ctx.r10.s64 = 80;
	// lvrx v12,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r30,96
	ctx.r6.s64 = r30.s64 + 96;
	// lvlx v10,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v11,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,96
	r11.s64 = 96;
	// lvlx v9,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,112
	ctx.r9.s64 = 112;
	// lvlx v8,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// li r10,128
	ctx.r10.s64 = 128;
	// lvlx v7,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r29,-128
	r29.s64 = r29.s64 + -128;
	// lvrx v9,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r30,16
	r11.s64 = r30.s64 + 16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r30,64
	ctx.r9.s64 = r30.s64 + 64;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r30,112
	r11.s64 = r30.s64 + 112;
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge cr6,0x8265797c
	if (!cr6.lt) goto loc_8265797C;
loc_82657A70:
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// blt cr6,0x82657ab0
	if (cr6.lt) goto loc_82657AB0;
	// rlwinm r11,r29,28,4,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
loc_82657A7C:
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lvrx v13,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bne cr6,0x82657a7c
	if (!cr6.eq) goto loc_82657A7C;
loc_82657AB0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82657ac8
	if (cr6.eq) goto loc_82657AC8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
loc_82657AC8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82657AD0"))) PPC_WEAK_FUNC(sub_82657AD0);
PPC_FUNC_IMPL(__imp__sub_82657AD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplwi cr6,r5,1024
	cr6.compare<uint32_t>(ctx.r5.u32, 1024, xer);
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// blt cr6,0x82657af4
	if (cr6.lt) goto loc_82657AF4;
	// li r27,1024
	r27.s64 = 1024;
loc_82657AF4:
	// neg r11,r30
	r11.s64 = -r30.s64;
	// clrlwi r28,r11,25
	r28.u64 = r11.u32 & 0x7F;
	// cmplw cr6,r5,r28
	cr6.compare<uint32_t>(ctx.r5.u32, r28.u32, xer);
	// bge cr6,0x82657b08
	if (!cr6.lt) goto loc_82657B08;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
loc_82657B08:
	// subf r29,r28,r5
	r29.s64 = ctx.r5.s64 - r28.s64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82657b2c
	if (cr6.eq) goto loc_82657B2C;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
	// add r31,r28,r31
	r31.u64 = r28.u64 + r31.u64;
	// add r30,r28,r30
	r30.u64 = r28.u64 + r30.u64;
loc_82657B2C:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82657b48
	if (cr6.eq) goto loc_82657B48;
loc_82657B38:
	// dcbt r11,r31
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// blt cr6,0x82657b38
	if (cr6.lt) goto loc_82657B38;
loc_82657B48:
	// clrlwi r11,r31,28
	r11.u64 = r31.u32 & 0xF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82657c34
	if (!cr6.eq) goto loc_82657C34;
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// blt cr6,0x82657bf8
	if (cr6.lt) goto loc_82657BF8;
loc_82657B5C:
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// ble cr6,0x82657b6c
	if (!cr6.gt) goto loc_82657B6C;
	// li r11,1024
	r11.s64 = 1024;
	// dcbt r11,r31
loc_82657B6C:
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r31,32
	ctx.r10.s64 = r31.s64 + 32;
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r31,48
	ctx.r9.s64 = r31.s64 + 48;
	// addi r8,r31,64
	ctx.r8.s64 = r31.s64 + 64;
	// addi r7,r31,80
	ctx.r7.s64 = r31.s64 + 80;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r31,96
	ctx.r6.s64 = r31.s64 + 96;
	// addi r11,r31,112
	r11.s64 = r31.s64 + 112;
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
	// lvx128 v10,r0,r8
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r30,48
	ctx.r9.s64 = r30.s64 + 48;
	// lvx128 v9,r0,r7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r30,64
	ctx.r8.s64 = r30.s64 + 64;
	// lvx128 v8,r0,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r30,80
	ctx.r7.s64 = r30.s64 + 80;
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r30,96
	ctx.r6.s64 = r30.s64 + 96;
	// addi r11,r30,16
	r11.s64 = r30.s64 + 16;
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r30,112
	ctx.r5.s64 = r30.s64 + 112;
	// stvx v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r29,-128
	r29.s64 = r29.s64 + -128;
	// stvx v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// stvx v9,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// stvx v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge cr6,0x82657b5c
	if (!cr6.lt) goto loc_82657B5C;
loc_82657BF8:
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// blt cr6,0x82657d60
	if (cr6.lt) goto loc_82657D60;
	// rlwinm r28,r29,28,4,31
	r28.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm r5,r28,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
	// rlwinm r11,r28,4,0,27
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
loc_82657C20:
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82657c20
	if (!cr6.eq) goto loc_82657C20;
	// b 0x82657d60
	goto loc_82657D60;
loc_82657C34:
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// blt cr6,0x82657d20
	if (cr6.lt) goto loc_82657D20;
loc_82657C3C:
	// cmplwi cr6,r29,1024
	cr6.compare<uint32_t>(r29.u32, 1024, xer);
	// ble cr6,0x82657c4c
	if (!cr6.gt) goto loc_82657C4C;
	// li r11,1024
	r11.s64 = 1024;
	// dcbt r11,r31
loc_82657C4C:
	// li r11,16
	r11.s64 = 16;
	// lvlx v13,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,16
	ctx.r9.s64 = 16;
	// li r10,32
	ctx.r10.s64 = 32;
	// addi r8,r30,48
	ctx.r8.s64 = r30.s64 + 48;
	// addi r7,r30,80
	ctx.r7.s64 = r30.s64 + 80;
	// lvrx v0,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,48
	r11.s64 = 48;
	// lvlx v12,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvrx v13,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,64
	ctx.r9.s64 = 64;
	// lvlx v11,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v13,v12,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// li r10,80
	ctx.r10.s64 = 80;
	// lvrx v12,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r6,r30,96
	ctx.r6.s64 = r30.s64 + 96;
	// lvlx v10,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v12,v11,v12
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvrx v11,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r11,96
	r11.s64 = 96;
	// lvlx v9,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v11,v10,v11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// lvrx v10,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r9,112
	ctx.r9.s64 = 112;
	// lvlx v8,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v10,v9,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// li r10,128
	ctx.r10.s64 = 128;
	// lvlx v7,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r29,-128
	r29.s64 = r29.s64 + -128;
	// lvrx v9,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r11,r30,16
	r11.s64 = r30.s64 + 16;
	// vor v9,v8,v9
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// lvrx v8,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v8,v7,v8
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// lvlx v6,r31,r9
	temp.u32 = r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvrx v7,r31,r10
	temp.u32 = r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r9,r30,64
	ctx.r9.s64 = r30.s64 + 64;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
	// vor v7,v6,v7
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r30,112
	r11.s64 = r30.s64 + 112;
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,128
	r31.s64 = r31.s64 + 128;
	// stvx v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,128
	r30.s64 = r30.s64 + 128;
	// stvx v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,128
	cr6.compare<uint32_t>(r29.u32, 128, xer);
	// stvx v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v9,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx v7,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bge cr6,0x82657c3c
	if (!cr6.lt) goto loc_82657C3C;
loc_82657D20:
	// cmplwi cr6,r29,16
	cr6.compare<uint32_t>(r29.u32, 16, xer);
	// blt cr6,0x82657d60
	if (cr6.lt) goto loc_82657D60;
	// rlwinm r11,r29,28,4,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 28) & 0xFFFFFFF;
loc_82657D2C:
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// addi r29,r29,-16
	r29.s64 = r29.s64 + -16;
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lvrx v13,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v0,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// bne cr6,0x82657d2c
	if (!cr6.eq) goto loc_82657D2C;
loc_82657D60:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82657d78
	if (cr6.eq) goto loc_82657D78;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8239d2a8
	sub_8239D2A8(ctx, base);
loc_82657D78:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd44
	return;
}

__attribute__((alias("__imp__sub_82657D80"))) PPC_WEAK_FUNC(sub_82657D80);
PPC_FUNC_IMPL(__imp__sub_82657D80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r3,256(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82658018
	if (cr6.eq) goto loc_82658018;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// beq cr6,0x82657eb8
	if (cr6.eq) goto loc_82657EB8;
	// li r29,8
	r29.s64 = 8;
loc_82657DA4:
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lbz r9,3(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r10,2(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lbz r31,0(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lwz r5,-8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbz r8,1(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lwz r30,-4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82657df4
	if (!cr6.eq) goto loc_82657DF4;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82657e08
	goto loc_82657E08;
loc_82657DF4:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82657E08:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r5,20(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lbz r9,7(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r10,6(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbz r8,5(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lwz r30,12(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82657e70
	if (!cr6.eq) goto loc_82657E70;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82657e84
	goto loc_82657E84;
loc_82657E70:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82657E84:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82657da4
	if (!cr6.eq) goto loc_82657DA4;
	// b 0x8239bd34
	return;
loc_82657EB8:
	// li r26,8
	r26.s64 = 8;
loc_82657EBC:
	// lbz r10,1(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r9,2(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r8,3(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r27,4(r6)
	r27.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r27,r27,r8
	r27.u64 = r27.u64 + ctx.r8.u64;
	// lwz r29,4(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// lwz r31,-8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// lwz r30,-4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r9.s64 = r27.s32 >> 1;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82657f30
	if (!cr6.eq) goto loc_82657F30;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82657f44
	goto loc_82657F44;
loc_82657F30:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82657F44:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lbz r10,5(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r9,6(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// lbz r8,7(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r27,8(r6)
	r27.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r28,16(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r27,r27,r8
	r27.u64 = r27.u64 + ctx.r8.u64;
	// lwz r29,20(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// srawi r8,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// lwz r30,12(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r9.s64 = r27.s32 >> 1;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82657fd0
	if (!cr6.eq) goto loc_82657FD0;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82657fe4
	goto loc_82657FE4;
loc_82657FD0:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82657FE4:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// addi r26,r26,-1
	r26.s64 = r26.s64 + -1;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82657ebc
	if (!cr6.eq) goto loc_82657EBC;
	// b 0x8239bd34
	return;
loc_82658018:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r10,r5,8
	ctx.r10.s64 = ctx.r5.s64 + 8;
	// beq cr6,0x826581a0
	if (cr6.eq) goto loc_826581A0;
	// li r23,8
	r23.s64 = 8;
loc_82658028:
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lbz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r25,1(r6)
	r25.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r31,2(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r8,3(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lwz r27,0(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r24,0(r11)
	r24.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r26,1(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r24,r9,r24
	r24.u64 = ctx.r9.u64 + r24.u64;
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r26,r26,r25
	r26.u64 = r26.u64 + r25.u64;
	// add r25,r5,r31
	r25.u64 = ctx.r5.u64 + r31.u64;
	// lwz r28,4(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r30,-8(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// srawi r31,r24,1
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x1) != 0);
	r31.s64 = r24.s32 >> 1;
	// lwz r29,-4(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// srawi r5,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r5.s64 = r26.s32 >> 1;
	// srawi r9,r25,1
	xer.ca = (r25.s32 < 0) & ((r25.u32 & 0x1) != 0);
	ctx.r9.s64 = r25.s32 >> 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 + r29.u64;
	// or r30,r8,r9
	r30.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r30,r30,r5
	r30.u64 = r30.u64 | ctx.r5.u64;
	// or r30,r30,r31
	r30.u64 = r30.u64 | r31.u64;
	// rlwinm r30,r30,0,0,23
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x826580ac
	if (!cr6.eq) goto loc_826580AC;
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x826580c0
	goto loc_826580C0;
loc_826580AC:
	// lbzx r31,r31,r3
	r31.u64 = PPC_LOAD_U8(r31.u32 + ctx.r3.u32);
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// rotlwi r31,r31,8
	r31.u64 = __builtin_rotateleft32(r31.u32, 8);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
loc_826580C0:
	// or r5,r31,r5
	ctx.r5.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// or r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 | ctx.r9.u64;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// lbz r31,4(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r8,5(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbz r30,6(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r29,r9,r8
	r29.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r31,6(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lbz r8,7(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// lbz r9,7(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// srawi r6,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r6.s64 = r29.s32 >> 1;
	// lwz r28,16(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r29,20(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// srawi r9,r31,1
	xer.ca = (r31.s32 < 0) & ((r31.u32 & 0x1) != 0);
	ctx.r9.s64 = r31.s32 >> 1;
	// lwz r31,8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lwz r30,12(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// or r31,r8,r9
	r31.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r31,r31,r6
	r31.u64 = r31.u64 | ctx.r6.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82658158
	if (!cr6.eq) goto loc_82658158;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8265816c
	goto loc_8265816C;
loc_82658158:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
loc_8265816C:
	// or r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 | ctx.r6.u64;
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// or r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 | ctx.r9.u64;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stw r9,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r9.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82658028
	if (!cr6.eq) goto loc_82658028;
	// b 0x8239bd34
	return;
loc_826581A0:
	// li r9,8
	ctx.r9.s64 = 8;
loc_826581A4:
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r31,-8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// stb r5,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r5.u8);
	// lbz r30,2(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r31,2(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lwz r5,-4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r8.u8);
	// lbz r31,3(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lbz r8,3(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbz r31,2(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// lbzx r5,r5,r30
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r30.u32);
	// stb r5,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r5.u8);
	// lbz r30,4(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r31,4(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r8.u8);
	// lbz r31,5(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r30,4(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r31,8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// stb r5,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r5.u8);
	// lbz r31,6(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// lbz r30,6(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lwz r5,12(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r8.u8);
	// lbz r31,7(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lbz r8,7(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lbz r5,6(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbz r31,6(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lwz r31,16(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// stb r5,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r5.u8);
	// lbz r31,8(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// lbz r11,8(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r5,20(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// lbzx r11,r11,r3
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r3.u32);
	// stb r11,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, r11.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x826581a4
	if (!cr6.eq) goto loc_826581A4;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_82658324"))) PPC_WEAK_FUNC(sub_82658324);
PPC_FUNC_IMPL(__imp__sub_82658324) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82658328"))) PPC_WEAK_FUNC(sub_82658328);
PPC_FUNC_IMPL(__imp__sub_82658328) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce0
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82658600
	if (cr6.eq) goto loc_82658600;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82658364
	if (cr6.eq) goto loc_82658364;
	// li r11,8
	r11.s64 = 8;
loc_82658344:
	// ld r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// std r10,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r10.u64);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// bne cr6,0x82658344
	if (!cr6.eq) goto loc_82658344;
	// b 0x8239bd30
	return;
loc_82658364:
	// rlwinm r3,r6,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r3,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// add r27,r9,r5
	r27.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r11,r28,r5
	r11.u64 = r28.u64 + ctx.r5.u64;
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// add r31,r9,r4
	r31.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// addi r25,r6,-1
	r25.s64 = ctx.r6.s64 + -1;
	// subfic r29,r6,-1
	xer.ca = ctx.r6.u32 <= 4294967295;
	r29.s64 = -1 - ctx.r6.s64;
	// li r30,2
	r30.s64 = 2;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r27,1
	ctx.r9.s64 = r27.s64 + 1;
	// add r5,r28,r4
	ctx.r5.u64 = r28.u64 + ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_826583B0:
	// lwz r4,-1(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + -1);
	// lis r12,-129
	r12.s64 = -8454144;
	// lwz r28,0(r8)
	r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// and r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stw r4,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r4.u32);
	// lwzx r4,r29,r10
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + ctx.r10.u32);
	// lwzx r28,r8,r6
	r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// and r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stwx r4,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r4,-1(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + -1);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// and r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stw r4,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r4.u32);
	// lwzx r4,r10,r6
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// lwzx r28,r29,r9
	r28.u64 = PPC_LOAD_U32(r29.u32 + ctx.r9.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r4,r28
	r28.u64 = ctx.r4.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r4,r27,r26
	ctx.r4.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stwx r4,r3,r6
	PPC_STORE_U32(ctx.r3.u32 + ctx.r6.u32, ctx.r4.u32);
	// lwz r4,-1(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + -1);
	// lwz r28,0(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// and r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r4,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r4.u32);
	// lwzx r4,r9,r6
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r28,r29,r11
	r28.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r4,r28
	r28.u64 = ctx.r4.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r4,r27,r26
	ctx.r4.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stwx r4,r31,r6
	PPC_STORE_U32(r31.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwz r4,-1(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + -1);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r4,r28
	r28.u64 = ctx.r4.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r4,r27,r26
	ctx.r4.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stw r4,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r4.u32);
	// lwzx r4,r25,r11
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + r11.u32);
	// lwzx r28,r11,r6
	r28.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r4,r28
	r28.u64 = ctx.r4.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r4,r27,r26
	ctx.r4.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stwx r4,r5,r6
	PPC_STORE_U32(ctx.r5.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x826583b0
	if (!cr6.eq) goto loc_826583B0;
	// b 0x8239bd30
	return;
loc_82658600:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x826588a8
	if (cr6.eq) goto loc_826588A8;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r5
	ctx.r3.u64 = r11.u64 + ctx.r5.u64;
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// rlwinm r4,r6,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r5
	ctx.r8.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r4
	r31.u64 = ctx.r6.u64 + ctx.r4.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r6,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r6,r10
	ctx.r4.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r6,r29
	ctx.r10.s64 = r29.s64 - ctx.r6.s64;
	// add r9,r5,r6
	ctx.r9.u64 = ctx.r5.u64 + ctx.r6.u64;
	// li r30,2
	r30.s64 = 2;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
loc_82658658:
	// lwz r29,0(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r12,-129
	r12.s64 = -8454144;
	// lwz r28,0(r5)
	r28.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r29,r28
	r28.u64 = r29.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stwx r29,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, r29.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lwz r29,0(r8)
	r29.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r28,0(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// and r29,r29,r28
	r29.u64 = r29.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r27,r29
	r29.u64 = r27.u64 + r29.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r29.u32);
	// lwz r29,0(r7)
	r29.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r28,0(r8)
	r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// and r29,r29,r28
	r29.u64 = r29.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, r29.u32);
	// lwz r29,0(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r28,0(r7)
	r28.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// and r29,r29,r28
	r29.u64 = r29.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, r29.u32);
	// lwz r29,0(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r28,0(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// and r29,r29,r28
	r29.u64 = r29.u64 & r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stwx r29,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r29.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r28,0(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r29,r28
	r28.u64 = r29.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stwx r29,r11,r4
	PPC_STORE_U32(r11.u32 + ctx.r4.u32, r29.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r29,r28
	r28.u64 = r29.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// stwx r29,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, r29.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwzx r29,r10,r6
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r26,r28,31,1,31
	r26.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// and r28,r29,r28
	r28.u64 = r29.u64 & r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r26,r26,r12
	r26.u64 = r26.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// add r29,r27,r26
	r29.u64 = r27.u64 + r26.u64;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r29.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82658658
	if (!cr6.eq) goto loc_82658658;
	// b 0x8239bd30
	return;
loc_826588A8:
	// lis r11,257
	r11.s64 = 16842752;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// addi r25,r5,1
	r25.s64 = ctx.r5.s64 + 1;
	// addi r23,r6,-1
	r23.s64 = ctx.r6.s64 + -1;
	// li r22,2
	r22.s64 = 2;
	// ori r11,r11,257
	r11.u64 = r11.u64 | 257;
loc_826588C0:
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// addi r5,r25,-1
	ctx.r5.s64 = r25.s64 + -1;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// add r8,r23,r25
	ctx.r8.u64 = r23.u64 + r25.u64;
	// add r9,r25,r6
	ctx.r9.u64 = r25.u64 + ctx.r6.u64;
	// li r10,2
	ctx.r10.s64 = 2;
loc_826588D8:
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// and r3,r30,r12
	ctx.r3.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r3,r26,r3
	ctx.r3.u64 = r26.u64 + ctx.r3.u64;
	// and r31,r29,r12
	r31.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// bne cr6,0x826588d8
	if (!cr6.eq) goto loc_826588D8;
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x826588c0
	if (!cr6.eq) goto loc_826588C0;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82658C1C"))) PPC_WEAK_FUNC(sub_82658C1C);
PPC_FUNC_IMPL(__imp__sub_82658C1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82658C20"))) PPC_WEAK_FUNC(sub_82658C20);
PPC_FUNC_IMPL(__imp__sub_82658C20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce4
	// lwz r3,256(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 256);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82658ed8
	if (cr6.eq) goto loc_82658ED8;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r11,r5,8
	r11.s64 = ctx.r5.s64 + 8;
	// beq cr6,0x82658d58
	if (cr6.eq) goto loc_82658D58;
	// li r29,8
	r29.s64 = 8;
loc_82658C44:
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lbz r9,3(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r10,2(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lbz r31,0(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lwz r5,-8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbz r8,1(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lwz r30,-4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82658c94
	if (!cr6.eq) goto loc_82658C94;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82658ca8
	goto loc_82658CA8;
loc_82658C94:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82658CA8:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r5,20(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lbz r9,7(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lbz r10,6(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lbz r8,5(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lwz r30,12(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82658d10
	if (!cr6.eq) goto loc_82658D10;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82658d24
	goto loc_82658D24;
loc_82658D10:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82658D24:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82658c44
	if (!cr6.eq) goto loc_82658C44;
	// b 0x8239bd34
	return;
loc_82658D58:
	// li r25,8
	r25.s64 = 8;
loc_82658D5C:
	// lbz r10,1(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r9,2(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r8,3(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// add r26,r5,r10
	r26.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r27,4(r6)
	r27.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r10,r27,r8
	ctx.r10.u64 = r27.u64 + ctx.r8.u64;
	// lwz r29,4(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r31,-8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// lwz r30,-4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r27,r10,1
	r27.s64 = ctx.r10.s64 + 1;
	// srawi r5,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r5.s64 = r26.s32 >> 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r9.s64 = r27.s32 >> 1;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82658de0
	if (!cr6.eq) goto loc_82658DE0;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82658df4
	goto loc_82658DF4;
loc_82658DE0:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82658DF4:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lbz r10,5(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r5,4(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r9,6(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// lbz r8,7(r6)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// add r26,r5,r10
	r26.u64 = ctx.r5.u64 + ctx.r10.u64;
	// lbz r27,8(r6)
	r27.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// add r5,r9,r10
	ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r28,16(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r10,r27,r8
	ctx.r10.u64 = r27.u64 + ctx.r8.u64;
	// lwz r29,20(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// lwz r30,12(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r27,r10,1
	r27.s64 = ctx.r10.s64 + 1;
	// srawi r5,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r5.s64 = r26.s32 >> 1;
	// srawi r8,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// srawi r10,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r9.s64 = r27.s32 >> 1;
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// or r31,r9,r10
	r31.u64 = ctx.r9.u64 | ctx.r10.u64;
	// or r31,r31,r8
	r31.u64 = r31.u64 | ctx.r8.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82658e90
	if (!cr6.eq) goto loc_82658E90;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82658ea4
	goto loc_82658EA4;
loc_82658E90:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// lbzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
loc_82658EA4:
	// or r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 | ctx.r8.u64;
	// addi r25,r25,-1
	r25.s64 = r25.s64 + -1;
	// rlwinm r8,r8,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82658d5c
	if (!cr6.eq) goto loc_82658D5C;
	// b 0x8239bd34
	return;
loc_82658ED8:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r10,r5,8
	ctx.r10.s64 = ctx.r5.s64 + 8;
	// beq cr6,0x82659080
	if (cr6.eq) goto loc_82659080;
	// li r23,8
	r23.s64 = 8;
loc_82658EE8:
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lbz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lbz r24,1(r6)
	r24.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r25,2(r6)
	r25.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r31,3(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lwz r27,0(r10)
	r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r26,0(r11)
	r26.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r5,1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// add r26,r9,r26
	r26.u64 = ctx.r9.u64 + r26.u64;
	// lbz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r5,r5,r24
	ctx.r5.u64 = ctx.r5.u64 + r24.u64;
	// add r8,r8,r25
	ctx.r8.u64 = ctx.r8.u64 + r25.u64;
	// lwz r28,4(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwz r30,-8(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lwz r29,-4(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r31,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	r31.s64 = r26.s32 >> 1;
	// addi r26,r9,1
	r26.s64 = ctx.r9.s64 + 1;
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r8.s64 = r26.s32 >> 1;
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// add r8,r8,r28
	ctx.r8.u64 = ctx.r8.u64 + r28.u64;
	// add r31,r31,r30
	r31.u64 = r31.u64 + r30.u64;
	// add r5,r5,r29
	ctx.r5.u64 = ctx.r5.u64 + r29.u64;
	// or r30,r8,r9
	r30.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r30,r30,r5
	r30.u64 = r30.u64 | ctx.r5.u64;
	// or r30,r30,r31
	r30.u64 = r30.u64 | r31.u64;
	// rlwinm r30,r30,0,0,23
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82658f7c
	if (!cr6.eq) goto loc_82658F7C;
	// rlwinm r31,r31,8,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x82658f90
	goto loc_82658F90;
loc_82658F7C:
	// lbzx r31,r31,r3
	r31.u64 = PPC_LOAD_U8(r31.u32 + ctx.r3.u32);
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// rotlwi r31,r31,8
	r31.u64 = __builtin_rotateleft32(r31.u32, 8);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
loc_82658F90:
	// or r5,r31,r5
	ctx.r5.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// or r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 | ctx.r9.u64;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// lbz r31,4(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lbz r5,5(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// add r30,r8,r31
	r30.u64 = ctx.r8.u64 + r31.u64;
	// lbz r31,6(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lbz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lbz r6,7(r6)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// addi r27,r30,1
	r27.s64 = r30.s64 + 1;
	// lbz r9,7(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// addi r26,r5,1
	r26.s64 = ctx.r5.s64 + 1;
	// lwz r28,16(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// lwz r29,20(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lwz r31,8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// srawi r5,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r5.s64 = r27.s32 >> 1;
	// lwz r30,12(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// addi r27,r9,1
	r27.s64 = ctx.r9.s64 + 1;
	// srawi r6,r26,1
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x1) != 0);
	ctx.r6.s64 = r26.s32 >> 1;
	// srawi r9,r8,1
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// srawi r8,r27,1
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0x1) != 0);
	ctx.r8.s64 = r27.s32 >> 1;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + r28.u64;
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r6,r6,r30
	ctx.r6.u64 = ctx.r6.u64 + r30.u64;
	// or r31,r8,r9
	r31.u64 = ctx.r8.u64 | ctx.r9.u64;
	// or r31,r31,r6
	r31.u64 = r31.u64 | ctx.r6.u64;
	// or r31,r31,r5
	r31.u64 = r31.u64 | ctx.r5.u64;
	// rlwinm r31,r31,0,0,23
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF00;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82659038
	if (!cr6.eq) goto loc_82659038;
	// rlwinm r5,r5,8,0,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0xFFFFFF00;
	// b 0x8265904c
	goto loc_8265904C;
loc_82659038:
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// lbzx r6,r6,r3
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r3.u32);
	// lbzx r9,r9,r3
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r3.u32);
	// rotlwi r5,r5,8
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r5.u32, 8);
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
loc_8265904C:
	// or r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 | ctx.r6.u64;
	// addi r23,r23,-1
	r23.s64 = r23.s64 + -1;
	// rlwinm r6,r6,8,0,23
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// or r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 | ctx.r9.u64;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// stw r9,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r9.u32);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82658ee8
	if (!cr6.eq) goto loc_82658EE8;
	// b 0x8239bd34
	return;
loc_82659080:
	// li r9,8
	ctx.r9.s64 = 8;
loc_82659084:
	// add r11,r6,r7
	r11.u64 = ctx.r6.u64 + ctx.r7.u64;
	// lbz r29,1(r6)
	r29.u64 = PPC_LOAD_U8(ctx.r6.u32 + 1);
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// lwz r31,-8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// lbz r30,0(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// add r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 + r29.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// stb r5,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r5.u8);
	// lbz r30,2(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 2);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lwz r31,-4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r8.u8);
	// lbz r30,3(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 3);
	// lbz r8,3(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r30,r8,r5
	r30.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r5,r8,2
	ctx.r5.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r30,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,2(r4)
	PPC_STORE_U8(ctx.r4.u32 + 2, ctx.r8.u8);
	// lbz r30,4(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 4);
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// lwz r31,4(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r30,r8,r5
	r30.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r5,r8,2
	ctx.r5.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r30,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,3(r4)
	PPC_STORE_U8(ctx.r4.u32 + 3, ctx.r8.u8);
	// lbz r30,5(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 5);
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lwz r31,8(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r30,r8,r5
	r30.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r5,r8,2
	ctx.r5.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r30,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r8.u8);
	// lbz r30,6(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 6);
	// lbz r8,6(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 6);
	// lwz r31,12(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r30,r8,r5
	r30.u64 = ctx.r8.u64 + ctx.r5.u64;
	// addi r5,r8,2
	ctx.r5.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r30,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// stb r8,5(r4)
	PPC_STORE_U8(ctx.r4.u32 + 5, ctx.r8.u8);
	// lbz r30,7(r6)
	r30.u64 = PPC_LOAD_U8(ctx.r6.u32 + 7);
	// lbz r8,7(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 7);
	// lwz r31,16(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + r30.u64;
	// add r5,r8,r5
	ctx.r5.u64 = ctx.r8.u64 + ctx.r5.u64;
	// rlwinm r5,r5,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// lbzx r5,r5,r3
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r3.u32);
	// stb r5,6(r4)
	PPC_STORE_U8(ctx.r4.u32 + 6, ctx.r5.u8);
	// lbz r31,8(r6)
	r31.u64 = PPC_LOAD_U8(ctx.r6.u32 + 8);
	// lbz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r6,20(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lbzx r8,r8,r3
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r3.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stb r8,7(r4)
	PPC_STORE_U8(ctx.r4.u32 + 7, ctx.r8.u8);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// bne cr6,0x82659084
	if (!cr6.eq) goto loc_82659084;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_826591EC"))) PPC_WEAK_FUNC(sub_826591EC);
PPC_FUNC_IMPL(__imp__sub_826591EC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_826591F0"))) PPC_WEAK_FUNC(sub_826591F0);
PPC_FUNC_IMPL(__imp__sub_826591F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x8239bce0
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x826594c8
	if (cr6.eq) goto loc_826594C8;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x8265922c
	if (cr6.eq) goto loc_8265922C;
	// li r11,8
	r11.s64 = 8;
loc_8265920C:
	// ld r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// std r10,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r10.u64);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// bne cr6,0x8265920c
	if (!cr6.eq) goto loc_8265920C;
	// b 0x8239bd30
	return;
loc_8265922C:
	// rlwinm r3,r6,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r6,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r3,1,0,30
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r5
	ctx.r10.u64 = r11.u64 + ctx.r5.u64;
	// add r3,r11,r4
	ctx.r3.u64 = r11.u64 + ctx.r4.u64;
	// add r27,r9,r5
	r27.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r11,r28,r5
	r11.u64 = r28.u64 + ctx.r5.u64;
	// addi r8,r5,1
	ctx.r8.s64 = ctx.r5.s64 + 1;
	// add r31,r9,r4
	r31.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// addi r26,r6,-1
	r26.s64 = ctx.r6.s64 + -1;
	// subfic r29,r6,-1
	xer.ca = ctx.r6.u32 <= 4294967295;
	r29.s64 = -1 - ctx.r6.s64;
	// li r30,2
	r30.s64 = 2;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r27,1
	ctx.r9.s64 = r27.s64 + 1;
	// add r5,r28,r4
	ctx.r5.u64 = r28.u64 + ctx.r4.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82659278:
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lis r12,-129
	r12.s64 = -8454144;
	// lwz r28,-1(r8)
	r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + -1);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r28,r4
	ctx.r4.u64 = r28.u64 | ctx.r4.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// stw r4,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r4.u32);
	// lwzx r4,r8,r6
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwzx r28,r29,r10
	r28.u64 = PPC_LOAD_U32(r29.u32 + ctx.r10.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stwx r4,r7,r6
	PPC_STORE_U32(ctx.r7.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r28,-1(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + -1);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r28,r4
	ctx.r4.u64 = r28.u64 | ctx.r4.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// stw r4,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r4.u32);
	// lwzx r4,r10,r6
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwzx r28,r29,r9
	r28.u64 = PPC_LOAD_U32(r29.u32 + ctx.r9.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stwx r4,r3,r6
	PPC_STORE_U32(ctx.r3.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r28,-1(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + -1);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r28,r4
	ctx.r4.u64 = r28.u64 | ctx.r4.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// lis r12,-129
	r12.s64 = -8454144;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// stw r4,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r4.u32);
	// lwzx r4,r9,r6
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwzx r28,r29,r11
	r28.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// stwx r4,r31,r6
	PPC_STORE_U32(r31.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r28,-1(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + -1);
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r28,r4
	ctx.r4.u64 = r28.u64 | ctx.r4.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// stw r4,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r4.u32);
	// lwzx r4,r11,r6
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// lwzx r28,r26,r11
	r28.u64 = PPC_LOAD_U32(r26.u32 + r11.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rlwinm r27,r4,31,1,31
	r27.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 31) & 0x7FFFFFFF;
	// or r4,r28,r4
	ctx.r4.u64 = r28.u64 | ctx.r4.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r4,r4,r12
	ctx.r4.u64 = ctx.r4.u64 & r12.u64;
	// add r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 + r28.u64;
	// add r4,r4,r27
	ctx.r4.u64 = ctx.r4.u64 + r27.u64;
	// stwx r4,r5,r6
	PPC_STORE_U32(ctx.r5.u32 + ctx.r6.u32, ctx.r4.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne cr6,0x82659278
	if (!cr6.eq) goto loc_82659278;
	// b 0x8239bd30
	return;
loc_826594C8:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82659770
	if (cr6.eq) goto loc_82659770;
	// rlwinm r11,r6,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r5
	ctx.r3.u64 = r11.u64 + ctx.r5.u64;
	// subf r11,r5,r4
	r11.s64 = ctx.r4.s64 - ctx.r5.s64;
	// rlwinm r4,r6,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r5
	ctx.r8.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r4
	r31.u64 = ctx.r6.u64 + ctx.r4.u64;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r6,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r4,r6,r10
	ctx.r4.u64 = ctx.r6.u64 + ctx.r10.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// rlwinm r31,r31,1,0,30
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r6,r29
	ctx.r10.s64 = r29.s64 - ctx.r6.s64;
	// add r9,r5,r6
	ctx.r9.u64 = ctx.r5.u64 + ctx.r6.u64;
	// li r30,2
	r30.s64 = 2;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// add r31,r31,r5
	r31.u64 = r31.u64 + ctx.r5.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
loc_82659520:
	// lwz r29,0(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r12,-129
	r12.s64 = -8454144;
	// lwz r28,0(r5)
	r28.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, r29.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lwz r29,0(r8)
	r29.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r28,0(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r29.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r29,0(r7)
	r29.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r28,0(r8)
	r28.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, r29.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwz r29,0(r3)
	r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r28,0(r7)
	r28.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, r29.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// lwz r29,0(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r28,0(r3)
	r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// lis r12,-129
	r12.s64 = -8454144;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r3
	PPC_STORE_U32(r11.u32 + ctx.r3.u32, r29.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r28,0(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r4
	PPC_STORE_U32(r11.u32 + ctx.r4.u32, r29.u32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// lwz r29,0(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, r29.u32);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwzx r29,r10,r6
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// lwz r28,0(r10)
	r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r27,r29,31,1,31
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 31) & 0x7FFFFFFF;
	// or r29,r29,r28
	r29.u64 = r29.u64 | r28.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-129
	r12.s64 = -8454144;
	// rlwinm r28,r28,31,1,31
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 31) & 0x7FFFFFFF;
	// ori r12,r12,32639
	r12.u64 = r12.u64 | 32639;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,257
	r12.s64 = 16842752;
	// ori r12,r12,257
	r12.u64 = r12.u64 | 257;
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// add r29,r29,r28
	r29.u64 = r29.u64 + r28.u64;
	// stwx r29,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r29.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82659520
	if (!cr6.eq) goto loc_82659520;
	// b 0x8239bd30
	return;
loc_82659770:
	// lis r11,514
	r11.s64 = 33685504;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// addi r25,r5,1
	r25.s64 = ctx.r5.s64 + 1;
	// addi r23,r6,-1
	r23.s64 = ctx.r6.s64 + -1;
	// li r22,2
	r22.s64 = 2;
	// ori r11,r11,514
	r11.u64 = r11.u64 | 514;
loc_82659788:
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// addi r5,r25,-1
	ctx.r5.s64 = r25.s64 + -1;
	// mr r7,r25
	ctx.r7.u64 = r25.u64;
	// add r8,r23,r25
	ctx.r8.u64 = r23.u64 + r25.u64;
	// add r9,r25,r6
	ctx.r9.u64 = r25.u64 + ctx.r6.u64;
	// li r10,2
	ctx.r10.s64 = 2;
loc_826597A0:
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r26
	r28.u64 = r28.u64 + r26.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r27,r30,r12
	r27.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r28,r28,r27
	r28.u64 = r28.u64 + r27.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// and r26,r29,r12
	r26.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r31,r28,r26
	r31.u64 = r28.u64 + r26.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r31,r11
	ctx.r3.u64 = r31.u64 + r11.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// lwz r31,0(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// and r26,r3,r12
	r26.u64 = ctx.r3.u64 & r12.u64;
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lis r12,771
	r12.s64 = 50528256;
	// lwz r29,0(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r28,r31,r12
	r28.u64 = r31.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r27,r31,30,2,31
	r27.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r26,r28,r26
	r26.u64 = r28.u64 + r26.u64;
	// and r27,r27,r12
	r27.u64 = r27.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// rlwinm r28,r3,30,2,31
	r28.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// and r28,r28,r12
	r28.u64 = r28.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// add r5,r5,r6
	ctx.r5.u64 = ctx.r5.u64 + ctx.r6.u64;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// and r3,r30,r12
	ctx.r3.u64 = r30.u64 & r12.u64;
	// lis r12,771
	r12.s64 = 50528256;
	// rlwinm r30,r30,30,2,31
	r30.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// add r3,r26,r3
	ctx.r3.u64 = r26.u64 + ctx.r3.u64;
	// and r31,r29,r12
	r31.u64 = r29.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r31,r29,30,2,31
	r31.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 30) & 0x3FFFFFFF;
	// and r30,r30,r12
	r30.u64 = r30.u64 & r12.u64;
	// lis r12,-193
	r12.s64 = -12648448;
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + r11.u64;
	// ori r12,r12,16191
	r12.u64 = r12.u64 | 16191;
	// rlwinm r3,r3,30,6,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFF;
	// and r31,r31,r12
	r31.u64 = r31.u64 & r12.u64;
	// lis r12,-253
	r12.s64 = -16580608;
	// ori r12,r12,771
	r12.u64 = r12.u64 | 771;
	// and r3,r3,r12
	ctx.r3.u64 = ctx.r3.u64 & r12.u64;
	// add r3,r3,r27
	ctx.r3.u64 = ctx.r3.u64 + r27.u64;
	// add r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 + r28.u64;
	// add r3,r3,r30
	ctx.r3.u64 = ctx.r3.u64 + r30.u64;
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + r31.u64;
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// add r4,r4,r6
	ctx.r4.u64 = ctx.r4.u64 + ctx.r6.u64;
	// bne cr6,0x826597a0
	if (!cr6.eq) goto loc_826597A0;
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// bne cr6,0x82659788
	if (!cr6.eq) goto loc_82659788;
	// b 0x8239bd30
	return;
}

__attribute__((alias("__imp__sub_82659AE4"))) PPC_WEAK_FUNC(sub_82659AE4);
PPC_FUNC_IMPL(__imp__sub_82659AE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82659AE8"))) PPC_WEAK_FUNC(sub_82659AE8);
PPC_FUNC_IMPL(__imp__sub_82659AE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// li r31,8
	r31.s64 = 8;
loc_82659AF4:
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// lwz r11,4(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// add r11,r6,r9
	r11.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lwz r30,12(r6)
	r30.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r6,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r6.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r6,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r6.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r6,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r6.u32);
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lwz r30,12(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r30,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r30.u32);
	// add r3,r3,r9
	ctx.r3.u64 = ctx.r3.u64 + ctx.r9.u64;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// lwz r11,4(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// add r4,r4,r10
	ctx.r4.u64 = ctx.r4.u64 + ctx.r10.u64;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r11.u32);
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// bne cr6,0x82659af4
	if (!cr6.eq) goto loc_82659AF4;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82659B8C"))) PPC_WEAK_FUNC(sub_82659B8C);
PPC_FUNC_IMPL(__imp__sub_82659B8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82659B90"))) PPC_WEAK_FUNC(sub_82659B90);
PPC_FUNC_IMPL(__imp__sub_82659B90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82659be4
	if (cr6.eq) goto loc_82659BE4;
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82659bbc
	if (cr6.eq) goto loc_82659BBC;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82659BBC:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82659bcc
	if (cr6.eq) goto loc_82659BCC;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82659BCC:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82659bdc
	if (cr6.eq) goto loc_82659BDC;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82659BDC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_82659BE4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82659BF8"))) PPC_WEAK_FUNC(sub_82659BF8);
PPC_FUNC_IMPL(__imp__sub_82659BF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// blt cr6,0x82659c0c
	if (cr6.lt) goto loc_82659C0C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// b 0x82659c10
	goto loc_82659C10;
loc_82659C0C:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82659C10:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// ble cr6,0x82659c20
	if (!cr6.gt) goto loc_82659C20;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x82659c2c
	goto loc_82659C2C;
loc_82659C20:
	// cmpw cr6,r5,r10
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659c2c
	if (!cr6.gt) goto loc_82659C2C;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
loc_82659C2C:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// ble cr6,0x82659c3c
	if (!cr6.gt) goto loc_82659C3C;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// b 0x82659c48
	goto loc_82659C48;
loc_82659C3C:
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659c48
	if (!cr6.gt) goto loc_82659C48;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_82659C48:
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// cmpw cr6,r5,r7
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r7.s32, xer);
	// bge cr6,0x82659cd8
	if (!cr6.lt) goto loc_82659CD8;
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// ble cr6,0x82659c68
	if (!cr6.gt) goto loc_82659C68;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x82659c74
	goto loc_82659C74;
loc_82659C68:
	// cmpw cr6,r4,r10
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659c74
	if (!cr6.gt) goto loc_82659C74;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_82659C74:
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// ble cr6,0x82659c84
	if (!cr6.gt) goto loc_82659C84;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// b 0x82659c90
	goto loc_82659C90;
loc_82659C84:
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659c90
	if (!cr6.gt) goto loc_82659C90;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_82659C90:
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// ble cr6,0x82659ca0
	if (!cr6.gt) goto loc_82659CA0;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// b 0x82659cac
	goto loc_82659CAC;
loc_82659CA0:
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659cac
	if (!cr6.gt) goto loc_82659CAC;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_82659CAC:
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x82659cbc
	if (!cr6.gt) goto loc_82659CBC;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// b 0x82659cc8
	goto loc_82659CC8;
loc_82659CBC:
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// ble cr6,0x82659cc8
	if (!cr6.gt) goto loc_82659CC8;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82659CC8:
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bltlr cr6
	if (cr6.lt) return;
loc_82659CD8:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82659CE0"))) PPC_WEAK_FUNC(sub_82659CE0);
PPC_FUNC_IMPL(__imp__sub_82659CE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpw cr6,r3,r8
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r8.s32, xer);
	// blt cr6,0x82659cf4
	if (cr6.lt) goto loc_82659CF4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// b 0x82659cf8
	goto loc_82659CF8;
loc_82659CF4:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82659CF8:
	// cmpw cr6,r11,r5
	cr6.compare<int32_t>(r11.s32, ctx.r5.s32, xer);
	// ble cr6,0x82659d08
	if (!cr6.gt) goto loc_82659D08;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// b 0x82659d14
	goto loc_82659D14;
loc_82659D08:
	// cmpw cr6,r5,r8
	cr6.compare<int32_t>(ctx.r5.s32, ctx.r8.s32, xer);
	// ble cr6,0x82659d14
	if (!cr6.gt) goto loc_82659D14;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
loc_82659D14:
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// ble cr6,0x82659d24
	if (!cr6.gt) goto loc_82659D24;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// b 0x82659d30
	goto loc_82659D30;
loc_82659D24:
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// ble cr6,0x82659d30
	if (!cr6.gt) goto loc_82659D30;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
loc_82659D30:
	// cmpw cr6,r11,r4
	cr6.compare<int32_t>(r11.s32, ctx.r4.s32, xer);
	// ble cr6,0x82659d40
	if (!cr6.gt) goto loc_82659D40;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x82659d4c
	goto loc_82659D4C;
loc_82659D40:
	// cmpw cr6,r4,r8
	cr6.compare<int32_t>(ctx.r4.s32, ctx.r8.s32, xer);
	// ble cr6,0x82659d4c
	if (!cr6.gt) goto loc_82659D4C;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
loc_82659D4C:
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// ble cr6,0x82659d5c
	if (!cr6.gt) goto loc_82659D5C;
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// b 0x82659d68
	goto loc_82659D68;
loc_82659D5C:
	// cmpw cr6,r6,r8
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r8.s32, xer);
	// ble cr6,0x82659d68
	if (!cr6.gt) goto loc_82659D68;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82659D68:
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bltlr cr6
	if (cr6.lt) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82659D80"))) PPC_WEAK_FUNC(sub_82659D80);
PPC_FUNC_IMPL(__imp__sub_82659D80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r5,10
	r11.s64 = ctx.r5.s64 + 10;
	// stw r5,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r5.u32);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// srawi r31,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r31.s64 = r11.s32 >> 3;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r16,r11,r25
	r16.s64 = r25.s64 - r11.s64;
	// li r11,8
	r11.s64 = 8;
	// rlwinm r14,r24,1,0,30
	r14.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r24,r10
	ctx.r10.u64 = r24.u64 + ctx.r10.u64;
	// subf r21,r24,r25
	r21.s64 = r25.s64 - r24.s64;
	// add r22,r25,r24
	r22.u64 = r25.u64 + r24.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// rlwinm r11,r24,1,0,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r31,1,0,30
	r30.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// subf r18,r14,r25
	r18.s64 = r25.s64 - r14.s64;
	// subf r15,r10,r25
	r15.s64 = r25.s64 - ctx.r10.s64;
	// add r20,r11,r25
	r20.u64 = r11.u64 + r25.u64;
	// subf r17,r11,r25
	r17.s64 = r25.s64 - r11.s64;
loc_82659DE0:
	// lbz r4,0(r17)
	ctx.r4.u64 = PPC_LOAD_U8(r17.u32 + 0);
	// lbz r27,0(r16)
	r27.u64 = PPC_LOAD_U8(r16.u32 + 0);
	// lbz r28,0(r18)
	r28.u64 = PPC_LOAD_U8(r18.u32 + 0);
	// subf r10,r4,r27
	ctx.r10.s64 = r27.s64 - ctx.r4.s64;
	// lbz r6,0(r21)
	ctx.r6.u64 = PPC_LOAD_U8(r21.u32 + 0);
	// subf r11,r28,r4
	r11.s64 = ctx.r4.s64 - r28.s64;
	// lbz r29,0(r25)
	r29.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lbz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// add r7,r11,r31
	ctx.r7.u64 = r11.u64 + r31.u64;
	// lbzx r9,r22,r24
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + r24.u32);
	// subfc r11,r10,r30
	xer.ca = r30.u32 >= ctx.r10.u32;
	r11.s64 = r30.s64 - ctx.r10.s64;
	// lbz r26,0(r20)
	r26.u64 = PPC_LOAD_U8(r20.u32 + 0);
	// subf r10,r6,r28
	ctx.r10.s64 = r28.s64 - ctx.r6.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r7,r7,r30
	xer.ca = r30.u32 >= ctx.r7.u32;
	ctx.r7.s64 = r30.s64 - ctx.r7.s64;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// subfe r7,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r7.u64 + ctx.r7.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r10,r10,r30
	xer.ca = r30.u32 >= ctx.r10.u32;
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// subf r23,r29,r6
	r23.s64 = ctx.r6.s64 - r29.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r10,r23,r31
	ctx.r10.u64 = r23.u64 + r31.u64;
	// subfc r10,r10,r30
	xer.ca = r30.u32 >= ctx.r10.u32;
	ctx.r10.s64 = r30.s64 - ctx.r10.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82659eec
	if (cr6.eq) goto loc_82659EEC;
	// lbzx r7,r20,r24
	ctx.r7.u64 = PPC_LOAD_U8(r20.u32 + r24.u32);
	// subf r10,r26,r9
	ctx.r10.s64 = ctx.r9.s64 - r26.s64;
	// lbz r3,0(r15)
	ctx.r3.u64 = PPC_LOAD_U8(r15.u32 + 0);
	// subf r5,r7,r26
	ctx.r5.s64 = r26.s64 - ctx.r7.s64;
	// subf r7,r27,r3
	ctx.r7.s64 = ctx.r3.s64 - r27.s64;
	// add r5,r5,r31
	ctx.r5.u64 = ctx.r5.u64 + r31.u64;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// subfc r5,r5,r30
	xer.ca = r30.u32 >= ctx.r5.u32;
	ctx.r5.s64 = r30.s64 - ctx.r5.s64;
	// add r19,r10,r31
	r19.u64 = ctx.r10.u64 + r31.u64;
	// subfe r10,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r5,r7,r30
	xer.ca = r30.u32 >= ctx.r7.u32;
	ctx.r5.s64 = r30.s64 - ctx.r7.s64;
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// subfe r3,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r3.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r5,r19,r30
	xer.ca = r30.u32 >= r19.u32;
	ctx.r5.s64 = r30.s64 - r19.s64;
	// add r19,r7,r31
	r19.u64 = ctx.r7.u64 + r31.u64;
	// subf r7,r8,r29
	ctx.r7.s64 = r29.s64 - ctx.r8.s64;
	// subfe r5,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r5.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + r31.u64;
	// subfc r19,r19,r30
	xer.ca = r30.u32 >= r19.u32;
	r19.s64 = r30.s64 - r19.s64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// subfe r7,r19,r19
	temp.u8 = (~r19.u32 + r19.u32 < ~r19.u32) | (~r19.u32 + r19.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~r19.u64 + r19.u64 + xer.ca;
	xer.ca = temp.u8;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r5,r7,1
	ctx.r5.s64 = ctx.r7.s64 + 1;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// lwz r19,100(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// subfc r3,r19,r30
	xer.ca = r30.u32 >= r19.u32;
	ctx.r3.s64 = r30.s64 - r19.s64;
	// subfe r7,r3,r3
	temp.u8 = (~ctx.r3.u32 + ctx.r3.u32 < ~ctx.r3.u32) | (~ctx.r3.u32 + ctx.r3.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r3.u64 + ctx.r3.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_82659EEC:
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x82659f94
	if (cr6.lt) goto loc_82659F94;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82659bf8
	sub_82659BF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82659f94
	if (cr6.eq) goto loc_82659F94;
	// add r10,r4,r28
	ctx.r10.u64 = ctx.r4.u64 + r28.u64;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r8,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r10,r8,r5
	ctx.r10.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + r11.u64;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r29,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r29,r8
	ctx.r8.u64 = r29.u64 + ctx.r8.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r6,r5,4
	ctx.r6.s64 = ctx.r5.s64 + 4;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r6,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 3;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// subf r3,r14,r25
	ctx.r3.s64 = r25.s64 - r14.s64;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stb r6,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r6.u8);
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// stb r7,0(r22)
	PPC_STORE_U8(r22.u32 + 0, ctx.r7.u8);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// b 0x8265a088
	goto loc_8265A088;
loc_82659F94:
	// subf r11,r8,r28
	r11.s64 = r28.s64 - ctx.r8.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r23,2,0,29
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r23,r11
	r11.u64 = r23.u64 + r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r19,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r19.s64 = r11.s32 >> 3;
	// mr r11,r19
	r11.u64 = r19.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,292(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bge cr6,0x8265a090
	if (!cr6.lt) goto loc_8265A090;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r11,r4,r28
	r11.s64 = r28.s64 - ctx.r4.s64;
	// subf r8,r6,r27
	ctx.r8.s64 = r27.s64 - ctx.r6.s64;
	// subf r9,r26,r29
	ctx.r9.s64 = r29.s64 - r26.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r5
	r11.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265a030
	if (!cr6.lt) goto loc_8265A030;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8265A030:
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8265a090
	if (!cr6.gt) goto loc_8265A090;
	// xor r10,r19,r23
	ctx.r10.u64 = r19.u64 ^ r23.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8265a090
	if (!cr6.lt) goto loc_8265A090;
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// srawi r10,r23,31
	xer.ca = (r23.s32 < 0) & ((r23.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r23.s32 >> 31;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x8265a078
	if (!cr6.gt) goto loc_8265A078;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_8265A078:
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
loc_8265A088:
	// stb r10,0(r21)
	PPC_STORE_U8(r21.u32 + 0, ctx.r10.u8);
	// stb r11,0(r25)
	PPC_STORE_U8(r25.u32 + 0, r11.u8);
loc_8265A090:
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r18,r18,1
	r18.s64 = r18.s64 + 1;
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// addi r16,r16,1
	r16.s64 = r16.s64 + 1;
	// addi r15,r15,1
	r15.s64 = r15.s64 + 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82659de0
	if (!cr6.eq) goto loc_82659DE0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239bd10
	return;
}

__attribute__((alias("__imp__sub_8265A0CC"))) PPC_WEAK_FUNC(sub_8265A0CC);
PPC_FUNC_IMPL(__imp__sub_8265A0CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265A0D0"))) PPC_WEAK_FUNC(sub_8265A0D0);
PPC_FUNC_IMPL(__imp__sub_8265A0D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcd4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r20,r4
	r20.u64 = ctx.r4.u64;
	// addi r11,r22,10
	r11.s64 = r22.s64 + 10;
	// addi r31,r3,-5
	r31.s64 = ctx.r3.s64 + -5;
	// srawi r30,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r30.s64 = r11.s32 >> 3;
	// li r21,8
	r21.s64 = 8;
	// rlwinm r29,r30,1,0,30
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
loc_8265A0F8:
	// lbz r4,2(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 2);
	// lbz r25,1(r31)
	r25.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// lbz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// lbz r27,3(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// subf r11,r4,r25
	r11.s64 = r25.s64 - ctx.r4.s64;
	// lbz r28,5(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 5);
	// subf r10,r6,r27
	ctx.r10.s64 = r27.s64 - ctx.r6.s64;
	// lbz r9,7(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 7);
	// add r7,r11,r30
	ctx.r7.u64 = r11.u64 + r30.u64;
	// lbz r26,8(r31)
	r26.u64 = PPC_LOAD_U8(r31.u32 + 8);
	// subf r11,r27,r4
	r11.s64 = ctx.r4.s64 - r27.s64;
	// lbz r8,6(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 6);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r5,r11,r30
	ctx.r5.u64 = r11.u64 + r30.u64;
	// subfc r11,r10,r29
	xer.ca = r29.u32 >= ctx.r10.u32;
	r11.s64 = r29.s64 - ctx.r10.s64;
	// subf r24,r28,r6
	r24.s64 = ctx.r6.s64 - r28.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r10,r5,r29
	xer.ca = r29.u32 >= ctx.r5.u32;
	ctx.r10.s64 = r29.s64 - ctx.r5.s64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r5,r7,r29
	xer.ca = r29.u32 >= ctx.r7.u32;
	ctx.r5.s64 = r29.s64 - ctx.r7.s64;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// subfe r10,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r10,r24,r30
	ctx.r10.u64 = r24.u64 + r30.u64;
	// subfc r10,r10,r29
	xer.ca = r29.u32 >= ctx.r10.u32;
	ctx.r10.s64 = r29.s64 - ctx.r10.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265a1fc
	if (cr6.eq) goto loc_8265A1FC;
	// lbz r7,9(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 9);
	// subf r10,r26,r9
	ctx.r10.s64 = ctx.r9.s64 - r26.s64;
	// lbz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// subf r5,r7,r26
	ctx.r5.s64 = r26.s64 - ctx.r7.s64;
	// subf r7,r25,r3
	ctx.r7.s64 = ctx.r3.s64 - r25.s64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + r30.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + r30.u64;
	// subfc r5,r5,r29
	xer.ca = r29.u32 >= ctx.r5.u32;
	ctx.r5.s64 = r29.s64 - ctx.r5.s64;
	// add r23,r10,r30
	r23.u64 = ctx.r10.u64 + r30.u64;
	// subfe r10,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r5,r7,r29
	xer.ca = r29.u32 >= ctx.r7.u32;
	ctx.r5.s64 = r29.s64 - ctx.r7.s64;
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// subfe r3,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r3.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r5,r23,r29
	xer.ca = r29.u32 >= r23.u32;
	ctx.r5.s64 = r29.s64 - r23.s64;
	// add r23,r7,r30
	r23.u64 = ctx.r7.u64 + r30.u64;
	// subf r7,r8,r28
	ctx.r7.s64 = r28.s64 - ctx.r8.s64;
	// subfe r5,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r5.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// subfc r23,r23,r29
	xer.ca = r29.u32 >= r23.u32;
	r23.s64 = r29.s64 - r23.s64;
	// add r19,r7,r30
	r19.u64 = ctx.r7.u64 + r30.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// subfe r7,r23,r23
	temp.u8 = (~r23.u32 + r23.u32 < ~r23.u32) | (~r23.u32 + r23.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~r23.u64 + r23.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfc r3,r19,r29
	xer.ca = r29.u32 >= r19.u32;
	ctx.r3.s64 = r29.s64 - r19.s64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r5,r7,1
	ctx.r5.s64 = ctx.r7.s64 + 1;
	// subfe r7,r3,r3
	temp.u8 = (~ctx.r3.u32 + ctx.r3.u32 < ~ctx.r3.u32) | (~ctx.r3.u32 + ctx.r3.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r3.u64 + ctx.r3.u64 + xer.ca;
	xer.ca = temp.u8;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
loc_8265A1FC:
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// blt cr6,0x8265a29c
	if (cr6.lt) goto loc_8265A29C;
	// rlwinm r11,r22,1,0,30
	r11.u64 = __builtin_rotateleft64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82659bf8
	sub_82659BF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x8265a29c
	if (cr6.eq) goto loc_8265A29C;
	// add r10,r27,r4
	ctx.r10.u64 = r27.u64 + ctx.r4.u64;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r8,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r9,r4
	r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r10,r8,r5
	ctx.r10.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + r11.u64;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r28,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r28,r8
	ctx.r8.u64 = r28.u64 + ctx.r8.u64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// addi r6,r5,4
	ctx.r6.s64 = ctx.r5.s64 + 4;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// srawi r6,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 3;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// srawi r7,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 3;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stb r6,3(r31)
	PPC_STORE_U8(r31.u32 + 3, ctx.r6.u8);
	// srawi r10,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// stb r7,6(r31)
	PPC_STORE_U8(r31.u32 + 6, ctx.r7.u8);
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// b 0x8265a38c
	goto loc_8265A38C;
loc_8265A29C:
	// subf r11,r8,r27
	r11.s64 = r27.s64 - ctx.r8.s64;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r24,2,0,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r24,r11
	r11.u64 = r24.u64 + r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r23,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r23.s64 = r11.s32 >> 3;
	// mr r11,r23
	r11.u64 = r23.u64;
	// srawi r10,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r11.s32 >> 31;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// cmpw cr6,r3,r22
	cr6.compare<int32_t>(ctx.r3.s32, r22.s32, xer);
	// bge cr6,0x8265a394
	if (!cr6.lt) goto loc_8265A394;
	// subf r10,r8,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r11,r4,r27
	r11.s64 = r27.s64 - ctx.r4.s64;
	// subf r8,r6,r25
	ctx.r8.s64 = r25.s64 - ctx.r6.s64;
	// subf r9,r26,r28
	ctx.r9.s64 = r28.s64 - r26.s64;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// add r7,r11,r7
	ctx.r7.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r5
	r11.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// srawi r11,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	r11.s64 = ctx.r10.s32 >> 3;
	// srawi r10,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// srawi r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r8,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = r11.s32 >> 31;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// xor r7,r11,r8
	ctx.r7.u64 = r11.u64 ^ ctx.r8.u64;
	// subf r11,r9,r10
	r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265a334
	if (!cr6.lt) goto loc_8265A334;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8265A334:
	// subf r11,r11,r3
	r11.s64 = ctx.r3.s64 - r11.s64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8265a394
	if (!cr6.gt) goto loc_8265A394;
	// xor r10,r23,r24
	ctx.r10.u64 = r23.u64 ^ r24.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x8265a394
	if (!cr6.lt) goto loc_8265A394;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// srawi r10,r24,31
	xer.ca = (r24.s32 < 0) & ((r24.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = r24.s32 >> 31;
	// srawi r8,r9,31
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// ble cr6,0x8265a37c
	if (!cr6.gt) goto loc_8265A37C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_8265A37C:
	// xor r11,r10,r11
	r11.u64 = ctx.r10.u64 ^ r11.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
loc_8265A38C:
	// stb r11,5(r31)
	PPC_STORE_U8(r31.u32 + 5, r11.u8);
	// stb r10,4(r31)
	PPC_STORE_U8(r31.u32 + 4, ctx.r10.u8);
loc_8265A394:
	// addi r21,r21,-1
	r21.s64 = r21.s64 + -1;
	// add r31,r31,r20
	r31.u64 = r31.u64 + r20.u64;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// bne cr6,0x8265a0f8
	if (!cr6.eq) goto loc_8265A0F8;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239bd24
	return;
}

__attribute__((alias("__imp__sub_8265A3AC"))) PPC_WEAK_FUNC(sub_8265A3AC);
PPC_FUNC_IMPL(__imp__sub_8265A3AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265A3B0"))) PPC_WEAK_FUNC(sub_8265A3B0);
PPC_FUNC_IMPL(__imp__sub_8265A3B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x8265a404
	if (!cr6.gt) goto loc_8265A404;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x8265a3f4
	if (!cr6.eq) goto loc_8265A3F4;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8265a404
	if (cr6.eq) goto loc_8265A404;
	// cmpwi cr6,r28,4
	cr6.compare<int32_t>(r28.s32, 4, xer);
	// beq cr6,0x8265a404
	if (cr6.eq) goto loc_8265A404;
loc_8265A3F4:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82659d80
	sub_82659D80(ctx, base);
loc_8265A404:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// ble cr6,0x8265a434
	if (!cr6.gt) goto loc_8265A434;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x8265a424
	if (!cr6.eq) goto loc_8265A424;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8265a434
	if (cr6.eq) goto loc_8265A434;
	// cmpwi cr6,r28,8
	cr6.compare<int32_t>(r28.s32, 8, xer);
	// beq cr6,0x8265a434
	if (cr6.eq) goto loc_8265A434;
loc_8265A424:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8265a0d0
	sub_8265A0D0(ctx, base);
loc_8265A434:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8265A43C"))) PPC_WEAK_FUNC(sub_8265A43C);
PPC_FUNC_IMPL(__imp__sub_8265A43C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265A440"))) PPC_WEAK_FUNC(sub_8265A440);
PPC_FUNC_IMPL(__imp__sub_8265A440) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265a4a8
	if (cr6.eq) goto loc_8265A4A8;
	// li r11,0
	r11.s64 = 0;
	// li r4,3
	ctx.r4.s64 = 3;
	// mullw r3,r30,r29
	ctx.r3.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// stw r3,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x8265a4b4
	if (!cr6.eq) goto loc_8265A4B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A4A8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_8265A4B4:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// li r11,2
	r11.s64 = 2;
	// bgt cr6,0x8265a4c8
	if (cr6.gt) goto loc_8265A4C8;
	// li r11,1
	r11.s64 = 1;
loc_8265A4C8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8265A4D8"))) PPC_WEAK_FUNC(sub_8265A4D8);
PPC_FUNC_IMPL(__imp__sub_8265A4D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265a50c
	if (cr6.eq) goto loc_8265A50C;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265a504
	if (cr6.eq) goto loc_8265A504;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A504:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A50C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265A520"))) PPC_WEAK_FUNC(sub_8265A520);
PPC_FUNC_IMPL(__imp__sub_8265A520) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r9,r5,-1
	ctx.r9.s64 = ctx.r5.s64 + -1;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// clrlwi r3,r5,31
	ctx.r3.u64 = ctx.r5.u32 & 0x1;
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// or r31,r4,r5
	r31.u64 = ctx.r4.u64 | ctx.r5.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// mullw r11,r3,r11
	r11.s64 = int64_t(ctx.r3.s32) * int64_t(r11.s32);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// add r31,r9,r10
	r31.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x8265a624
	if (cr6.eq) goto loc_8265A624;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x8265a568
	if (!cr6.eq) goto loc_8265A568;
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// b 0x8265a634
	goto loc_8265A634;
loc_8265A568:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8265a578
	if (!cr6.eq) goto loc_8265A578;
	// li r11,2
	r11.s64 = 2;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A578:
	// add r9,r31,r4
	ctx.r9.u64 = r31.u64 + ctx.r4.u64;
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// lbz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// clrlwi r10,r10,30
	ctx.r10.u64 = ctx.r10.u32 & 0x3;
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x8265a5a0
	if (!cr6.eq) goto loc_8265A5A0;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// b 0x8265a62c
	goto loc_8265A62C;
loc_8265A5A0:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8265a5b4
	if (!cr6.eq) goto loc_8265A5B4;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x8265a5cc
	if (!cr6.eq) goto loc_8265A5CC;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A5B4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265a5dc
	if (!cr6.eq) goto loc_8265A5DC;
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8265a624
	if (!cr6.eq) goto loc_8265A624;
	// li r11,1
	r11.s64 = 1;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A5CC:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x8265a624
	if (!cr6.eq) goto loc_8265A624;
	// li r11,2
	r11.s64 = 2;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A5DC:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8265a624
	if (!cr6.eq) goto loc_8265A624;
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x8265a624
	if (!cr6.eq) goto loc_8265A624;
	// lbz r11,-1(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x8265a604
	if (!cr6.eq) goto loc_8265A604;
	// li r11,2
	r11.s64 = 2;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A604:
	// cmpwi cr6,r6,12
	cr6.compare<int32_t>(ctx.r6.s32, 12, xer);
	// ble cr6,0x8265a614
	if (!cr6.gt) goto loc_8265A614;
	// li r11,2
	r11.s64 = 2;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A614:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x8265a628
	if (!cr6.eq) goto loc_8265A628;
	// li r11,1
	r11.s64 = 1;
	// b 0x8265a628
	goto loc_8265A628;
loc_8265A624:
	// li r11,0
	r11.s64 = 0;
loc_8265A628:
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
loc_8265A62C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x8265a660
	if (!cr6.eq) goto loc_8265A660;
loc_8265A634:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne cr6,0x8265a64c
	if (!cr6.eq) goto loc_8265A64C;
	// li r11,16
	r11.s64 = 16;
	// clrlwi r7,r11,24
	ctx.r7.u64 = r11.u32 & 0xFF;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// b 0x8265a688
	goto loc_8265A688;
loc_8265A64C:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// clrlwi r7,r11,24
	ctx.r7.u64 = r11.u32 & 0xFF;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// b 0x8265a688
	goto loc_8265A688;
loc_8265A660:
	// add r11,r3,r4
	r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// rlwinm r7,r11,30,2,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// bne cr6,0x8265a67c
	if (!cr6.eq) goto loc_8265A67C;
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// b 0x8265a684
	goto loc_8265A684;
loc_8265A67C:
	// lbzx r11,r31,r4
	r11.u64 = PPC_LOAD_U8(r31.u32 + ctx.r4.u32);
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
loc_8265A684:
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
loc_8265A688:
	// and r11,r4,r5
	r11.u64 = ctx.r4.u64 & ctx.r5.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265a69c
	if (!cr6.eq) goto loc_8265A69C;
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// b 0x8265a6a8
	goto loc_8265A6A8;
loc_8265A69C:
	// add r11,r31,r4
	r11.u64 = r31.u64 + ctx.r4.u64;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// rlwinm r9,r11,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
loc_8265A6A8:
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x8265a6dc
	if (!cr6.lt) goto loc_8265A6DC;
	// clrlwi r10,r9,24
	ctx.r10.u64 = ctx.r9.u32 & 0xFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8265a6ec
	if (cr6.lt) goto loc_8265A6EC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_8265A6DC:
	// clrlwi r11,r9,24
	r11.u64 = ctx.r9.u32 & 0xFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x8265a6ec
	if (!cr6.lt) goto loc_8265A6EC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8265A6EC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265A6FC"))) PPC_WEAK_FUNC(sub_8265A6FC);
PPC_FUNC_IMPL(__imp__sub_8265A6FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265A700"))) PPC_WEAK_FUNC(sub_8265A700);
PPC_FUNC_IMPL(__imp__sub_8265A700) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32137
	r11.s64 = -2106130432;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// clrlwi r8,r5,31
	ctx.r8.u64 = ctx.r5.u32 & 0x1;
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r11,-17392
	r11.s64 = r11.s64 + -17392;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
	// lbzx r11,r6,r11
	r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + r11.u32);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwimi r11,r7,2,0,29
	r11.u64 = (__builtin_rotateleft32(ctx.r7.u32, 2) & 0xFFFFFFFC) | (r11.u64 & 0xFFFFFFFF00000003);
	// stbx r11,r10,r4
	PPC_STORE_U8(ctx.r10.u32 + ctx.r4.u32, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265A730"))) PPC_WEAK_FUNC(sub_8265A730);
PPC_FUNC_IMPL(__imp__sub_8265A730) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,1
	cr6.compare<int32_t>(ctx.r4.s32, 1, xer);
	// ble cr6,0x8265a754
	if (!cr6.gt) goto loc_8265A754;
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// ble cr6,0x8265a760
	if (!cr6.gt) goto loc_8265A760;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lbz r11,-1(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// clrlwi r3,r11,30
	ctx.r3.u64 = r11.u32 & 0x3;
	// blr 
	return;
loc_8265A754:
	// cmpwi cr6,r5,1
	cr6.compare<int32_t>(ctx.r5.s32, 1, xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// bgtlr cr6
	if (cr6.gt) return;
loc_8265A760:
	// li r3,2
	ctx.r3.s64 = 2;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265A768"))) PPC_WEAK_FUNC(sub_8265A768);
PPC_FUNC_IMPL(__imp__sub_8265A768) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,56
	ctx.r3.s64 = 56;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265a838
	if (cr6.eq) goto loc_8265A838;
	// li r5,56
	ctx.r5.s64 = 56;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,64
	ctx.r3.s64 = 64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r3.u32);
	// beq cr6,0x8265a800
	if (cr6.eq) goto loc_8265A800;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// beq cr6,0x8265a800
	if (cr6.eq) goto loc_8265A800;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,128
	ctx.r3.s64 = 128;
	// addi r10,r11,31
	ctx.r10.s64 = r11.s64 + 31;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// stw r3,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// bne cr6,0x8265a850
	if (!cr6.eq) goto loc_8265A850;
loc_8265A800:
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265a810
	if (cr6.eq) goto loc_8265A810;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A810:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265a820
	if (cr6.eq) goto loc_8265A820;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A820:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265a830
	if (cr6.eq) goto loc_8265A830;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A830:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265A838:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8265A850:
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265A870"))) PPC_WEAK_FUNC(sub_8265A870);
PPC_FUNC_IMPL(__imp__sub_8265A870) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x8239bcf8
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r31,0
	r31.s64 = 0;
	// addi r11,r1,-128
	r11.s64 = ctx.r1.s64 + -128;
	// rlwimi r10,r4,6,0,25
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r4.u32, 6) & 0xFFFFFFC0) | (ctx.r10.u64 & 0xFFFFFFFF0000003F);
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// li r10,10
	ctx.r10.s64 = 10;
	// addi r5,r3,4
	ctx.r5.s64 = ctx.r3.s64 + 4;
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_8265A8A0:
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bdnz 0x8265a8a0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8265A8A0;
	// lis r29,16
	r29.s64 = 1048576;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r29,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, r29.u32);
	// ble cr6,0x8265a9c4
	if (!cr6.gt) goto loc_8265A9C4;
	// li r30,1
	r30.s64 = 1;
loc_8265A8C0:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// clrlwi r7,r11,26
	ctx.r7.u64 = r11.u32 & 0x3F;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subfic r11,r7,20
	xer.ca = ctx.r7.u32 <= 20;
	r11.s64 = 20 - ctx.r7.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// slw r8,r30,r11
	ctx.r8.u64 = r11.u8 & 0x20 ? 0 : (r30.u32 << (r11.u8 & 0x3F));
	// and r9,r8,r6
	ctx.r9.u64 = ctx.r8.u64 & ctx.r6.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// sraw r11,r6,r11
	temp.u32 = r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	r11.s64 = ctx.r6.s32 >> temp.u32;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// or r11,r11,r7
	r11.u64 = r11.u64 | ctx.r7.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// beq cr6,0x8265a908
	if (cr6.eq) goto loc_8265A908;
	// lwz r3,-4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// b 0x8265a90c
	goto loc_8265A90C;
loc_8265A908:
	// add r3,r8,r6
	ctx.r3.u64 = ctx.r8.u64 + ctx.r6.u64;
loc_8265A90C:
	// cmpw cr6,r3,r29
	cr6.compare<int32_t>(ctx.r3.s32, r29.s32, xer);
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8265a91c
	if (!cr6.eq) goto loc_8265A91C;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
loc_8265A91C:
	// addi r9,r7,-1
	ctx.r9.s64 = ctx.r7.s64 + -1;
	// cmpw cr6,r9,r31
	cr6.compare<int32_t>(ctx.r9.s32, r31.s32, xer);
	// ble cr6,0x8265a980
	if (!cr6.gt) goto loc_8265A980;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_8265A934:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// bne cr6,0x8265a980
	if (!cr6.eq) goto loc_8265A980;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// and r28,r10,r8
	r28.u64 = ctx.r10.u64 & ctx.r8.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8265a958
	if (cr6.eq) goto loc_8265A958;
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// b 0x8265a95c
	goto loc_8265A95C;
loc_8265A958:
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_8265A95C:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmpw cr6,r10,r29
	cr6.compare<int32_t>(ctx.r10.s32, r29.s32, xer);
	// bne cr6,0x8265a970
	if (!cr6.eq) goto loc_8265A970;
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
loc_8265A970:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// cmpw cr6,r9,r31
	cr6.compare<int32_t>(ctx.r9.s32, r31.s32, xer);
	// bgt cr6,0x8265a934
	if (cr6.gt) goto loc_8265A934;
loc_8265A980:
	// addi r10,r7,1
	ctx.r10.s64 = ctx.r7.s64 + 1;
	// cmpwi cr6,r10,20
	cr6.compare<int32_t>(ctx.r10.s32, 20, xer);
	// bge cr6,0x8265a9b8
	if (!cr6.lt) goto loc_8265A9B8;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_8265A998:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r9,r6
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, xer);
	// bne cr6,0x8265a9b8
	if (!cr6.eq) goto loc_8265A9B8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpwi cr6,r10,20
	cr6.compare<int32_t>(ctx.r10.s32, 20, xer);
	// blt cr6,0x8265a998
	if (cr6.lt) goto loc_8265A998;
loc_8265A9B8:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8265a8c0
	if (!cr6.eq) goto loc_8265A8C0;
loc_8265A9C4:
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8265A9C8"))) PPC_WEAK_FUNC(sub_8265A9C8);
PPC_FUNC_IMPL(__imp__sub_8265A9C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265aa7c
	if (cr6.eq) goto loc_8265AA7C;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x8265aa7c
	if (cr6.eq) goto loc_8265AA7C;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x8265aa5c
	if (cr6.eq) goto loc_8265AA5C;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x8265aa5c
	if (cr6.eq) goto loc_8265AA5C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x8265aa14
	if (!cr6.eq) goto loc_8265AA14;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// cmpwi cr6,r4,13
	cr6.compare<int32_t>(ctx.r4.s32, 13, xer);
	// addi r11,r11,-15896
	r11.s64 = r11.s64 + -15896;
	// bge cr6,0x8265aa0c
	if (!cr6.lt) goto loc_8265AA0C;
	// addi r11,r11,-1452
	r11.s64 = r11.s64 + -1452;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA0C:
	// addi r11,r11,-1468
	r11.s64 = r11.s64 + -1468;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA14:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x8265aa4c
	if (cr6.eq) goto loc_8265AA4C;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x8265aa44
	if (cr6.eq) goto loc_8265AA44;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// ble cr6,0x8265aa9c
	if (!cr6.gt) goto loc_8265AA9C;
	// lis r11,-32137
	r11.s64 = -2106130432;
	// cmpwi cr6,r4,13
	cr6.compare<int32_t>(ctx.r4.s32, 13, xer);
	// addi r11,r11,-15896
	r11.s64 = r11.s64 + -15896;
	// bge cr6,0x8265aa98
	if (!cr6.lt) goto loc_8265AA98;
	// addi r11,r11,-144
	r11.s64 = r11.s64 + -144;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA44:
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bnelr cr6
	if (!cr6.eq) return;
loc_8265AA4C:
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,3544
	r11.s64 = r11.s64 + 3544;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// blr 
	return;
loc_8265AA5C:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// cmpwi cr6,r4,13
	cr6.compare<int32_t>(ctx.r4.s32, 13, xer);
	// addi r11,r11,-15896
	r11.s64 = r11.s64 + -15896;
	// bge cr6,0x8265aa74
	if (!cr6.lt) goto loc_8265AA74;
	// addi r11,r11,-464
	r11.s64 = r11.s64 + -464;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA74:
	// addi r11,r11,-784
	r11.s64 = r11.s64 + -784;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA7C:
	// lis r11,-32137
	r11.s64 = -2106130432;
	// cmpwi cr6,r4,13
	cr6.compare<int32_t>(ctx.r4.s32, 13, xer);
	// addi r11,r11,-15896
	r11.s64 = r11.s64 + -15896;
	// bge cr6,0x8265aa94
	if (!cr6.lt) goto loc_8265AA94;
	// addi r11,r11,-1104
	r11.s64 = r11.s64 + -1104;
	// b 0x8265aa98
	goto loc_8265AA98;
loc_8265AA94:
	// addi r11,r11,-1424
	r11.s64 = r11.s64 + -1424;
loc_8265AA98:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
loc_8265AA9C:
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r9,1
	ctx.r9.s64 = 1;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stw r10,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r10.u32);
	// slw r11,r9,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265AAC0"))) PPC_WEAK_FUNC(sub_8265AAC0);
PPC_FUNC_IMPL(__imp__sub_8265AAC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265ab1c
	if (cr6.eq) goto loc_8265AB1C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x82655c10
	sub_82655C10(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265aaf4
	if (cr6.eq) goto loc_8265AAF4;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265AAF4:
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265ab04
	if (cr6.eq) goto loc_8265AB04;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265AB04:
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265ab14
	if (cr6.eq) goto loc_8265AB14;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265AB14:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265AB1C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265AB30"))) PPC_WEAK_FUNC(sub_8265AB30);
PPC_FUNC_IMPL(__imp__sub_8265AB30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcfc
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,48
	ctx.r3.s64 = 48;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265ac80
	if (cr6.eq) goto loc_8265AC80;
	// li r5,48
	ctx.r5.s64 = 48;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,12
	ctx.r3.s64 = 12;
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r3.u32);
	// bne cr6,0x8265ab98
	if (!cr6.eq) goto loc_8265AB98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_8265AB98:
	// bl 0x82390038
	sub_82390038(ctx, base);
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r30,4
	cr6.compare<int32_t>(r30.s32, 4, xer);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// slw r11,r29,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (r29.u32 << (r30.u8 & 0x3F));
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lis r11,-32243
	r11.s64 = -2113077248;
	// addi r11,r11,3692
	r11.s64 = r11.s64 + 3692;
	// bge cr6,0x8265abc8
	if (!cr6.lt) goto loc_8265ABC8;
	// addi r11,r11,-228
	r11.s64 = r11.s64 + -228;
	// li r10,77
	ctx.r10.s64 = 77;
	// b 0x8265abdc
	goto loc_8265ABDC;
loc_8265ABC8:
	// bne cr6,0x8265abd8
	if (!cr6.eq) goto loc_8265ABD8;
	// addi r11,r11,40
	r11.s64 = r11.s64 + 40;
	// li r10,12
	ctx.r10.s64 = 12;
	// b 0x8265abdc
	goto loc_8265ABDC;
loc_8265ABD8:
	// li r10,34
	ctx.r10.s64 = 34;
loc_8265ABDC:
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// li r4,3
	ctx.r4.s64 = 3;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r4,3
	ctx.r4.s64 = 3;
	// rlwinm r3,r10,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// bl 0x82604080
	sub_82604080(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// stw r3,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r3.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// beq cr6,0x8265ac78
	if (cr6.eq) goto loc_8265AC78;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265ac78
	if (cr6.eq) goto loc_8265AC78;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8265ac6c
	if (!cr6.gt) goto loc_8265AC6C;
loc_8265AC40:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// lbzx r9,r9,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// slw r9,r29,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r9.u8 & 0x3F));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x8265ac40
	if (cr6.lt) goto loc_8265AC40;
loc_8265AC6C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
loc_8265AC78:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8265aac0
	sub_8265AAC0(ctx, base);
loc_8265AC80:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239bd4c
	return;
}

__attribute__((alias("__imp__sub_8265AC8C"))) PPC_WEAK_FUNC(sub_8265AC8C);
PPC_FUNC_IMPL(__imp__sub_8265AC8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265AC90"))) PPC_WEAK_FUNC(sub_8265AC90);
PPC_FUNC_IMPL(__imp__sub_8265AC90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r30,24(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x8265acc4
	if (!cr6.eq) goto loc_8265ACC4;
	// li r11,0
	r11.s64 = 0;
	// b 0x8265ad64
	goto loc_8265AD64;
loc_8265ACC4:
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8265ad24
	if (!cr6.gt) goto loc_8265AD24;
loc_8265ACCC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8265ad24
	if (cr6.eq) goto loc_8265AD24;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x8265ad14
	if (!cr0.lt) goto loc_8265AD14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265AD14:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8265accc
	if (cr6.gt) goto loc_8265ACCC;
loc_8265AD24:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r28
	r30.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8265ad60
	if (!cr0.lt) goto loc_8265AD60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265AD60:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_8265AD64:
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// lwz r8,32(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// srawi r7,r7,1
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// mullw r11,r7,r11
	r11.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// bne cr6,0x8265ad94
	if (!cr6.eq) goto loc_8265AD94;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8265AD94:
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,32(r29)
	PPC_STORE_U32(r29.u32 + 32, r11.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x8265ae04
	if (!cr6.gt) goto loc_8265AE04;
	// li r9,0
	ctx.r9.s64 = 0;
loc_8265ADA8:
	// lwz r8,44(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// clrlwi r6,r7,28
	ctx.r6.u64 = ctx.r7.u32 & 0xF;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// rlwinm r7,r7,28,4,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwimi r6,r5,0,0,25
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r5.u32, 0) & 0xFFFFFFC0) | (ctx.r6.u64 & 0xFFFFFFFF0000003F);
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// lwz r8,44(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// rlwimi r7,r6,0,0,25
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r6.u32, 0) & 0xFFFFFFC0) | (ctx.r7.u64 & 0xFFFFFFFF0000003F);
	// stw r7,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r7.u32);
	// lwz r8,4(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// blt cr6,0x8265ada8
	if (cr6.lt) goto loc_8265ADA8;
loc_8265AE04:
	// lwz r4,4(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r3,44(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// bl 0x8265a870
	sub_8265A870(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r6,6
	ctx.r6.s64 = 6;
	// lwz r5,44(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// lwz r3,40(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82655f48
	sub_82655F48(ctx, base);
	// subfic r11,r3,0
	xer.ca = ctx.r3.u32 <= 0;
	r11.s64 = 0 - ctx.r3.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r3,r11,0,30,30
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8265AE3C"))) PPC_WEAK_FUNC(sub_8265AE3C);
PPC_FUNC_IMPL(__imp__sub_8265AE3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265AE40"))) PPC_WEAK_FUNC(sub_8265AE40);
PPC_FUNC_IMPL(__imp__sub_8265AE40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bce4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// lwz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265aea0
	if (!cr6.eq) goto loc_8265AEA0;
	// bl 0x8265ac90
	sub_8265AC90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// bne cr6,0x8265b3ac
	if (!cr6.eq) goto loc_8265B3AC;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r10,16(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 16);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8265AEA0:
	// lwz r11,40(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 40);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8265af90
	if (cr6.lt) goto loc_8265AF90;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8265af88
	if (!cr6.lt) goto loc_8265AF88;
loc_8265AEF0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8265af1c
	if (cr6.lt) goto loc_8265AF1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8265aef0
	if (cr6.eq) goto loc_8265AEF0;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8265afd4
	goto loc_8265AFD4;
loc_8265AF1C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8265AF88:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// b 0x8265afd4
	goto loc_8265AFD4;
loc_8265AF90:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_8265AFA0:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8265afa0
	if (cr6.lt) goto loc_8265AFA0;
loc_8265AFD4:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// li r27,0
	r27.s64 = 0;
	// lbzx r29,r11,r30
	r29.u64 = PPC_LOAD_U8(r11.u32 + r30.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x8265b090
	if (cr6.eq) goto loc_8265B090;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r28,r27
	r28.u64 = r27.u64;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x8265b054
	if (!cr6.gt) goto loc_8265B054;
loc_8265AFFC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8265b054
	if (cr6.eq) goto loc_8265B054;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	r29.s64 = r29.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r29.u8 & 0x3F));
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// bge 0x8265b044
	if (!cr0.lt) goto loc_8265B044;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265B044:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x8265affc
	if (cr6.gt) goto loc_8265AFFC;
loc_8265B054:
	// subfic r9,r29,64
	xer.ca = r29.u32 <= 64;
	ctx.r9.s64 = 64 - r29.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	r29.u64 = r11.u64 + r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8265b090
	if (!cr0.lt) goto loc_8265B090;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265B090:
	// lwz r10,36(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 36);
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmpwi cr6,r30,46
	cr6.compare<int32_t>(r30.s32, 46, xer);
	// bge cr6,0x8265b114
	if (!cr6.lt) goto loc_8265B114;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r30,23
	cr6.compare<int32_t>(r30.s32, 23, xer);
	// bge cr6,0x8265b0b4
	if (!cr6.lt) goto loc_8265B0B4;
	// stw r27,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r27.u32);
	// b 0x8265b0bc
	goto loc_8265B0BC;
loc_8265B0B4:
	// addi r11,r30,-23
	r11.s64 = r30.s64 + -23;
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
loc_8265B0BC:
	// cmpwi cr6,r11,16
	cr6.compare<int32_t>(r11.s32, 16, xer);
	// bge cr6,0x8265b0d0
	if (!cr6.lt) goto loc_8265B0D0;
	// stw r27,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r27.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B0D0:
	// cmpwi cr6,r11,20
	cr6.compare<int32_t>(r11.s32, 20, xer);
	// bge cr6,0x8265b0e8
	if (!cr6.lt) goto loc_8265B0E8;
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B0E8:
	// cmpwi cr6,r11,22
	cr6.compare<int32_t>(r11.s32, 22, xer);
	// bge cr6,0x8265b104
	if (!cr6.lt) goto loc_8265B104;
	// li r10,2
	ctx.r10.s64 = 2;
	// addi r11,r11,-20
	r11.s64 = r11.s64 + -20;
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B104:
	// li r11,3
	r11.s64 = 3;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// stw r27,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r27.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B114:
	// rlwinm r11,r30,2,0,29
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r30,59
	cr6.compare<int32_t>(r30.s32, 59, xer);
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r27,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r27.u32);
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// bge cr6,0x8265b1f8
	if (!cr6.lt) goto loc_8265B1F8;
	// cmpwi cr6,r30,47
	cr6.compare<int32_t>(r30.s32, 47, xer);
	// stw r27,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r27.u32);
	// bgt cr6,0x8265b158
	if (cr6.gt) goto loc_8265B158;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r10,r10,3780
	ctx.r10.s64 = ctx.r10.s64 + 3780;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// lbz r10,-46(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -46);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B158:
	// cmpwi cr6,r30,49
	cr6.compare<int32_t>(r30.s32, 49, xer);
	// bgt cr6,0x8265b188
	if (cr6.gt) goto loc_8265B188;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r10,r10,3780
	ctx.r10.s64 = ctx.r10.s64 + 3780;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// stw r9,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r9.u32);
	// lbz r10,-46(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -46);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B188:
	// cmpwi cr6,r30,50
	cr6.compare<int32_t>(r30.s32, 50, xer);
	// bne cr6,0x8265b19c
	if (!cr6.eq) goto loc_8265B19C;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B19C:
	// cmpwi cr6,r30,51
	cr6.compare<int32_t>(r30.s32, 51, xer);
	// bne cr6,0x8265b1b8
	if (!cr6.eq) goto loc_8265B1B8;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B1B8:
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// cmpwi cr6,r30,56
	cr6.compare<int32_t>(r30.s32, 56, xer);
	// addi r10,r10,3780
	ctx.r10.s64 = ctx.r10.s64 + 3780;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// bgt cr6,0x8265b1e0
	if (cr6.gt) goto loc_8265B1E0;
	// stw r27,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r27.u32);
	// lbz r10,-46(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -46);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8265b35c
	goto loc_8265B35C;
loc_8265B1E0:
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r9,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r9.u32);
	// lbz r10,-46(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -46);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8265b35c
	goto loc_8265B35C;
loc_8265B1F8:
	// cmpwi cr6,r30,73
	cr6.compare<int32_t>(r30.s32, 73, xer);
	// bge cr6,0x8265b2b4
	if (!cr6.lt) goto loc_8265B2B4;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r30,64
	cr6.compare<int32_t>(r30.s32, 64, xer);
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// bgt cr6,0x8265b234
	if (cr6.gt) goto loc_8265B234;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r10,r10,3780
	ctx.r10.s64 = ctx.r10.s64 + 3780;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// lbz r10,-59(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -59);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B234:
	// cmpwi cr6,r30,67
	cr6.compare<int32_t>(r30.s32, 67, xer);
	// bgt cr6,0x8265b264
	if (cr6.gt) goto loc_8265B264;
	// lis r9,-32243
	ctx.r9.s64 = -2113077248;
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// addi r9,r9,3780
	ctx.r9.s64 = ctx.r9.s64 + 3780;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// lbz r10,-59(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -59);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B264:
	// cmpwi cr6,r30,70
	cr6.compare<int32_t>(r30.s32, 70, xer);
	// bgt cr6,0x8265b290
	if (cr6.gt) goto loc_8265B290;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// stw r27,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r27.u32);
	// addi r10,r10,3780
	ctx.r10.s64 = ctx.r10.s64 + 3780;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// lbz r10,-59(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + -59);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8265b35c
	goto loc_8265B35C;
loc_8265B290:
	// lis r9,-32243
	ctx.r9.s64 = -2113077248;
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
	// addi r9,r9,3780
	ctx.r9.s64 = ctx.r9.s64 + 3780;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// lbz r10,-59(r9)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + -59);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x8265b35c
	goto loc_8265B35C;
loc_8265B2B4:
	// cmpwi cr6,r30,75
	cr6.compare<int32_t>(r30.s32, 75, xer);
	// bge cr6,0x8265b344
	if (!cr6.lt) goto loc_8265B344;
	// addi r9,r30,-73
	ctx.r9.s64 = r30.s64 + -73;
	// clrlwi r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// stw r9,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r9.u32);
	// bgt cr6,0x8265b360
	if (cr6.gt) goto loc_8265B360;
	// lis r12,-32154
	r12.s64 = -2107244544;
	// addi r12,r12,-19732
	r12.s64 = r12.s64 + -19732;
	// rlwinm r0,r10,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r10.u64) {
	case 0:
		goto loc_8265B2FC;
	case 1:
		goto loc_8265B308;
	case 2:
		goto loc_8265B31C;
	case 3:
		goto loc_8265B330;
	default:
		__builtin_unreachable();
	}
	// lwz r19,-19716(r5)
	r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + -19716);
	// lwz r19,-19704(r5)
	r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + -19704);
	// lwz r19,-19684(r5)
	r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + -19684);
	// lwz r19,-19664(r5)
	r19.u64 = PPC_LOAD_U32(ctx.r5.u32 + -19664);
loc_8265B2FC:
	// li r10,2
	ctx.r10.s64 = 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// b 0x8265b358
	goto loc_8265B358;
loc_8265B308:
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// stw r10,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r10.u32);
	// b 0x8265b360
	goto loc_8265B360;
loc_8265B31C:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// b 0x8265b358
	goto loc_8265B358;
loc_8265B330:
	// clrlwi r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addi r10,r10,5
	ctx.r10.s64 = ctx.r10.s64 + 5;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// b 0x8265b358
	goto loc_8265B358;
loc_8265B344:
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// stw r10,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r10.u32);
	// clrlwi r10,r11,26
	ctx.r10.u64 = r11.u32 & 0x3F;
	// srawi r11,r11,6
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3F) != 0);
	r11.s64 = r11.s32 >> 6;
loc_8265B358:
	// stw r10,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r10.u32);
loc_8265B35C:
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
loc_8265B360:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge 0x8265b398
	if (!cr0.lt) goto loc_8265B398;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265B398:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// neg r11,r30
	r11.s64 = -r30.s64;
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ r11.u64;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// stw r11,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r11.u32);
loc_8265B3AC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239bd34
	return;
}

__attribute__((alias("__imp__sub_8265B3B8"))) PPC_WEAK_FUNC(sub_8265B3B8);
PPC_FUNC_IMPL(__imp__sub_8265B3B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265b418
	if (!cr6.eq) goto loc_8265B418;
	// bl 0x8265ac90
	sub_8265AC90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// beq cr6,0x8265b404
	if (cr6.eq) goto loc_8265B404;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8265B404:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8265B418:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8265b518
	if (cr6.lt) goto loc_8265B518;
	// clrlwi r11,r30,28
	r11.u64 = r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8265b508
	if (!cr6.lt) goto loc_8265B508;
loc_8265B468:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8265b49c
	if (cr6.lt) goto loc_8265B49C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8265b468
	if (cr6.eq) goto loc_8265B468;
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8265B49C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8265B508:
	// srawi r30,r30,4
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0xF) != 0);
	r30.s64 = r30.s32 >> 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8265B518:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_8265B528:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r30,r11
	r30.s64 = r11.s16;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x8265b528
	if (cr6.lt) goto loc_8265B528;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8265B568"))) PPC_WEAK_FUNC(sub_8265B568);
PPC_FUNC_IMPL(__imp__sub_8265B568) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r10,16(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265b5c8
	if (!cr6.eq) goto loc_8265B5C8;
	// bl 0x8265ac90
	sub_8265AC90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x8265b5b4
	if (cr6.eq) goto loc_8265B5B4;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8265B5B4:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r10,16(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 16);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_8265B5C8:
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 8);
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subfic r11,r4,64
	xer.ca = ctx.r4.u32 <= 64;
	r11.s64 = 64 - ctx.r4.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x8265b6b8
	if (cr6.lt) goto loc_8265B6B8;
	// clrlwi r11,r27,28
	r11.u64 = r27.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// sld r10,r10,r11
	ctx.r10.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x8265b6b0
	if (!cr6.lt) goto loc_8265B6B0;
loc_8265B618:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x8265b644
	if (cr6.lt) goto loc_8265B644;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb840
	sub_825EB840(ctx, base);
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x8265b618
	if (cr6.eq) goto loc_8265B618;
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x8265b6fc
	goto loc_8265B6FC;
loc_8265B644:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// rldicr r11,r9,8,55
	r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	r11.u64 = __builtin_rotateleft64(r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
loc_8265B6B0:
	// srawi r27,r27,4
	xer.ca = (r27.s32 < 0) & ((r27.u32 & 0xF) != 0);
	r27.s64 = r27.s32 >> 4;
	// b 0x8265b6fc
	goto loc_8265B6FC;
loc_8265B6B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// lis r11,0
	r11.s64 = 0;
	// ori r29,r11,32768
	r29.u64 = r11.u64 | 32768;
loc_8265B6C8:
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rldicl r11,r11,1,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r30,r11,r27
	r30.u64 = r11.u64 + r27.u64;
	// bl 0x825eb9d0
	sub_825EB9D0(ctx, base);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	r11.u64 = PPC_LOAD_U16(r11.u32 + r28.u32);
	// extsh r27,r11
	r27.s64 = r11.s16;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// blt cr6,0x8265b6c8
	if (cr6.lt) goto loc_8265B6C8;
loc_8265B6FC:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbzx r30,r11,r27
	r30.u64 = PPC_LOAD_U8(r11.u32 + r27.u32);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x8265b7b4
	if (cr6.eq) goto loc_8265B7B4;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r29,0
	r29.s64 = 0;
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x8265b778
	if (!cr6.gt) goto loc_8265B778;
loc_8265B720:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8265b778
	if (cr6.eq) goto loc_8265B778;
	// subfic r8,r11,64
	xer.ca = r11.u32 <= 64;
	ctx.r8.s64 = 64 - r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	r30.s64 = r30.s64 - r11.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	r11.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// slw r11,r10,r30
	r11.u64 = r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r30.u8 & 0x3F));
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// bge 0x8265b768
	if (!cr0.lt) goto loc_8265B768;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265B768:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r9,16
	r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x8265b720
	if (cr6.gt) goto loc_8265B720;
loc_8265B778:
	// subfic r9,r30,64
	xer.ca = r30.u32 <= 64;
	ctx.r9.s64 = 64 - r30.s64;
	// ld r11,0(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	r30.u64 = r11.u64 + r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r8.u64);
	// bge 0x8265b7b4
	if (!cr0.lt) goto loc_8265B7B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825eb900
	sub_825EB900(ctx, base);
loc_8265B7B4:
	// mr r11,r27
	r11.u64 = r27.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r27,17
	cr6.compare<int32_t>(r27.s32, 17, xer);
	// blt cr6,0x8265b7cc
	if (cr6.lt) goto loc_8265B7CC;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r11,r27,-17
	r11.s64 = r27.s64 + -17;
loc_8265B7CC:
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// blt cr6,0x8265b7ec
	if (cr6.lt) goto loc_8265B7EC;
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// srawi r9,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r9.s64 = r30.s32 >> 1;
	// addi r10,r10,3812
	ctx.r10.s64 = ctx.r10.s64 + 3812;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r11,-5(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -5);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
loc_8265B7EC:
	// clrlwi r10,r30,31
	ctx.r10.u64 = r30.u32 & 0x1;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r8
	ctx.r3.u64 = r11.u64 + ctx.r8.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8265B80C"))) PPC_WEAK_FUNC(sub_8265B80C);
PPC_FUNC_IMPL(__imp__sub_8265B80C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265B810"))) PPC_WEAK_FUNC(sub_8265B810);
PPC_FUNC_IMPL(__imp__sub_8265B810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,80
	ctx.r3.s64 = 80;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8265b924
	if (cr6.eq) goto loc_8265B924;
	// li r5,80
	ctx.r5.s64 = 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239cd50
	sub_8239CD50(ctx, base);
	// li r31,0
	r31.s64 = 0;
	// li r11,1
	r11.s64 = 1;
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// li r10,8
	ctx.r10.s64 = 8;
	// addi r29,r30,12
	r29.s64 = r30.s64 + 12;
	// stw r31,60(r30)
	PPC_STORE_U32(r30.u32 + 60, r31.u32);
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// stw r11,72(r30)
	PPC_STORE_U32(r30.u32 + 72, r11.u32);
	// stw r10,64(r30)
	PPC_STORE_U32(r30.u32 + 64, ctx.r10.u32);
loc_8265B86C:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8265ab30
	sub_8265AB30(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// beq cr6,0x8265b8a8
	if (cr6.eq) goto loc_8265B8A8;
	// lwz r11,64(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 64);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x8265b86c
	if (cr6.lt) goto loc_8265B86C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
loc_8265B8A8:
	// addi r29,r31,-1
	r29.s64 = r31.s64 + -1;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x8265b91c
	if (cr6.lt) goto loc_8265B91C;
	// addi r11,r29,3
	r11.s64 = r29.s64 + 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r30
	r28.u64 = r11.u64 + r30.u64;
loc_8265B8C0:
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265b90c
	if (cr6.eq) goto loc_8265B90C;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x82655c10
	sub_82655C10(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b8e4
	if (cr6.eq) goto loc_8265B8E4;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B8E4:
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b8f4
	if (cr6.eq) goto loc_8265B8F4;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B8F4:
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b904
	if (cr6.eq) goto loc_8265B904;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B904:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B90C:
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r28,r28,-4
	r28.s64 = r28.s64 + -4;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bge cr6,0x8265b8c0
	if (!cr6.lt) goto loc_8265B8C0;
loc_8265B91C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B924:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8265B930"))) PPC_WEAK_FUNC(sub_8265B930);
PPC_FUNC_IMPL(__imp__sub_8265B930) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8265b9c4
	if (cr6.eq) goto loc_8265B9C4;
	// lwz r11,64(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8265b9bc
	if (!cr6.gt) goto loc_8265B9BC;
	// addi r30,r28,12
	r30.s64 = r28.s64 + 12;
loc_8265B95C:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8265b9a8
	if (cr6.eq) goto loc_8265B9A8;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// bl 0x82655c10
	sub_82655C10(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b980
	if (cr6.eq) goto loc_8265B980;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B980:
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b990
	if (cr6.eq) goto loc_8265B990;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B990:
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265b9a0
	if (cr6.eq) goto loc_8265B9A0;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B9A0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B9A8:
	// lwz r11,64(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r29,r11
	cr6.compare<int32_t>(r29.s32, r11.s32, xer);
	// blt cr6,0x8265b95c
	if (cr6.lt) goto loc_8265B95C;
loc_8265B9BC:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82604090
	sub_82604090(ctx, base);
loc_8265B9C4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239bd48
	return;
}

__attribute__((alias("__imp__sub_8265B9CC"))) PPC_WEAK_FUNC(sub_8265B9CC);
PPC_FUNC_IMPL(__imp__sub_8265B9CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265B9D0"))) PPC_WEAK_FUNC(sub_8265B9D0);
PPC_FUNC_IMPL(__imp__sub_8265B9D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// li r11,1024
	r11.s64 = 1024;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r4,68(r8)
	PPC_STORE_U32(ctx.r8.u32 + 68, ctx.r4.u32);
	// stw r11,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r11.u32);
	// beq cr6,0x8265ba08
	if (cr6.eq) goto loc_8265BA08;
	// li r11,1
	r11.s64 = 1;
	// stw r6,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r6.u32);
	// stw r11,60(r8)
	PPC_STORE_U32(ctx.r8.u32 + 60, r11.u32);
	// b 0x8265ba0c
	goto loc_8265BA0C;
loc_8265BA08:
	// stw r6,60(r8)
	PPC_STORE_U32(ctx.r8.u32 + 60, ctx.r6.u32);
loc_8265BA0C:
	// lwz r11,64(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 64);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x8265ba38
	if (!cr6.gt) goto loc_8265BA38;
	// addi r7,r8,12
	ctx.r7.s64 = ctx.r8.s64 + 12;
loc_8265BA1C:
	// lwz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// bl 0x8265a9c8
	sub_8265A9C8(ctx, base);
	// lwz r11,64(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 64);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmpw cr6,r6,r11
	cr6.compare<int32_t>(ctx.r6.s32, r11.s32, xer);
	// blt cr6,0x8265ba1c
	if (cr6.lt) goto loc_8265BA1C;
loc_8265BA38:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BA48"))) PPC_WEAK_FUNC(sub_8265BA48);
PPC_FUNC_IMPL(__imp__sub_8265BA48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,128(r3)
	PPC_STORE_U32(ctx.r3.u32 + 128, r11.u32);
	// stw r11,124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 124, r11.u32);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, r11.u32);
	// stw r11,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, r11.u32);
	// stw r10,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BA68"))) PPC_WEAK_FUNC(sub_8265BA68);
PPC_FUNC_IMPL(__imp__sub_8265BA68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r3,128(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265ba98
	if (cr6.eq) goto loc_8265BA98;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r30.u32);
loc_8265BA98:
	// lwz r3,124(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 124);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265baac
	if (cr6.eq) goto loc_8265BAAC;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r30,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r30.u32);
loc_8265BAAC:
	// li r11,1
	r11.s64 = 1;
	// stw r11,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BACC"))) PPC_WEAK_FUNC(sub_8265BACC);
PPC_FUNC_IMPL(__imp__sub_8265BACC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265BAD0"))) PPC_WEAK_FUNC(sub_8265BAD0);
PPC_FUNC_IMPL(__imp__sub_8265BAD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bge cr6,0x8265badc
	if (!cr6.lt) goto loc_8265BADC;
	// neg r4,r4
	ctx.r4.s64 = -ctx.r4.s64;
loc_8265BADC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x8265bae8
	if (!cr6.lt) goto loc_8265BAE8;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
loc_8265BAE8:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bge cr6,0x8265baf4
	if (!cr6.lt) goto loc_8265BAF4;
	// neg r6,r6
	ctx.r6.s64 = -ctx.r6.s64;
loc_8265BAF4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x8265bb00
	if (!cr6.lt) goto loc_8265BB00;
	// neg r7,r7
	ctx.r7.s64 = -ctx.r7.s64;
loc_8265BB00:
	// stw r4,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r4.u32);
	// stw r5,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r5.u32);
	// stw r6,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, ctx.r6.u32);
	// stw r7,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BB14"))) PPC_WEAK_FUNC(sub_8265BB14);
PPC_FUNC_IMPL(__imp__sub_8265BB14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265BB18"))) PPC_WEAK_FUNC(sub_8265BB18);
PPC_FUNC_IMPL(__imp__sub_8265BB18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, r11.u32);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BB34"))) PPC_WEAK_FUNC(sub_8265BB34);
PPC_FUNC_IMPL(__imp__sub_8265BB34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265BB38"))) PPC_WEAK_FUNC(sub_8265BB38);
PPC_FUNC_IMPL(__imp__sub_8265BB38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8265bb80
	if (cr6.eq) goto loc_8265BB80;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8265BB80:
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BB9C"))) PPC_WEAK_FUNC(sub_8265BB9C);
PPC_FUNC_IMPL(__imp__sub_8265BB9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265BBA0"))) PPC_WEAK_FUNC(sub_8265BBA0);
PPC_FUNC_IMPL(__imp__sub_8265BBA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x8239bcf0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x8265bbcc
	if (!cr6.lt) goto loc_8265BBCC;
	// neg r30,r30
	r30.s64 = -r30.s64;
loc_8265BBCC:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bge cr6,0x8265bbd8
	if (!cr6.lt) goto loc_8265BBD8;
	// neg r26,r26
	r26.s64 = -r26.s64;
loc_8265BBD8:
	// li r27,0
	r27.s64 = 0;
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// stw r29,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r29.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r30.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r28,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r28.u32);
	// stw r26,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r26.u32);
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// beq cr6,0x8265bc04
	if (cr6.eq) goto loc_8265BC04;
	// bl 0x82604090
	sub_82604090(ctx, base);
	// stw r27,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r27.u32);
loc_8265BC04:
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// bgt cr6,0x8265bc10
	if (cr6.gt) goto loc_8265BC10;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_8265BC10:
	// cmpw cr6,r30,r26
	cr6.compare<int32_t>(r30.s32, r26.s32, xer);
	// bgt cr6,0x8265bc1c
	if (cr6.gt) goto loc_8265BC1C;
	// mr r30,r26
	r30.u64 = r26.u64;
loc_8265BC1C:
	// addi r10,r29,2
	ctx.r10.s64 = r29.s64 + 2;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// li r4,0
	ctx.r4.s64 = 0;
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82604080
	sub_82604080(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,60(r31)
	PPC_STORE_U32(r31.u32 + 60, ctx.r3.u32);
	// bne cr6,0x8265bc4c
	if (!cr6.eq) goto loc_8265BC4C;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
loc_8265BC4C:
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239bd40
	return;
}

__attribute__((alias("__imp__sub_8265BC60"))) PPC_WEAK_FUNC(sub_8265BC60);
PPC_FUNC_IMPL(__imp__sub_8265BC60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bge cr6,0x8265bc6c
	if (!cr6.lt) goto loc_8265BC6C;
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
loc_8265BC6C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x8265bc78
	if (!cr6.lt) goto loc_8265BC78;
	// neg r7,r7
	ctx.r7.s64 = -ctx.r7.s64;
loc_8265BC78:
	// stw r4,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r4.u32);
	// stw r5,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r5.u32);
	// stw r6,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r6.u32);
	// stw r7,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265BC8C"))) PPC_WEAK_FUNC(sub_8265BC8C);
PPC_FUNC_IMPL(__imp__sub_8265BC8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265BC90"))) PPC_WEAK_FUNC(sub_8265BC90);
PPC_FUNC_IMPL(__imp__sub_8265BC90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, ctx.r9.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-7
	r11.s64 = r11.s64 + -7;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265bd50
	if (!cr6.lt) goto loc_8265BD50;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
loc_8265BD50:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265bda0
	goto loc_8265BDA0;
loc_8265BD94:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265BDA0:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c35c
	if (!cr6.lt) goto loc_8265C35C;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265bdd0
	goto loc_8265BDD0;
loc_8265BDC4:
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
loc_8265BDD0:
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265bfc0
	if (!cr6.lt) goto loc_8265BFC0;
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,3(r10)
	PPC_STORE_U8(ctx.r10.u32 + 3, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// b 0x8265bdc4
	goto loc_8265BDC4;
loc_8265BFC0:
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265bfd8
	goto loc_8265BFD8;
loc_8265BFCC:
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
loc_8265BFD8:
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c348
	if (!cr6.lt) goto loc_8265C348;
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c030
	if (!cr6.gt) goto loc_8265C030;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// b 0x8265c038
	goto loc_8265C038;
loc_8265C030:
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265C038:
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c060
	if (!cr6.gt) goto loc_8265C060;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265c068
	goto loc_8265C068;
loc_8265C060:
	// lwz r11,-112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265C068:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c110
	if (!cr6.gt) goto loc_8265C110;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// b 0x8265c118
	goto loc_8265C118;
loc_8265C110:
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
loc_8265C118:
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c140
	if (!cr6.gt) goto loc_8265C140;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265c148
	goto loc_8265C148;
loc_8265C140:
	// lwz r11,-112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265C148:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,2(r10)
	PPC_STORE_U8(ctx.r10.u32 + 2, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c204
	if (!cr6.gt) goto loc_8265C204;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265c20c
	goto loc_8265C20C;
loc_8265C204:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265C20C:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c234
	if (!cr6.gt) goto loc_8265C234;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265c23c
	goto loc_8265C23C;
loc_8265C234:
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
loc_8265C23C:
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c264
	if (!cr6.gt) goto loc_8265C264;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265c26c
	goto loc_8265C26C;
loc_8265C264:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
loc_8265C26C:
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c294
	if (!cr6.gt) goto loc_8265C294;
	// lwz r11,-104(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265c29c
	goto loc_8265C29C;
loc_8265C294:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
loc_8265C29C:
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,1(r10)
	PPC_STORE_U8(ctx.r10.u32 + 1, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r9,-88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stb r11,3(r10)
	PPC_STORE_U8(ctx.r10.u32 + 3, r11.u8);
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// b 0x8265bfcc
	goto loc_8265BFCC;
loc_8265C348:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265bd94
	goto loc_8265BD94;
loc_8265C35C:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265C360"))) PPC_WEAK_FUNC(sub_8265C360);
PPC_FUNC_IMPL(__imp__sub_8265C360) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, ctx.r9.u32);
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, ctx.r9.u32);
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r9.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265c484
	if (!cr6.gt) goto loc_8265C484;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265C484:
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x8265c494
	if (cr6.gt) goto loc_8265C494;
	// b 0x8265c864
	goto loc_8265C864;
loc_8265C494:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265c4bc
	if (cr6.eq) goto loc_8265C4BC;
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265c4c4
	goto loc_8265C4C4;
loc_8265C4BC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
loc_8265C4C4:
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-64(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265c5f4
	if (!cr6.lt) goto loc_8265C5F4;
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r9.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265c550
	goto loc_8265C550;
loc_8265C544:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265C550:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265c5cc
	if (!cr6.lt) goto loc_8265C5CC;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// b 0x8265c58c
	goto loc_8265C58C;
loc_8265C580:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
loc_8265C58C:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c5c8
	if (!cr6.lt) goto loc_8265C5C8;
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265c580
	goto loc_8265C580;
loc_8265C5C8:
	// b 0x8265c544
	goto loc_8265C544;
loc_8265C5CC:
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, r11.u32);
loc_8265C5F4:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265c60c
	goto loc_8265C60C;
loc_8265C600:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265C60C:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c6ec
	if (!cr6.lt) goto loc_8265C6EC;
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// b 0x8265c674
	goto loc_8265C674;
loc_8265C668:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
loc_8265C674:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c6d8
	if (!cr6.lt) goto loc_8265C6D8;
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-56(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r9,-64(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265c668
	goto loc_8265C668;
loc_8265C6D8:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265c600
	goto loc_8265C600;
loc_8265C6EC:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c864
	if (!cr6.lt) goto loc_8265C864;
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265c714
	goto loc_8265C714;
loc_8265C708:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265C714:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c864
	if (!cr6.lt) goto loc_8265C864;
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c7fc
	if (!cr6.lt) goto loc_8265C7FC;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// b 0x8265c794
	goto loc_8265C794;
loc_8265C788:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
loc_8265C794:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c7f8
	if (!cr6.lt) goto loc_8265C7F8;
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-56(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// lwz r9,-64(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-68(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265c788
	goto loc_8265C788;
loc_8265C7F8:
	// b 0x8265c850
	goto loc_8265C850;
loc_8265C7FC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// b 0x8265c814
	goto loc_8265C814;
loc_8265C808:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
loc_8265C814:
	// lwz r11,-40(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265c850
	if (!cr6.lt) goto loc_8265C850;
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265c808
	goto loc_8265C808;
loc_8265C850:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265c708
	goto loc_8265C708;
loc_8265C864:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265C868"))) PPC_WEAK_FUNC(sub_8265C868);
PPC_FUNC_IMPL(__imp__sub_8265C868) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, ctx.r9.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, ctx.r9.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, ctx.r9.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-60(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-76(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265c9c4
	if (!cr6.lt) goto loc_8265C9C4;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265C9C4:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// b 0x8265c9dc
	goto loc_8265C9DC;
loc_8265C9D0:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
loc_8265C9DC:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265cb28
	if (!cr6.lt) goto loc_8265CB28;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265ca0c
	goto loc_8265CA0C;
loc_8265CA00:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CA0C:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,-56(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265caa4
	if (!cr6.lt) goto loc_8265CAA4;
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265ca00
	goto loc_8265CA00;
loc_8265CAA4:
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265cabc
	goto loc_8265CABC;
loc_8265CAB0:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CABC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-24(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265cb14
	if (!cr6.lt) goto loc_8265CB14;
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265cab0
	goto loc_8265CAB0;
loc_8265CB14:
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265c9d0
	goto loc_8265C9D0;
loc_8265CB28:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-76(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// b 0x8265cb90
	goto loc_8265CB90;
loc_8265CB84:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
loc_8265CB90:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ccd8
	if (!cr6.lt) goto loc_8265CCD8;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265cbc0
	goto loc_8265CBC0;
loc_8265CBB4:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CBC0:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265cc58
	if (!cr6.lt) goto loc_8265CC58;
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265cbb4
	goto loc_8265CBB4;
loc_8265CC58:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265cc70
	goto loc_8265CC70;
loc_8265CC64:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CC70:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,-20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ccc4
	if (!cr6.lt) goto loc_8265CCC4;
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265cc64
	goto loc_8265CC64;
loc_8265CCC4:
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265cb84
	goto loc_8265CB84;
loc_8265CCD8:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-76(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// b 0x8265cd50
	goto loc_8265CD50;
loc_8265CD44:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
loc_8265CD50:
	// lwz r11,-68(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ce98
	if (!cr6.lt) goto loc_8265CE98;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265cd80
	goto loc_8265CD80;
loc_8265CD74:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CD80:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ce18
	if (!cr6.lt) goto loc_8265CE18;
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// lwz r9,-48(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265cd74
	goto loc_8265CD74;
loc_8265CE18:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265ce30
	goto loc_8265CE30;
loc_8265CE24:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265CE30:
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lwz r10,-20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265ce84
	if (!cr6.lt) goto loc_8265CE84;
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265ce24
	goto loc_8265CE24;
loc_8265CE84:
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// b 0x8265cd44
	goto loc_8265CD44;
loc_8265CE98:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265CE9C"))) PPC_WEAK_FUNC(sub_8265CE9C);
PPC_FUNC_IMPL(__imp__sub_8265CE9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265CEA0"))) PPC_WEAK_FUNC(sub_8265CEA0);
PPC_FUNC_IMPL(__imp__sub_8265CEA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, ctx.r9.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// rlwinm r11,r11,8,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-92(r1)
	PPC_STORE_U32(ctx.r1.u32 + -92, ctx.r9.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, ctx.r9.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, ctx.r9.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,-104(r1)
	PPC_STORE_U32(ctx.r1.u32 + -104, r11.u32);
	// lwz r11,-84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265d024
	if (!cr6.gt) goto loc_8265D024;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
loc_8265D024:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265d034
	if (!cr6.lt) goto loc_8265D034;
	// b 0x8265db68
	goto loc_8265DB68;
loc_8265D034:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265d068
	if (cr6.eq) goto loc_8265D068;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265d070
	goto loc_8265D070;
loc_8265D068:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265D070:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265d178
	if (!cr6.lt) goto loc_8265D178;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d0e0
	goto loc_8265D0E0;
loc_8265D0D4:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D0E0:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d160
	if (!cr6.lt) goto loc_8265D160;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d11c
	goto loc_8265D11C;
loc_8265D110:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D11C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d15c
	if (!cr6.lt) goto loc_8265D15C;
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
	// b 0x8265d110
	goto loc_8265D110;
loc_8265D15C:
	// b 0x8265d0d4
	goto loc_8265D0D4;
loc_8265D160:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265D178:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d198
	goto loc_8265D198;
loc_8265D18C:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D198:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d284
	if (!cr6.lt) goto loc_8265D284;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d204
	goto loc_8265D204;
loc_8265D1F8:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D204:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d270
	if (!cr6.lt) goto loc_8265D270;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265d1f8
	goto loc_8265D1F8;
loc_8265D270:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265d18c
	goto loc_8265D18C;
loc_8265D284:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265d40c
	if (!cr6.gt) goto loc_8265D40C;
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d2ac
	goto loc_8265D2AC;
loc_8265D2A0:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D2AC:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d40c
	if (!cr6.lt) goto loc_8265D40C;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r10,r10,8
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,256
	xer.ca = r11.u32 <= 256;
	r11.s64 = 256 - r11.s64;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// lwz r10,-84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d3a0
	if (!cr6.lt) goto loc_8265D3A0;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d330
	goto loc_8265D330;
loc_8265D324:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D330:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d39c
	if (!cr6.lt) goto loc_8265D39C;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,8
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xFF) != 0);
	r11.s64 = r11.s32 >> 8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265d324
	goto loc_8265D324;
loc_8265D39C:
	// b 0x8265d3f8
	goto loc_8265D3F8;
loc_8265D3A0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d3b8
	goto loc_8265D3B8;
loc_8265D3AC:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D3B8:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d3f8
	if (!cr6.lt) goto loc_8265D3F8;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265d3ac
	goto loc_8265D3AC;
loc_8265D3F8:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265d2a0
	goto loc_8265D2A0;
loc_8265D40C:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265d434
	if (!cr6.gt) goto loc_8265D434;
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
loc_8265D434:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265d4c0
	if (cr6.eq) goto loc_8265D4C0;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// b 0x8265d4c8
	goto loc_8265D4C8;
loc_8265D4C0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
loc_8265D4C8:
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265d5e0
	if (!cr6.lt) goto loc_8265D5E0;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d548
	goto loc_8265D548;
loc_8265D53C:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D548:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d5c8
	if (!cr6.lt) goto loc_8265D5C8;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d588
	goto loc_8265D588;
loc_8265D57C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D588:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d5c4
	if (!cr6.lt) goto loc_8265D5C4;
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// b 0x8265d57c
	goto loc_8265D57C;
loc_8265D5C4:
	// b 0x8265d53c
	goto loc_8265D53C;
loc_8265D5C8:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265D5E0:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d608
	goto loc_8265D608;
loc_8265D5FC:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D608:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d6e4
	if (!cr6.lt) goto loc_8265D6E4;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d66c
	goto loc_8265D66C;
loc_8265D660:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D66C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d6d0
	if (!cr6.lt) goto loc_8265D6D0;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265d660
	goto loc_8265D660;
loc_8265D6D0:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265d5fc
	goto loc_8265D5FC;
loc_8265D6E4:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265d7b8
	if (!cr6.gt) goto loc_8265D7B8;
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d714
	goto loc_8265D714;
loc_8265D708:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D714:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d7b8
	if (!cr6.lt) goto loc_8265D7B8;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d768
	goto loc_8265D768;
loc_8265D75C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D768:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d7a4
	if (!cr6.lt) goto loc_8265D7A4;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265d75c
	goto loc_8265D75C;
loc_8265D7A4:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265d708
	goto loc_8265D708;
loc_8265D7B8:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// lwz r9,32(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// lwz r9,-84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -84);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,40(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265d870
	if (cr6.eq) goto loc_8265D870;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// addi r11,r11,-256
	r11.s64 = r11.s64 + -256;
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265d878
	goto loc_8265D878;
loc_8265D870:
	// li r11,0
	r11.s64 = 0;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
loc_8265D878:
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265d990
	if (!cr6.lt) goto loc_8265D990;
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-112(r1)
	PPC_STORE_U32(ctx.r1.u32 + -112, ctx.r9.u32);
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d8f8
	goto loc_8265D8F8;
loc_8265D8EC:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D8F8:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265d978
	if (!cr6.lt) goto loc_8265D978;
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265d938
	goto loc_8265D938;
loc_8265D92C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265D938:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265d974
	if (!cr6.lt) goto loc_8265D974;
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-24(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265d92c
	goto loc_8265D92C;
loc_8265D974:
	// b 0x8265d8ec
	goto loc_8265D8EC;
loc_8265D978:
	// lwz r11,-92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,-88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
loc_8265D990:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265d9b8
	goto loc_8265D9B8;
loc_8265D9AC:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265D9B8:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265da94
	if (!cr6.lt) goto loc_8265DA94;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265da1c
	goto loc_8265DA1C;
loc_8265DA10:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265DA1C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265da80
	if (!cr6.lt) goto loc_8265DA80;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265da10
	goto loc_8265DA10;
loc_8265DA80:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265d9ac
	goto loc_8265D9AC;
loc_8265DA94:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x8265db68
	if (!cr6.gt) goto loc_8265DB68;
	// lwz r11,-64(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
	// b 0x8265dac4
	goto loc_8265DAC4;
loc_8265DAB8:
	// lwz r11,-100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-100(r1)
	PPC_STORE_U32(ctx.r1.u32 + -100, r11.u32);
loc_8265DAC4:
	// lwz r11,36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -100);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265db68
	if (!cr6.lt) goto loc_8265DB68;
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// stw r11,-108(r1)
	PPC_STORE_U32(ctx.r1.u32 + -108, r11.u32);
	// lwz r11,-96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
	// lwz r10,-108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -108);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// b 0x8265db18
	goto loc_8265DB18;
loc_8265DB0C:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
loc_8265DB18:
	// lwz r11,-44(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265db54
	if (!cr6.lt) goto loc_8265DB54;
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265db0c
	goto loc_8265DB0C;
loc_8265DB54:
	// lwz r11,-88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
	// lwz r10,-92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -92);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, r11.u32);
	// b 0x8265dab8
	goto loc_8265DAB8;
loc_8265DB68:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8265DB6C"))) PPC_WEAK_FUNC(sub_8265DB6C);
PPC_FUNC_IMPL(__imp__sub_8265DB6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8265DB70"))) PPC_WEAK_FUNC(sub_8265DB70);
PPC_FUNC_IMPL(__imp__sub_8265DB70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// stw r3,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r3.u32);
	// stw r4,28(r1)
	PPC_STORE_U32(ctx.r1.u32 + 28, ctx.r4.u32);
	// stw r5,36(r1)
	PPC_STORE_U32(ctx.r1.u32 + 36, ctx.r5.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,-60(r1)
	PPC_STORE_U32(ctx.r1.u32 + -60, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// stw r9,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r9.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-36(r1)
	PPC_STORE_U32(ctx.r1.u32 + -36, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// stw r11,-64(r1)
	PPC_STORE_U32(ctx.r1.u32 + -64, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,7,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// divw r9,r11,r10
	ctx.r9.s32 = r11.s32 / ctx.r10.s32;
	// twllei r10,0
	// rotlwi r11,r11,1
	r11.u64 = __builtin_rotateleft32(r11.u32, 1);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// andc r11,r10,r11
	r11.u64 = ctx.r10.u64 & ~r11.u64;
	// twlgei r11,-1
	// addi r11,r9,-1
	r11.s64 = ctx.r9.s64 + -1;
	// stw r11,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265dccc
	if (cr6.eq) goto loc_8265DCCC;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265dccc
	if (!cr6.eq) goto loc_8265DCCC;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265dccc
	if (!cr6.eq) goto loc_8265DCCC;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,80(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,92(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265dcf8
	goto loc_8265DCF8;
loc_8265DCCC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265DCF8:
	// lwz r11,-32(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8265dd0c
	if (!cr6.lt) goto loc_8265DD0C;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, r11.u32);
loc_8265DD0C:
	// lwz r11,28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265dd24
	goto loc_8265DD24;
loc_8265DD18:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
loc_8265DD24:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265deec
	if (!cr6.lt) goto loc_8265DEEC;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265dd54
	goto loc_8265DD54;
loc_8265DD48:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265DD54:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265dde0
	if (!cr6.lt) goto loc_8265DDE0;
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265dd48
	goto loc_8265DD48;
loc_8265DDE0:
	// b 0x8265ddf0
	goto loc_8265DDF0;
loc_8265DDE4:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265DDF0:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265dec8
	if (!cr6.lt) goto loc_8265DEC8;
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265de68
	if (!cr6.gt) goto loc_8265DE68;
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265de68
	if (!cr6.gt) goto loc_8265DE68;
	// lwz r11,-28(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
loc_8265DE68:
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-12(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, r11.u32);
	// b 0x8265dde4
	goto loc_8265DDE4;
loc_8265DEC8:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-24(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265dd18
	goto loc_8265DD18;
loc_8265DEEC:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265df78
	if (cr6.eq) goto loc_8265DF78;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265df78
	if (!cr6.eq) goto loc_8265DF78;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265df78
	if (!cr6.eq) goto loc_8265DF78;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,84(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,96(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265dfc4
	goto loc_8265DFC4;
loc_8265DF78:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265DFC4:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265dff0
	goto loc_8265DFF0;
loc_8265DFE4:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
loc_8265DFF0:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265e1d0
	if (!cr6.lt) goto loc_8265E1D0;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265e034
	goto loc_8265E034;
loc_8265E028:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265E034:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265e0c4
	if (!cr6.lt) goto loc_8265E0C4;
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// li r10,128
	ctx.r10.s64 = 128;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265e028
	goto loc_8265E028;
loc_8265E0C4:
	// b 0x8265e0d4
	goto loc_8265E0D4;
loc_8265E0C8:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265E0D4:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265e1ac
	if (!cr6.lt) goto loc_8265E1AC;
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// li r10,128
	ctx.r10.s64 = 128;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265e14c
	if (!cr6.gt) goto loc_8265E14C;
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265e14c
	if (!cr6.gt) goto loc_8265E14C;
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
loc_8265E14C:
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r11.u32);
	// b 0x8265e0c8
	goto loc_8265E0C8;
loc_8265E1AC:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-24(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265dfe4
	goto loc_8265DFE4;
loc_8265E1D0:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,-60(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r9,-28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r11,52(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r9,-36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,44(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-36(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8265e278
	if (cr6.eq) goto loc_8265E278;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265e278
	if (!cr6.eq) goto loc_8265E278;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8265e278
	if (!cr6.eq) goto loc_8265E278;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,88(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r11,100(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 100);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
	// b 0x8265e2e0
	goto loc_8265E2E0;
loc_8265E278:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,-60(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// lwz r10,32(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r11,64(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// lwz r9,20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,44(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,20(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r9,28(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// li r9,2
	ctx.r9.s64 = 2;
	// divw r10,r10,r9
	ctx.r10.s32 = ctx.r10.s32 / ctx.r9.s32;
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// stw r11,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r11.u32);
loc_8265E2E0:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,28(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
	// b 0x8265e30c
	goto loc_8265E30C;
loc_8265E300:
	// lwz r11,-76(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-76(r1)
	PPC_STORE_U32(ctx.r1.u32 + -76, r11.u32);
loc_8265E30C:
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// lwz r10,36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 36);
	// lwz r11,44(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// li r10,2
	ctx.r10.s64 = 2;
	// divw r11,r11,r10
	r11.s32 = r11.s32 / ctx.r10.s32;
	// lwz r10,-76(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -76);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bge cr6,0x8265e4e4
	if (!cr6.lt) goto loc_8265E4E4;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
	// b 0x8265e350
	goto loc_8265E350;
loc_8265E344:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265E350:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r10,-68(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -68);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265e3dc
	if (!cr6.lt) goto loc_8265E3DC;
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,-52(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lbzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, r11.u32);
	// b 0x8265e344
	goto loc_8265E344;
loc_8265E3DC:
	// b 0x8265e3ec
	goto loc_8265E3EC;
loc_8265E3E0:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, r11.u32);
loc_8265E3EC:
	// lwz r11,-20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x8265e4c0
	if (!cr6.lt) goto loc_8265E4C0;
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stw r11,-56(r1)
	PPC_STORE_U32(ctx.r1.u32 + -56, r11.u32);
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
	// lwz r11,-80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-40(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265e460
	if (!cr6.gt) goto loc_8265E460;
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, r11.u32);
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x8265e460
	if (!cr6.gt) goto loc_8265E460;
	// lwz r11,-36(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, r11.u32);
loc_8265E460:
	// lwz r11,-56(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// subfic r11,r11,128
	xer.ca = r11.u32 <= 128;
	r11.s64 = 128 - r11.s64;
	// stw r11,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -80);
	// lbzx r11,r11,r10
	r11.u64 = PPC_LOAD_U8(r11.u32 + ctx.r10.u32);
	// lwz r10,-44(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r10,-52(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r9,-40(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// lbzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// lwz r9,-56(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// srawi r11,r11,7
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7F) != 0);
	r11.s64 = r11.s32 >> 7;
	// lwz r10,-72(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r9,-20(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -20);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stbx r11,r10,r9
	PPC_STORE_U8(ctx.r10.u32 + ctx.r9.u32, r11.u8);
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, r11.u32);
	// b 0x8265e3e0
	goto loc_8265E3E0;
loc_8265E4C0:
	// lwz r11,-72(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -72);
	// lwz r10,-24(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, r11.u32);
	// lwz r11,-52(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -52);
	// lwz r10,-36(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -36);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,-52(r1)
	PPC_STORE_U32(ctx.r1.u32 + -52, r11.u32);
	// b 0x8265e300
	goto loc_8265E300;
loc_8265E4E4:
	// blr 
	return;
}

